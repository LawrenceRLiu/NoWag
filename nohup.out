/home/lliu/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.34s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.18it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.08it/s]
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=True`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Traceback (most recent call last):
  File "/home/lliu/huffman/llama.py", line 358, in <module>
    dataloader, testloader = get_loaders(
                             ^^^^^^^^^^^^
  File "/home/lliu/huffman/datautils.py", line 100, in get_loaders
    return get_c4(nsamples, seed, seqlen, model, tokenizer)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lliu/huffman/datautils.py", line 65, in get_c4
    traindata = load_dataset('allenai/c4', data_files={'train': 'en/c4-train.00000-of-01024.json.gz'}, split='train')
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lliu/anaconda3/lib/python3.11/site-packages/datasets/load.py", line 2606, in load_dataset
    builder_instance = load_dataset_builder(
                       ^^^^^^^^^^^^^^^^^^^^^
  File "/home/lliu/anaconda3/lib/python3.11/site-packages/datasets/load.py", line 2277, in load_dataset_builder
    dataset_module = dataset_module_factory(
                     ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lliu/anaconda3/lib/python3.11/site-packages/datasets/load.py", line 1905, in dataset_module_factory
    ).get_module()
      ^^^^^^^^^^^^
  File "/home/lliu/anaconda3/lib/python3.11/site-packages/datasets/load.py", line 1206, in get_module
    hfh_dataset_info = HfApi(config.HF_ENDPOINT).dataset_info(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lliu/anaconda3/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/lliu/anaconda3/lib/python3.11/site-packages/huggingface_hub/hf_api.py", line 2445, in dataset_info
    r = get_session().get(path, headers=headers, timeout=timeout, params=params)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lliu/anaconda3/lib/python3.11/site-packages/requests/sessions.py", line 602, in get
    return self.request("GET", url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lliu/anaconda3/lib/python3.11/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lliu/anaconda3/lib/python3.11/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lliu/anaconda3/lib/python3.11/site-packages/huggingface_hub/utils/_http.py", line 66, in send
    return super().send(request, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lliu/anaconda3/lib/python3.11/site-packages/requests/adapters.py", line 667, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/lliu/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py", line 714, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File "/home/lliu/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py", line 466, in _make_request
    six.raise_from(e, None)
  File "<string>", line 3, in raise_from
  File "/home/lliu/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py", line 461, in _make_request
    httplib_response = conn.getresponse()
                       ^^^^^^^^^^^^^^^^^^
  File "/home/lliu/anaconda3/lib/python3.11/http/client.py", line 1378, in getresponse
    response.begin()
  File "/home/lliu/anaconda3/lib/python3.11/http/client.py", line 318, in begin
    version, status, reason = self._read_status()
                              ^^^^^^^^^^^^^^^^^^^
  File "/home/lliu/anaconda3/lib/python3.11/http/client.py", line 279, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lliu/anaconda3/lib/python3.11/socket.py", line 706, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lliu/anaconda3/lib/python3.11/ssl.py", line 1311, in recv_into
    return self.read(nbytes, buffer)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lliu/anaconda3/lib/python3.11/ssl.py", line 1167, in read
    return self._sslobj.read(len, buffer)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
usage: llama.py [-h] [--seed SEED] [--device DEVICE] [--nsamples NSAMPLES]
                [--percdamp PERCDAMP] [--sparsity SPARSITY] [--prunen PRUNEN]
                [--prunem PRUNEM] [--blocksize BLOCKSIZE] [--gmp]
                [--wbits WBITS] [--minlayer MINLAYER] [--maxlayer MAXLAYER]
                [--prune_only PRUNE_ONLY] [--invert] [--save SAVE]
                [--true-sequential] [--log_wandb]
                [--subvector_dim SUBVECTOR_DIM]
                [--k_magnitude_codebook K_MAGNITUDE_CODEBOOK]
                [--k_cosine_codebook K_COSINE_CODEBOOK] [--keep_top KEEP_TOP]
                [--lr LR] [--lr_multiple LR_MULTIPLE] [--n_iters N_ITERS]
                [--clamp_gradients CLAMP_GRADIENTS] [--quantize]
                model {wikitext2,ptb,c4}
llama.py: error: unrecognized arguments: ----subvector_dim 8 --k_cosine_codebk 16
usage: llama_low_rank_and_prune.py [-h] [--seed SEED] [--device DEVICE]
                                   [--nsamples NSAMPLES]
                                   [--nsamples_val NSAMPLES_VAL]
                                   [--percdamp PERCDAMP] [--sparsity SPARSITY]
                                   [--prunen PRUNEN] [--prunem PRUNEM]
                                   [--blocksize BLOCKSIZE] [--gmp]
                                   [--wbits WBITS] [--minlayer MINLAYER]
                                   [--maxlayer MAXLAYER]
                                   [--prune_only PRUNE_ONLY] [--invert]
                                   [--save SAVE] [--true_sequential]
                                   [--log_wandb]
                                   [--keep_top_frac KEEP_TOP_FRAC]
                                   [--keep_bottom_frac KEEP_BOTTOM_FRAC]
                                   [--mha_low_rank MHA_LOW_RANK]
                                   [--sparse_rowwise SPARSE_ROWWISE]
                                   [--sparse_rowwise_conditions SPARSE_ROWWISE_CONDITIONS [SPARSE_ROWWISE_CONDITIONS ...]]
                                   [--sparse_colwise SPARSE_COLWISE]
                                   [--sparse_colwise_conditions SPARSE_COLWISE_CONDITIONS [SPARSE_COLWISE_CONDITIONS ...]]
                                   [--lr LR] [--lr_multiple LR_MULTIPLE]
                                   [--n_iters N_ITERS]
                                   [--clamp_gradients CLAMP_GRADIENTS]
                                   [--quantize]
                                   [--regularization REGULARIZATION]
                                   [--fine_tune] [--fine_tune_quip_like]
                                   [--fnn_device FNN_DEVICE]
                                   [--finetune_epochs FINETUNE_EPOCHS]
                                   [--finetune_early_stop FINETUNE_EARLY_STOP]
                                   [--finetune_lr FINETUNE_LR]
                                   [--finetune_batch_size FINETUNE_BATCH_SIZE]
                                   [--offload_activations]
                                   [--finetune_adam_beta1 FINETUNE_ADAM_BETA1]
                                   [--finetune_adam_beta2 FINETUNE_ADAM_BETA2]
                                   [--finetune_keep_best]
                                   [--local_batch_size LOCAL_BATCH_SIZE]
                                   model {wikitext2,ptb,c4}
llama_low_rank_and_prune.py: error: argument --n_iters: expected one argument
