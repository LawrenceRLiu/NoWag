/home/lliu/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.15s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.34it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.24it/s]
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=True`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Token indices sequence length is longer than the specified maximum sequence length for this model (2824491 > 2048). Running this sequence through the model will result in indexing errors
Starting...
Ready.
0 self_attn.q_proj
Pruning ...
using 0.01 of the magnitude
47 H_error tensor(2.1090, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3920, device='cuda:6', grad_fn=<DivBackward0>)
43351 MiB free out of 48676 MiB total
0 self_attn.k_proj
Pruning ...
using 0.01 of the magnitude
46 H_error tensor(2.0293, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3845, device='cuda:6', grad_fn=<DivBackward0>)
43365 MiB free out of 48676 MiB total
0 self_attn.v_proj
Pruning ...
using 0.01 of the magnitude
45 H_error tensor(0.2065, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3452, device='cuda:6', grad_fn=<DivBackward0>)
43397 MiB free out of 48676 MiB total
0 self_attn.o_proj
Pruning ...
using 0.01 of the magnitude
47 H_error tensor(0.0153, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3836, device='cuda:6', grad_fn=<DivBackward0>)
43429 MiB free out of 48676 MiB total
0 mlp.gate_proj
Pruning ...
using 0.01 of the magnitude
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
116 H_error tensor(2.3030, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3141, device='cuda:6', grad_fn=<DivBackward0>)
43461 MiB free out of 48676 MiB total
0 mlp.up_proj
Pruning ...
using 0.01 of the magnitude
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
116 H_error tensor(2.1506, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3134, device='cuda:6', grad_fn=<DivBackward0>)
43493 MiB free out of 48676 MiB total
0 mlp.down_proj
Pruning ...
using 0.01 of the magnitude
122 H_error tensor(0.0221, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3181, device='cuda:6', grad_fn=<DivBackward0>)
43347 MiB free out of 48676 MiB total
1 self_attn.q_proj
Pruning ...
using 0.01 of the magnitude
46 H_error tensor(14.2932, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3742, device='cuda:6', grad_fn=<DivBackward0>)
43127 MiB free out of 48676 MiB total
1 self_attn.k_proj
Pruning ...
using 0.01 of the magnitude
Traceback (most recent call last):
  File "/home/lliu/huffman/llama.py", line 391, in <module>
    llama_sequential(model, dataloader, args.device)
  File "/home/lliu/huffman/llama.py", line 136, in llama_sequential
    n_bits, n_params = gpts[name].fastquant(
                       ^^^^^^^^^^^^^^^^^^^^^
  File "/home/lliu/huffman/vector_quantizer.py", line 382, in fastquant
    H_error.backward()
  File "/home/lliu/anaconda3/lib/python3.11/site-packages/torch/_tensor.py", line 492, in backward
    torch.autograd.backward(
  File "/home/lliu/anaconda3/lib/python3.11/site-packages/torch/autograd/__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
