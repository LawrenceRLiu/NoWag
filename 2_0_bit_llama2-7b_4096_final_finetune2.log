/home/lliu/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.27s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.23it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.14it/s]
Model loaded. LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
4096
Loading tokenizer for meta-llama/Llama-2-7b-hf
Starting...
/home/lliu/anaconda3/lib/python3.11/site-packages/torch/cuda/__init__.py:611: UserWarning: Can't initialize NVML
  warnings.warn("Can't initialize NVML")
39011 MiB free out of 48676 MiB total
attention_mask torch.Size([1, 1, 4096, 4096]) cuda:7 torch.float16
position_ids torch.Size([1, 4096]) cuda:7 torch.int64
position_ids torch.Size([1, 4096])
attention_mask torch.Size([1, 1, 4096, 4096])
Ready.
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
0 self_attn.q_proj
Pruning ...
realigning
initial loss 1.56924569606781
final loss 1.1642513275146484
quantized
not here
quantized in 22.66327476501465 seconds
36577 MiB free out of 48676 MiB total
0 self_attn.k_proj
Pruning ...
realigning
initial loss 1.8915133476257324
final loss 1.5419642925262451
quantized
not here
quantized in 20.21381187438965 seconds
36641 MiB free out of 48676 MiB total
0 self_attn.v_proj
Pruning ...
realigning
initial loss 0.11391407996416092
final loss 0.09255972504615784
quantized
not here
quantized in 19.38123846054077 seconds
36705 MiB free out of 48676 MiB total
0 self_attn.o_proj
Pruning ...
realigning
initial loss 0.016694191843271255
final loss 0.01346983015537262
quantized
not here
quantized in 18.981043100357056 seconds
36705 MiB free out of 48676 MiB total
0 mlp.gate_proj
Pruning ...
realigning
initial loss 5.378171443939209
final loss 4.615377426147461
quantized
not here
quantized in 54.071107387542725 seconds
36425 MiB free out of 48676 MiB total
0 mlp.up_proj
Pruning ...
realigning
initial loss 3.686795473098755
final loss 3.4293406009674072
quantized
not here
quantized in 51.32206726074219 seconds
36145 MiB free out of 48676 MiB total
0 mlp.down_proj
Pruning ...
realigning
initial loss 0.020199565216898918
final loss 0.016655195504426956
quantized
not here
quantized in 49.537675619125366 seconds
36037 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 8.524706316848096e-06 val loss: 7.2420090191371855e-06
8385 MiB free out of 48676 MiB total
epoch 1 loss: 6.963146272909171e-06 val loss: 6.613146325662456e-06
8385 MiB free out of 48676 MiB total
epoch 2 loss: 6.5559819901750416e-06 val loss: 6.356890025926987e-06
8385 MiB free out of 48676 MiB total
epoch 3 loss: 6.354178932355126e-06 val loss: 6.198482907393554e-06
8385 MiB free out of 48676 MiB total
epoch 4 loss: 6.219573904786557e-06 val loss: 6.08395069434664e-06
8385 MiB free out of 48676 MiB total
36037 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8385 MiB free out of 48676 MiB total
after cast to cpu
38579 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
1 self_attn.q_proj
Pruning ...
realigning
initial loss 23.14562225341797
final loss 17.854764938354492
quantized
not here
quantized in 22.893721103668213 seconds
36543 MiB free out of 48676 MiB total
1 self_attn.k_proj
Pruning ...
realigning
initial loss 24.939071655273438
final loss 19.801544189453125
quantized
not here
quantized in 21.80155873298645 seconds
36629 MiB free out of 48676 MiB total
1 self_attn.v_proj
Pruning ...
realigning
initial loss 1.0507758855819702
final loss 0.9199187159538269
quantized
not here
quantized in 20.593744039535522 seconds
36715 MiB free out of 48676 MiB total
1 self_attn.o_proj
Pruning ...
realigning
initial loss 0.08773843199014664
final loss 0.08273354172706604
quantized
not here
quantized in 19.621506452560425 seconds
36801 MiB free out of 48676 MiB total
1 mlp.gate_proj
Pruning ...
realigning
initial loss 25.727558135986328
final loss 18.473936080932617
quantized
not here
quantized in 55.16277194023132 seconds
36521 MiB free out of 48676 MiB total
1 mlp.up_proj
Pruning ...
realigning
initial loss 14.278097152709961
final loss 13.456363677978516
quantized
not here
quantized in 54.62078309059143 seconds
36241 MiB free out of 48676 MiB total
1 mlp.down_proj
Pruning ...
realigning
initial loss 0.1126033365726471
final loss 0.09226929396390915
quantized
not here
quantized in 56.35271239280701 seconds
35961 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0956418986315839 val loss: 0.0003320495507068699
8309 MiB free out of 48676 MiB total
epoch 1 loss: 0.04830801127536688 val loss: 0.0005926625926804263
8309 MiB free out of 48676 MiB total
epoch 2 loss: 0.020548518854411668 val loss: 0.0007858712124289013
8309 MiB free out of 48676 MiB total
epoch 3 loss: 0.006841140283995628 val loss: 0.0009189236916427035
8309 MiB free out of 48676 MiB total
epoch 4 loss: 0.0026769089135996182 val loss: 0.0008748341351747513
8309 MiB free out of 48676 MiB total
35961 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8309 MiB free out of 48676 MiB total
after cast to cpu
38579 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
2 self_attn.q_proj
Pruning ...
realigning
initial loss 74.73880767822266
final loss 63.034488677978516
quantized
not here
quantized in 22.993551015853882 seconds
36575 MiB free out of 48676 MiB total
2 self_attn.k_proj
Pruning ...
realigning
initial loss 92.23189544677734
final loss 71.81217956542969
quantized
not here
quantized in 23.105775117874146 seconds
36661 MiB free out of 48676 MiB total
2 self_attn.v_proj
Pruning ...
realigning
initial loss 14.237030029296875
final loss 13.633190155029297
quantized
not here
quantized in 20.232243061065674 seconds
36747 MiB free out of 48676 MiB total
2 self_attn.o_proj
Pruning ...
realigning
initial loss 0.20712126791477203
final loss 0.1866941899061203
quantized
not here
quantized in 19.986396312713623 seconds
36811 MiB free out of 48676 MiB total
2 mlp.gate_proj
Pruning ...
realigning
initial loss 37.695343017578125
final loss 35.23109436035156
quantized
not here
quantized in 55.12806749343872 seconds
36531 MiB free out of 48676 MiB total
2 mlp.up_proj
Pruning ...
realigning
initial loss 28.16720199584961
final loss 27.750104904174805
quantized
not here
quantized in 52.44483256340027 seconds
36251 MiB free out of 48676 MiB total
2 mlp.down_proj
Pruning ...
realigning
initial loss 0.15605150163173676
final loss 0.15501798689365387
quantized
not here
quantized in 51.24290156364441 seconds
35971 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.00014442847668760805 val loss: 0.0007368051883531734
8319 MiB free out of 48676 MiB total
epoch 1 loss: 0.00013688217541130143 val loss: 0.0007350856394623406
8319 MiB free out of 48676 MiB total
epoch 2 loss: 0.00013498615908247302 val loss: 0.0007331614178838208
8319 MiB free out of 48676 MiB total
epoch 3 loss: 0.00013390269947421984 val loss: 0.0007318203824979719
8319 MiB free out of 48676 MiB total
epoch 4 loss: 0.00013313274376969275 val loss: 0.0007308287931664381
8319 MiB free out of 48676 MiB total
35971 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8319 MiB free out of 48676 MiB total
after cast to cpu
38579 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
3 self_attn.q_proj
Pruning ...
realigning
initial loss 175.34593200683594
final loss 155.7941436767578
quantized
not here
quantized in 23.09592366218567 seconds
36575 MiB free out of 48676 MiB total
3 self_attn.k_proj
Pruning ...
realigning
initial loss 207.64328002929688
final loss 169.66824340820312
quantized
not here
quantized in 22.755821704864502 seconds
36661 MiB free out of 48676 MiB total
3 self_attn.v_proj
Pruning ...
realigning
initial loss 36.10301971435547
final loss 35.425559997558594
quantized
not here
quantized in 20.4003267288208 seconds
36661 MiB free out of 48676 MiB total
3 self_attn.o_proj
Pruning ...
realigning
initial loss 0.3616723120212555
final loss 0.28115853667259216
quantized
not here
quantized in 21.153621196746826 seconds
36725 MiB free out of 48676 MiB total
3 mlp.gate_proj
Pruning ...
realigning
initial loss 60.49747085571289
final loss 58.33769989013672
quantized
not here
quantized in 54.86758375167847 seconds
36445 MiB free out of 48676 MiB total
3 mlp.up_proj
Pruning ...
realigning
initial loss 46.40015411376953
final loss 46.14289855957031
quantized
not here
quantized in 52.968241691589355 seconds
36165 MiB free out of 48676 MiB total
3 mlp.down_proj
Pruning ...
realigning
initial loss 0.35951176285743713
final loss 0.3566865921020508
quantized
not here
quantized in 49.41716432571411 seconds
35885 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0003629567088410113 val loss: 0.0018869923806050792
9257 MiB free out of 48676 MiB total
epoch 1 loss: 0.00032114023315443774 val loss: 0.0018852181019610725
9257 MiB free out of 48676 MiB total
epoch 2 loss: 0.00031066777319210814 val loss: 0.0018753775730147026
9257 MiB free out of 48676 MiB total
epoch 3 loss: 0.0003061095053453755 val loss: 0.0018675128376344219
9257 MiB free out of 48676 MiB total
epoch 4 loss: 0.0003034804010439984 val loss: 0.0018616983870742843
9257 MiB free out of 48676 MiB total
35885 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
9257 MiB free out of 48676 MiB total
after cast to cpu
38579 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
4 self_attn.q_proj
Pruning ...
realigning
initial loss 150.35292053222656
final loss 138.24395751953125
quantized
not here
quantized in 22.865640878677368 seconds
36575 MiB free out of 48676 MiB total
4 self_attn.k_proj
Pruning ...
realigning
initial loss 171.4586639404297
final loss 149.26687622070312
quantized
not here
quantized in 22.29226040840149 seconds
36661 MiB free out of 48676 MiB total
4 self_attn.v_proj
Pruning ...
realigning
initial loss 34.68959426879883
final loss 34.02410888671875
quantized
not here
quantized in 20.613221645355225 seconds
36747 MiB free out of 48676 MiB total
4 self_attn.o_proj
Pruning ...
realigning
initial loss 0.6784555315971375
final loss 0.5777918100357056
quantized
not here
quantized in 21.359273672103882 seconds
36833 MiB free out of 48676 MiB total
4 mlp.gate_proj
Pruning ...
realigning
initial loss 86.38809967041016
final loss 81.23365783691406
quantized
not here
quantized in 55.15128707885742 seconds
36553 MiB free out of 48676 MiB total
4 mlp.up_proj
Pruning ...
realigning
initial loss 60.215091705322266
final loss 59.6048583984375
quantized
not here
quantized in 53.43142819404602 seconds
36273 MiB free out of 48676 MiB total
4 mlp.down_proj
Pruning ...
realigning
initial loss 0.6638007164001465
final loss 0.6465693116188049
quantized
not here
quantized in 52.81621551513672 seconds
35993 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0005732650515710702 val loss: 0.0021977264404995367
8341 MiB free out of 48676 MiB total
epoch 1 loss: 0.0005440162462946319 val loss: 0.0022009256936144084
8341 MiB free out of 48676 MiB total
epoch 2 loss: 0.0005341159981071542 val loss: 0.002201268755015917
8341 MiB free out of 48676 MiB total
epoch 3 loss: 0.0005288311149342917 val loss: 0.0022003025806043297
8341 MiB free out of 48676 MiB total
epoch 4 loss: 0.0005252558371466876 val loss: 0.002198866699473001
8341 MiB free out of 48676 MiB total
35993 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8341 MiB free out of 48676 MiB total
after cast to cpu
38579 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
5 self_attn.q_proj
Pruning ...
realigning
initial loss 171.9491729736328
final loss 154.53695678710938
quantized
not here
quantized in 22.85910677909851 seconds
36575 MiB free out of 48676 MiB total
5 self_attn.k_proj
Pruning ...
realigning
initial loss 214.30938720703125
final loss 177.16014099121094
quantized
not here
quantized in 22.005975246429443 seconds
36661 MiB free out of 48676 MiB total
5 self_attn.v_proj
Pruning ...
realigning
initial loss 40.240074157714844
final loss 39.76367950439453
quantized
not here
quantized in 20.34577202796936 seconds
36661 MiB free out of 48676 MiB total
5 self_attn.o_proj
Pruning ...
realigning
initial loss 1.2311172485351562
final loss 1.0308657884597778
quantized
not here
quantized in 21.15981888771057 seconds
36725 MiB free out of 48676 MiB total
5 mlp.gate_proj
Pruning ...
realigning
initial loss 105.74771881103516
final loss 101.08625793457031
quantized
not here
quantized in 53.93402695655823 seconds
36445 MiB free out of 48676 MiB total
5 mlp.up_proj
Pruning ...
realigning
initial loss 74.73980712890625
final loss 74.22325134277344
quantized
not here
quantized in 52.97908353805542 seconds
36165 MiB free out of 48676 MiB total
5 mlp.down_proj
Pruning ...
realigning
initial loss 1.1530262231826782
final loss 1.1239897012710571
quantized
not here
quantized in 52.90398645401001 seconds
36057 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0009193084979415289 val loss: 0.002614952973090112
9429 MiB free out of 48676 MiB total
epoch 1 loss: 0.0008602197954132862 val loss: 0.0027287820557830855
9429 MiB free out of 48676 MiB total
epoch 2 loss: 0.0008385640608139511 val loss: 0.0027997461293125525
9429 MiB free out of 48676 MiB total
epoch 3 loss: 0.0008279358821710048 val loss: 0.0028408509388100356
9429 MiB free out of 48676 MiB total
epoch 4 loss: 0.0008217110216719448 val loss: 0.002867462797439657
9429 MiB free out of 48676 MiB total
36057 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
9429 MiB free out of 48676 MiB total
after cast to cpu
38579 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
6 self_attn.q_proj
Pruning ...
realigning
initial loss 283.2332763671875
final loss 248.3167724609375
quantized
not here
quantized in 22.736191034317017 seconds
36575 MiB free out of 48676 MiB total
6 self_attn.k_proj
Pruning ...
realigning
initial loss 315.1747741699219
final loss 253.43128967285156
quantized
not here
quantized in 21.896856546401978 seconds
36661 MiB free out of 48676 MiB total
6 self_attn.v_proj
Pruning ...
realigning
initial loss 58.644203186035156
final loss 57.85211944580078
quantized
not here
quantized in 20.429353713989258 seconds
36747 MiB free out of 48676 MiB total
6 self_attn.o_proj
Pruning ...
realigning
initial loss 1.8263545036315918
final loss 1.617005705833435
quantized
not here
quantized in 21.29197335243225 seconds
36833 MiB free out of 48676 MiB total
6 mlp.gate_proj
Pruning ...
realigning
initial loss 136.80787658691406
final loss 129.2947998046875
quantized
not here
quantized in 55.02966332435608 seconds
36617 MiB free out of 48676 MiB total
6 mlp.up_proj
Pruning ...
realigning
initial loss 90.76448059082031
final loss 89.99251556396484
quantized
not here
quantized in 52.841203689575195 seconds
36337 MiB free out of 48676 MiB total
6 mlp.down_proj
Pruning ...
realigning
initial loss 1.666305422782898
final loss 1.628153920173645
quantized
not here
quantized in 53.24752163887024 seconds
36229 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0014696548651045305 val loss: 0.003159611005685292
9601 MiB free out of 48676 MiB total
epoch 1 loss: 0.0013506905552276294 val loss: 0.003204588239896111
9601 MiB free out of 48676 MiB total
epoch 2 loss: 0.0013066825122223236 val loss: 0.0032398461480624974
9601 MiB free out of 48676 MiB total
epoch 3 loss: 0.0012851781621066038 val loss: 0.003271991154178977
9601 MiB free out of 48676 MiB total
epoch 4 loss: 0.0012721003104161355 val loss: 0.003301312943222001
9601 MiB free out of 48676 MiB total
36229 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
9601 MiB free out of 48676 MiB total
after cast to cpu
38579 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
7 self_attn.q_proj
Pruning ...
realigning
initial loss 314.9444580078125
final loss 273.32806396484375
quantized
not here
quantized in 22.94575071334839 seconds
36575 MiB free out of 48676 MiB total
7 self_attn.k_proj
Pruning ...
realigning
initial loss 322.86883544921875
final loss 268.35845947265625
quantized
not here
quantized in 21.954259634017944 seconds
36661 MiB free out of 48676 MiB total
7 self_attn.v_proj
Pruning ...
realigning
initial loss 66.0579833984375
final loss 65.23307037353516
quantized
not here
quantized in 20.87347674369812 seconds
36747 MiB free out of 48676 MiB total
7 self_attn.o_proj
Pruning ...
realigning
initial loss 2.732358932495117
final loss 2.49613094329834
quantized
not here
quantized in 21.203914642333984 seconds
36747 MiB free out of 48676 MiB total
7 mlp.gate_proj
Pruning ...
realigning
initial loss 157.63131713867188
final loss 149.78143310546875
quantized
not here
quantized in 54.81487202644348 seconds
36531 MiB free out of 48676 MiB total
7 mlp.up_proj
Pruning ...
realigning
initial loss 106.60490417480469
final loss 105.795654296875
quantized
not here
quantized in 52.9292995929718 seconds
36251 MiB free out of 48676 MiB total
7 mlp.down_proj
Pruning ...
realigning
initial loss 2.2598259449005127
final loss 2.2271647453308105
quantized
not here
quantized in 53.06017303466797 seconds
35971 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0020576224524120335 val loss: 0.0046320545370690525
8319 MiB free out of 48676 MiB total
epoch 1 loss: 0.0019217890139771043 val loss: 0.0047506054397672415
8319 MiB free out of 48676 MiB total
epoch 2 loss: 0.0018649111434569932 val loss: 0.004804208903806284
8319 MiB free out of 48676 MiB total
epoch 3 loss: 0.0018351629114476964 val loss: 0.004848563316045329
8319 MiB free out of 48676 MiB total
epoch 4 loss: 0.0018170250205002958 val loss: 0.004894435580354184
8319 MiB free out of 48676 MiB total
35971 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8319 MiB free out of 48676 MiB total
after cast to cpu
38579 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
8 self_attn.q_proj
Pruning ...
realigning
initial loss 307.1496276855469
final loss 276.9915466308594
quantized
not here
quantized in 22.70806908607483 seconds
36575 MiB free out of 48676 MiB total
8 self_attn.k_proj
Pruning ...
realigning
initial loss 304.36627197265625
final loss 263.21636962890625
quantized
not here
quantized in 21.7722430229187 seconds
36661 MiB free out of 48676 MiB total
8 self_attn.v_proj
Pruning ...
realigning
initial loss 67.343017578125
final loss 66.44236755371094
quantized
not here
quantized in 20.51464080810547 seconds
36661 MiB free out of 48676 MiB total
8 self_attn.o_proj
Pruning ...
realigning
initial loss 4.097017288208008
final loss 3.964287757873535
quantized
not here
quantized in 21.2025465965271 seconds
36725 MiB free out of 48676 MiB total
8 mlp.gate_proj
Pruning ...
realigning
initial loss 168.242919921875
final loss 159.09051513671875
quantized
not here
quantized in 55.03007435798645 seconds
36445 MiB free out of 48676 MiB total
8 mlp.up_proj
Pruning ...
realigning
initial loss 119.12931823730469
final loss 117.93160247802734
quantized
not here
quantized in 52.744712829589844 seconds
36165 MiB free out of 48676 MiB total
8 mlp.down_proj
Pruning ...
realigning
initial loss 2.79940128326416
final loss 2.7629621028900146
quantized
not here
quantized in 51.4224169254303 seconds
36057 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0026556212578725535 val loss: 0.004429587192134932
9429 MiB free out of 48676 MiB total
epoch 1 loss: 0.0024988774603116326 val loss: 0.004483376920688897
9429 MiB free out of 48676 MiB total
epoch 2 loss: 0.002430008598821587 val loss: 0.004540412832284346
9429 MiB free out of 48676 MiB total
epoch 3 loss: 0.0023897753926576115 val loss: 0.004599454638082534
9429 MiB free out of 48676 MiB total
epoch 4 loss: 0.002363698918998125 val loss: 0.0046595582098234445
9429 MiB free out of 48676 MiB total
36057 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
9429 MiB free out of 48676 MiB total
after cast to cpu
38579 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
9 self_attn.q_proj
Pruning ...
realigning
initial loss 288.0062561035156
final loss 261.51239013671875
quantized
not here
quantized in 22.724015474319458 seconds
36575 MiB free out of 48676 MiB total
9 self_attn.k_proj
Pruning ...
realigning
initial loss 321.44085693359375
final loss 281.50018310546875
quantized
not here
quantized in 21.840767860412598 seconds
36661 MiB free out of 48676 MiB total
9 self_attn.v_proj
Pruning ...
realigning
initial loss 73.9227294921875
final loss 73.0709457397461
quantized
not here
quantized in 20.480274200439453 seconds
36747 MiB free out of 48676 MiB total
9 self_attn.o_proj
Pruning ...
realigning
initial loss 7.055806636810303
final loss 6.955780982971191
quantized
not here
quantized in 20.01209330558777 seconds
36833 MiB free out of 48676 MiB total
9 mlp.gate_proj
Pruning ...
realigning
initial loss 176.4702911376953
final loss 166.64736938476562
quantized
not here
quantized in 54.76811361312866 seconds
36553 MiB free out of 48676 MiB total
9 mlp.up_proj
Pruning ...
realigning
initial loss 129.27133178710938
final loss 127.86331939697266
quantized
not here
quantized in 52.629263162612915 seconds
36273 MiB free out of 48676 MiB total
9 mlp.down_proj
Pruning ...
realigning
initial loss 3.2030129432678223
final loss 3.1722071170806885
quantized
not here
quantized in 52.708637714385986 seconds
35993 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0030929142430977663 val loss: 0.0059028824616689235
8341 MiB free out of 48676 MiB total
epoch 1 loss: 0.0029511817992897704 val loss: 0.0059199463576078415
8341 MiB free out of 48676 MiB total
epoch 2 loss: 0.002891437436119304 val loss: 0.00593580657732673
8341 MiB free out of 48676 MiB total
epoch 3 loss: 0.0028566333294293145 val loss: 0.005959618196357042
8341 MiB free out of 48676 MiB total
epoch 4 loss: 0.0028338951651676325 val loss: 0.005989546451019123
8341 MiB free out of 48676 MiB total
35993 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8341 MiB free out of 48676 MiB total
after cast to cpu
38579 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
10 self_attn.q_proj
Pruning ...
realigning
initial loss 302.1548156738281
final loss 272.9256896972656
quantized
not here
quantized in 22.903733253479004 seconds
36575 MiB free out of 48676 MiB total
10 self_attn.k_proj
Pruning ...
realigning
initial loss 338.039306640625
final loss 298.4505920410156
quantized
not here
quantized in 22.054498195648193 seconds
36661 MiB free out of 48676 MiB total
10 self_attn.v_proj
Pruning ...
realigning
initial loss 74.914794921875
final loss 73.89388275146484
quantized
not here
quantized in 20.378246784210205 seconds
36747 MiB free out of 48676 MiB total
10 self_attn.o_proj
Pruning ...
realigning
initial loss 9.224563598632812
final loss 9.119576454162598
quantized
not here
quantized in 20.15355682373047 seconds
36747 MiB free out of 48676 MiB total
10 mlp.gate_proj
Pruning ...
realigning
initial loss 193.12069702148438
final loss 181.2257843017578
quantized
not here
quantized in 55.20358872413635 seconds
36531 MiB free out of 48676 MiB total
10 mlp.up_proj
Pruning ...
realigning
initial loss 140.7053985595703
final loss 138.93179321289062
quantized
not here
quantized in 53.94025135040283 seconds
36251 MiB free out of 48676 MiB total
10 mlp.down_proj
Pruning ...
realigning
initial loss 3.761190891265869
final loss 3.698826313018799
quantized
not here
quantized in 54.19859552383423 seconds
35971 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.003759477756830165 val loss: 0.008443232043646276
8319 MiB free out of 48676 MiB total
epoch 1 loss: 0.0036194657695887145 val loss: 0.008389257418457419
8319 MiB free out of 48676 MiB total
epoch 2 loss: 0.003555719282303471 val loss: 0.008387598674744368
8319 MiB free out of 48676 MiB total
epoch 3 loss: 0.0035160048901161645 val loss: 0.008428643981460482
8319 MiB free out of 48676 MiB total
epoch 4 loss: 0.0034887654710473726 val loss: 0.008495433838106692
8319 MiB free out of 48676 MiB total
35971 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8319 MiB free out of 48676 MiB total
after cast to cpu
38579 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
11 self_attn.q_proj
Pruning ...
realigning
initial loss 379.41644287109375
final loss 339.009765625
quantized
not here
quantized in 23.195971250534058 seconds
36575 MiB free out of 48676 MiB total
11 self_attn.k_proj
Pruning ...
realigning
initial loss 371.09375
final loss 324.6380615234375
quantized
not here
quantized in 22.094024896621704 seconds
36661 MiB free out of 48676 MiB total
11 self_attn.v_proj
Pruning ...
realigning
initial loss 102.2728271484375
final loss 101.13854217529297
quantized
not here
quantized in 20.99683403968811 seconds
36747 MiB free out of 48676 MiB total
11 self_attn.o_proj
Pruning ...
realigning
initial loss 8.407790184020996
final loss 7.973227500915527
quantized
not here
quantized in 21.30351734161377 seconds
36811 MiB free out of 48676 MiB total
11 mlp.gate_proj
Pruning ...
realigning
initial loss 209.08261108398438
final loss 194.75601196289062
quantized
not here
quantized in 55.494545459747314 seconds
36531 MiB free out of 48676 MiB total
11 mlp.up_proj
Pruning ...
realigning
initial loss 156.3981170654297
final loss 154.819091796875
quantized
not here
quantized in 53.72470188140869 seconds
36251 MiB free out of 48676 MiB total
11 mlp.down_proj
Pruning ...
realigning
initial loss 4.4315643310546875
final loss 4.3848161697387695
quantized
not here
quantized in 52.93819856643677 seconds
35971 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.00426184573007049 val loss: 0.006632521777646616
9343 MiB free out of 48676 MiB total
epoch 1 loss: 0.004089632693649037 val loss: 0.0066859736980404705
9343 MiB free out of 48676 MiB total
epoch 2 loss: 0.004007004243248957 val loss: 0.006717683107126504
9343 MiB free out of 48676 MiB total
epoch 3 loss: 0.003956681306590326 val loss: 0.006746595696313307
9343 MiB free out of 48676 MiB total
epoch 4 loss: 0.003922504678484984 val loss: 0.00678030212293379
9343 MiB free out of 48676 MiB total
35971 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
9343 MiB free out of 48676 MiB total
after cast to cpu
38579 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
12 self_attn.q_proj
Pruning ...
realigning
initial loss 365.2572021484375
final loss 329.46380615234375
quantized
not here
quantized in 22.851291179656982 seconds
36575 MiB free out of 48676 MiB total
12 self_attn.k_proj
Pruning ...
realigning
initial loss 402.2062683105469
final loss 350.8739013671875
quantized
not here
quantized in 21.993112564086914 seconds
36661 MiB free out of 48676 MiB total
12 self_attn.v_proj
Pruning ...
realigning
initial loss 100.15287780761719
final loss 99.27018737792969
quantized
not here
quantized in 20.846229553222656 seconds
36661 MiB free out of 48676 MiB total
12 self_attn.o_proj
Pruning ...
realigning
initial loss 10.114914894104004
final loss 9.853521347045898
quantized
not here
quantized in 21.13603401184082 seconds
36725 MiB free out of 48676 MiB total
12 mlp.gate_proj
Pruning ...
realigning
initial loss 215.9215850830078
final loss 204.05392456054688
quantized
not here
quantized in 54.96991944313049 seconds
36445 MiB free out of 48676 MiB total
12 mlp.up_proj
Pruning ...
realigning
initial loss 169.9322967529297
final loss 168.27647399902344
quantized
not here
quantized in 53.435028076171875 seconds
36165 MiB free out of 48676 MiB total
12 mlp.down_proj
Pruning ...
realigning
initial loss 4.5407915115356445
final loss 4.513850212097168
quantized
not here
quantized in 51.812464237213135 seconds
36057 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.00477064739970956 val loss: 0.007282536796992645
9429 MiB free out of 48676 MiB total
epoch 1 loss: 0.004619134833774297 val loss: 0.007366894482402131
9429 MiB free out of 48676 MiB total
epoch 2 loss: 0.004544137460470665 val loss: 0.00745420329621993
9429 MiB free out of 48676 MiB total
epoch 3 loss: 0.004496918536460726 val loss: 0.00754507229430601
9429 MiB free out of 48676 MiB total
epoch 4 loss: 0.004464535420993343 val loss: 0.007633790199179202
9429 MiB free out of 48676 MiB total
36057 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
9429 MiB free out of 48676 MiB total
after cast to cpu
38579 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
13 self_attn.q_proj
Pruning ...
realigning
initial loss 367.74053955078125
final loss 328.92095947265625
quantized
not here
quantized in 22.773119926452637 seconds
36575 MiB free out of 48676 MiB total
13 self_attn.k_proj
Pruning ...
realigning
initial loss 404.8157958984375
final loss 346.8262939453125
quantized
not here
quantized in 21.83306908607483 seconds
36661 MiB free out of 48676 MiB total
13 self_attn.v_proj
Pruning ...
realigning
initial loss 110.55995178222656
final loss 109.94886779785156
quantized
not here
quantized in 20.30934429168701 seconds
36661 MiB free out of 48676 MiB total
13 self_attn.o_proj
Pruning ...
realigning
initial loss 10.266094207763672
final loss 10.045186042785645
quantized
not here
quantized in 21.140931844711304 seconds
36725 MiB free out of 48676 MiB total
13 mlp.gate_proj
Pruning ...
realigning
initial loss 229.73980712890625
final loss 215.71673583984375
quantized
not here
quantized in 55.22104001045227 seconds
36445 MiB free out of 48676 MiB total
13 mlp.up_proj
Pruning ...
realigning
initial loss 185.297119140625
final loss 183.49024963378906
quantized
not here
quantized in 53.4973886013031 seconds
36165 MiB free out of 48676 MiB total
13 mlp.down_proj
Pruning ...
realigning
initial loss 5.170514106750488
final loss 5.156327247619629
quantized
not here
quantized in 53.544007539749146 seconds
36057 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.005403430546721211 val loss: 0.007581662357551977
9429 MiB free out of 48676 MiB total
epoch 1 loss: 0.0052398897205421235 val loss: 0.007667140132980421
9429 MiB free out of 48676 MiB total
epoch 2 loss: 0.005157315754331648 val loss: 0.007758589374134317
9429 MiB free out of 48676 MiB total
epoch 3 loss: 0.00510343423957238 val loss: 0.007857196091208607
9429 MiB free out of 48676 MiB total
epoch 4 loss: 0.0050647850475797895 val loss: 0.007958206377224997
9429 MiB free out of 48676 MiB total
36057 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
9429 MiB free out of 48676 MiB total
after cast to cpu
38579 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
14 self_attn.q_proj
Pruning ...
realigning
initial loss 398.47052001953125
final loss 348.2706604003906
quantized
not here
quantized in 22.680176258087158 seconds
36575 MiB free out of 48676 MiB total
14 self_attn.k_proj
Pruning ...
realigning
initial loss 432.1650085449219
final loss 369.52685546875
quantized
not here
quantized in 21.74714756011963 seconds
36661 MiB free out of 48676 MiB total
14 self_attn.v_proj
Pruning ...
realigning
initial loss 111.32024383544922
final loss 110.67434692382812
quantized
not here
quantized in 20.317904233932495 seconds
36661 MiB free out of 48676 MiB total
14 self_attn.o_proj
Pruning ...
realigning
initial loss 13.732635498046875
final loss 13.64440631866455
quantized
not here
quantized in 20.060261011123657 seconds
36725 MiB free out of 48676 MiB total
14 mlp.gate_proj
Pruning ...
realigning
initial loss 257.42120361328125
final loss 237.5520477294922
quantized
not here
quantized in 55.300963163375854 seconds
36445 MiB free out of 48676 MiB total
14 mlp.up_proj
Pruning ...
realigning
initial loss 201.8955078125
final loss 199.82826232910156
quantized
not here
quantized in 52.95341420173645 seconds
36165 MiB free out of 48676 MiB total
14 mlp.down_proj
Pruning ...
realigning
initial loss 5.923989295959473
final loss 5.911447525024414
quantized
not here
quantized in 54.24061894416809 seconds
35885 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0064711970335338265 val loss: 0.009002302482258528
9257 MiB free out of 48676 MiB total
epoch 1 loss: 0.006283838050876511 val loss: 0.009216454287525266
9257 MiB free out of 48676 MiB total
epoch 2 loss: 0.0061930915180710144 val loss: 0.009375501424074173
9257 MiB free out of 48676 MiB total
epoch 3 loss: 0.006133698796475073 val loss: 0.009517288242932409
9257 MiB free out of 48676 MiB total
epoch 4 loss: 0.0060909081948921084 val loss: 0.009648795065004379
9257 MiB free out of 48676 MiB total
35885 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
9257 MiB free out of 48676 MiB total
after cast to cpu
38579 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
15 self_attn.q_proj
Pruning ...
realigning
initial loss 370.15447998046875
final loss 316.1837463378906
quantized
not here
quantized in 22.5252468585968 seconds
36575 MiB free out of 48676 MiB total
15 self_attn.k_proj
Pruning ...
realigning
initial loss 418.7039794921875
final loss 349.85107421875
quantized
not here
quantized in 21.61153221130371 seconds
36661 MiB free out of 48676 MiB total
15 self_attn.v_proj
Pruning ...
realigning
initial loss 115.05896759033203
final loss 114.3560791015625
quantized
not here
quantized in 20.195615768432617 seconds
36661 MiB free out of 48676 MiB total
15 self_attn.o_proj
Pruning ...
realigning
initial loss 12.885653495788574
final loss 12.713008880615234
quantized
not here
quantized in 21.04090690612793 seconds
36725 MiB free out of 48676 MiB total
15 mlp.gate_proj
Pruning ...
realigning
initial loss 279.53717041015625
final loss 260.75360107421875
quantized
not here
quantized in 55.01776671409607 seconds
36445 MiB free out of 48676 MiB total
15 mlp.up_proj
Pruning ...
realigning
initial loss 222.94650268554688
final loss 220.83169555664062
quantized
not here
quantized in 54.378215312957764 seconds
36165 MiB free out of 48676 MiB total
15 mlp.down_proj
Pruning ...
realigning
initial loss 7.166049957275391
final loss 7.152257919311523
quantized
not here
quantized in 53.955934286117554 seconds
36057 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.007523986951127881 val loss: 0.010402286250609905
8405 MiB free out of 48676 MiB total
epoch 1 loss: 0.007314094789762748 val loss: 0.0105494256131351
8405 MiB free out of 48676 MiB total
epoch 2 loss: 0.007220271701953607 val loss: 0.0107144367066212
8405 MiB free out of 48676 MiB total
epoch 3 loss: 0.007158609958423767 val loss: 0.010883408947847784
8405 MiB free out of 48676 MiB total
epoch 4 loss: 0.007113493917131564 val loss: 0.011047648673411459
8405 MiB free out of 48676 MiB total
36057 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8405 MiB free out of 48676 MiB total
after cast to cpu
38579 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
16 self_attn.q_proj
Pruning ...
realigning
initial loss 403.56256103515625
final loss 346.3369140625
quantized
not here
quantized in 22.660040140151978 seconds
36575 MiB free out of 48676 MiB total
16 self_attn.k_proj
Pruning ...
realigning
initial loss 436.55780029296875
final loss 365.7746276855469
quantized
not here
quantized in 21.723678827285767 seconds
36661 MiB free out of 48676 MiB total
16 self_attn.v_proj
Pruning ...
realigning
initial loss 132.54306030273438
final loss 132.16339111328125
quantized
not here
quantized in 20.35861825942993 seconds
36747 MiB free out of 48676 MiB total
16 self_attn.o_proj
Pruning ...
realigning
initial loss 17.086685180664062
final loss 16.996788024902344
quantized
not here
quantized in 19.88837480545044 seconds
36833 MiB free out of 48676 MiB total
16 mlp.gate_proj
Pruning ...
realigning
initial loss 336.32049560546875
final loss 307.511962890625
quantized
not here
quantized in 55.20615792274475 seconds
36617 MiB free out of 48676 MiB total
16 mlp.up_proj
Pruning ...
realigning
initial loss 255.70230102539062
final loss 252.51608276367188
quantized
not here
quantized in 53.659727334976196 seconds
36337 MiB free out of 48676 MiB total
16 mlp.down_proj
Pruning ...
realigning
initial loss 9.457647323608398
final loss 9.441237449645996
quantized
not here
quantized in 54.24542188644409 seconds
36229 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.010153538474696688 val loss: 0.016186207067221403
8233 MiB free out of 48676 MiB total
epoch 1 loss: 0.009874465598841198 val loss: 0.01645745476707816
8233 MiB free out of 48676 MiB total
epoch 2 loss: 0.009742998117872048 val loss: 0.016721365740522742
8233 MiB free out of 48676 MiB total
epoch 3 loss: 0.009654227033024654 val loss: 0.01699391205329448
8233 MiB free out of 48676 MiB total
epoch 4 loss: 0.009586793945345562 val loss: 0.017270836047828197
8233 MiB free out of 48676 MiB total
36229 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8233 MiB free out of 48676 MiB total
after cast to cpu
38579 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
17 self_attn.q_proj
Pruning ...
realigning
initial loss 424.225341796875
final loss 355.09234619140625
quantized
not here
quantized in 22.51323127746582 seconds
36575 MiB free out of 48676 MiB total
17 self_attn.k_proj
Pruning ...
realigning
initial loss 471.8890686035156
final loss 382.6073913574219
quantized
not here
quantized in 21.66930055618286 seconds
36661 MiB free out of 48676 MiB total
17 self_attn.v_proj
Pruning ...
realigning
initial loss 141.72360229492188
final loss 141.13877868652344
quantized
not here
quantized in 20.48245358467102 seconds
36661 MiB free out of 48676 MiB total
17 self_attn.o_proj
Pruning ...
realigning
initial loss 14.95577621459961
final loss 14.500272750854492
quantized
not here
quantized in 21.051668643951416 seconds
36725 MiB free out of 48676 MiB total
17 mlp.gate_proj
Pruning ...
realigning
initial loss 386.00787353515625
final loss 355.3316955566406
quantized
not here
quantized in 54.87326097488403 seconds
36445 MiB free out of 48676 MiB total
17 mlp.up_proj
Pruning ...
realigning
initial loss 282.3348083496094
final loss 279.908935546875
quantized
not here
quantized in 53.41710543632507 seconds
36165 MiB free out of 48676 MiB total
17 mlp.down_proj
Pruning ...
realigning
initial loss 10.358760833740234
final loss 10.351656913757324
quantized
not here
quantized in 52.76564383506775 seconds
36057 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.01039033368579112 val loss: 0.01741781027521938
9429 MiB free out of 48676 MiB total
epoch 1 loss: 0.01013011003669817 val loss: 0.01740524941124022
9429 MiB free out of 48676 MiB total
epoch 2 loss: 0.01000707386265276 val loss: 0.017408402753062546
9429 MiB free out of 48676 MiB total
epoch 3 loss: 0.009921644268615637 val loss: 0.017430218402296305
9429 MiB free out of 48676 MiB total
epoch 4 loss: 0.009856322372797877 val loss: 0.01747518195770681
9429 MiB free out of 48676 MiB total
36057 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
9429 MiB free out of 48676 MiB total
after cast to cpu
38579 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
18 self_attn.q_proj
Pruning ...
realigning
initial loss 448.2845153808594
final loss 366.4017028808594
quantized
not here
quantized in 22.127209663391113 seconds
36575 MiB free out of 48676 MiB total
18 self_attn.k_proj
Pruning ...
realigning
initial loss 502.7359619140625
final loss 399.48394775390625
quantized
not here
quantized in 21.37203574180603 seconds
36661 MiB free out of 48676 MiB total
18 self_attn.v_proj
Pruning ...
realigning
initial loss 172.9954376220703
final loss 172.52670288085938
quantized
not here
quantized in 19.971131324768066 seconds
36661 MiB free out of 48676 MiB total
18 self_attn.o_proj
Pruning ...
realigning
initial loss 13.96526050567627
final loss 13.805399894714355
quantized
not here
quantized in 19.608554363250732 seconds
36725 MiB free out of 48676 MiB total
18 mlp.gate_proj
Pruning ...
realigning
initial loss 433.2205810546875
final loss 402.02301025390625
quantized
not here
quantized in 54.19345211982727 seconds
36445 MiB free out of 48676 MiB total
18 mlp.up_proj
Pruning ...
realigning
initial loss 313.217041015625
final loss 311.22802734375
quantized
not here
quantized in 52.78349256515503 seconds
36165 MiB free out of 48676 MiB total
18 mlp.down_proj
Pruning ...
realigning
initial loss 12.433698654174805
final loss 12.415319442749023
quantized
not here
quantized in 53.47583556175232 seconds
35885 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.011738985303964 val loss: 0.018387081450782716
9257 MiB free out of 48676 MiB total
epoch 1 loss: 0.011452995968284085 val loss: 0.018974513979628682
9257 MiB free out of 48676 MiB total
epoch 2 loss: 0.011307650893286336 val loss: 0.01952372118830681
9257 MiB free out of 48676 MiB total
epoch 3 loss: 0.011208145646378398 val loss: 0.020047813653945923
9257 MiB free out of 48676 MiB total
epoch 4 loss: 0.01113288584019756 val loss: 0.02055597642902285
9257 MiB free out of 48676 MiB total
35885 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
9257 MiB free out of 48676 MiB total
after cast to cpu
38579 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
19 self_attn.q_proj
Pruning ...
realigning
initial loss 428.07568359375
final loss 359.7648010253906
quantized
not here
quantized in 22.17327332496643 seconds
36575 MiB free out of 48676 MiB total
19 self_attn.k_proj
Pruning ...
realigning
initial loss 482.89068603515625
final loss 387.2212829589844
quantized
not here
quantized in 21.368793487548828 seconds
36661 MiB free out of 48676 MiB total
19 self_attn.v_proj
Pruning ...
realigning
initial loss 176.32980346679688
final loss 175.90953063964844
quantized
not here
quantized in 20.33840847015381 seconds
36661 MiB free out of 48676 MiB total
19 self_attn.o_proj
Pruning ...
realigning
initial loss 13.66511344909668
final loss 13.357393264770508
quantized
not here
quantized in 20.738764762878418 seconds
36725 MiB free out of 48676 MiB total
19 mlp.gate_proj
Pruning ...
realigning
initial loss 457.92974853515625
final loss 431.8970947265625
quantized
not here
quantized in 53.23837614059448 seconds
36445 MiB free out of 48676 MiB total
19 mlp.up_proj
Pruning ...
realigning
initial loss 338.8251647949219
final loss 336.5479431152344
quantized
not here
quantized in 52.737510681152344 seconds
36165 MiB free out of 48676 MiB total
19 mlp.down_proj
Pruning ...
realigning
initial loss 13.316551208496094
final loss 13.303682327270508
quantized
not here
quantized in 53.92043089866638 seconds
36057 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.013079497184662614 val loss: 0.01959607924800366
8405 MiB free out of 48676 MiB total
epoch 1 loss: 0.012786227198375855 val loss: 0.019907403038814664
8405 MiB free out of 48676 MiB total
epoch 2 loss: 0.012647841635043733 val loss: 0.020153352757915854
8405 MiB free out of 48676 MiB total
epoch 3 loss: 0.012551535983220674 val loss: 0.020378575776703656
8405 MiB free out of 48676 MiB total
epoch 4 loss: 0.012477755684813019 val loss: 0.020593731314875185
8405 MiB free out of 48676 MiB total
36057 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8405 MiB free out of 48676 MiB total
after cast to cpu
38579 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
20 self_attn.q_proj
Pruning ...
realigning
initial loss 458.6862487792969
final loss 380.66021728515625
quantized
not here
quantized in 22.273042678833008 seconds
36575 MiB free out of 48676 MiB total
20 self_attn.k_proj
Pruning ...
realigning
initial loss 480.2998352050781
final loss 396.4537048339844
quantized
not here
quantized in 21.59791660308838 seconds
36661 MiB free out of 48676 MiB total
20 self_attn.v_proj
Pruning ...
realigning
initial loss 181.7576141357422
final loss 181.45657348632812
quantized
not here
quantized in 20.161154985427856 seconds
36661 MiB free out of 48676 MiB total
20 self_attn.o_proj
Pruning ...
realigning
initial loss 30.302631378173828
final loss 29.134929656982422
quantized
not here
quantized in 20.443968772888184 seconds
36725 MiB free out of 48676 MiB total
20 mlp.gate_proj
Pruning ...
realigning
initial loss 490.46612548828125
final loss 461.94000244140625
quantized
not here
quantized in 54.14648485183716 seconds
36445 MiB free out of 48676 MiB total
20 mlp.up_proj
Pruning ...
realigning
initial loss 361.1605529785156
final loss 358.20257568359375
quantized
not here
quantized in 51.861050605773926 seconds
36165 MiB free out of 48676 MiB total
20 mlp.down_proj
Pruning ...
realigning
initial loss 17.126914978027344
final loss 17.078765869140625
quantized
not here
quantized in 54.59990930557251 seconds
36057 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.01809982930717524 val loss: 0.02492464182432741
8405 MiB free out of 48676 MiB total
epoch 1 loss: 0.01750308858754579 val loss: 0.025423391489312053
8405 MiB free out of 48676 MiB total
epoch 2 loss: 0.0172011706745252 val loss: 0.02595110039692372
8405 MiB free out of 48676 MiB total
epoch 3 loss: 0.01699797055334784 val loss: 0.026454296428710222
8405 MiB free out of 48676 MiB total
epoch 4 loss: 0.01684202585602179 val loss: 0.026934665045700967
8405 MiB free out of 48676 MiB total
36057 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8405 MiB free out of 48676 MiB total
after cast to cpu
38579 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
21 self_attn.q_proj
Pruning ...
realigning
initial loss 463.98016357421875
final loss 402.3395690917969
quantized
not here
quantized in 22.385344743728638 seconds
36575 MiB free out of 48676 MiB total
21 self_attn.k_proj
Pruning ...
realigning
initial loss 492.21484375
final loss 420.1015319824219
quantized
not here
quantized in 21.545087575912476 seconds
36661 MiB free out of 48676 MiB total
21 self_attn.v_proj
Pruning ...
realigning
initial loss 220.15460205078125
final loss 219.7954864501953
quantized
not here
quantized in 20.65738010406494 seconds
36661 MiB free out of 48676 MiB total
21 self_attn.o_proj
Pruning ...
realigning
initial loss 18.181915283203125
final loss 16.355934143066406
quantized
not here
quantized in 20.966261386871338 seconds
36725 MiB free out of 48676 MiB total
21 mlp.gate_proj
Pruning ...
realigning
initial loss 524.1506958007812
final loss 499.4945983886719
quantized
not here
quantized in 53.91709351539612 seconds
36445 MiB free out of 48676 MiB total
21 mlp.up_proj
Pruning ...
realigning
initial loss 387.49176025390625
final loss 384.8739929199219
quantized
not here
quantized in 53.3591845035553 seconds
36165 MiB free out of 48676 MiB total
21 mlp.down_proj
Pruning ...
realigning
initial loss 17.791322708129883
final loss 17.776409149169922
quantized
not here
quantized in 54.483872413635254 seconds
36057 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.01718831787002273 val loss: 0.0326921280939132
9429 MiB free out of 48676 MiB total
epoch 1 loss: 0.01663520857982803 val loss: 0.03309732582420111
9429 MiB free out of 48676 MiB total
epoch 2 loss: 0.01635149495996302 val loss: 0.03336734091863036
9429 MiB free out of 48676 MiB total
epoch 3 loss: 0.016189826543268282 val loss: 0.033535485388711095
9429 MiB free out of 48676 MiB total
epoch 4 loss: 0.01607628034253139 val loss: 0.03368257451802492
9429 MiB free out of 48676 MiB total
36057 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
9429 MiB free out of 48676 MiB total
after cast to cpu
38579 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
22 self_attn.q_proj
Pruning ...
realigning
initial loss 503.272216796875
final loss 432.2140197753906
quantized
not here
quantized in 22.386452198028564 seconds
36575 MiB free out of 48676 MiB total
22 self_attn.k_proj
Pruning ...
realigning
initial loss 533.3994140625
final loss 447.1567687988281
quantized
not here
quantized in 21.588968515396118 seconds
36661 MiB free out of 48676 MiB total
22 self_attn.v_proj
Pruning ...
realigning
initial loss 228.7890625
final loss 228.35745239257812
quantized
not here
quantized in 20.3245370388031 seconds
36747 MiB free out of 48676 MiB total
22 self_attn.o_proj
Pruning ...
realigning
initial loss 69.3885726928711
final loss 67.45882415771484
quantized
not here
quantized in 19.991606950759888 seconds
36747 MiB free out of 48676 MiB total
22 mlp.gate_proj
Pruning ...
realigning
initial loss 555.2994995117188
final loss 532.830078125
quantized
not here
quantized in 53.99394249916077 seconds
36531 MiB free out of 48676 MiB total
22 mlp.up_proj
Pruning ...
realigning
initial loss 410.1252136230469
final loss 407.6700134277344
quantized
not here
quantized in 53.204660415649414 seconds
36251 MiB free out of 48676 MiB total
22 mlp.down_proj
Pruning ...
realigning
initial loss 20.04344367980957
final loss 20.031692504882812
quantized
not here
quantized in 52.82162094116211 seconds
35971 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.027060394582804292 val loss: 0.03249221504665911
8319 MiB free out of 48676 MiB total
epoch 1 loss: 0.02623629978916142 val loss: 0.033023016760125756
8319 MiB free out of 48676 MiB total
epoch 2 loss: 0.02576051485084463 val loss: 0.03352377167902887
8319 MiB free out of 48676 MiB total
epoch 3 loss: 0.025404625353985466 val loss: 0.03392346412874758
8319 MiB free out of 48676 MiB total
epoch 4 loss: 0.025107425390160643 val loss: 0.03424105141311884
8319 MiB free out of 48676 MiB total
35971 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8319 MiB free out of 48676 MiB total
after cast to cpu
38579 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
23 self_attn.q_proj
Pruning ...
realigning
initial loss 520.9573364257812
final loss 467.2633056640625
quantized
not here
quantized in 22.51093292236328 seconds
36575 MiB free out of 48676 MiB total
23 self_attn.k_proj
Pruning ...
realigning
initial loss 567.7652587890625
final loss 483.28167724609375
quantized
not here
quantized in 21.668373346328735 seconds
36661 MiB free out of 48676 MiB total
23 self_attn.v_proj
Pruning ...
realigning
initial loss 288.0093078613281
final loss 287.5989685058594
quantized
not here
quantized in 20.374545574188232 seconds
36661 MiB free out of 48676 MiB total
23 self_attn.o_proj
Pruning ...
realigning
initial loss 23.06806182861328
final loss 22.854536056518555
quantized
not here
quantized in 19.955679893493652 seconds
36725 MiB free out of 48676 MiB total
23 mlp.gate_proj
Pruning ...
realigning
initial loss 578.9822998046875
final loss 562.0653076171875
quantized
not here
quantized in 53.879568338394165 seconds
36445 MiB free out of 48676 MiB total
23 mlp.up_proj
Pruning ...
realigning
initial loss 446.88177490234375
final loss 445.18585205078125
quantized
not here
quantized in 53.0486786365509 seconds
36165 MiB free out of 48676 MiB total
23 mlp.down_proj
Pruning ...
realigning
initial loss 22.101926803588867
final loss 22.09408950805664
quantized
not here
quantized in 52.77409768104553 seconds
35885 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.02260968170594424 val loss: 0.035175579600036144
9257 MiB free out of 48676 MiB total
epoch 1 loss: 0.022184773901244625 val loss: 0.03533581714145839
9257 MiB free out of 48676 MiB total
epoch 2 loss: 0.02197645745764021 val loss: 0.035472938092425466
9257 MiB free out of 48676 MiB total
epoch 3 loss: 0.021830602578120306 val loss: 0.035590226063504815
9257 MiB free out of 48676 MiB total
epoch 4 loss: 0.021715138558647595 val loss: 0.0357025065459311
9257 MiB free out of 48676 MiB total
35885 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
9257 MiB free out of 48676 MiB total
after cast to cpu
38579 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
24 self_attn.q_proj
Pruning ...
realigning
initial loss 495.5367126464844
final loss 444.9390563964844
quantized
not here
quantized in 22.615434169769287 seconds
36575 MiB free out of 48676 MiB total
24 self_attn.k_proj
Pruning ...
realigning
initial loss 523.1002807617188
final loss 457.16961669921875
quantized
not here
quantized in 21.737021684646606 seconds
36661 MiB free out of 48676 MiB total
24 self_attn.v_proj
Pruning ...
realigning
initial loss 272.6005554199219
final loss 272.17694091796875
quantized
not here
quantized in 20.70781135559082 seconds
36661 MiB free out of 48676 MiB total
24 self_attn.o_proj
Pruning ...
realigning
initial loss 42.39238739013672
final loss 34.91933822631836
quantized
not here
quantized in 20.973583459854126 seconds
36725 MiB free out of 48676 MiB total
24 mlp.gate_proj
Pruning ...
realigning
initial loss 622.37890625
final loss 602.3324584960938
quantized
not here
quantized in 53.754701375961304 seconds
36445 MiB free out of 48676 MiB total
24 mlp.up_proj
Pruning ...
realigning
initial loss 472.7460632324219
final loss 471.09161376953125
quantized
not here
quantized in 54.66995334625244 seconds
36165 MiB free out of 48676 MiB total
24 mlp.down_proj
Pruning ...
realigning
initial loss 23.61444854736328
final loss 23.60832977294922
quantized
not here
quantized in 52.84557390213013 seconds
36057 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.026829898633877747 val loss: 0.041578026954084635
9429 MiB free out of 48676 MiB total
epoch 1 loss: 0.025489885971182957 val loss: 0.04203382204286754
9429 MiB free out of 48676 MiB total
epoch 2 loss: 0.02482350164791569 val loss: 0.042461792938411236
9429 MiB free out of 48676 MiB total
epoch 3 loss: 0.02444681258930359 val loss: 0.042722283862531185
9429 MiB free out of 48676 MiB total
epoch 4 loss: 0.02419664252374787 val loss: 0.04284153878688812
9429 MiB free out of 48676 MiB total
36057 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
9429 MiB free out of 48676 MiB total
after cast to cpu
38579 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
25 self_attn.q_proj
Pruning ...
realigning
initial loss 564.3990478515625
final loss 515.9783935546875
quantized
not here
quantized in 22.423293590545654 seconds
36575 MiB free out of 48676 MiB total
25 self_attn.k_proj
Pruning ...
realigning
initial loss 602.697509765625
final loss 531.351806640625
quantized
not here
quantized in 21.42526602745056 seconds
36661 MiB free out of 48676 MiB total
25 self_attn.v_proj
Pruning ...
realigning
initial loss 346.9989318847656
final loss 346.5674133300781
quantized
not here
quantized in 20.229666233062744 seconds
36661 MiB free out of 48676 MiB total
25 self_attn.o_proj
Pruning ...
realigning
initial loss 21.73531150817871
final loss 21.23810386657715
quantized
not here
quantized in 20.107983112335205 seconds
36725 MiB free out of 48676 MiB total
25 mlp.gate_proj
Pruning ...
realigning
initial loss 681.1453857421875
final loss 656.6613159179688
quantized
not here
quantized in 54.39300560951233 seconds
36445 MiB free out of 48676 MiB total
25 mlp.up_proj
Pruning ...
realigning
initial loss 514.1871337890625
final loss 512.3228149414062
quantized
not here
quantized in 54.477163314819336 seconds
36165 MiB free out of 48676 MiB total
25 mlp.down_proj
Pruning ...
realigning
initial loss 25.431114196777344
final loss 25.417036056518555
quantized
not here
quantized in 53.154545545578 seconds
36057 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.02734194984077476 val loss: 0.04948945832438767
8405 MiB free out of 48676 MiB total
epoch 1 loss: 0.026596820083796047 val loss: 0.049370725406333804
8405 MiB free out of 48676 MiB total
epoch 2 loss: 0.02625971556699369 val loss: 0.0492709253448993
8405 MiB free out of 48676 MiB total
epoch 3 loss: 0.026042343582957983 val loss: 0.0491921785287559
8405 MiB free out of 48676 MiB total
epoch 4 loss: 0.025878264830680564 val loss: 0.049140224466100335
8405 MiB free out of 48676 MiB total
36057 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8405 MiB free out of 48676 MiB total
after cast to cpu
38579 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
26 self_attn.q_proj
Pruning ...
realigning
initial loss 547.8695068359375
final loss 484.2515869140625
quantized
not here
quantized in 22.45675039291382 seconds
36575 MiB free out of 48676 MiB total
26 self_attn.k_proj
Pruning ...
realigning
initial loss 602.4893798828125
final loss 506.4205322265625
quantized
not here
quantized in 21.40373682975769 seconds
36661 MiB free out of 48676 MiB total
26 self_attn.v_proj
Pruning ...
realigning
initial loss 341.34228515625
final loss 340.97344970703125
quantized
not here
quantized in 20.289296627044678 seconds
36661 MiB free out of 48676 MiB total
26 self_attn.o_proj
Pruning ...
realigning
initial loss 43.08562088012695
final loss 29.592449188232422
quantized
not here
quantized in 20.95100712776184 seconds
36725 MiB free out of 48676 MiB total
26 mlp.gate_proj
Pruning ...
realigning
initial loss 754.877197265625
final loss 714.0392456054688
quantized
not here
quantized in 54.48669409751892 seconds
36445 MiB free out of 48676 MiB total
26 mlp.up_proj
Pruning ...
realigning
initial loss 542.48095703125
final loss 539.9207153320312
quantized
not here
quantized in 52.94233584403992 seconds
36165 MiB free out of 48676 MiB total
26 mlp.down_proj
Pruning ...
realigning
initial loss 27.068464279174805
final loss 27.058740615844727
quantized
not here
quantized in 53.31022119522095 seconds
36057 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.030331491900142282 val loss: 0.058408253360539675
9429 MiB free out of 48676 MiB total
epoch 1 loss: 0.029059947875794023 val loss: 0.058523913845419884
9429 MiB free out of 48676 MiB total
epoch 2 loss: 0.02866715390700847 val loss: 0.05843495624139905
9429 MiB free out of 48676 MiB total
epoch 3 loss: 0.02842146529292222 val loss: 0.05835035955533385
9429 MiB free out of 48676 MiB total
epoch 4 loss: 0.028237581878784113 val loss: 0.05828375951386988
9429 MiB free out of 48676 MiB total
36057 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
9429 MiB free out of 48676 MiB total
after cast to cpu
38579 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
27 self_attn.q_proj
Pruning ...
realigning
initial loss 625.992431640625
final loss 520.8906860351562
quantized
not here
quantized in 22.180460453033447 seconds
36575 MiB free out of 48676 MiB total
27 self_attn.k_proj
Pruning ...
realigning
initial loss 766.9337158203125
final loss 559.6173095703125
quantized
not here
quantized in 21.405224323272705 seconds
36661 MiB free out of 48676 MiB total
27 self_attn.v_proj
Pruning ...
realigning
initial loss 355.8121337890625
final loss 355.03826904296875
quantized
not here
quantized in 20.321816205978394 seconds
36747 MiB free out of 48676 MiB total
27 self_attn.o_proj
Pruning ...
realigning
initial loss 34.17448425292969
final loss 33.43169021606445
quantized
not here
quantized in 20.05485773086548 seconds
36811 MiB free out of 48676 MiB total
27 mlp.gate_proj
Pruning ...
realigning
initial loss 827.8421020507812
final loss 772.5028076171875
quantized
not here
quantized in 54.24126100540161 seconds
36531 MiB free out of 48676 MiB total
27 mlp.up_proj
Pruning ...
realigning
initial loss 581.6635131835938
final loss 577.2351684570312
quantized
not here
quantized in 54.19142484664917 seconds
36251 MiB free out of 48676 MiB total
27 mlp.down_proj
Pruning ...
realigning
initial loss 30.781112670898438
final loss 30.755149841308594
quantized
not here
quantized in 53.168041706085205 seconds
35971 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.032460287562571466 val loss: 0.07347057433798909
9343 MiB free out of 48676 MiB total
epoch 1 loss: 0.03179943450959399 val loss: 0.07413714192807674
9343 MiB free out of 48676 MiB total
epoch 2 loss: 0.03146489191567525 val loss: 0.07458017580211163
9343 MiB free out of 48676 MiB total
epoch 3 loss: 0.03121855236531701 val loss: 0.07493830192834139
9343 MiB free out of 48676 MiB total
epoch 4 loss: 0.031014254709589295 val loss: 0.07530010072514415
9343 MiB free out of 48676 MiB total
35971 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
9343 MiB free out of 48676 MiB total
after cast to cpu
38579 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
28 self_attn.q_proj
Pruning ...
realigning
initial loss 626.1300659179688
final loss 530.147705078125
quantized
not here
quantized in 22.295046091079712 seconds
36575 MiB free out of 48676 MiB total
28 self_attn.k_proj
Pruning ...
realigning
initial loss 756.3696899414062
final loss 569.7238159179688
quantized
not here
quantized in 21.427106618881226 seconds
36661 MiB free out of 48676 MiB total
28 self_attn.v_proj
Pruning ...
realigning
initial loss 395.4738464355469
final loss 394.79998779296875
quantized
not here
quantized in 20.424235105514526 seconds
36661 MiB free out of 48676 MiB total
28 self_attn.o_proj
Pruning ...
realigning
initial loss 50.49137878417969
final loss 48.18159866333008
quantized
not here
quantized in 20.693824529647827 seconds
36725 MiB free out of 48676 MiB total
28 mlp.gate_proj
Pruning ...
realigning
initial loss 875.4490966796875
final loss 814.4794921875
quantized
not here
quantized in 54.35665678977966 seconds
36445 MiB free out of 48676 MiB total
28 mlp.up_proj
Pruning ...
realigning
initial loss 654.654296875
final loss 644.4114990234375
quantized
not here
quantized in 53.210700035095215 seconds
36165 MiB free out of 48676 MiB total
28 mlp.down_proj
Pruning ...
realigning
initial loss 36.78832244873047
final loss 36.73493194580078
quantized
not here
quantized in 54.1995735168457 seconds
35885 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.043155561841558665 val loss: 0.08250607550144196
9257 MiB free out of 48676 MiB total
epoch 1 loss: 0.041666292236186564 val loss: 0.0830832808278501
9257 MiB free out of 48676 MiB total
epoch 2 loss: 0.04099951504031196 val loss: 0.08364280173555017
9257 MiB free out of 48676 MiB total
epoch 3 loss: 0.04053530705277808 val loss: 0.08413751097396016
9257 MiB free out of 48676 MiB total
epoch 4 loss: 0.04015284319757484 val loss: 0.08465179102495313
9257 MiB free out of 48676 MiB total
35885 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
9257 MiB free out of 48676 MiB total
after cast to cpu
38579 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
29 self_attn.q_proj
Pruning ...
realigning
initial loss 588.79931640625
final loss 485.1527099609375
quantized
not here
quantized in 22.464255571365356 seconds
36575 MiB free out of 48676 MiB total
29 self_attn.k_proj
Pruning ...
realigning
initial loss 678.0206298828125
final loss 512.2445068359375
quantized
not here
quantized in 21.465566635131836 seconds
36661 MiB free out of 48676 MiB total
29 self_attn.v_proj
Pruning ...
realigning
initial loss 371.5067138671875
final loss 370.9834899902344
quantized
not here
quantized in 20.53792142868042 seconds
36661 MiB free out of 48676 MiB total
29 self_attn.o_proj
Pruning ...
realigning
initial loss 77.03919982910156
final loss 57.10155487060547
quantized
not here
quantized in 21.04586434364319 seconds
36725 MiB free out of 48676 MiB total
29 mlp.gate_proj
Pruning ...
realigning
initial loss 1006.5970458984375
final loss 913.955322265625
quantized
not here
quantized in 54.77073311805725 seconds
36445 MiB free out of 48676 MiB total
29 mlp.up_proj
Pruning ...
realigning
initial loss 768.6719970703125
final loss 751.341552734375
quantized
not here
quantized in 54.16246843338013 seconds
36165 MiB free out of 48676 MiB total
29 mlp.down_proj
Pruning ...
realigning
initial loss 43.94883346557617
final loss 43.83441162109375
quantized
not here
quantized in 54.35297703742981 seconds
36057 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.05431163069442846 val loss: 0.33320846129208803
8405 MiB free out of 48676 MiB total
epoch 1 loss: 0.05203314105165191 val loss: 0.33782446291297674
8405 MiB free out of 48676 MiB total
epoch 2 loss: 0.050857250578701496 val loss: 0.3431702069938183
8405 MiB free out of 48676 MiB total
epoch 3 loss: 0.05008840520167723 val loss: 0.348643128760159
8405 MiB free out of 48676 MiB total
epoch 4 loss: 0.04951304112910293 val loss: 0.35376549046486616
8405 MiB free out of 48676 MiB total
36057 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8405 MiB free out of 48676 MiB total
after cast to cpu
38579 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
30 self_attn.q_proj
Pruning ...
realigning
initial loss 665.9231567382812
final loss 527.6755981445312
quantized
not here
quantized in 22.18979835510254 seconds
36575 MiB free out of 48676 MiB total
30 self_attn.k_proj
Pruning ...
realigning
initial loss 803.47265625
final loss 568.6896362304688
quantized
not here
quantized in 21.230152368545532 seconds
36661 MiB free out of 48676 MiB total
30 self_attn.v_proj
Pruning ...
realigning
initial loss 425.45367431640625
final loss 423.5400085449219
quantized
not here
quantized in 20.25371217727661 seconds
36747 MiB free out of 48676 MiB total
30 self_attn.o_proj
Pruning ...
realigning
initial loss 79.95265197753906
final loss 64.98870849609375
quantized
not here
quantized in 20.89488983154297 seconds
36747 MiB free out of 48676 MiB total
30 mlp.gate_proj
Pruning ...
realigning
initial loss 1285.58984375
final loss 1007.4742431640625
quantized
not here
quantized in 54.46099925041199 seconds
36531 MiB free out of 48676 MiB total
30 mlp.up_proj
Pruning ...
realigning
initial loss 1092.6845703125
final loss 904.791748046875
quantized
not here
quantized in 54.64565658569336 seconds
36251 MiB free out of 48676 MiB total
30 mlp.down_proj
Pruning ...
realigning
initial loss 63.88444519042969
final loss 62.46360778808594
quantized
not here
quantized in 59.467567920684814 seconds
35971 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.16801321576349437 val loss: 29.96295540779829
8319 MiB free out of 48676 MiB total
epoch 1 loss: 0.15542976919095963 val loss: 24.80684319883585
8319 MiB free out of 48676 MiB total
epoch 2 loss: 0.1447793435654603 val loss: 18.841952178627253
8319 MiB free out of 48676 MiB total
epoch 3 loss: 0.13540144270518795 val loss: 14.027042999863625
8319 MiB free out of 48676 MiB total
epoch 4 loss: 0.12765852705342695 val loss: 11.533912636339664
8319 MiB free out of 48676 MiB total
35971 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8319 MiB free out of 48676 MiB total
after cast to cpu
38579 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
31 self_attn.q_proj
Pruning ...
realigning
initial loss 501.291748046875
final loss 387.2103271484375
quantized
not here
quantized in 22.25378131866455 seconds
36575 MiB free out of 48676 MiB total
31 self_attn.k_proj
Pruning ...
realigning
initial loss 759.6373901367188
final loss 474.9237060546875
quantized
not here
quantized in 21.341781854629517 seconds
36661 MiB free out of 48676 MiB total
31 self_attn.v_proj
Pruning ...
realigning
initial loss 238.45852661132812
final loss 237.77459716796875
quantized
not here
quantized in 20.391375303268433 seconds
36661 MiB free out of 48676 MiB total
31 self_attn.o_proj
Pruning ...
realigning
initial loss 99.97566223144531
final loss 62.22745895385742
quantized
not here
quantized in 21.024927377700806 seconds
36725 MiB free out of 48676 MiB total
31 mlp.gate_proj
Pruning ...
realigning
initial loss 1202.72802734375
final loss 872.2694702148438
quantized
not here
quantized in 54.45172357559204 seconds
36445 MiB free out of 48676 MiB total
31 mlp.up_proj
Pruning ...
realigning
initial loss 1714.5572509765625
final loss 969.664306640625
quantized
not here
quantized in 54.788963317871094 seconds
36165 MiB free out of 48676 MiB total
31 mlp.down_proj
Pruning ...
realigning
initial loss 127.80453491210938
final loss 109.76118469238281
quantized
not here
quantized in 59.20716691017151 seconds
36057 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.1890521364985034 val loss: 4.240888059139252
9429 MiB free out of 48676 MiB total
epoch 1 loss: 0.1671639111591503 val loss: 4.1023761332035065
9429 MiB free out of 48676 MiB total
epoch 2 loss: 0.15888932289090008 val loss: 4.036254093050957
9429 MiB free out of 48676 MiB total
epoch 3 loss: 0.15396182460244745 val loss: 3.993872717022896
9429 MiB free out of 48676 MiB total
epoch 4 loss: 0.15039395191706717 val loss: 3.9629620015621185
9429 MiB free out of 48676 MiB total
36057 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
9429 MiB free out of 48676 MiB total
after cast to cpu
38579 MiB free out of 48676 MiB total
Total bits: 12995657728 Total params: 6476005376
average bits per value: 2.0067397992227978
total time taken: 15857.305698871613
Loading tokenizer for meta-llama/Llama-2-7b-hf
/home/lliu/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Dataset: wikitext2
Evaluating ...
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
Perplexity: 7.363367
