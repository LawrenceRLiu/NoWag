{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np \n",
    "import random\n",
    "\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:5\")\n",
    "\n",
    "H = torch.load(\"test/original_weights.pt\")[\"H\"].to(device).float()\n",
    "weights = torch.load(\"test/original_weights.pt\")[\"weights\"].to(device).float()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row_mask.sum() =  tensor(4014, device='cuda:5')\n",
      "column_mask.sum() =  tensor(3957, device='cuda:5')\n",
      "row_mask.sum() =  tensor(3968, device='cuda:5')\n",
      "column_mask.sum() =  tensor(3904, device='cuda:5')\n"
     ]
    }
   ],
   "source": [
    "torch.random.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "random.seed(0)\n",
    "def create_mask(data,percent_top):\n",
    "    \"\"\"\n",
    "    data: torch.tensor of shape (n)\n",
    "    percent_top: float, the percentage of the top values to keep\n",
    "    \"\"\"\n",
    "\n",
    "    threshold = torch.quantile(data, 1-percent_top/100)\n",
    "    return data < threshold\n",
    "\n",
    "\n",
    "d = 64\n",
    "percent_dense_rowise = 2\n",
    "percent_dense_columnwise = 3\n",
    "\n",
    "\n",
    "row_mask = create_mask(torch.norm(weights, dim = 1), percent_dense_rowise)\n",
    "column_mask = create_mask(torch.norm(weights, dim = 0), percent_dense_columnwise) & create_mask(torch.norm(H, dim = 0), percent_dense_columnwise)\n",
    "\n",
    "print(\"row_mask.sum() = \", row_mask.sum())\n",
    "print(\"column_mask.sum() = \", column_mask.sum())\n",
    "\n",
    "\n",
    "def mask_round(mask, d):\n",
    "\n",
    "    while mask.sum() % d != 0:\n",
    "        mask[torch.randint(0, mask.shape[0], (1,))] = False\n",
    "\n",
    "    return mask\n",
    "\n",
    "row_mask = mask_round(row_mask, d)\n",
    "column_mask = mask_round(column_mask, d)\n",
    "\n",
    "mask = row_mask.unsqueeze(1) & column_mask.unsqueeze(0)\n",
    "\n",
    "print(\"row_mask.sum() = \", row_mask.sum())\n",
    "print(\"column_mask.sum() = \", column_mask.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparse bits: 1.2265625 encoding bits: 0.6925048828125 overhead: 0.00018310546875\n",
      "bits per value: 1.91925048828125\n",
      "weights_masked.shape =  torch.Size([3968, 3904])\n"
     ]
    }
   ],
   "source": [
    "torch.random.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "random.seed(0)\n",
    "import tqdm\n",
    "import sklearn.cluster\n",
    "\n",
    "n_iters = 25\n",
    "d = 64\n",
    "k_low_rank = 3\n",
    "keep_top = 0.01\n",
    "\n",
    "# print(\"average bits:\", (np.log2(k_magnitude_codebook) + np.log2(k_coseine_codebook))/d)\n",
    "\n",
    "def get_bytes(bits):\n",
    "    #return as MB\n",
    "    return bits/8/1024/1024\n",
    "\n",
    "\n",
    "overhead = k_low_rank * 16 * d\n",
    "\n",
    "encoding_bits = (16 * k_low_rank)/d * torch.sum(mask).item()\n",
    "\n",
    "sparse_bits = 16 * torch.sum(~mask).item()\n",
    "print(\"sparse bits:\", sparse_bits/(weights.shape[0] * weights.shape[1]),\n",
    "        \"encoding bits:\", encoding_bits/(weights.shape[0] * weights.shape[1]),\n",
    "        \"overhead:\", overhead/(weights.shape[0] * weights.shape[1]))\n",
    "print(\"bits per value:\", (sparse_bits + encoding_bits + overhead)/(weights.shape[0] * weights.shape[1]))\n",
    "# raise ValueError\n",
    "\n",
    "\n",
    "weights_masked = weights[row_mask,:][:,column_mask]\n",
    "print(\"weights_masked.shape = \", weights_masked.shape)\n",
    "subvector_assignments = torch.arange(weights_masked.shape[1]).reshape((-1, d))\n",
    "\n",
    "weights_reshaped = weights_masked[:,subvector_assignments] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3968, 61, 64])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_reshaped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class low_rank_quantizers(nn.Module):\n",
    "    def __init__(self, d1,d2, k, d3): \n",
    "        super(low_rank_quantizers, self).__init__()\n",
    "\n",
    "        self.weights = nn.parameter.Parameter(torch.randn(d1 * d2, k))\n",
    "        self.codebook = nn.parameter.Parameter(torch.rand(k, d3))\n",
    "\n",
    "        self.d1, self.d2, self.k, self.d3 = d1, d2, k, d3\n",
    "\n",
    "    def forward(self):\n",
    "\n",
    "        return (self.weights @ self.codebook).reshape(self.d1, self.d2, self.d3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "free = 46849.0, total = 48676.75\n",
      "average error 34.04465866088867, H error 8654.181640625\n",
      "H error increased\n",
      "lr =  [0.09000000000000001]\n",
      "H error increased\n",
      "lr =  [0.08100000000000002]\n",
      "H error increased\n",
      "lr =  [0.07290000000000002]\n",
      "H error increased\n",
      "lr =  [0.06561000000000002]\n",
      "H error increased\n",
      "lr =  [0.05904900000000002]\n",
      "H error increased\n",
      "lr =  [0.05314410000000002]\n",
      "average error 1.4173794984817505, H error 23.564693450927734\n",
      "average error 1.407180666923523, H error 20.67806053161621\n",
      "H error increased\n",
      "lr =  [0.04782969000000002]\n",
      "H error increased\n",
      "lr =  [0.043046721000000024]\n",
      "average error 1.415610432624817, H error 20.336143493652344\n",
      "H error increased\n",
      "lr =  [0.03874204890000002]\n",
      "H error increased\n",
      "lr =  [0.03486784401000002]\n",
      "H error increased\n",
      "lr =  [0.03138105960900001]\n",
      "H error increased\n",
      "lr =  [0.028242953648100012]\n",
      "H error increased\n",
      "lr =  [0.025418658283290013]\n",
      "H error increased\n",
      "lr =  [0.022876792454961013]\n",
      "H error increased\n",
      "lr =  [0.020589113209464913]\n",
      "average error 1.421230673789978, H error 20.247394561767578\n",
      "H error increased\n",
      "lr =  [0.01853020188851842]\n",
      "H error increased\n",
      "lr =  [0.01667718169966658]\n",
      "H error increased\n",
      "lr =  [0.015009463529699923]\n",
      "average error 1.4231770038604736, H error 20.22320556640625\n",
      "H error increased\n",
      "lr =  [0.013508517176729932]\n",
      "H error increased\n",
      "lr =  [0.01215766545905694]\n",
      "H error increased\n",
      "lr =  [0.010941898913151246]\n",
      "average error 1.423905611038208, H error 20.210254669189453\n",
      "H error increased\n",
      "lr =  [0.009847709021836121]\n",
      "H error increased\n",
      "lr =  [0.00886293811965251]\n",
      "H error increased\n",
      "lr =  [0.007976644307687259]\n",
      "H error increased\n",
      "lr =  [0.007178979876918534]\n",
      "H error increased\n",
      "lr =  [0.006461081889226681]\n",
      "H error increased\n",
      "lr =  [0.005814973700304013]\n",
      "H error increased\n",
      "lr =  [0.005233476330273611]\n",
      "average error 1.4239535331726074, H error 20.202014923095703\n",
      "H error increased\n",
      "lr =  [0.00471012869724625]\n",
      "H error increased\n",
      "lr =  [0.004239115827521626]\n",
      "H error increased\n",
      "lr =  [0.0038152042447694634]\n",
      "average error 1.4234681129455566, H error 20.191356658935547\n",
      "H error increased\n",
      "lr =  [0.003433683820292517]\n",
      "H error increased\n",
      "lr =  [0.0030903154382632653]\n",
      "H error increased\n",
      "lr =  [0.002781283894436939]\n",
      "H error increased\n",
      "lr =  [0.002503155504993245]\n",
      "average error 1.422497034072876, H error 20.182313919067383\n",
      "H error increased\n",
      "lr =  [0.0022528399544939205]\n",
      "H error increased\n",
      "lr =  [0.0020275559590445286]\n"
     ]
    }
   ],
   "source": [
    "model = low_rank_quantizers(weights_reshaped.shape[0], weights_reshaped.shape[1], k_low_rank, d)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-1)\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = 1, gamma = 0.9)\n",
    "free, total = torch.cuda.mem_get_info(device)\n",
    "print(f\"free = {free/1024/1024}, total = {total/1024/1024}\")\n",
    "\n",
    "# raise ValueError\n",
    "\n",
    "n_iters = 10000\n",
    "lr = 1e-3\n",
    "clamp_gradients = 1e-1\n",
    "prev_loss = 1e10\n",
    "\n",
    "lambda_1 = 1\n",
    "lambda_2 = 1000\n",
    "\n",
    "prev_H_error = 1e10\n",
    "\n",
    "for i in range(n_iters):\n",
    "    weights_reconstructed = torch.empty_like(weights_masked)\n",
    "    \n",
    "    tmp = model()\n",
    "    weights_reconstructed[:,subvector_assignments] = tmp\n",
    "\n",
    "\n",
    "\n",
    "    weights_quantized = torch.empty_like(weights)\n",
    "\n",
    "    weights_quantized[mask] = weights_reconstructed.flatten()\n",
    "    weights_quantized[~mask] = weights[~mask]\n",
    "\n",
    "    diff = weights - weights_quantized\n",
    "    average_error = torch.sum(torch.abs(diff)**1)/torch.sum(torch.abs(weights)**1)\n",
    "\n",
    "    H_error = torch.einsum('ik,kl,il->', diff, H/H.shape[0], diff)\n",
    "\n",
    "\n",
    "    if i % (n_iters//10) == 0:\n",
    "        print(f\"average error {average_error}, H error {H_error}\")\n",
    "    # print(f\"average error {average_error}, H error {H_error}\")\n",
    "\n",
    "    if H_error > prev_H_error:\n",
    "        print(\"H error increased\")\n",
    "        lr_scheduler.step()\n",
    "        print(\"lr = \", lr_scheduler.get_last_lr())\n",
    "\n",
    "    prev_H_error = H_error\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    H_error.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(20.1729, device='cuda:5', grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0460,  0.0039, -0.0506,  ..., -0.0152, -0.0212, -0.0279],\n",
       "        [-0.0129,  0.0014, -0.0127,  ..., -0.0024, -0.0125, -0.0106],\n",
       "        [ 0.0112,  0.0043,  0.0134,  ...,  0.0100,  0.0020,  0.0076],\n",
       "        ...,\n",
       "        [ 0.0147, -0.0099, -0.0214,  ..., -0.0049,  0.0427,  0.0095],\n",
       "        [-0.0204, -0.0035, -0.0539,  ..., -0.0182,  0.0262,  0.0080],\n",
       "        [ 0.0287, -0.0051,  0.0744,  ...,  0.0021, -0.0246, -0.0282]],\n",
       "       device='cuda:5', grad_fn=<IndexPutBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0096, -0.0301,  0.0085,  ...,  0.0178, -0.0052, -0.0365],\n",
       "        [-0.0029, -0.0101,  0.0100,  ...,  0.0147,  0.0040, -0.0104],\n",
       "        [-0.0004,  0.0139, -0.0074,  ..., -0.0083, -0.0070,  0.0146],\n",
       "        ...,\n",
       "        [-0.0107, -0.0061,  0.0310,  ..., -0.0052, -0.0143,  0.0236],\n",
       "        [-0.0104, -0.0213, -0.0129,  ..., -0.0199, -0.0143, -0.0103],\n",
       "        [ 0.0184,  0.0119,  0.0195,  ...,  0.0343, -0.0327, -0.0355]],\n",
       "       device='cuda:5')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
