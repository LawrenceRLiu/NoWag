/home/lliu/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.16s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.36it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.25it/s]
Model loaded. LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
4096
Loading tokenizer for meta-llama/Llama-2-7b-hf
Starting...
38748 MiB free out of 48676 MiB total
attention_mask torch.Size([1, 1, 4096, 4096]) cuda:7 torch.float16
position_ids torch.Size([1, 4096]) cuda:7 torch.int64
position_ids torch.Size([1, 4096])
attention_mask torch.Size([1, 1, 4096, 4096])
Ready.
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
0 self_attn.q_proj
Pruning ...
256
iter 0, train loss 1.539240837097168, val loss 1.5551683902740479
iter 10, train loss 1.3275091648101807, val loss 1.3391706943511963
iter 20, train loss 1.3247530460357666, val loss 1.334544062614441
iter 30, train loss 1.0055323839187622, val loss 1.0133731365203857
iter 40, train loss 0.8627423048019409, val loss 0.8695060014724731
iter 50, train loss 0.9098653197288513, val loss 0.9162530899047852
iter 60, train loss 0.7742913961410522, val loss 0.7799912691116333
iter 70, train loss 0.7714749574661255, val loss 0.7776618599891663
iter 80, train loss 0.772995114326477, val loss 0.7791616916656494
iter 90, train loss 0.7368890047073364, val loss 0.7423539757728577
best loss 0.7024013996124268
not here
quantized in 74.0297303199768 seconds
20329 MiB free out of 48676 MiB total
0 self_attn.k_proj
Pruning ...
256
iter 0, train loss 1.238694429397583, val loss 1.249128818511963
iter 10, train loss 0.9339669942855835, val loss 0.9410219192504883
iter 20, train loss 0.8166754245758057, val loss 0.8223265409469604
iter 30, train loss 0.7943630814552307, val loss 0.7983620762825012
iter 40, train loss 0.687296450138092, val loss 0.6897246837615967
iter 50, train loss 0.6478574275970459, val loss 0.6498303413391113
iter 60, train loss 0.6012144684791565, val loss 0.6028896570205688
iter 70, train loss 0.577594518661499, val loss 0.5793419480323792
iter 80, train loss 0.5741013288497925, val loss 0.5755456686019897
iter 90, train loss 0.5419108867645264, val loss 0.5433346033096313
best loss 0.5275446176528931
not here
quantized in 73.75956726074219 seconds
24313 MiB free out of 48676 MiB total
0 self_attn.v_proj
Pruning ...
256
iter 0, train loss 0.0800636038184166, val loss 0.07972458750009537
iter 10, train loss 0.07562747597694397, val loss 0.07512325048446655
iter 20, train loss 0.07391969859600067, val loss 0.07341375946998596
iter 30, train loss 0.07260031253099442, val loss 0.07206330448389053
iter 40, train loss 0.07057321071624756, val loss 0.07006508111953735
iter 50, train loss 0.06959658861160278, val loss 0.06908179819583893
iter 60, train loss 0.06845349073410034, val loss 0.06795002520084381
iter 70, train loss 0.06807161867618561, val loss 0.0675438642501831
iter 80, train loss 0.06756667792797089, val loss 0.06710143387317657
iter 90, train loss 0.06704296171665192, val loss 0.06659063696861267
best loss 0.06644072383642197
not here
quantized in 75.25442862510681 seconds
24281 MiB free out of 48676 MiB total
0 self_attn.o_proj
Pruning ...
256
iter 0, train loss 0.010799665004014969, val loss 0.010869271121919155
iter 10, train loss 0.010423609986901283, val loss 0.010483304969966412
iter 20, train loss 0.009595566429197788, val loss 0.009591903537511826
iter 30, train loss 0.010089416988193989, val loss 0.010201715864241123
iter 40, train loss 0.008834700100123882, val loss 0.008835170418024063
iter 50, train loss 0.008693552576005459, val loss 0.008686389774084091
iter 60, train loss 0.00831171777099371, val loss 0.00830650795251131
iter 70, train loss 0.008381731808185577, val loss 0.00838275533169508
iter 80, train loss 0.008335040882229805, val loss 0.00832438562065363
iter 90, train loss 0.008234450593590736, val loss 0.008271959610283375
best loss 0.00810930784791708
not here
quantized in 74.26839709281921 seconds
24217 MiB free out of 48676 MiB total
0 mlp.gate_proj
Pruning ...
256
iter 0, train loss 3.9480443000793457, val loss 3.9192986488342285
iter 10, train loss 4.103921413421631, val loss 4.071527481079102
iter 20, train loss 4.0965776443481445, val loss 4.062873840332031
iter 30, train loss 4.034256458282471, val loss 4.00203275680542
iter 40, train loss 3.991262197494507, val loss 3.9600884914398193
iter 50, train loss 3.9763901233673096, val loss 3.9438300132751465
iter 60, train loss 3.9398303031921387, val loss 3.9056570529937744
iter 70, train loss 3.9244437217712402, val loss 3.891392230987549
iter 80, train loss 3.9134325981140137, val loss 3.881011724472046
iter 90, train loss 3.9058918952941895, val loss 3.8738362789154053
best loss 3.746044158935547
not here
quantized in 121.61192798614502 seconds
36124 MiB free out of 48676 MiB total
0 mlp.up_proj
Pruning ...
256
iter 0, train loss 3.5419487953186035, val loss 3.5142924785614014
iter 10, train loss 3.7418277263641357, val loss 3.709853172302246
iter 20, train loss 3.5561232566833496, val loss 3.5260727405548096
iter 30, train loss 3.5499653816223145, val loss 3.5196423530578613
iter 40, train loss 3.5339508056640625, val loss 3.5031657218933105
iter 50, train loss 3.5250306129455566, val loss 3.4944939613342285
iter 60, train loss 3.517113208770752, val loss 3.4856178760528564
iter 70, train loss 3.5172061920166016, val loss 3.4852499961853027
iter 80, train loss 3.5197296142578125, val loss 3.4877350330352783
iter 90, train loss 3.5140538215637207, val loss 3.4823131561279297
best loss 3.362034797668457
not here
quantized in 86.9582200050354 seconds
35930 MiB free out of 48676 MiB total
0 mlp.down_proj
Pruning ...
256
iter 0, train loss 0.010733775794506073, val loss 0.010662795975804329
iter 10, train loss 0.011761096306145191, val loss 0.011644423007965088
iter 20, train loss 0.011560875922441483, val loss 0.011430333368480206
iter 30, train loss 0.0112678911536932, val loss 0.01114411186426878
iter 40, train loss 0.011172942817211151, val loss 0.011053776368498802
iter 50, train loss 0.011102825403213501, val loss 0.01098722405731678
iter 60, train loss 0.01100496668368578, val loss 0.01088614109903574
iter 70, train loss 0.010963489301502705, val loss 0.010848484933376312
iter 80, train loss 0.010969397611916065, val loss 0.010853652842342854
iter 90, train loss 0.010969669558107853, val loss 0.010854902677237988
best loss 0.010006040334701538
not here
quantized in 97.05093741416931 seconds
35736 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.codebook', 'self_attn.q_proj.quantizer.norms_1', 'self_attn.q_proj.quantizer.norms_0', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.codebook', 'self_attn.k_proj.quantizer.norms_1', 'self_attn.k_proj.quantizer.norms_0', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.codebook', 'self_attn.v_proj.quantizer.norms_1', 'self_attn.v_proj.quantizer.norms_0', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.codebook', 'self_attn.o_proj.quantizer.norms_1', 'self_attn.o_proj.quantizer.norms_0', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.codebook', 'mlp.gate_proj.quantizer.norms_1', 'mlp.gate_proj.quantizer.norms_0', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.codebook', 'mlp.up_proj.quantizer.norms_1', 'mlp.up_proj.quantizer.norms_0', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.codebook', 'mlp.down_proj.quantizer.norms_1', 'mlp.down_proj.quantizer.norms_0', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135936
n accumulation steps: 1
n batches: 128
epoch 0 loss: 5.790370995129024e-06 val loss: 5.327184624093206e-06
7666 MiB free out of 48676 MiB total
epoch 1 loss: 5.329599293446563e-06 val loss: 5.196272468310781e-06
7666 MiB free out of 48676 MiB total
epoch 2 loss: 5.240987462684643e-06 val loss: 5.137675174182732e-06
7666 MiB free out of 48676 MiB total
epoch 3 loss: 5.194906368188867e-06 val loss: 5.102874752083153e-06
7666 MiB free out of 48676 MiB total
epoch 4 loss: 5.165585054811572e-06 val loss: 5.078927642898634e-06
7666 MiB free out of 48676 MiB total
35736 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
7666 MiB free out of 48676 MiB total
after cast to cpu
37850 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
1 self_attn.q_proj
Pruning ...
256
iter 0, train loss 19.086734771728516, val loss 17.883745193481445
iter 10, train loss 17.0435848236084, val loss 18.043907165527344
iter 20, train loss 15.70231819152832, val loss 19.85098648071289
iter 30, train loss 14.83294677734375, val loss 19.88935089111328
iter 40, train loss 14.167024612426758, val loss 20.227684020996094
iter 50, train loss 13.855323791503906, val loss 20.608654022216797
iter 60, train loss 13.842150688171387, val loss 20.930763244628906
iter 70, train loss 14.287254333496094, val loss 20.975788116455078
iter 80, train loss 14.186971664428711, val loss 20.992084503173828
iter 90, train loss 14.005767822265625, val loss 21.076114654541016
best loss 15.928887367248535
not here
quantized in 36.53860831260681 seconds
36422 MiB free out of 48676 MiB total
1 self_attn.k_proj
Pruning ...
256
iter 0, train loss 17.663463592529297, val loss 17.877328872680664
iter 10, train loss 14.646749496459961, val loss 17.921672821044922
iter 20, train loss 13.941329956054688, val loss 19.302581787109375
iter 30, train loss 13.378944396972656, val loss 19.198348999023438
iter 40, train loss 12.949676513671875, val loss 19.399227142333984
iter 50, train loss 12.756324768066406, val loss 19.539121627807617
iter 60, train loss 12.576925277709961, val loss 19.578235626220703
iter 70, train loss 12.627251625061035, val loss 19.54263687133789
iter 80, train loss 12.525619506835938, val loss 19.516399383544922
iter 90, train loss 12.504490852355957, val loss 19.529186248779297
best loss 15.85973072052002
not here
quantized in 35.60917353630066 seconds
36412 MiB free out of 48676 MiB total
1 self_attn.v_proj
Pruning ...
256
iter 0, train loss 0.8410146832466125, val loss 1.469487190246582
iter 10, train loss 0.8738386631011963, val loss 1.5852620601654053
iter 20, train loss 0.820243239402771, val loss 1.4973746538162231
iter 30, train loss 0.8091675043106079, val loss 1.4827351570129395
iter 40, train loss 0.7981868386268616, val loss 1.4615436792373657
iter 50, train loss 0.7926811575889587, val loss 1.4527432918548584
iter 60, train loss 0.7913623452186584, val loss 1.4489922523498535
iter 70, train loss 0.789756178855896, val loss 1.4459829330444336
iter 80, train loss 0.7893232703208923, val loss 1.450471043586731
iter 90, train loss 0.7882031202316284, val loss 1.4485342502593994
best loss 1.4132113456726074
not here
quantized in 32.04234957695007 seconds
36402 MiB free out of 48676 MiB total
1 self_attn.o_proj
Pruning ...
256
iter 0, train loss 0.0869402140378952, val loss 0.06295543164014816
iter 10, train loss 0.0844842940568924, val loss 0.07301931083202362
iter 20, train loss 0.0821051299571991, val loss 0.06778284907341003
iter 30, train loss 0.08115912973880768, val loss 0.07190293073654175
iter 40, train loss 0.08013061434030533, val loss 0.06720231473445892
iter 50, train loss 0.07940992712974548, val loss 0.06780216097831726
iter 60, train loss 0.07921585440635681, val loss 0.06769072264432907
iter 70, train loss 0.07896514236927032, val loss 0.06825828552246094
iter 80, train loss 0.07847530394792557, val loss 0.06605444848537445
iter 90, train loss 0.07811924815177917, val loss 0.06674216687679291
best loss 0.06295543164014816
not here
quantized in 31.31449794769287 seconds
36370 MiB free out of 48676 MiB total
1 mlp.gate_proj
Pruning ...
256
iter 0, train loss 16.034746170043945, val loss 31.601985931396484
iter 10, train loss 16.924612045288086, val loss 32.43345260620117
iter 20, train loss 16.4257755279541, val loss 31.719972610473633
iter 30, train loss 16.590145111083984, val loss 31.862323760986328
iter 40, train loss 16.763425827026367, val loss 32.46164321899414
iter 50, train loss 16.877782821655273, val loss 32.59165954589844
iter 60, train loss 16.804019927978516, val loss 32.43517303466797
iter 70, train loss 16.706680297851562, val loss 32.50477981567383
iter 80, train loss 16.606220245361328, val loss 32.30632019042969
iter 90, train loss 16.536212921142578, val loss 32.192543029785156
best loss 28.663265228271484
not here
quantized in 88.08095073699951 seconds
36068 MiB free out of 48676 MiB total
1 mlp.up_proj
Pruning ...
256
iter 0, train loss 13.22529411315918, val loss 26.297103881835938
iter 10, train loss 13.689289093017578, val loss 26.568531036376953
iter 20, train loss 13.391719818115234, val loss 26.12830352783203
iter 30, train loss 13.452970504760742, val loss 26.203947067260742
iter 40, train loss 13.433486938476562, val loss 26.2747802734375
iter 50, train loss 13.425646781921387, val loss 26.0577392578125
iter 60, train loss 13.422311782836914, val loss 26.14849853515625
iter 70, train loss 13.408954620361328, val loss 26.08641815185547
iter 80, train loss 13.407835960388184, val loss 25.872074127197266
iter 90, train loss 13.40103530883789, val loss 25.75345802307129
best loss 25.228473663330078
not here
quantized in 86.07022714614868 seconds
35874 MiB free out of 48676 MiB total
1 mlp.down_proj
Pruning ...
256
iter 0, train loss 0.08624126017093658, val loss 0.20467813313007355
iter 10, train loss 0.39902016520500183, val loss 0.26751646399497986
iter 20, train loss 0.2700845003128052, val loss 0.2603881359100342
iter 30, train loss 0.24405261874198914, val loss 0.25337904691696167
iter 40, train loss 0.19137516617774963, val loss 0.24921086430549622
iter 50, train loss 0.1315564215183258, val loss 0.24519407749176025
iter 60, train loss 0.115676149725914, val loss 0.24129454791545868
iter 70, train loss 0.10693007707595825, val loss 0.23708568513393402
iter 80, train loss 0.1026703268289566, val loss 0.23217937350273132
iter 90, train loss 0.101352259516716, val loss 0.2284654825925827
best loss 0.20467813313007355
not here
quantized in 96.18833231925964 seconds
35680 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.codebook', 'self_attn.q_proj.quantizer.norms_1', 'self_attn.q_proj.quantizer.norms_0', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.codebook', 'self_attn.k_proj.quantizer.norms_1', 'self_attn.k_proj.quantizer.norms_0', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.codebook', 'self_attn.v_proj.quantizer.norms_1', 'self_attn.v_proj.quantizer.norms_0', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.codebook', 'self_attn.o_proj.quantizer.norms_1', 'self_attn.o_proj.quantizer.norms_0', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.codebook', 'mlp.gate_proj.quantizer.norms_1', 'mlp.gate_proj.quantizer.norms_0', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.codebook', 'mlp.up_proj.quantizer.norms_1', 'mlp.up_proj.quantizer.norms_0', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.codebook', 'mlp.down_proj.quantizer.norms_1', 'mlp.down_proj.quantizer.norms_0', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135936
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0009116843789342965 val loss: 0.00016841552587720798
6652 MiB free out of 48676 MiB total
epoch 1 loss: 0.00021075425627259392 val loss: 0.00017008336089929799
6652 MiB free out of 48676 MiB total
epoch 2 loss: 0.000171180208099031 val loss: 0.00017128773197327973
6652 MiB free out of 48676 MiB total
epoch 3 loss: 0.00014389257523816923 val loss: 0.00017219241726706969
6652 MiB free out of 48676 MiB total
epoch 4 loss: 0.0001241399364175777 val loss: 0.00017291370750172064
6652 MiB free out of 48676 MiB total
35680 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
6652 MiB free out of 48676 MiB total
after cast to cpu
37850 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
2 self_attn.q_proj
Pruning ...
256
iter 0, train loss 64.09185791015625, val loss 79.3119125366211
iter 10, train loss 62.42497253417969, val loss 89.48388671875
iter 20, train loss 63.6900520324707, val loss 93.49712371826172
iter 30, train loss 61.718841552734375, val loss 91.64537048339844
iter 40, train loss 61.06245803833008, val loss 90.5967788696289
iter 50, train loss 60.607666015625, val loss 90.23774719238281
iter 60, train loss 60.39903259277344, val loss 90.09872436523438
iter 70, train loss 60.468318939208984, val loss 90.20375061035156
iter 80, train loss 60.398868560791016, val loss 90.13992309570312
iter 90, train loss 60.13226318359375, val loss 90.0523681640625
best loss 76.12316131591797
not here
quantized in 34.70818114280701 seconds
36390 MiB free out of 48676 MiB total
2 self_attn.k_proj
Pruning ...
256
iter 0, train loss 76.21247863769531, val loss 90.35240936279297
iter 10, train loss 70.53482055664062, val loss 100.54188537597656
iter 20, train loss 73.76168823242188, val loss 108.70343017578125
iter 30, train loss 71.83082580566406, val loss 105.76199340820312
iter 40, train loss 71.5470962524414, val loss 105.81416320800781
iter 50, train loss 71.2240219116211, val loss 105.65887451171875
iter 60, train loss 70.476806640625, val loss 105.07894897460938
iter 70, train loss 69.51958465576172, val loss 103.9698486328125
iter 80, train loss 69.3727035522461, val loss 103.7651138305664
iter 90, train loss 69.3324966430664, val loss 103.88674926757812
best loss 86.6215591430664
not here
quantized in 33.63262677192688 seconds
36380 MiB free out of 48676 MiB total
2 self_attn.v_proj
Pruning ...
256
iter 0, train loss 13.711109161376953, val loss 20.851900100708008
iter 10, train loss 14.35951042175293, val loss 22.271562576293945
iter 20, train loss 13.921146392822266, val loss 21.5881404876709
iter 30, train loss 13.94398021697998, val loss 21.680858612060547
iter 40, train loss 13.886557579040527, val loss 21.64720916748047
iter 50, train loss 13.904609680175781, val loss 21.657100677490234
iter 60, train loss 13.854095458984375, val loss 21.618301391601562
iter 70, train loss 13.850603103637695, val loss 21.56122398376465
iter 80, train loss 13.842568397521973, val loss 21.586023330688477
iter 90, train loss 13.837569236755371, val loss 21.57178497314453
best loss 20.77344512939453
not here
quantized in 32.01510262489319 seconds
36370 MiB free out of 48676 MiB total
2 self_attn.o_proj
Pruning ...
256
iter 0, train loss 0.18220670521259308, val loss 0.9421975016593933
iter 10, train loss 0.17944960296154022, val loss 0.9873835444450378
iter 20, train loss 0.17428970336914062, val loss 0.9662083387374878
iter 30, train loss 0.17290736734867096, val loss 0.950940728187561
iter 40, train loss 0.1719357967376709, val loss 0.9883882999420166
iter 50, train loss 0.17124900221824646, val loss 0.9508960247039795
iter 60, train loss 0.17108450829982758, val loss 0.9677731394767761
iter 70, train loss 0.17077340185642242, val loss 0.967351496219635
iter 80, train loss 0.1705738604068756, val loss 0.975144624710083
iter 90, train loss 0.17027749121189117, val loss 0.9627745747566223
best loss 0.9196768999099731
not here
quantized in 31.878766775131226 seconds
36370 MiB free out of 48676 MiB total
2 mlp.gate_proj
Pruning ...
256
iter 0, train loss 34.23703384399414, val loss 55.99903106689453
iter 10, train loss 35.69145202636719, val loss 60.580169677734375
iter 20, train loss 34.32292938232422, val loss 59.591453552246094
iter 30, train loss 34.56459045410156, val loss 59.38733673095703
iter 40, train loss 34.41914367675781, val loss 59.34313201904297
iter 50, train loss 34.4439697265625, val loss 59.50841522216797
iter 60, train loss 34.544639587402344, val loss 59.526031494140625
iter 70, train loss 34.528411865234375, val loss 59.77204895019531
iter 80, train loss 34.529449462890625, val loss 59.97225570678711
iter 90, train loss 34.51524353027344, val loss 59.68243408203125
best loss 54.218379974365234
not here
quantized in 86.57047915458679 seconds
36068 MiB free out of 48676 MiB total
2 mlp.up_proj
Pruning ...
256
iter 0, train loss 27.967496871948242, val loss 45.89086151123047
iter 10, train loss 28.175527572631836, val loss 47.49620819091797
iter 20, train loss 28.112499237060547, val loss 47.36563491821289
iter 30, train loss 28.043516159057617, val loss 47.72166442871094
iter 40, train loss 28.1097412109375, val loss 47.93791961669922
iter 50, train loss 28.092138290405273, val loss 47.813087463378906
iter 60, train loss 28.09076499938965, val loss 47.259098052978516
iter 70, train loss 28.09001922607422, val loss 47.391380310058594
iter 80, train loss 28.08301544189453, val loss 47.65612030029297
iter 90, train loss 28.0946044921875, val loss 47.273197174072266
best loss 45.89086151123047
not here
quantized in 84.36837148666382 seconds
35874 MiB free out of 48676 MiB total
2 mlp.down_proj
Pruning ...
256
iter 0, train loss 0.16037505865097046, val loss 0.36497610807418823
iter 10, train loss 0.15988463163375854, val loss 0.3673245906829834
iter 20, train loss 0.1592063009738922, val loss 0.36834874749183655
iter 30, train loss 0.15794160962104797, val loss 0.3613676130771637
iter 40, train loss 0.1573285311460495, val loss 0.36110997200012207
iter 50, train loss 0.15710042417049408, val loss 0.3645104467868805
iter 60, train loss 0.15684616565704346, val loss 0.35763800144195557
iter 70, train loss 0.1567871868610382, val loss 0.36589565873146057
iter 80, train loss 0.15666930377483368, val loss 0.3645144999027252
iter 90, train loss 0.15664467215538025, val loss 0.3632636070251465
best loss 0.3553309738636017
not here
quantized in 92.71772074699402 seconds
35680 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.codebook', 'self_attn.q_proj.quantizer.norms_1', 'self_attn.q_proj.quantizer.norms_0', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.codebook', 'self_attn.k_proj.quantizer.norms_1', 'self_attn.k_proj.quantizer.norms_0', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.codebook', 'self_attn.v_proj.quantizer.norms_1', 'self_attn.v_proj.quantizer.norms_0', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.codebook', 'self_attn.o_proj.quantizer.norms_1', 'self_attn.o_proj.quantizer.norms_0', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.codebook', 'mlp.gate_proj.quantizer.norms_1', 'mlp.gate_proj.quantizer.norms_0', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.codebook', 'mlp.up_proj.quantizer.norms_1', 'mlp.up_proj.quantizer.norms_0', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.codebook', 'mlp.down_proj.quantizer.norms_1', 'mlp.down_proj.quantizer.norms_0', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135936
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0001549448007835963 val loss: 0.000680608722177567
6652 MiB free out of 48676 MiB total
epoch 1 loss: 0.00014500761778890592 val loss: 0.0006907833121658769
6652 MiB free out of 48676 MiB total
epoch 2 loss: 0.00014273948386289703 val loss: 0.00069532383713522
6652 MiB free out of 48676 MiB total
epoch 3 loss: 0.0001416548628867531 val loss: 0.0006972054034122266
6652 MiB free out of 48676 MiB total
epoch 4 loss: 0.00014094467167069524 val loss: 0.0006980534271860961
6652 MiB free out of 48676 MiB total
35680 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
6652 MiB free out of 48676 MiB total
after cast to cpu
37850 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
3 self_attn.q_proj
Pruning ...
256
iter 0, train loss 148.5655517578125, val loss 172.82379150390625
iter 10, train loss 156.14370727539062, val loss 204.0655517578125
iter 20, train loss 151.48886108398438, val loss 200.0634765625
iter 30, train loss 147.04176330566406, val loss 194.79566955566406
iter 40, train loss 146.00555419921875, val loss 194.39222717285156
iter 50, train loss 146.39820861816406, val loss 194.20909118652344
iter 60, train loss 145.64796447753906, val loss 194.54190063476562
iter 70, train loss 145.2636260986328, val loss 193.9177703857422
iter 80, train loss 144.77711486816406, val loss 193.59185791015625
iter 90, train loss 144.40283203125, val loss 193.63876342773438
best loss 172.42066955566406
not here
quantized in 33.83631944656372 seconds
36422 MiB free out of 48676 MiB total
3 self_attn.k_proj
Pruning ...
256
iter 0, train loss 171.1180877685547, val loss 193.88345336914062
iter 10, train loss 171.76177978515625, val loss 223.80299377441406
iter 20, train loss 175.33009338378906, val loss 231.92474365234375
iter 30, train loss 169.8236083984375, val loss 226.07797241210938
iter 40, train loss 165.13914489746094, val loss 221.40699768066406
iter 50, train loss 164.41598510742188, val loss 220.36029052734375
iter 60, train loss 163.5743408203125, val loss 219.63287353515625
iter 70, train loss 163.2987060546875, val loss 218.97207641601562
iter 80, train loss 162.7102508544922, val loss 218.781494140625
iter 90, train loss 161.86407470703125, val loss 217.5040283203125
best loss 192.0829620361328
not here
quantized in 32.84959101676941 seconds
36412 MiB free out of 48676 MiB total
3 self_attn.v_proj
Pruning ...
256
iter 0, train loss 35.335655212402344, val loss 48.07655334472656
iter 10, train loss 35.86748504638672, val loss 49.49091339111328
iter 20, train loss 35.67024230957031, val loss 49.160194396972656
iter 30, train loss 35.63527297973633, val loss 49.257904052734375
iter 40, train loss 35.529911041259766, val loss 49.08599090576172
iter 50, train loss 35.53789520263672, val loss 49.1334228515625
iter 60, train loss 35.46135711669922, val loss 49.105072021484375
iter 70, train loss 35.519317626953125, val loss 49.19224548339844
iter 80, train loss 35.540523529052734, val loss 49.03365707397461
iter 90, train loss 35.496376037597656, val loss 48.921043395996094
best loss 48.07655334472656
not here
quantized in 31.503863096237183 seconds
36402 MiB free out of 48676 MiB total
3 self_attn.o_proj
Pruning ...
256
iter 0, train loss 0.26159805059432983, val loss 2.1491622924804688
iter 10, train loss 0.24741381406784058, val loss 2.198561191558838
iter 20, train loss 0.24038779735565186, val loss 2.2071378231048584
iter 30, train loss 0.24023652076721191, val loss 2.2461204528808594
iter 40, train loss 0.23980775475502014, val loss 2.257185935974121
iter 50, train loss 0.24195410311222076, val loss 2.2623701095581055
iter 60, train loss 0.2464139461517334, val loss 2.271005153656006
iter 70, train loss 0.24675552546977997, val loss 2.2834370136260986
iter 80, train loss 0.24555036425590515, val loss 2.261369466781616
iter 90, train loss 0.245279923081398, val loss 2.3074684143066406
best loss 2.1308014392852783
not here
quantized in 31.485442876815796 seconds
36370 MiB free out of 48676 MiB total
3 mlp.gate_proj
Pruning ...
256
iter 0, train loss 54.97553634643555, val loss 98.61480712890625
iter 10, train loss 56.035301208496094, val loss 100.45088958740234
iter 20, train loss 55.4450569152832, val loss 100.58777618408203
iter 30, train loss 55.49394989013672, val loss 99.96604919433594
iter 40, train loss 55.44969177246094, val loss 100.17484283447266
iter 50, train loss 55.431396484375, val loss 100.99699401855469
iter 60, train loss 55.49618148803711, val loss 100.97613525390625
iter 70, train loss 55.37519073486328, val loss 101.16786193847656
iter 80, train loss 55.46258544921875, val loss 101.29161834716797
iter 90, train loss 55.48046875, val loss 101.2086410522461
best loss 98.2504653930664
not here
quantized in 84.84786820411682 seconds
36068 MiB free out of 48676 MiB total
3 mlp.up_proj
Pruning ...
256
iter 0, train loss 45.10738754272461, val loss 80.40276336669922
iter 10, train loss 45.14982986450195, val loss 81.50040435791016
iter 20, train loss 45.2191162109375, val loss 82.05416870117188
iter 30, train loss 45.18239974975586, val loss 81.95747375488281
iter 40, train loss 45.22602844238281, val loss 82.04733276367188
iter 50, train loss 45.218475341796875, val loss 82.25810241699219
iter 60, train loss 45.18439483642578, val loss 81.88543701171875
iter 70, train loss 45.214202880859375, val loss 81.95032501220703
iter 80, train loss 45.18910598754883, val loss 81.70448303222656
iter 90, train loss 45.19474411010742, val loss 80.8622055053711
best loss 80.19060516357422
not here
quantized in 83.46171379089355 seconds
35874 MiB free out of 48676 MiB total
3 mlp.down_proj
Pruning ...
256
iter 0, train loss 0.32310792803764343, val loss 0.869875431060791
iter 10, train loss 0.32194721698760986, val loss 0.8888528943061829
iter 20, train loss 0.3186635375022888, val loss 0.8742340207099915
iter 30, train loss 0.3166523575782776, val loss 0.8989909887313843
iter 40, train loss 0.3160523474216461, val loss 0.9005023837089539
iter 50, train loss 0.3151623010635376, val loss 0.9045411944389343
iter 60, train loss 0.3145785927772522, val loss 0.9135254621505737
iter 70, train loss 0.3141881227493286, val loss 0.9117869734764099
iter 80, train loss 0.3134082555770874, val loss 0.9078558683395386
iter 90, train loss 0.31297582387924194, val loss 0.9085286855697632
best loss 0.8671164512634277
not here
quantized in 92.47238993644714 seconds
35680 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.codebook', 'self_attn.q_proj.quantizer.norms_1', 'self_attn.q_proj.quantizer.norms_0', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.codebook', 'self_attn.k_proj.quantizer.norms_1', 'self_attn.k_proj.quantizer.norms_0', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.codebook', 'self_attn.v_proj.quantizer.norms_1', 'self_attn.v_proj.quantizer.norms_0', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.codebook', 'self_attn.o_proj.quantizer.norms_1', 'self_attn.o_proj.quantizer.norms_0', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.codebook', 'mlp.gate_proj.quantizer.norms_1', 'mlp.gate_proj.quantizer.norms_0', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.codebook', 'mlp.up_proj.quantizer.norms_1', 'mlp.up_proj.quantizer.norms_0', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.codebook', 'mlp.down_proj.quantizer.norms_1', 'mlp.down_proj.quantizer.norms_0', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135936
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.00032902256020861387 val loss: 0.0018490440415916964
6652 MiB free out of 48676 MiB total
epoch 1 loss: 0.00029131961446182686 val loss: 0.0018619736001710407
6652 MiB free out of 48676 MiB total
epoch 2 loss: 0.00028232662816662923 val loss: 0.0018691884906729683
6652 MiB free out of 48676 MiB total
epoch 3 loss: 0.00027877048023583484 val loss: 0.0018720737425610423
6652 MiB free out of 48676 MiB total
epoch 4 loss: 0.000276776041573612 val loss: 0.0018737213322310708
6652 MiB free out of 48676 MiB total
35680 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
6652 MiB free out of 48676 MiB total
after cast to cpu
37850 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
4 self_attn.q_proj
Pruning ...
256
iter 0, train loss 133.33763122558594, val loss 152.07032775878906
iter 10, train loss 141.05667114257812, val loss 178.2758331298828
iter 20, train loss 137.44180297851562, val loss 173.5902099609375
iter 30, train loss 135.44485473632812, val loss 172.55979919433594
iter 40, train loss 134.80599975585938, val loss 171.57614135742188
iter 50, train loss 133.768798828125, val loss 170.90509033203125
iter 60, train loss 133.42562866210938, val loss 170.8372802734375
iter 70, train loss 133.05242919921875, val loss 170.16554260253906
iter 80, train loss 132.84115600585938, val loss 170.13681030273438
iter 90, train loss 132.6883087158203, val loss 170.2330322265625
best loss 152.07032775878906
not here
quantized in 34.20104217529297 seconds
36422 MiB free out of 48676 MiB total
4 self_attn.k_proj
Pruning ...
256
iter 0, train loss 147.755859375, val loss 164.74978637695312
iter 10, train loss 153.93310546875, val loss 191.1773681640625
iter 20, train loss 153.37948608398438, val loss 193.33273315429688
iter 30, train loss 149.72482299804688, val loss 190.51290893554688
iter 40, train loss 147.1808624267578, val loss 187.57675170898438
iter 50, train loss 146.73486328125, val loss 187.61691284179688
iter 60, train loss 146.38893127441406, val loss 187.16455078125
iter 70, train loss 145.75164794921875, val loss 186.85079956054688
iter 80, train loss 145.7080078125, val loss 186.9036102294922
iter 90, train loss 145.766845703125, val loss 186.8316650390625
best loss 164.4693145751953
not here
quantized in 32.68911790847778 seconds
36412 MiB free out of 48676 MiB total
4 self_attn.v_proj
Pruning ...
256
iter 0, train loss 34.009525299072266, val loss 43.961116790771484
iter 10, train loss 34.40637969970703, val loss 45.601505279541016
iter 20, train loss 34.17430877685547, val loss 45.250144958496094
iter 30, train loss 34.11066436767578, val loss 45.28349304199219
iter 40, train loss 34.045379638671875, val loss 45.12763595581055
iter 50, train loss 33.992034912109375, val loss 45.03428649902344
iter 60, train loss 34.00713348388672, val loss 44.98683166503906
iter 70, train loss 34.011444091796875, val loss 45.04399108886719
iter 80, train loss 33.92021942138672, val loss 45.027706146240234
iter 90, train loss 33.904266357421875, val loss 45.082801818847656
best loss 43.961116790771484
not here
quantized in 32.78774118423462 seconds
36402 MiB free out of 48676 MiB total
4 self_attn.o_proj
Pruning ...
256
iter 0, train loss 0.5835289359092712, val loss 1.8507522344589233
iter 10, train loss 0.5409789085388184, val loss 1.9379664659500122
iter 20, train loss 0.5308409929275513, val loss 1.8089239597320557
iter 30, train loss 0.5211032629013062, val loss 1.7166988849639893
iter 40, train loss 0.5152958035469055, val loss 1.7055983543395996
iter 50, train loss 0.5119827389717102, val loss 1.6791375875473022
iter 60, train loss 0.5118083953857422, val loss 1.767636775970459
iter 70, train loss 0.5102449655532837, val loss 1.7426667213439941
iter 80, train loss 0.5079379081726074, val loss 1.7017478942871094
iter 90, train loss 0.505753755569458, val loss 1.7457292079925537
best loss 1.6687438488006592
not here
quantized in 38.78201913833618 seconds
36370 MiB free out of 48676 MiB total
4 mlp.gate_proj
Pruning ...
256
iter 0, train loss 76.7990493774414, val loss 118.46583557128906
iter 10, train loss 79.38336181640625, val loss 124.43894958496094
iter 20, train loss 77.95264434814453, val loss 126.00360870361328
iter 30, train loss 78.06863403320312, val loss 126.3219223022461
iter 40, train loss 77.90484619140625, val loss 124.6121826171875
iter 50, train loss 77.85895538330078, val loss 125.18673706054688
iter 60, train loss 77.68077087402344, val loss 123.07466125488281
iter 70, train loss 77.64447021484375, val loss 123.4146728515625
iter 80, train loss 77.62113189697266, val loss 123.94861602783203
iter 90, train loss 77.63799285888672, val loss 123.44286346435547
best loss 117.26127624511719
not here
quantized in 104.29185199737549 seconds
36068 MiB free out of 48676 MiB total
4 mlp.up_proj
Pruning ...
256
iter 0, train loss 60.01136016845703, val loss 94.28404235839844
iter 10, train loss 59.99702072143555, val loss 94.84584045410156
iter 20, train loss 59.95761489868164, val loss 94.86058044433594
iter 30, train loss 60.082096099853516, val loss 94.90393829345703
iter 40, train loss 60.05313491821289, val loss 95.23338317871094
iter 50, train loss 60.06535339355469, val loss 94.7008056640625
iter 60, train loss 60.03101348876953, val loss 94.83367919921875
iter 70, train loss 60.10422134399414, val loss 94.54393768310547
iter 80, train loss 60.08510208129883, val loss 94.63365936279297
iter 90, train loss 60.08625030517578, val loss 95.0566177368164
best loss 94.09098815917969
not here
quantized in 83.47856521606445 seconds
35874 MiB free out of 48676 MiB total
4 mlp.down_proj
Pruning ...
256
iter 0, train loss 0.5723158121109009, val loss 1.1558858156204224
iter 10, train loss 0.5726552605628967, val loss 1.1776386499404907
iter 20, train loss 0.567501425743103, val loss 1.1841455698013306
iter 30, train loss 0.5685319900512695, val loss 1.1781649589538574
iter 40, train loss 0.5672526955604553, val loss 1.1849968433380127
iter 50, train loss 0.5733230710029602, val loss 1.199855923652649
iter 60, train loss 0.5744709968566895, val loss 1.1794404983520508
iter 70, train loss 0.5734520554542542, val loss 1.171306848526001
iter 80, train loss 0.5727112293243408, val loss 1.1790106296539307
iter 90, train loss 0.5727560520172119, val loss 1.1772352457046509
best loss 1.1558858156204224
not here
quantized in 92.08280396461487 seconds
35680 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.codebook', 'self_attn.q_proj.quantizer.norms_1', 'self_attn.q_proj.quantizer.norms_0', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.codebook', 'self_attn.k_proj.quantizer.norms_1', 'self_attn.k_proj.quantizer.norms_0', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.codebook', 'self_attn.v_proj.quantizer.norms_1', 'self_attn.v_proj.quantizer.norms_0', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.codebook', 'self_attn.o_proj.quantizer.norms_1', 'self_attn.o_proj.quantizer.norms_0', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.codebook', 'mlp.gate_proj.quantizer.norms_1', 'mlp.gate_proj.quantizer.norms_0', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.codebook', 'mlp.up_proj.quantizer.norms_1', 'mlp.up_proj.quantizer.norms_0', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.codebook', 'mlp.down_proj.quantizer.norms_1', 'mlp.down_proj.quantizer.norms_0', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135936
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.000581153914481547 val loss: 0.002134195383405313
6652 MiB free out of 48676 MiB total
epoch 1 loss: 0.0005197503160161432 val loss: 0.0022430503886425868
6652 MiB free out of 48676 MiB total
epoch 2 loss: 0.0005037717185132351 val loss: 0.0023272258404176682
6652 MiB free out of 48676 MiB total
epoch 3 loss: 0.0004966690357832704 val loss: 0.002380296486080624
6652 MiB free out of 48676 MiB total
epoch 4 loss: 0.000492580302989154 val loss: 0.0024135820131050423
6652 MiB free out of 48676 MiB total
35680 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
6652 MiB free out of 48676 MiB total
after cast to cpu
37850 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
5 self_attn.q_proj
Pruning ...
256
iter 0, train loss 153.46324157714844, val loss 160.37432861328125
iter 10, train loss 161.24264526367188, val loss 187.93252563476562
iter 20, train loss 158.41058349609375, val loss 186.9156036376953
iter 30, train loss 154.4310302734375, val loss 182.52069091796875
iter 40, train loss 152.16299438476562, val loss 180.8052978515625
iter 50, train loss 151.9833221435547, val loss 179.81021118164062
iter 60, train loss 151.72010803222656, val loss 180.06332397460938
iter 70, train loss 151.39817810058594, val loss 179.74034118652344
iter 80, train loss 151.16725158691406, val loss 179.56143188476562
iter 90, train loss 150.972900390625, val loss 179.7655792236328
best loss 160.37432861328125
not here
quantized in 37.49900770187378 seconds
36422 MiB free out of 48676 MiB total
5 self_attn.k_proj
Pruning ...
256
iter 0, train loss 179.03482055664062, val loss 180.50689697265625
iter 10, train loss 182.79989624023438, val loss 211.16505432128906
iter 20, train loss 185.88552856445312, val loss 219.2122039794922
iter 30, train loss 180.48265075683594, val loss 213.21902465820312
iter 40, train loss 176.57492065429688, val loss 208.51409912109375
iter 50, train loss 175.47654724121094, val loss 207.34117126464844
iter 60, train loss 175.57574462890625, val loss 208.70635986328125
iter 70, train loss 175.54800415039062, val loss 208.825439453125
iter 80, train loss 174.934814453125, val loss 208.53671264648438
iter 90, train loss 174.55654907226562, val loss 208.26449584960938
best loss 180.4515380859375
not here
quantized in 36.36538052558899 seconds
36412 MiB free out of 48676 MiB total
5 self_attn.v_proj
Pruning ...
256
iter 0, train loss 39.931793212890625, val loss 48.08978271484375
iter 10, train loss 40.32181167602539, val loss 49.109771728515625
iter 20, train loss 40.186683654785156, val loss 49.10860824584961
iter 30, train loss 40.12887954711914, val loss 48.9799919128418
iter 40, train loss 40.058536529541016, val loss 49.130897521972656
iter 50, train loss 40.016292572021484, val loss 49.05222702026367
iter 60, train loss 39.930721282958984, val loss 49.076168060302734
iter 70, train loss 39.984519958496094, val loss 49.067508697509766
iter 80, train loss 39.937652587890625, val loss 49.01552200317383
iter 90, train loss 39.921512603759766, val loss 49.041324615478516
best loss 48.08978271484375
not here
quantized in 35.00143885612488 seconds
36402 MiB free out of 48676 MiB total
5 self_attn.o_proj
Pruning ...
256
iter 0, train loss 0.8566852807998657, val loss 2.335998058319092
iter 10, train loss 0.8400629162788391, val loss 2.426255702972412
iter 20, train loss 0.8282638192176819, val loss 2.3332340717315674
iter 30, train loss 0.8277631998062134, val loss 2.326319456100464
iter 40, train loss 0.8231750726699829, val loss 2.3015966415405273
iter 50, train loss 0.818863570690155, val loss 2.2796902656555176
iter 60, train loss 0.8184770345687866, val loss 2.3343043327331543
iter 70, train loss 0.815752387046814, val loss 2.298818826675415
iter 80, train loss 0.8151368498802185, val loss 2.270108938217163
iter 90, train loss 0.8135753870010376, val loss 2.2887611389160156
best loss 2.2621166706085205
not here
quantized in 34.7828586101532 seconds
36370 MiB free out of 48676 MiB total
5 mlp.gate_proj
Pruning ...
256
iter 0, train loss 99.1263427734375, val loss 137.55628967285156
iter 10, train loss 102.81430053710938, val loss 146.72833251953125
iter 20, train loss 101.32318115234375, val loss 145.1271514892578
iter 30, train loss 100.88540649414062, val loss 143.96652221679688
iter 40, train loss 100.79472351074219, val loss 144.65481567382812
iter 50, train loss 100.4608154296875, val loss 143.98106384277344
iter 60, train loss 100.3023681640625, val loss 143.21197509765625
iter 70, train loss 100.18678283691406, val loss 144.310791015625
iter 80, train loss 100.083740234375, val loss 142.92703247070312
iter 90, train loss 100.02709197998047, val loss 143.68923950195312
best loss 136.59490966796875
not here
quantized in 95.03284072875977 seconds
36068 MiB free out of 48676 MiB total
5 mlp.up_proj
Pruning ...
256
iter 0, train loss 76.29163360595703, val loss 106.99543762207031
iter 10, train loss 76.57785034179688, val loss 106.71453094482422
iter 20, train loss 76.6956558227539, val loss 108.31007385253906
iter 30, train loss 76.7147216796875, val loss 106.3348388671875
iter 40, train loss 76.71891021728516, val loss 106.87151336669922
iter 50, train loss 76.678955078125, val loss 106.21034240722656
iter 60, train loss 76.60237121582031, val loss 106.96467590332031
iter 70, train loss 76.63790893554688, val loss 107.2442855834961
iter 80, train loss 76.61802673339844, val loss 106.74932861328125
iter 90, train loss 76.5906982421875, val loss 106.92626953125
best loss 105.71517944335938
not here
quantized in 93.8405396938324 seconds
35874 MiB free out of 48676 MiB total
5 mlp.down_proj
Pruning ...
256
iter 0, train loss 0.8799866437911987, val loss 1.2568893432617188
iter 10, train loss 0.8766818642616272, val loss 1.279181957244873
iter 20, train loss 0.8728784918785095, val loss 1.2655227184295654
iter 30, train loss 0.8673189878463745, val loss 1.262805461883545
iter 40, train loss 0.8642911911010742, val loss 1.2611396312713623
iter 50, train loss 0.8623696565628052, val loss 1.2703951597213745
iter 60, train loss 0.8602595329284668, val loss 1.2698683738708496
iter 70, train loss 0.8595083951950073, val loss 1.267166018486023
iter 80, train loss 0.8577362298965454, val loss 1.2744184732437134
iter 90, train loss 0.8562366366386414, val loss 1.2708183526992798
best loss 1.2431226968765259
not here
quantized in 102.23267078399658 seconds
35680 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.codebook', 'self_attn.q_proj.quantizer.norms_1', 'self_attn.q_proj.quantizer.norms_0', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.codebook', 'self_attn.k_proj.quantizer.norms_1', 'self_attn.k_proj.quantizer.norms_0', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.codebook', 'self_attn.v_proj.quantizer.norms_1', 'self_attn.v_proj.quantizer.norms_0', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.codebook', 'self_attn.o_proj.quantizer.norms_1', 'self_attn.o_proj.quantizer.norms_0', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.codebook', 'mlp.gate_proj.quantizer.norms_1', 'mlp.gate_proj.quantizer.norms_0', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.codebook', 'mlp.up_proj.quantizer.norms_1', 'mlp.up_proj.quantizer.norms_0', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.codebook', 'mlp.down_proj.quantizer.norms_1', 'mlp.down_proj.quantizer.norms_0', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135936
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0009240161734851426 val loss: 0.0026350816333433613
6652 MiB free out of 48676 MiB total
epoch 1 loss: 0.0008182689698514878 val loss: 0.0027570145030040294
6652 MiB free out of 48676 MiB total
epoch 2 loss: 0.0007885011932557973 val loss: 0.0028697110974462703
6652 MiB free out of 48676 MiB total
epoch 3 loss: 0.0007758964193271822 val loss: 0.002963677980005741
6652 MiB free out of 48676 MiB total
epoch 4 loss: 0.0007694184214415145 val loss: 0.0030358671356225386
6652 MiB free out of 48676 MiB total
35680 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
6652 MiB free out of 48676 MiB total
after cast to cpu
37850 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
6 self_attn.q_proj
Pruning ...
256
iter 0, train loss 229.78488159179688, val loss 218.84906005859375
iter 10, train loss 240.84271240234375, val loss 265.5453186035156
iter 20, train loss 236.72669982910156, val loss 261.37353515625
iter 30, train loss 232.66285705566406, val loss 256.94775390625
iter 40, train loss 228.64193725585938, val loss 252.17349243164062
iter 50, train loss 228.27728271484375, val loss 252.59861755371094
iter 60, train loss 227.2969970703125, val loss 252.77748107910156
iter 70, train loss 226.50424194335938, val loss 251.92701721191406
iter 80, train loss 226.23764038085938, val loss 251.777099609375
iter 90, train loss 225.894287109375, val loss 250.83270263671875
best loss 218.84906005859375
not here
quantized in 37.69879603385925 seconds
36422 MiB free out of 48676 MiB total
6 self_attn.k_proj
Pruning ...
256
iter 0, train loss 251.69735717773438, val loss 230.0082244873047
iter 10, train loss 258.2433166503906, val loss 283.41180419921875
iter 20, train loss 259.7012023925781, val loss 287.3556213378906
iter 30, train loss 253.13662719726562, val loss 281.021728515625
iter 40, train loss 248.32818603515625, val loss 274.4230041503906
iter 50, train loss 246.780517578125, val loss 274.2208251953125
iter 60, train loss 244.6534881591797, val loss 272.25115966796875
iter 70, train loss 243.3560791015625, val loss 270.75665283203125
iter 80, train loss 242.82244873046875, val loss 270.121826171875
iter 90, train loss 242.7262725830078, val loss 270.59234619140625
best loss 230.0082244873047
not here
quantized in 36.96556901931763 seconds
36412 MiB free out of 48676 MiB total
6 self_attn.v_proj
Pruning ...
256
iter 0, train loss 57.761940002441406, val loss 63.81672286987305
iter 10, train loss 58.30406951904297, val loss 66.19833374023438
iter 20, train loss 58.13779067993164, val loss 65.90365600585938
iter 30, train loss 57.956687927246094, val loss 65.83036804199219
iter 40, train loss 58.019432067871094, val loss 65.92283630371094
iter 50, train loss 57.94780731201172, val loss 65.77163696289062
iter 60, train loss 57.96971130371094, val loss 65.74420166015625
iter 70, train loss 57.85147476196289, val loss 65.74258422851562
iter 80, train loss 57.79417419433594, val loss 65.77910614013672
iter 90, train loss 57.78341293334961, val loss 65.77469635009766
best loss 63.81672286987305
not here
quantized in 35.082881927490234 seconds
36402 MiB free out of 48676 MiB total
6 self_attn.o_proj
Pruning ...
256
iter 0, train loss 1.382455825805664, val loss 2.637085437774658
iter 10, train loss 1.3665510416030884, val loss 2.719698905944824
iter 20, train loss 1.3850295543670654, val loss 3.073451280593872
iter 30, train loss 1.369629144668579, val loss 2.812056303024292
iter 40, train loss 1.3659964799880981, val loss 2.614603042602539
iter 50, train loss 1.3574306964874268, val loss 2.62355375289917
iter 60, train loss 1.3475072383880615, val loss 2.6155896186828613
iter 70, train loss 1.3424915075302124, val loss 2.5927233695983887
iter 80, train loss 1.342860221862793, val loss 2.638878345489502
iter 90, train loss 1.341863751411438, val loss 2.650479316711426
best loss 2.5650408267974854
not here
quantized in 34.8485541343689 seconds
36370 MiB free out of 48676 MiB total
6 mlp.gate_proj
Pruning ...
256
iter 0, train loss 126.4920883178711, val loss 149.7890167236328
iter 10, train loss 133.4888153076172, val loss 165.87435913085938
iter 20, train loss 130.00816345214844, val loss 163.0928955078125
iter 30, train loss 129.98275756835938, val loss 163.60861206054688
iter 40, train loss 129.4768524169922, val loss 160.0123748779297
iter 50, train loss 129.08409118652344, val loss 160.2550048828125
iter 60, train loss 128.9482421875, val loss 158.1275177001953
iter 70, train loss 128.70046997070312, val loss 160.34197998046875
iter 80, train loss 128.73228454589844, val loss 161.1497802734375
iter 90, train loss 128.63421630859375, val loss 161.92822265625
best loss 148.88836669921875
not here
quantized in 96.00145626068115 seconds
36068 MiB free out of 48676 MiB total
6 mlp.up_proj
Pruning ...
256
iter 0, train loss 92.9269790649414, val loss 112.01338195800781
iter 10, train loss 93.16218566894531, val loss 115.0403823852539
iter 20, train loss 93.22906494140625, val loss 115.99978637695312
iter 30, train loss 93.21162414550781, val loss 116.41639709472656
iter 40, train loss 93.11961364746094, val loss 116.58633422851562
iter 50, train loss 93.06643676757812, val loss 114.9886474609375
iter 60, train loss 93.09835815429688, val loss 116.33029174804688
iter 70, train loss 93.10340881347656, val loss 115.825439453125
iter 80, train loss 93.10321044921875, val loss 116.78923797607422
iter 90, train loss 93.07831573486328, val loss 116.85515594482422
best loss 112.01338195800781
not here
quantized in 93.49967002868652 seconds
35874 MiB free out of 48676 MiB total
6 mlp.down_proj
Pruning ...
256
iter 0, train loss 1.362700343132019, val loss 1.4016880989074707
iter 10, train loss 1.358730435371399, val loss 1.4085289239883423
iter 20, train loss 1.3499751091003418, val loss 1.4220466613769531
iter 30, train loss 1.3360543251037598, val loss 1.428034782409668
iter 40, train loss 1.3342630863189697, val loss 1.4263445138931274
iter 50, train loss 1.331841230392456, val loss 1.4253675937652588
iter 60, train loss 1.3282949924468994, val loss 1.4219263792037964
iter 70, train loss 1.3243099451065063, val loss 1.4068984985351562
iter 80, train loss 1.3219505548477173, val loss 1.4318103790283203
iter 90, train loss 1.3207558393478394, val loss 1.4145002365112305
best loss 1.3784905672073364
not here
quantized in 121.51398944854736 seconds
35680 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.codebook', 'self_attn.q_proj.quantizer.norms_1', 'self_attn.q_proj.quantizer.norms_0', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.codebook', 'self_attn.k_proj.quantizer.norms_1', 'self_attn.k_proj.quantizer.norms_0', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.codebook', 'self_attn.v_proj.quantizer.norms_1', 'self_attn.v_proj.quantizer.norms_0', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.codebook', 'self_attn.o_proj.quantizer.norms_1', 'self_attn.o_proj.quantizer.norms_0', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.codebook', 'mlp.gate_proj.quantizer.norms_1', 'mlp.gate_proj.quantizer.norms_0', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.codebook', 'mlp.up_proj.quantizer.norms_1', 'mlp.up_proj.quantizer.norms_0', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.codebook', 'mlp.down_proj.quantizer.norms_1', 'mlp.down_proj.quantizer.norms_0', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135936
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0014858077993267216 val loss: 0.003180077052093111
6652 MiB free out of 48676 MiB total
epoch 1 loss: 0.001275766645449039 val loss: 0.0032466705015394837
6652 MiB free out of 48676 MiB total
epoch 2 loss: 0.001230641032634594 val loss: 0.0032958914089249447
6652 MiB free out of 48676 MiB total
epoch 3 loss: 0.0012096297805328504 val loss: 0.0033554232359165326
6652 MiB free out of 48676 MiB total
epoch 4 loss: 0.0011973278515142738 val loss: 0.003409073964576237
6652 MiB free out of 48676 MiB total
35680 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
6652 MiB free out of 48676 MiB total
after cast to cpu
37850 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
7 self_attn.q_proj
Pruning ...
256
iter 0, train loss 250.90646362304688, val loss 233.38182067871094
iter 10, train loss 265.9183654785156, val loss 287.34393310546875
iter 20, train loss 260.2818603515625, val loss 280.9484558105469
iter 30, train loss 255.820068359375, val loss 276.2972106933594
iter 40, train loss 252.74887084960938, val loss 274.24322509765625
iter 50, train loss 251.75030517578125, val loss 273.67974853515625
iter 60, train loss 251.30996704101562, val loss 273.5245361328125
iter 70, train loss 251.00205993652344, val loss 272.52899169921875
iter 80, train loss 250.52252197265625, val loss 271.44891357421875
iter 90, train loss 249.88156127929688, val loss 271.1925048828125
best loss 233.38182067871094
not here
quantized in 33.97950839996338 seconds
36422 MiB free out of 48676 MiB total
7 self_attn.k_proj
Pruning ...
256
iter 0, train loss 265.13397216796875, val loss 237.4954376220703
iter 10, train loss 274.14666748046875, val loss 291.76947021484375
iter 20, train loss 276.2697448730469, val loss 295.7513732910156
iter 30, train loss 267.3375244140625, val loss 287.5936279296875
iter 40, train loss 260.5013427734375, val loss 280.5330810546875
iter 50, train loss 259.0472412109375, val loss 278.1100769042969
iter 60, train loss 258.8829650878906, val loss 279.1551818847656
iter 70, train loss 258.16680908203125, val loss 279.2158203125
iter 80, train loss 258.02655029296875, val loss 279.7167053222656
iter 90, train loss 257.50592041015625, val loss 279.2037658691406
best loss 237.4954376220703
not here
quantized in 33.01605176925659 seconds
36412 MiB free out of 48676 MiB total
7 self_attn.v_proj
Pruning ...
256
iter 0, train loss 64.58587646484375, val loss 69.40513610839844
iter 10, train loss 65.01032257080078, val loss 71.63057708740234
iter 20, train loss 64.81199645996094, val loss 71.5953598022461
iter 30, train loss 64.65328979492188, val loss 71.38146209716797
iter 40, train loss 64.73898315429688, val loss 71.65093231201172
iter 50, train loss 64.63123321533203, val loss 71.46829986572266
iter 60, train loss 64.5232925415039, val loss 71.36908721923828
iter 70, train loss 64.4849624633789, val loss 71.40546417236328
iter 80, train loss 64.51490783691406, val loss 71.37776947021484
iter 90, train loss 64.477294921875, val loss 71.50048828125
best loss 69.40513610839844
not here
quantized in 31.518908262252808 seconds
36402 MiB free out of 48676 MiB total
7 self_attn.o_proj
Pruning ...
256
iter 0, train loss 2.104841709136963, val loss 2.4174516201019287
iter 10, train loss 2.0572543144226074, val loss 2.448554754257202
iter 20, train loss 2.0435783863067627, val loss 2.4234511852264404
iter 30, train loss 2.0553855895996094, val loss 2.483780860900879
iter 40, train loss 2.043008327484131, val loss 2.457951068878174
iter 50, train loss 2.0331804752349854, val loss 2.4728541374206543
iter 60, train loss 2.0304527282714844, val loss 2.429049491882324
iter 70, train loss 2.0261669158935547, val loss 2.481387138366699
iter 80, train loss 2.024778127670288, val loss 2.47605037689209
iter 90, train loss 2.018451452255249, val loss 2.5195326805114746
best loss 2.380124807357788
not here
quantized in 31.148948907852173 seconds
36370 MiB free out of 48676 MiB total
7 mlp.gate_proj
Pruning ...
256
iter 0, train loss 146.49220275878906, val loss 182.60391235351562
iter 10, train loss 154.48582458496094, val loss 199.65245056152344
iter 20, train loss 150.58787536621094, val loss 193.63034057617188
iter 30, train loss 150.14747619628906, val loss 193.826416015625
iter 40, train loss 149.79385375976562, val loss 194.7210235595703
iter 50, train loss 149.56272888183594, val loss 192.97958374023438
iter 60, train loss 149.22842407226562, val loss 194.72438049316406
iter 70, train loss 148.7987060546875, val loss 195.77737426757812
iter 80, train loss 148.53814697265625, val loss 194.79029846191406
iter 90, train loss 148.35704040527344, val loss 195.53106689453125
best loss 179.28457641601562
not here
quantized in 85.49030637741089 seconds
36068 MiB free out of 48676 MiB total
7 mlp.up_proj
Pruning ...
256
iter 0, train loss 108.90827178955078, val loss 136.33447265625
iter 10, train loss 109.18975830078125, val loss 141.07366943359375
iter 20, train loss 109.27753448486328, val loss 144.45721435546875
iter 30, train loss 109.35273742675781, val loss 142.6994171142578
iter 40, train loss 109.34905242919922, val loss 141.78541564941406
iter 50, train loss 109.17897033691406, val loss 143.73789978027344
iter 60, train loss 109.0862045288086, val loss 144.2456817626953
iter 70, train loss 109.13431549072266, val loss 143.3968048095703
iter 80, train loss 109.06869506835938, val loss 144.39041137695312
iter 90, train loss 109.03680419921875, val loss 145.43128967285156
best loss 136.33447265625
not here
quantized in 84.02518939971924 seconds
35874 MiB free out of 48676 MiB total
7 mlp.down_proj
Pruning ...
256
iter 0, train loss 1.8372324705123901, val loss 1.9311248064041138
iter 10, train loss 1.8405438661575317, val loss 1.9292263984680176
iter 20, train loss 1.8296372890472412, val loss 1.9726672172546387
iter 30, train loss 1.8188414573669434, val loss 1.9382346868515015
iter 40, train loss 1.814439296722412, val loss 1.961613416671753
iter 50, train loss 1.8064372539520264, val loss 1.942683458328247
iter 60, train loss 1.8061163425445557, val loss 1.937837839126587
iter 70, train loss 1.8056788444519043, val loss 1.9506316184997559
iter 80, train loss 1.8034312725067139, val loss 1.9391436576843262
iter 90, train loss 1.8036739826202393, val loss 1.933772325515747
best loss 1.8960189819335938
not here
quantized in 91.76940417289734 seconds
35680 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.codebook', 'self_attn.q_proj.quantizer.norms_1', 'self_attn.q_proj.quantizer.norms_0', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.codebook', 'self_attn.k_proj.quantizer.norms_1', 'self_attn.k_proj.quantizer.norms_0', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.codebook', 'self_attn.v_proj.quantizer.norms_1', 'self_attn.v_proj.quantizer.norms_0', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.codebook', 'self_attn.o_proj.quantizer.norms_1', 'self_attn.o_proj.quantizer.norms_0', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.codebook', 'mlp.gate_proj.quantizer.norms_1', 'mlp.gate_proj.quantizer.norms_0', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.codebook', 'mlp.up_proj.quantizer.norms_1', 'mlp.up_proj.quantizer.norms_0', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.codebook', 'mlp.down_proj.quantizer.norms_1', 'mlp.down_proj.quantizer.norms_0', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135936
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0019523812952684239 val loss: 0.00418976042419672
6652 MiB free out of 48676 MiB total
epoch 1 loss: 0.0017644258714426542 val loss: 0.004245908901793882
6652 MiB free out of 48676 MiB total
epoch 2 loss: 0.0017158094997284934 val loss: 0.0043212033924646676
6652 MiB free out of 48676 MiB total
epoch 3 loss: 0.0016923137918638531 val loss: 0.004399320139782503
6652 MiB free out of 48676 MiB total
epoch 4 loss: 0.0016780237856437452 val loss: 0.004464402765734121
6652 MiB free out of 48676 MiB total
35680 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
6652 MiB free out of 48676 MiB total
after cast to cpu
37850 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
8 self_attn.q_proj
Pruning ...
256
iter 0, train loss 244.8104705810547, val loss 226.97372436523438
iter 10, train loss 258.3359375, val loss 274.07183837890625
iter 20, train loss 251.51248168945312, val loss 265.32708740234375
iter 30, train loss 248.13099670410156, val loss 262.3118896484375
iter 40, train loss 244.81948852539062, val loss 259.935302734375
iter 50, train loss 245.17294311523438, val loss 260.4068603515625
iter 60, train loss 244.58987426757812, val loss 260.4681396484375
iter 70, train loss 244.096435546875, val loss 260.8310546875
iter 80, train loss 243.49835205078125, val loss 260.5385437011719
iter 90, train loss 243.4800567626953, val loss 259.9593811035156
best loss 226.97372436523438
not here
quantized in 33.637633085250854 seconds
36422 MiB free out of 48676 MiB total
8 self_attn.k_proj
Pruning ...
256
iter 0, train loss 256.4966735839844, val loss 230.95968627929688
iter 10, train loss 265.0151672363281, val loss 277.06927490234375
iter 20, train loss 267.5671691894531, val loss 280.34698486328125
iter 30, train loss 262.59942626953125, val loss 276.4595947265625
iter 40, train loss 257.97137451171875, val loss 272.7522888183594
iter 50, train loss 256.3884582519531, val loss 271.60455322265625
iter 60, train loss 254.77503967285156, val loss 269.9315490722656
iter 70, train loss 253.5682830810547, val loss 269.4133605957031
iter 80, train loss 253.43063354492188, val loss 269.94708251953125
iter 90, train loss 252.09072875976562, val loss 268.6983337402344
best loss 230.95968627929688
not here
quantized in 32.93216609954834 seconds
36412 MiB free out of 48676 MiB total
8 self_attn.v_proj
Pruning ...
256
iter 0, train loss 67.15370178222656, val loss 70.67007446289062
iter 10, train loss 68.08200073242188, val loss 73.85545349121094
iter 20, train loss 67.58389282226562, val loss 73.29383850097656
iter 30, train loss 67.3016128540039, val loss 73.13317108154297
iter 40, train loss 67.2280502319336, val loss 72.9968032836914
iter 50, train loss 67.09687805175781, val loss 73.09559631347656
iter 60, train loss 67.05577850341797, val loss 73.07550048828125
iter 70, train loss 67.02845001220703, val loss 73.0062255859375
iter 80, train loss 67.03882598876953, val loss 73.04867553710938
iter 90, train loss 66.9620590209961, val loss 72.98114013671875
best loss 70.67007446289062
not here
quantized in 31.755839824676514 seconds
36402 MiB free out of 48676 MiB total
8 self_attn.o_proj
Pruning ...
256
iter 0, train loss 3.782618522644043, val loss 2.9455013275146484
iter 10, train loss 3.6003029346466064, val loss 3.0480775833129883
iter 20, train loss 3.5332465171813965, val loss 3.1225085258483887
iter 30, train loss 3.492356300354004, val loss 3.1099424362182617
iter 40, train loss 3.442828416824341, val loss 3.0408554077148438
iter 50, train loss 3.4285945892333984, val loss 3.0403077602386475
iter 60, train loss 3.41188645362854, val loss 3.131446361541748
iter 70, train loss 3.4012410640716553, val loss 3.0657100677490234
iter 80, train loss 3.3878209590911865, val loss 3.0571322441101074
iter 90, train loss 3.382519006729126, val loss 3.0104963779449463
best loss 2.9455013275146484
not here
quantized in 31.231600522994995 seconds
36370 MiB free out of 48676 MiB total
8 mlp.gate_proj
Pruning ...
256
iter 0, train loss 153.91738891601562, val loss 179.6628875732422
iter 10, train loss 161.692626953125, val loss 197.94593811035156
iter 20, train loss 157.45138549804688, val loss 193.69996643066406
iter 30, train loss 157.632080078125, val loss 192.22622680664062
iter 40, train loss 157.17037963867188, val loss 195.53558349609375
iter 50, train loss 157.03057861328125, val loss 193.5544891357422
iter 60, train loss 156.40699768066406, val loss 193.77493286132812
iter 70, train loss 156.07229614257812, val loss 192.725341796875
iter 80, train loss 156.01251220703125, val loss 192.6681671142578
iter 90, train loss 155.71392822265625, val loss 191.82655334472656
best loss 179.6628875732422
not here
quantized in 92.9728615283966 seconds
36068 MiB free out of 48676 MiB total
8 mlp.up_proj
Pruning ...
256
iter 0, train loss 121.6241455078125, val loss 148.7410125732422
iter 10, train loss 122.36944580078125, val loss 149.70973205566406
iter 20, train loss 122.56085205078125, val loss 152.2376708984375
iter 30, train loss 122.38329315185547, val loss 149.2659454345703
iter 40, train loss 122.29313659667969, val loss 152.19522094726562
iter 50, train loss 122.3034439086914, val loss 150.5614013671875
iter 60, train loss 122.3070297241211, val loss 148.88330078125
iter 70, train loss 122.2189712524414, val loss 150.45774841308594
iter 80, train loss 122.29256439208984, val loss 152.01544189453125
iter 90, train loss 122.19979858398438, val loss 151.35586547851562
best loss 148.65599060058594
not here
quantized in 94.70095300674438 seconds
35874 MiB free out of 48676 MiB total
8 mlp.down_proj
Pruning ...
256
iter 0, train loss 2.311636447906494, val loss 2.134742259979248
iter 10, train loss 2.3111541271209717, val loss 2.1590380668640137
iter 20, train loss 2.2995283603668213, val loss 2.2053515911102295
iter 30, train loss 2.2925095558166504, val loss 2.214031934738159
iter 40, train loss 2.2876482009887695, val loss 2.267084836959839
iter 50, train loss 2.2872514724731445, val loss 2.246511459350586
iter 60, train loss 2.286736488342285, val loss 2.265488624572754
iter 70, train loss 2.2871816158294678, val loss 2.2494149208068848
iter 80, train loss 2.284968852996826, val loss 2.202615261077881
iter 90, train loss 2.282586097717285, val loss 2.206045150756836
best loss 2.134742259979248
not here
quantized in 101.74697089195251 seconds
35680 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.codebook', 'self_attn.q_proj.quantizer.norms_1', 'self_attn.q_proj.quantizer.norms_0', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.codebook', 'self_attn.k_proj.quantizer.norms_1', 'self_attn.k_proj.quantizer.norms_0', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.codebook', 'self_attn.v_proj.quantizer.norms_1', 'self_attn.v_proj.quantizer.norms_0', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.codebook', 'self_attn.o_proj.quantizer.norms_1', 'self_attn.o_proj.quantizer.norms_0', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.codebook', 'mlp.gate_proj.quantizer.norms_1', 'mlp.gate_proj.quantizer.norms_0', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.codebook', 'mlp.up_proj.quantizer.norms_1', 'mlp.up_proj.quantizer.norms_0', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.codebook', 'mlp.down_proj.quantizer.norms_1', 'mlp.down_proj.quantizer.norms_0', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135936
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0025063082630367717 val loss: 0.004214699089061469
6652 MiB free out of 48676 MiB total
epoch 1 loss: 0.002310418492925237 val loss: 0.004268125689122826
6652 MiB free out of 48676 MiB total
epoch 2 loss: 0.0022553953931492288 val loss: 0.004316863982239738
6652 MiB free out of 48676 MiB total
epoch 3 loss: 0.002227845470770262 val loss: 0.004371042683487758
6652 MiB free out of 48676 MiB total
epoch 4 loss: 0.0022104544168541906 val loss: 0.004420481185661629
6652 MiB free out of 48676 MiB total
35680 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
6652 MiB free out of 48676 MiB total
after cast to cpu
37850 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
9 self_attn.q_proj
Pruning ...
256
iter 0, train loss 256.0075988769531, val loss 235.00363159179688
iter 10, train loss 271.3702392578125, val loss 281.15374755859375
iter 20, train loss 261.2416687011719, val loss 271.4162292480469
iter 30, train loss 256.0019226074219, val loss 265.7174377441406
iter 40, train loss 255.69947814941406, val loss 265.4375
iter 50, train loss 255.302978515625, val loss 266.2337951660156
iter 60, train loss 255.16119384765625, val loss 266.341064453125
iter 70, train loss 254.41429138183594, val loss 265.9797058105469
iter 80, train loss 253.6753387451172, val loss 265.080810546875
iter 90, train loss 253.3054962158203, val loss 264.75445556640625
best loss 235.00363159179688
not here
quantized in 37.710904359817505 seconds
36422 MiB free out of 48676 MiB total
9 self_attn.k_proj
Pruning ...
256
iter 0, train loss 279.4910888671875, val loss 249.47970581054688
iter 10, train loss 293.95416259765625, val loss 301.148193359375
iter 20, train loss 293.38519287109375, val loss 304.98162841796875
iter 30, train loss 282.3743896484375, val loss 293.25225830078125
iter 40, train loss 278.1182861328125, val loss 289.43695068359375
iter 50, train loss 277.6097106933594, val loss 288.7110595703125
iter 60, train loss 277.33837890625, val loss 288.3563537597656
iter 70, train loss 276.7044982910156, val loss 288.2261962890625
iter 80, train loss 276.74969482421875, val loss 288.1168518066406
iter 90, train loss 276.11859130859375, val loss 287.6966552734375
best loss 249.47970581054688
not here
quantized in 36.59506106376648 seconds
36412 MiB free out of 48676 MiB total
9 self_attn.v_proj
Pruning ...
256
iter 0, train loss 73.58360290527344, val loss 75.94666290283203
iter 10, train loss 74.13782501220703, val loss 78.48873901367188
iter 20, train loss 73.75320434570312, val loss 78.42240905761719
iter 30, train loss 73.42264556884766, val loss 78.1290283203125
iter 40, train loss 73.50916290283203, val loss 78.3486328125
iter 50, train loss 73.4744644165039, val loss 78.20335388183594
iter 60, train loss 73.4255142211914, val loss 78.27386474609375
iter 70, train loss 73.39336395263672, val loss 78.22962188720703
iter 80, train loss 73.32264709472656, val loss 78.0746078491211
iter 90, train loss 73.3121109008789, val loss 78.0892562866211
best loss 75.94666290283203
not here
quantized in 35.217140674591064 seconds
36402 MiB free out of 48676 MiB total
9 self_attn.o_proj
Pruning ...
256
iter 0, train loss 4.941402912139893, val loss 2.807152032852173
iter 10, train loss 4.866880893707275, val loss 2.857940673828125
iter 20, train loss 4.798925399780273, val loss 2.9196910858154297
iter 30, train loss 4.7490949630737305, val loss 2.887502670288086
iter 40, train loss 4.721203804016113, val loss 2.893372058868408
iter 50, train loss 4.694351673126221, val loss 2.8532660007476807
iter 60, train loss 4.682990074157715, val loss 2.902791738510132
iter 70, train loss 4.676121234893799, val loss 2.910952091217041
iter 80, train loss 4.66270637512207, val loss 2.9297351837158203
iter 90, train loss 4.670803070068359, val loss 2.8801417350769043
best loss 2.794086456298828
not here
quantized in 34.649308919906616 seconds
36370 MiB free out of 48676 MiB total
9 mlp.gate_proj
Pruning ...
256
iter 0, train loss 163.05648803710938, val loss 191.52175903320312
iter 10, train loss 171.72352600097656, val loss 205.0240478515625
iter 20, train loss 166.85601806640625, val loss 200.3968963623047
iter 30, train loss 167.39752197265625, val loss 205.52279663085938
iter 40, train loss 166.4715576171875, val loss 203.2726593017578
iter 50, train loss 166.18356323242188, val loss 203.00173950195312
iter 60, train loss 165.894287109375, val loss 204.74615478515625
iter 70, train loss 165.7578887939453, val loss 205.81153869628906
iter 80, train loss 165.42691040039062, val loss 206.10337829589844
iter 90, train loss 165.43685913085938, val loss 204.66897583007812
best loss 190.33645629882812
not here
quantized in 96.332359790802 seconds
36068 MiB free out of 48676 MiB total
9 mlp.up_proj
Pruning ...
256
iter 0, train loss 133.1640625, val loss 161.84095764160156
iter 10, train loss 133.9427032470703, val loss 160.68875122070312
iter 20, train loss 134.16275024414062, val loss 163.23318481445312
iter 30, train loss 134.09603881835938, val loss 167.0386962890625
iter 40, train loss 134.10598754882812, val loss 166.23403930664062
iter 50, train loss 134.169189453125, val loss 165.5665740966797
iter 60, train loss 134.00865173339844, val loss 163.30528259277344
iter 70, train loss 134.0348663330078, val loss 161.482177734375
iter 80, train loss 133.97010803222656, val loss 160.32814025878906
iter 90, train loss 133.86468505859375, val loss 159.89712524414062
best loss 159.5511474609375
not here
quantized in 94.24166202545166 seconds
35874 MiB free out of 48676 MiB total
9 mlp.down_proj
Pruning ...
256
iter 0, train loss 2.7744998931884766, val loss 2.7452392578125
iter 10, train loss 2.7601158618927, val loss 2.79453182220459
iter 20, train loss 2.760451555252075, val loss 2.793694019317627
iter 30, train loss 2.749553680419922, val loss 2.906716823577881
iter 40, train loss 2.7384676933288574, val loss 2.8103652000427246
iter 50, train loss 2.734529972076416, val loss 2.7776529788970947
iter 60, train loss 2.7333102226257324, val loss 2.8006772994995117
iter 70, train loss 2.7268199920654297, val loss 2.8322136402130127
iter 80, train loss 2.724327325820923, val loss 2.840543270111084
iter 90, train loss 2.724813938140869, val loss 2.819218635559082
best loss 2.7452392578125
not here
quantized in 102.06124877929688 seconds
35680 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.codebook', 'self_attn.q_proj.quantizer.norms_1', 'self_attn.q_proj.quantizer.norms_0', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.codebook', 'self_attn.k_proj.quantizer.norms_1', 'self_attn.k_proj.quantizer.norms_0', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.codebook', 'self_attn.v_proj.quantizer.norms_1', 'self_attn.v_proj.quantizer.norms_0', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.codebook', 'self_attn.o_proj.quantizer.norms_1', 'self_attn.o_proj.quantizer.norms_0', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.codebook', 'mlp.gate_proj.quantizer.norms_1', 'mlp.gate_proj.quantizer.norms_0', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.codebook', 'mlp.up_proj.quantizer.norms_1', 'mlp.up_proj.quantizer.norms_0', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.codebook', 'mlp.down_proj.quantizer.norms_1', 'mlp.down_proj.quantizer.norms_0', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135936
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.003107763428488397 val loss: 0.006142633210401982
6652 MiB free out of 48676 MiB total
epoch 1 loss: 0.0028622718127735425 val loss: 0.006225095537956804
6652 MiB free out of 48676 MiB total
epoch 2 loss: 0.0027953308781434316 val loss: 0.00631852651713416
6652 MiB free out of 48676 MiB total
epoch 3 loss: 0.002760648623734596 val loss: 0.006418789766030386
6652 MiB free out of 48676 MiB total
epoch 4 loss: 0.0027383939559513237 val loss: 0.006506807869300246
6652 MiB free out of 48676 MiB total
35680 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
6652 MiB free out of 48676 MiB total
after cast to cpu
37850 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
10 self_attn.q_proj
Pruning ...
256
iter 0, train loss 261.2856750488281, val loss 233.446044921875
iter 10, train loss 275.049072265625, val loss 279.9132995605469
iter 20, train loss 265.4995422363281, val loss 270.4042663574219
iter 30, train loss 262.2370300292969, val loss 267.74835205078125
iter 40, train loss 261.27813720703125, val loss 266.4848937988281
iter 50, train loss 261.23956298828125, val loss 267.34747314453125
iter 60, train loss 260.4299011230469, val loss 266.92742919921875
iter 70, train loss 260.4405517578125, val loss 267.0345153808594
iter 80, train loss 260.62158203125, val loss 267.028564453125
iter 90, train loss 260.5061340332031, val loss 266.72760009765625
best loss 233.446044921875
not here
quantized in 37.8090033531189 seconds
36422 MiB free out of 48676 MiB total
10 self_attn.k_proj
Pruning ...
256
iter 0, train loss 290.1566467285156, val loss 253.46267700195312
iter 10, train loss 303.97998046875, val loss 306.6408386230469
iter 20, train loss 304.64361572265625, val loss 308.8728332519531
iter 30, train loss 296.9617919921875, val loss 302.1631774902344
iter 40, train loss 292.197509765625, val loss 297.0738220214844
iter 50, train loss 291.6220703125, val loss 297.3624267578125
iter 60, train loss 290.9200439453125, val loss 296.983154296875
iter 70, train loss 290.932861328125, val loss 296.9432067871094
iter 80, train loss 289.7119140625, val loss 296.5187683105469
iter 90, train loss 289.22760009765625, val loss 295.39239501953125
best loss 253.46267700195312
not here
quantized in 36.340617179870605 seconds
36412 MiB free out of 48676 MiB total
10 self_attn.v_proj
Pruning ...
256
iter 0, train loss 73.8278579711914, val loss 75.05072784423828
iter 10, train loss 74.35394287109375, val loss 77.24491882324219
iter 20, train loss 74.45455169677734, val loss 77.5618667602539
iter 30, train loss 74.19538879394531, val loss 77.35731506347656
iter 40, train loss 74.15509033203125, val loss 77.38109588623047
iter 50, train loss 74.00169372558594, val loss 77.30200958251953
iter 60, train loss 73.94477844238281, val loss 77.33736419677734
iter 70, train loss 73.9956283569336, val loss 77.36805725097656
iter 80, train loss 74.02625274658203, val loss 77.29971313476562
iter 90, train loss 73.98553466796875, val loss 77.31502532958984
best loss 75.05072784423828
not here
quantized in 35.16147255897522 seconds
36402 MiB free out of 48676 MiB total
10 self_attn.o_proj
Pruning ...
256
iter 0, train loss 7.2818169593811035, val loss 2.745495319366455
iter 10, train loss 6.983760833740234, val loss 2.676753044128418
iter 20, train loss 6.840572357177734, val loss 2.6947343349456787
iter 30, train loss 6.739986896514893, val loss 2.67417049407959
iter 40, train loss 6.677224159240723, val loss 2.6593408584594727
iter 50, train loss 6.6230010986328125, val loss 2.6236348152160645
iter 60, train loss 6.5753679275512695, val loss 2.576411247253418
iter 70, train loss 6.539120197296143, val loss 2.643784761428833
iter 80, train loss 6.512541770935059, val loss 2.6182169914245605
iter 90, train loss 6.500521659851074, val loss 2.6324901580810547
best loss 2.57300066947937
not here
quantized in 34.420554876327515 seconds
36370 MiB free out of 48676 MiB total
10 mlp.gate_proj
Pruning ...
256
iter 0, train loss 170.4746856689453, val loss 188.0628204345703
iter 10, train loss 180.0924835205078, val loss 211.16806030273438
iter 20, train loss 176.28201293945312, val loss 204.1543426513672
iter 30, train loss 175.7157745361328, val loss 207.9130859375
iter 40, train loss 175.5021209716797, val loss 207.35443115234375
iter 50, train loss 174.98178100585938, val loss 204.9254913330078
iter 60, train loss 174.71273803710938, val loss 206.932861328125
iter 70, train loss 174.4010009765625, val loss 204.975341796875
iter 80, train loss 174.36239624023438, val loss 206.4355010986328
iter 90, train loss 174.256103515625, val loss 207.49632263183594
best loss 188.0628204345703
not here
quantized in 95.40126729011536 seconds
36068 MiB free out of 48676 MiB total
10 mlp.up_proj
Pruning ...
256
iter 0, train loss 143.0230255126953, val loss 164.5465850830078
iter 10, train loss 144.00970458984375, val loss 167.69876098632812
iter 20, train loss 144.0645751953125, val loss 170.22410583496094
iter 30, train loss 143.9326629638672, val loss 170.11752319335938
iter 40, train loss 144.00160217285156, val loss 172.65151977539062
iter 50, train loss 143.8769989013672, val loss 173.4508514404297
iter 60, train loss 143.7705841064453, val loss 171.68727111816406
iter 70, train loss 143.7236328125, val loss 170.2264404296875
iter 80, train loss 143.697021484375, val loss 170.76951599121094
iter 90, train loss 143.65737915039062, val loss 170.4392852783203
best loss 163.98455810546875
not here
quantized in 94.70417761802673 seconds
35874 MiB free out of 48676 MiB total
10 mlp.down_proj
Pruning ...
256
iter 0, train loss 3.2250285148620605, val loss 3.7386069297790527
iter 10, train loss 3.2287981510162354, val loss 3.8092262744903564
iter 20, train loss 3.210524082183838, val loss 3.859032154083252
iter 30, train loss 3.1865110397338867, val loss 3.9201176166534424
iter 40, train loss 3.1702704429626465, val loss 3.9334750175476074
iter 50, train loss 3.1711854934692383, val loss 3.889707088470459
iter 60, train loss 3.160550117492676, val loss 3.9039273262023926
iter 70, train loss 3.1536142826080322, val loss 3.909651041030884
iter 80, train loss 3.152580976486206, val loss 3.8820714950561523
iter 90, train loss 3.1525585651397705, val loss 3.8832216262817383
best loss 3.7386069297790527
not here
quantized in 101.96720576286316 seconds
35680 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.codebook', 'self_attn.q_proj.quantizer.norms_1', 'self_attn.q_proj.quantizer.norms_0', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.codebook', 'self_attn.k_proj.quantizer.norms_1', 'self_attn.k_proj.quantizer.norms_0', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.codebook', 'self_attn.v_proj.quantizer.norms_1', 'self_attn.v_proj.quantizer.norms_0', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.codebook', 'self_attn.o_proj.quantizer.norms_1', 'self_attn.o_proj.quantizer.norms_0', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.codebook', 'mlp.gate_proj.quantizer.norms_1', 'mlp.gate_proj.quantizer.norms_0', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.codebook', 'mlp.up_proj.quantizer.norms_1', 'mlp.up_proj.quantizer.norms_0', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.codebook', 'mlp.down_proj.quantizer.norms_1', 'mlp.down_proj.quantizer.norms_0', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135936
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0036548929838318145 val loss: 0.007289926434168592
6652 MiB free out of 48676 MiB total
epoch 1 loss: 0.0034226031766593223 val loss: 0.007369297032710165
6652 MiB free out of 48676 MiB total
epoch 2 loss: 0.003347361473061028 val loss: 0.007452820718754083
6652 MiB free out of 48676 MiB total
epoch 3 loss: 0.0033060780187952332 val loss: 0.0075502397667150944
6652 MiB free out of 48676 MiB total
epoch 4 loss: 0.003278568548921612 val loss: 0.007652432075701654
6652 MiB free out of 48676 MiB total
35680 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
6652 MiB free out of 48676 MiB total
after cast to cpu
37850 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
11 self_attn.q_proj
Pruning ...
256
iter 0, train loss 296.6717224121094, val loss 257.89678955078125
iter 10, train loss 317.5212707519531, val loss 317.3265075683594
iter 20, train loss 307.66162109375, val loss 307.5273132324219
iter 30, train loss 304.6424560546875, val loss 305.17498779296875
iter 40, train loss 301.64215087890625, val loss 303.28375244140625
iter 50, train loss 300.21368408203125, val loss 303.1326599121094
iter 60, train loss 300.9477233886719, val loss 303.3076171875
iter 70, train loss 299.9128112792969, val loss 301.93646240234375
iter 80, train loss 299.5362548828125, val loss 301.7245788574219
iter 90, train loss 299.8033142089844, val loss 301.6547546386719
best loss 257.89678955078125
not here
quantized in 37.74133253097534 seconds
36422 MiB free out of 48676 MiB total
11 self_attn.k_proj
Pruning ...
256
iter 0, train loss 303.7160339355469, val loss 258.1589660644531
iter 10, train loss 320.4696350097656, val loss 314.81201171875
iter 20, train loss 318.31787109375, val loss 315.5943908691406
iter 30, train loss 314.297607421875, val loss 312.45233154296875
iter 40, train loss 310.08544921875, val loss 307.908447265625
iter 50, train loss 308.55535888671875, val loss 307.12164306640625
iter 60, train loss 307.32049560546875, val loss 305.8356628417969
iter 70, train loss 306.70196533203125, val loss 305.6820068359375
iter 80, train loss 306.3957824707031, val loss 305.98004150390625
iter 90, train loss 306.3401794433594, val loss 306.3485107421875
best loss 258.1589660644531
not here
quantized in 36.531739473342896 seconds
36412 MiB free out of 48676 MiB total
11 self_attn.v_proj
Pruning ...
256
iter 0, train loss 99.67786407470703, val loss 99.38835906982422
iter 10, train loss 100.26751708984375, val loss 102.27900695800781
iter 20, train loss 100.4348373413086, val loss 102.40643310546875
iter 30, train loss 100.08270263671875, val loss 102.33992004394531
iter 40, train loss 99.9336166381836, val loss 102.09103393554688
iter 50, train loss 99.93456268310547, val loss 102.21607971191406
iter 60, train loss 99.99060821533203, val loss 102.29000091552734
iter 70, train loss 99.89651489257812, val loss 102.24610900878906
iter 80, train loss 99.8891372680664, val loss 102.20067596435547
iter 90, train loss 99.81149291992188, val loss 101.94493865966797
best loss 99.38835906982422
not here
quantized in 35.18622851371765 seconds
36402 MiB free out of 48676 MiB total
11 self_attn.o_proj
Pruning ...
256
iter 0, train loss 7.835483551025391, val loss 4.225260257720947
iter 10, train loss 7.779422283172607, val loss 4.4578399658203125
iter 20, train loss 7.662210464477539, val loss 4.477006912231445
iter 30, train loss 7.7248029708862305, val loss 4.344749450683594
iter 40, train loss 7.660335540771484, val loss 4.440359592437744
iter 50, train loss 7.608492374420166, val loss 4.575526237487793
iter 60, train loss 7.59703254699707, val loss 4.572553634643555
iter 70, train loss 7.56355619430542, val loss 4.587194919586182
iter 80, train loss 7.543097019195557, val loss 4.644455909729004
iter 90, train loss 7.535989761352539, val loss 4.5634613037109375
best loss 4.154173374176025
not here
quantized in 34.464959383010864 seconds
36370 MiB free out of 48676 MiB total
11 mlp.gate_proj
Pruning ...
256
iter 0, train loss 181.1951904296875, val loss 195.38926696777344
iter 10, train loss 190.84097290039062, val loss 215.44546508789062
iter 20, train loss 187.3729248046875, val loss 211.97645568847656
iter 30, train loss 186.26412963867188, val loss 210.1103515625
iter 40, train loss 185.07342529296875, val loss 207.37063598632812
iter 50, train loss 184.5408477783203, val loss 210.0795135498047
iter 60, train loss 184.13217163085938, val loss 213.00987243652344
iter 70, train loss 183.96990966796875, val loss 208.38720703125
iter 80, train loss 184.1503143310547, val loss 209.17233276367188
iter 90, train loss 184.1610565185547, val loss 208.1809539794922
best loss 192.87823486328125
not here
quantized in 95.95684027671814 seconds
36068 MiB free out of 48676 MiB total
11 mlp.up_proj
Pruning ...
256
iter 0, train loss 155.89794921875, val loss 171.35153198242188
iter 10, train loss 156.8546600341797, val loss 178.46221923828125
iter 20, train loss 156.9713897705078, val loss 178.76187133789062
iter 30, train loss 156.89736938476562, val loss 182.13479614257812
iter 40, train loss 156.87884521484375, val loss 183.24658203125
iter 50, train loss 156.7098388671875, val loss 184.13897705078125
iter 60, train loss 156.80128479003906, val loss 181.55496215820312
iter 70, train loss 156.6604461669922, val loss 182.5595245361328
iter 80, train loss 156.53469848632812, val loss 182.93104553222656
iter 90, train loss 156.6180419921875, val loss 181.82008361816406
best loss 171.35153198242188
not here
quantized in 94.47082924842834 seconds
35874 MiB free out of 48676 MiB total
11 mlp.down_proj
Pruning ...
256
iter 0, train loss 3.4942100048065186, val loss 2.7537436485290527
iter 10, train loss 3.4954562187194824, val loss 2.867436408996582
iter 20, train loss 3.4659781455993652, val loss 2.8853330612182617
iter 30, train loss 3.455170154571533, val loss 2.8689076900482178
iter 40, train loss 3.4398248195648193, val loss 2.8728256225585938
iter 50, train loss 3.4326086044311523, val loss 2.832252025604248
iter 60, train loss 3.4278955459594727, val loss 2.8012375831604004
iter 70, train loss 3.42252516746521, val loss 2.828115463256836
iter 80, train loss 3.4214377403259277, val loss 2.8360700607299805
iter 90, train loss 3.4215874671936035, val loss 2.884159564971924
best loss 2.7537436485290527
not here
quantized in 102.36445665359497 seconds
35680 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.codebook', 'self_attn.q_proj.quantizer.norms_1', 'self_attn.q_proj.quantizer.norms_0', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.codebook', 'self_attn.k_proj.quantizer.norms_1', 'self_attn.k_proj.quantizer.norms_0', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.codebook', 'self_attn.v_proj.quantizer.norms_1', 'self_attn.v_proj.quantizer.norms_0', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.codebook', 'self_attn.o_proj.quantizer.norms_1', 'self_attn.o_proj.quantizer.norms_0', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.codebook', 'mlp.gate_proj.quantizer.norms_1', 'mlp.gate_proj.quantizer.norms_0', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.codebook', 'mlp.up_proj.quantizer.norms_1', 'mlp.up_proj.quantizer.norms_0', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.codebook', 'mlp.down_proj.quantizer.norms_1', 'mlp.down_proj.quantizer.norms_0', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135936
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.004103167866560398 val loss: 0.006283947732299566
6652 MiB free out of 48676 MiB total
epoch 1 loss: 0.003836789768683957 val loss: 0.0063339038752019405
6652 MiB free out of 48676 MiB total
epoch 2 loss: 0.0037510701422434067 val loss: 0.006342073203995824
6652 MiB free out of 48676 MiB total
epoch 3 loss: 0.0037047933055873727 val loss: 0.0063751552370376885
6652 MiB free out of 48676 MiB total
epoch 4 loss: 0.0036731709969899384 val loss: 0.006417404016247019
6652 MiB free out of 48676 MiB total
35680 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
6652 MiB free out of 48676 MiB total
after cast to cpu
37850 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
12 self_attn.q_proj
Pruning ...
256
iter 0, train loss 305.1787109375, val loss 271.43359375
iter 10, train loss 323.8351135253906, val loss 324.32012939453125
iter 20, train loss 312.70281982421875, val loss 313.50836181640625
iter 30, train loss 308.29962158203125, val loss 310.2081604003906
iter 40, train loss 306.3656921386719, val loss 308.55255126953125
iter 50, train loss 306.09808349609375, val loss 308.60235595703125
iter 60, train loss 305.39263916015625, val loss 308.5251770019531
iter 70, train loss 304.2935791015625, val loss 307.6838073730469
iter 80, train loss 303.6746520996094, val loss 307.3594665527344
iter 90, train loss 303.6864929199219, val loss 306.864501953125
best loss 271.43359375
not here
quantized in 37.67444157600403 seconds
36422 MiB free out of 48676 MiB total
12 self_attn.k_proj
Pruning ...
256
iter 0, train loss 337.1496276855469, val loss 289.9884033203125
iter 10, train loss 359.925537109375, val loss 355.3728332519531
iter 20, train loss 354.70294189453125, val loss 353.75830078125
iter 30, train loss 347.64385986328125, val loss 348.1882019042969
iter 40, train loss 344.2506103515625, val loss 343.74127197265625
iter 50, train loss 341.802978515625, val loss 343.1617736816406
iter 60, train loss 339.9818115234375, val loss 342.238525390625
iter 70, train loss 338.91094970703125, val loss 340.010498046875
iter 80, train loss 338.0245666503906, val loss 339.3076171875
iter 90, train loss 337.3255920410156, val loss 338.9134521484375
best loss 289.9884033203125
not here
quantized in 36.73109459877014 seconds
36412 MiB free out of 48676 MiB total
12 self_attn.v_proj
Pruning ...
256
iter 0, train loss 97.3477554321289, val loss 98.14276123046875
iter 10, train loss 97.8895034790039, val loss 100.64361572265625
iter 20, train loss 97.77547454833984, val loss 100.9388198852539
iter 30, train loss 97.58304595947266, val loss 100.696533203125
iter 40, train loss 97.35586547851562, val loss 100.70206451416016
iter 50, train loss 97.38766479492188, val loss 100.61395263671875
iter 60, train loss 97.23369598388672, val loss 100.75225830078125
iter 70, train loss 97.37275695800781, val loss 100.5628662109375
iter 80, train loss 97.38124084472656, val loss 100.51573181152344
iter 90, train loss 97.35440826416016, val loss 100.63768768310547
best loss 98.14276123046875
not here
quantized in 35.288376331329346 seconds
36402 MiB free out of 48676 MiB total
12 self_attn.o_proj
Pruning ...
256
iter 0, train loss 8.684466361999512, val loss 4.190830230712891
iter 10, train loss 8.568096160888672, val loss 4.265557289123535
iter 20, train loss 8.51830768585205, val loss 4.272851467132568
iter 30, train loss 8.452147483825684, val loss 4.123802661895752
iter 40, train loss 8.403403282165527, val loss 4.169926643371582
iter 50, train loss 8.408679962158203, val loss 4.201192855834961
iter 60, train loss 8.398371696472168, val loss 4.212637901306152
iter 70, train loss 8.373699188232422, val loss 4.178732872009277
iter 80, train loss 8.363502502441406, val loss 4.009421348571777
iter 90, train loss 8.3330659866333, val loss 4.0121750831604
best loss 3.9719223976135254
not here
quantized in 34.44240689277649 seconds
36402 MiB free out of 48676 MiB total
12 mlp.gate_proj
Pruning ...
256
iter 0, train loss 190.95553588867188, val loss 206.87420654296875
iter 10, train loss 200.7411346435547, val loss 226.9898681640625
iter 20, train loss 196.71530151367188, val loss 221.2928466796875
iter 30, train loss 196.3170166015625, val loss 222.60140991210938
iter 40, train loss 195.87197875976562, val loss 225.40371704101562
iter 50, train loss 195.46661376953125, val loss 225.1004638671875
iter 60, train loss 194.9111328125, val loss 227.59725952148438
iter 70, train loss 194.64297485351562, val loss 226.9351806640625
iter 80, train loss 194.32098388671875, val loss 226.52593994140625
iter 90, train loss 194.18682861328125, val loss 228.39697265625
best loss 206.87420654296875
not here
quantized in 95.72515106201172 seconds
36100 MiB free out of 48676 MiB total
12 mlp.up_proj
Pruning ...
256
iter 0, train loss 170.4058074951172, val loss 188.61497497558594
iter 10, train loss 171.11282348632812, val loss 197.1934356689453
iter 20, train loss 171.35202026367188, val loss 197.435546875
iter 30, train loss 171.02406311035156, val loss 195.22825622558594
iter 40, train loss 170.83758544921875, val loss 195.07516479492188
iter 50, train loss 170.84873962402344, val loss 193.39129638671875
iter 60, train loss 170.725830078125, val loss 196.07376098632812
iter 70, train loss 170.74310302734375, val loss 196.53636169433594
iter 80, train loss 170.72744750976562, val loss 197.70755004882812
iter 90, train loss 170.73248291015625, val loss 197.36489868164062
best loss 188.61497497558594
not here
quantized in 94.50594401359558 seconds
35906 MiB free out of 48676 MiB total
12 mlp.down_proj
Pruning ...
256
iter 0, train loss 3.878199577331543, val loss 2.802570343017578
iter 10, train loss 3.8985369205474854, val loss 2.869422674179077
iter 20, train loss 3.8890254497528076, val loss 2.821903944015503
iter 30, train loss 3.8834753036499023, val loss 2.8053712844848633
iter 40, train loss 3.892817258834839, val loss 2.8483822345733643
iter 50, train loss 3.8844237327575684, val loss 2.8601484298706055
iter 60, train loss 3.8842101097106934, val loss 2.8667819499969482
iter 70, train loss 3.8826467990875244, val loss 2.836353063583374
iter 80, train loss 3.8779406547546387, val loss 2.807554244995117
iter 90, train loss 3.881739854812622, val loss 2.801974296569824
best loss 2.770061492919922
not here
quantized in 101.96318578720093 seconds
35712 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.codebook', 'self_attn.q_proj.quantizer.norms_1', 'self_attn.q_proj.quantizer.norms_0', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.codebook', 'self_attn.k_proj.quantizer.norms_1', 'self_attn.k_proj.quantizer.norms_0', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.codebook', 'self_attn.v_proj.quantizer.norms_1', 'self_attn.v_proj.quantizer.norms_0', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.codebook', 'self_attn.o_proj.quantizer.norms_1', 'self_attn.o_proj.quantizer.norms_0', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.codebook', 'mlp.gate_proj.quantizer.norms_1', 'mlp.gate_proj.quantizer.norms_0', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.codebook', 'mlp.up_proj.quantizer.norms_1', 'mlp.up_proj.quantizer.norms_0', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.codebook', 'mlp.down_proj.quantizer.norms_1', 'mlp.down_proj.quantizer.norms_0', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135936
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.004805771124665625 val loss: 0.007042298122541979
7644 MiB free out of 48676 MiB total
epoch 1 loss: 0.004415451352542732 val loss: 0.0072317020676564425
7644 MiB free out of 48676 MiB total
epoch 2 loss: 0.004295910544897197 val loss: 0.007309304259251803
7644 MiB free out of 48676 MiB total
epoch 3 loss: 0.004238136310959817 val loss: 0.007397928100544959
7644 MiB free out of 48676 MiB total
epoch 4 loss: 0.004201619785817456 val loss: 0.00748225805000402
7644 MiB free out of 48676 MiB total
35712 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
7644 MiB free out of 48676 MiB total
after cast to cpu
37850 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
13 self_attn.q_proj
Pruning ...
256
iter 0, train loss 312.54290771484375, val loss 276.39324951171875
iter 10, train loss 330.6144714355469, val loss 331.7364196777344
iter 20, train loss 317.00103759765625, val loss 317.07562255859375
iter 30, train loss 313.9338684082031, val loss 315.7684020996094
iter 40, train loss 313.0187072753906, val loss 316.5630798339844
iter 50, train loss 311.4864501953125, val loss 314.06707763671875
iter 60, train loss 310.3299865722656, val loss 313.0367431640625
iter 70, train loss 310.18408203125, val loss 312.6977233886719
iter 80, train loss 309.8309020996094, val loss 312.893310546875
iter 90, train loss 309.74188232421875, val loss 312.57342529296875
best loss 276.39324951171875
not here
quantized in 37.5242338180542 seconds
36422 MiB free out of 48676 MiB total
13 self_attn.k_proj
Pruning ...
256
iter 0, train loss 336.1888732910156, val loss 287.1064147949219
iter 10, train loss 354.986572265625, val loss 352.9983825683594
iter 20, train loss 350.0765380859375, val loss 350.339111328125
iter 30, train loss 340.9273681640625, val loss 341.63519287109375
iter 40, train loss 335.1421813964844, val loss 336.0412292480469
iter 50, train loss 333.015380859375, val loss 333.964599609375
iter 60, train loss 332.6149597167969, val loss 334.0072326660156
iter 70, train loss 332.9243469238281, val loss 333.0656433105469
iter 80, train loss 332.37127685546875, val loss 333.5950927734375
iter 90, train loss 332.32196044921875, val loss 333.9788818359375
best loss 287.1064147949219
not here
quantized in 36.763614654541016 seconds
36412 MiB free out of 48676 MiB total
13 self_attn.v_proj
Pruning ...
256
iter 0, train loss 108.08676147460938, val loss 109.06289672851562
iter 10, train loss 108.34780883789062, val loss 110.90511322021484
iter 20, train loss 108.4045181274414, val loss 111.25836181640625
iter 30, train loss 107.90834045410156, val loss 110.69429779052734
iter 40, train loss 107.81616973876953, val loss 110.59152221679688
iter 50, train loss 107.57695770263672, val loss 110.63164520263672
iter 60, train loss 107.54798889160156, val loss 110.53657531738281
iter 70, train loss 107.50875091552734, val loss 110.60585021972656
iter 80, train loss 107.55754852294922, val loss 110.6102294921875
iter 90, train loss 107.53034973144531, val loss 110.68751525878906
best loss 109.06289672851562
not here
quantized in 35.132713079452515 seconds
36402 MiB free out of 48676 MiB total
13 self_attn.o_proj
Pruning ...
256
iter 0, train loss 9.158586502075195, val loss 4.304649829864502
iter 10, train loss 8.844108581542969, val loss 4.591421127319336
iter 20, train loss 8.734411239624023, val loss 4.469106674194336
iter 30, train loss 8.610654830932617, val loss 4.646463871002197
iter 40, train loss 8.576744079589844, val loss 4.4633002281188965
iter 50, train loss 8.52932357788086, val loss 4.602148056030273
iter 60, train loss 8.502762794494629, val loss 4.459258079528809
iter 70, train loss 8.497051239013672, val loss 4.501162528991699
iter 80, train loss 8.452314376831055, val loss 4.520670413970947
iter 90, train loss 8.447418212890625, val loss 4.5141754150390625
best loss 4.304649829864502
not here
quantized in 34.70377278327942 seconds
36370 MiB free out of 48676 MiB total
13 mlp.gate_proj
Pruning ...
256
iter 0, train loss 201.27474975585938, val loss 216.1953125
iter 10, train loss 211.37525939941406, val loss 237.16049194335938
iter 20, train loss 207.32363891601562, val loss 236.30397033691406
iter 30, train loss 206.72576904296875, val loss 231.74305725097656
iter 40, train loss 206.43699645996094, val loss 235.2718963623047
iter 50, train loss 205.5515899658203, val loss 235.19610595703125
iter 60, train loss 205.32565307617188, val loss 232.88307189941406
iter 70, train loss 204.6761932373047, val loss 236.46847534179688
iter 80, train loss 204.49740600585938, val loss 235.6255340576172
iter 90, train loss 204.464111328125, val loss 234.6363067626953
best loss 215.84478759765625
not here
quantized in 96.11750340461731 seconds
36068 MiB free out of 48676 MiB total
13 mlp.up_proj
Pruning ...
256
iter 0, train loss 183.8065185546875, val loss 209.2620391845703
iter 10, train loss 184.5259246826172, val loss 209.46279907226562
iter 20, train loss 184.88682556152344, val loss 214.0109405517578
iter 30, train loss 185.00245666503906, val loss 213.7420196533203
iter 40, train loss 184.59490966796875, val loss 214.68882751464844
iter 50, train loss 184.62368774414062, val loss 213.34495544433594
iter 60, train loss 184.65057373046875, val loss 218.6800994873047
iter 70, train loss 184.5029296875, val loss 219.113037109375
iter 80, train loss 184.51715087890625, val loss 218.92593383789062
iter 90, train loss 184.5183563232422, val loss 218.21363830566406
best loss 207.8975830078125
not here
quantized in 94.72250080108643 seconds
35874 MiB free out of 48676 MiB total
13 mlp.down_proj
Pruning ...
256
iter 0, train loss 4.592185974121094, val loss 3.478468894958496
iter 10, train loss 4.598329544067383, val loss 3.5855534076690674
iter 20, train loss 4.577274799346924, val loss 3.557044267654419
iter 30, train loss 4.560950756072998, val loss 3.548006772994995
iter 40, train loss 4.555991172790527, val loss 3.549999952316284
iter 50, train loss 4.550705909729004, val loss 3.5962014198303223
iter 60, train loss 4.541664123535156, val loss 3.5617082118988037
iter 70, train loss 4.540457248687744, val loss 3.4954795837402344
iter 80, train loss 4.53975772857666, val loss 3.5072288513183594
iter 90, train loss 4.535417079925537, val loss 3.5874483585357666
best loss 3.4683737754821777
not here
quantized in 101.57265663146973 seconds
35680 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.codebook', 'self_attn.q_proj.quantizer.norms_1', 'self_attn.q_proj.quantizer.norms_0', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.codebook', 'self_attn.k_proj.quantizer.norms_1', 'self_attn.k_proj.quantizer.norms_0', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.codebook', 'self_attn.v_proj.quantizer.norms_1', 'self_attn.v_proj.quantizer.norms_0', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.codebook', 'self_attn.o_proj.quantizer.norms_1', 'self_attn.o_proj.quantizer.norms_0', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.codebook', 'mlp.gate_proj.quantizer.norms_1', 'mlp.gate_proj.quantizer.norms_0', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.codebook', 'mlp.up_proj.quantizer.norms_1', 'mlp.up_proj.quantizer.norms_0', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.codebook', 'mlp.down_proj.quantizer.norms_1', 'mlp.down_proj.quantizer.norms_0', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135936
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.005453882011352107 val loss: 0.008129668538458645
6652 MiB free out of 48676 MiB total
epoch 1 loss: 0.004951962207996985 val loss: 0.008638982253614813
6652 MiB free out of 48676 MiB total
epoch 2 loss: 0.004811811057152227 val loss: 0.008818184898700565
6652 MiB free out of 48676 MiB total
epoch 3 loss: 0.004743224093544995 val loss: 0.008980894985143095
6652 MiB free out of 48676 MiB total
epoch 4 loss: 0.004697934604337206 val loss: 0.009130041173193604
6652 MiB free out of 48676 MiB total
35680 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
6652 MiB free out of 48676 MiB total
after cast to cpu
37850 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
14 self_attn.q_proj
Pruning ...
256
iter 0, train loss 328.41387939453125, val loss 285.5601501464844
iter 10, train loss 351.48028564453125, val loss 351.9881591796875
iter 20, train loss 337.31671142578125, val loss 339.76605224609375
iter 30, train loss 331.9923095703125, val loss 334.3274230957031
iter 40, train loss 328.8481750488281, val loss 331.05206298828125
iter 50, train loss 327.9042663574219, val loss 330.4228210449219
iter 60, train loss 326.77947998046875, val loss 328.95635986328125
iter 70, train loss 326.30255126953125, val loss 328.43048095703125
iter 80, train loss 326.4241943359375, val loss 328.0710754394531
iter 90, train loss 326.418212890625, val loss 328.2027587890625
best loss 285.5601501464844
not here
quantized in 37.68138885498047 seconds
36422 MiB free out of 48676 MiB total
14 self_attn.k_proj
Pruning ...
256
iter 0, train loss 359.52783203125, val loss 299.3228454589844
iter 10, train loss 380.2439270019531, val loss 376.30633544921875
iter 20, train loss 375.11712646484375, val loss 374.3526306152344
iter 30, train loss 366.26910400390625, val loss 366.24981689453125
iter 40, train loss 359.8240966796875, val loss 360.21905517578125
iter 50, train loss 357.5738525390625, val loss 357.30523681640625
iter 60, train loss 356.2110595703125, val loss 355.70159912109375
iter 70, train loss 356.072021484375, val loss 355.95440673828125
iter 80, train loss 355.8209228515625, val loss 356.29315185546875
iter 90, train loss 355.39288330078125, val loss 355.273193359375
best loss 299.3228454589844
not here
quantized in 36.89957523345947 seconds
36412 MiB free out of 48676 MiB total
14 self_attn.v_proj
Pruning ...
256
iter 0, train loss 108.55691528320312, val loss 110.1424560546875
iter 10, train loss 108.55440521240234, val loss 111.62017059326172
iter 20, train loss 108.3612289428711, val loss 112.03829956054688
iter 30, train loss 108.3451156616211, val loss 112.2719955444336
iter 40, train loss 108.23625183105469, val loss 112.22412109375
iter 50, train loss 108.00833129882812, val loss 111.84007263183594
iter 60, train loss 108.08465576171875, val loss 112.12107849121094
iter 70, train loss 108.10633850097656, val loss 112.15451049804688
iter 80, train loss 108.01692962646484, val loss 112.04918670654297
iter 90, train loss 107.93959045410156, val loss 111.77313995361328
best loss 110.1424560546875
not here
quantized in 35.01440167427063 seconds
36402 MiB free out of 48676 MiB total
14 self_attn.o_proj
Pruning ...
256
iter 0, train loss 11.528130531311035, val loss 4.142917633056641
iter 10, train loss 11.447965621948242, val loss 4.211165428161621
iter 20, train loss 11.307360649108887, val loss 4.208718776702881
iter 30, train loss 11.319090843200684, val loss 4.114252090454102
iter 40, train loss 11.262113571166992, val loss 4.040985584259033
iter 50, train loss 11.215658187866211, val loss 4.140635013580322
iter 60, train loss 11.175079345703125, val loss 4.096128940582275
iter 70, train loss 11.162181854248047, val loss 4.012854099273682
iter 80, train loss 11.104255676269531, val loss 4.061074733734131
iter 90, train loss 11.090851783752441, val loss 4.152243614196777
best loss 4.0037689208984375
not here
quantized in 34.42286419868469 seconds
36402 MiB free out of 48676 MiB total
14 mlp.gate_proj
Pruning ...
256
iter 0, train loss 220.66650390625, val loss 237.29525756835938
iter 10, train loss 231.66885375976562, val loss 255.38552856445312
iter 20, train loss 227.08584594726562, val loss 258.16754150390625
iter 30, train loss 226.18698120117188, val loss 256.76666259765625
iter 40, train loss 225.51162719726562, val loss 256.18402099609375
iter 50, train loss 225.25924682617188, val loss 257.3909606933594
iter 60, train loss 224.9894256591797, val loss 255.9974365234375
iter 70, train loss 224.56076049804688, val loss 252.59375
iter 80, train loss 224.2335968017578, val loss 252.95391845703125
iter 90, train loss 224.03421020507812, val loss 256.09765625
best loss 237.29525756835938
not here
quantized in 95.54690194129944 seconds
36100 MiB free out of 48676 MiB total
14 mlp.up_proj
Pruning ...
256
iter 0, train loss 201.88711547851562, val loss 223.98944091796875
iter 10, train loss 202.88160705566406, val loss 233.24099731445312
iter 20, train loss 203.07266235351562, val loss 232.23744201660156
iter 30, train loss 202.81719970703125, val loss 236.17840576171875
iter 40, train loss 202.63015747070312, val loss 231.49476623535156
iter 50, train loss 202.7146759033203, val loss 232.07327270507812
iter 60, train loss 202.58335876464844, val loss 230.57659912109375
iter 70, train loss 202.65289306640625, val loss 232.49151611328125
iter 80, train loss 202.66151428222656, val loss 231.34329223632812
iter 90, train loss 202.66061401367188, val loss 231.57064819335938
best loss 223.98944091796875
not here
quantized in 94.73740410804749 seconds
35906 MiB free out of 48676 MiB total
14 mlp.down_proj
Pruning ...
256
iter 0, train loss 5.288307189941406, val loss 3.9923095703125
iter 10, train loss 5.294682502746582, val loss 3.911513328552246
iter 20, train loss 5.290482521057129, val loss 4.0280537605285645
iter 30, train loss 5.274634838104248, val loss 4.123034477233887
iter 40, train loss 5.267117977142334, val loss 3.983994245529175
iter 50, train loss 5.265682697296143, val loss 3.9613566398620605
iter 60, train loss 5.261347770690918, val loss 4.078615188598633
iter 70, train loss 5.258913040161133, val loss 4.064278602600098
iter 80, train loss 5.256898880004883, val loss 3.993845224380493
iter 90, train loss 5.252442359924316, val loss 3.978217601776123
best loss 3.911513328552246
not here
quantized in 101.71889185905457 seconds
35712 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.codebook', 'self_attn.q_proj.quantizer.norms_1', 'self_attn.q_proj.quantizer.norms_0', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.codebook', 'self_attn.k_proj.quantizer.norms_1', 'self_attn.k_proj.quantizer.norms_0', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.codebook', 'self_attn.v_proj.quantizer.norms_1', 'self_attn.v_proj.quantizer.norms_0', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.codebook', 'self_attn.o_proj.quantizer.norms_1', 'self_attn.o_proj.quantizer.norms_0', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.codebook', 'mlp.gate_proj.quantizer.norms_1', 'mlp.gate_proj.quantizer.norms_0', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.codebook', 'mlp.up_proj.quantizer.norms_1', 'mlp.up_proj.quantizer.norms_0', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.codebook', 'mlp.down_proj.quantizer.norms_1', 'mlp.down_proj.quantizer.norms_0', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135936
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0063502732664346695 val loss: 0.009790414478629827
7644 MiB free out of 48676 MiB total
epoch 1 loss: 0.005964067291643005 val loss: 0.01016317802714184
7644 MiB free out of 48676 MiB total
epoch 2 loss: 0.005835445022967178 val loss: 0.010414320800919086
7644 MiB free out of 48676 MiB total
epoch 3 loss: 0.005767088197899284 val loss: 0.010645742760971189
7644 MiB free out of 48676 MiB total
epoch 4 loss: 0.005720752877095947 val loss: 0.010852960229385644
7644 MiB free out of 48676 MiB total
35712 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
7644 MiB free out of 48676 MiB total
after cast to cpu
37850 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
15 self_attn.q_proj
Pruning ...
256
iter 0, train loss 309.03466796875, val loss 266.547119140625
iter 10, train loss 327.25213623046875, val loss 327.7454833984375
iter 20, train loss 313.45977783203125, val loss 312.11322021484375
iter 30, train loss 308.27655029296875, val loss 308.6732177734375
iter 40, train loss 307.9275207519531, val loss 307.69720458984375
iter 50, train loss 305.7200927734375, val loss 304.2681884765625
iter 60, train loss 303.04296875, val loss 302.48870849609375
iter 70, train loss 303.7084045410156, val loss 303.43292236328125
iter 80, train loss 304.05810546875, val loss 304.2739562988281
iter 90, train loss 304.05963134765625, val loss 303.6526794433594
best loss 266.547119140625
not here
quantized in 37.682679891586304 seconds
36422 MiB free out of 48676 MiB total
15 self_attn.k_proj
Pruning ...
256
iter 0, train loss 349.9046630859375, val loss 289.0372009277344
iter 10, train loss 368.45599365234375, val loss 364.62799072265625
iter 20, train loss 366.57025146484375, val loss 364.13482666015625
iter 30, train loss 358.61798095703125, val loss 357.0682067871094
iter 40, train loss 353.3071594238281, val loss 351.4361572265625
iter 50, train loss 350.0986328125, val loss 349.3878173828125
iter 60, train loss 347.8989562988281, val loss 346.8328857421875
iter 70, train loss 347.2956237792969, val loss 347.2445373535156
iter 80, train loss 346.321044921875, val loss 345.6910095214844
iter 90, train loss 345.99017333984375, val loss 345.8904724121094
best loss 289.0372009277344
not here
quantized in 36.769840240478516 seconds
36412 MiB free out of 48676 MiB total
15 self_attn.v_proj
Pruning ...
256
iter 0, train loss 113.22418975830078, val loss 113.90247344970703
iter 10, train loss 113.53837585449219, val loss 115.96034240722656
iter 20, train loss 113.4576416015625, val loss 115.94882202148438
iter 30, train loss 113.30813598632812, val loss 115.82575988769531
iter 40, train loss 113.35010528564453, val loss 116.14505767822266
iter 50, train loss 113.17575073242188, val loss 115.66781616210938
iter 60, train loss 112.84165954589844, val loss 115.60734558105469
iter 70, train loss 112.78724670410156, val loss 115.72117614746094
iter 80, train loss 112.83807373046875, val loss 115.5441665649414
iter 90, train loss 112.90669250488281, val loss 115.54325103759766
best loss 113.90247344970703
not here
quantized in 35.10408091545105 seconds
36402 MiB free out of 48676 MiB total
15 self_attn.o_proj
Pruning ...
256
iter 0, train loss 11.434671401977539, val loss 5.3361287117004395
iter 10, train loss 11.34770679473877, val loss 5.287112712860107
iter 20, train loss 11.278594970703125, val loss 5.3025736808776855
iter 30, train loss 11.247859954833984, val loss 5.434088230133057
iter 40, train loss 11.235631942749023, val loss 5.431445598602295
iter 50, train loss 11.181205749511719, val loss 5.342404365539551
iter 60, train loss 11.167349815368652, val loss 5.231471538543701
iter 70, train loss 11.158476829528809, val loss 5.248476505279541
iter 80, train loss 11.14222526550293, val loss 5.3088154792785645
iter 90, train loss 11.12469482421875, val loss 5.281360626220703
best loss 5.187198638916016
not here
quantized in 35.75565767288208 seconds
36402 MiB free out of 48676 MiB total
15 mlp.gate_proj
Pruning ...
256
iter 0, train loss 241.15916442871094, val loss 259.6736755371094
iter 10, train loss 253.78411865234375, val loss 284.43133544921875
iter 20, train loss 249.48904418945312, val loss 283.9737548828125
iter 30, train loss 248.31939697265625, val loss 286.34564208984375
iter 40, train loss 248.7548370361328, val loss 282.0684814453125
iter 50, train loss 247.9368133544922, val loss 282.0296630859375
iter 60, train loss 247.25027465820312, val loss 276.3470764160156
iter 70, train loss 246.53118896484375, val loss 279.6701354980469
iter 80, train loss 245.96090698242188, val loss 277.65264892578125
iter 90, train loss 245.76364135742188, val loss 277.6244812011719
best loss 259.6736755371094
not here
quantized in 121.14774703979492 seconds
36100 MiB free out of 48676 MiB total
15 mlp.up_proj
Pruning ...
256
iter 0, train loss 220.74139404296875, val loss 247.48153686523438
iter 10, train loss 222.3125, val loss 252.32452392578125
iter 20, train loss 222.4984130859375, val loss 255.31631469726562
iter 30, train loss 222.228515625, val loss 252.7080078125
iter 40, train loss 222.3162384033203, val loss 251.6036834716797
iter 50, train loss 222.19760131835938, val loss 252.48287963867188
iter 60, train loss 222.2108154296875, val loss 253.30479431152344
iter 70, train loss 222.09353637695312, val loss 253.3579559326172
iter 80, train loss 222.01881408691406, val loss 251.08026123046875
iter 90, train loss 222.02066040039062, val loss 250.88406372070312
best loss 247.48153686523438
not here
quantized in 104.66691827774048 seconds
35906 MiB free out of 48676 MiB total
15 mlp.down_proj
Pruning ...
256
iter 0, train loss 6.541802406311035, val loss 4.376558303833008
iter 10, train loss 6.547516822814941, val loss 4.388767242431641
iter 20, train loss 6.520841598510742, val loss 4.462338924407959
iter 30, train loss 6.522873878479004, val loss 4.485752582550049
iter 40, train loss 6.51560640335083, val loss 4.501250267028809
iter 50, train loss 6.50701904296875, val loss 4.436967849731445
iter 60, train loss 6.503724098205566, val loss 4.423911094665527
iter 70, train loss 6.496480941772461, val loss 4.450472354888916
iter 80, train loss 6.494570255279541, val loss 4.484933853149414
iter 90, train loss 6.4975128173828125, val loss 4.4767351150512695
best loss 4.376558303833008
not here
quantized in 107.72635912895203 seconds
35712 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.codebook', 'self_attn.q_proj.quantizer.norms_1', 'self_attn.q_proj.quantizer.norms_0', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.codebook', 'self_attn.k_proj.quantizer.norms_1', 'self_attn.k_proj.quantizer.norms_0', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.codebook', 'self_attn.v_proj.quantizer.norms_1', 'self_attn.v_proj.quantizer.norms_0', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.codebook', 'self_attn.o_proj.quantizer.norms_1', 'self_attn.o_proj.quantizer.norms_0', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.codebook', 'mlp.gate_proj.quantizer.norms_1', 'mlp.gate_proj.quantizer.norms_0', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.codebook', 'mlp.up_proj.quantizer.norms_1', 'mlp.up_proj.quantizer.norms_0', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.codebook', 'mlp.down_proj.quantizer.norms_1', 'mlp.down_proj.quantizer.norms_0', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135936
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.007530429673352046 val loss: 0.010725289350375533
7644 MiB free out of 48676 MiB total
epoch 1 loss: 0.006913337863807101 val loss: 0.011348224768880755
7644 MiB free out of 48676 MiB total
epoch 2 loss: 0.006735388738889014 val loss: 0.01153936400078237
7644 MiB free out of 48676 MiB total
epoch 3 loss: 0.006654212298599305 val loss: 0.011710899416357279
7644 MiB free out of 48676 MiB total
epoch 4 loss: 0.0066000019178318325 val loss: 0.01186872780090198
7644 MiB free out of 48676 MiB total
35712 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
7644 MiB free out of 48676 MiB total
after cast to cpu
37850 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
16 self_attn.q_proj
Pruning ...
256
iter 0, train loss 328.34088134765625, val loss 280.10308837890625
iter 10, train loss 341.3954772949219, val loss 341.5486145019531
iter 20, train loss 331.42401123046875, val loss 330.23980712890625
iter 30, train loss 326.5740966796875, val loss 327.1948547363281
iter 40, train loss 323.614013671875, val loss 322.782470703125
iter 50, train loss 321.82598876953125, val loss 321.6493225097656
iter 60, train loss 320.7398986816406, val loss 321.2611999511719
iter 70, train loss 319.8563232421875, val loss 321.51788330078125
iter 80, train loss 319.41845703125, val loss 321.21954345703125
iter 90, train loss 319.0216369628906, val loss 321.24395751953125
best loss 280.10308837890625
not here
quantized in 34.506439447402954 seconds
36422 MiB free out of 48676 MiB total
16 self_attn.k_proj
Pruning ...
256
iter 0, train loss 361.681640625, val loss 298.1468200683594
iter 10, train loss 382.33673095703125, val loss 376.64788818359375
iter 20, train loss 375.52325439453125, val loss 372.431640625
iter 30, train loss 370.08587646484375, val loss 368.03167724609375
iter 40, train loss 365.8243103027344, val loss 363.60833740234375
iter 50, train loss 362.14642333984375, val loss 360.63153076171875
iter 60, train loss 359.7337646484375, val loss 359.4634094238281
iter 70, train loss 357.63055419921875, val loss 357.1529846191406
iter 80, train loss 354.8035888671875, val loss 354.7048034667969
iter 90, train loss 355.3570251464844, val loss 355.1806335449219
best loss 298.1468200683594
not here
quantized in 32.97964286804199 seconds
36412 MiB free out of 48676 MiB total
16 self_attn.v_proj
Pruning ...
256
iter 0, train loss 129.22930908203125, val loss 133.197021484375
iter 10, train loss 129.163818359375, val loss 133.91668701171875
iter 20, train loss 128.67288208007812, val loss 133.4173583984375
iter 30, train loss 128.38385009765625, val loss 133.20736694335938
iter 40, train loss 127.98866271972656, val loss 133.38595581054688
iter 50, train loss 128.1769256591797, val loss 133.35498046875
iter 60, train loss 128.0164794921875, val loss 133.04550170898438
iter 70, train loss 127.9176025390625, val loss 133.3887939453125
iter 80, train loss 127.93903350830078, val loss 133.4637451171875
iter 90, train loss 127.9080810546875, val loss 133.08384704589844
best loss 132.9890899658203
not here
quantized in 31.35259246826172 seconds
36402 MiB free out of 48676 MiB total
16 self_attn.o_proj
Pruning ...
256
iter 0, train loss 13.640012741088867, val loss 6.702399253845215
iter 10, train loss 13.529966354370117, val loss 6.7697954177856445
iter 20, train loss 13.477659225463867, val loss 6.688294410705566
iter 30, train loss 13.488736152648926, val loss 6.73495626449585
iter 40, train loss 13.445394515991211, val loss 6.840207099914551
iter 50, train loss 13.357308387756348, val loss 6.620206832885742
iter 60, train loss 13.328215599060059, val loss 6.581788063049316
iter 70, train loss 13.316669464111328, val loss 6.71060848236084
iter 80, train loss 13.316368103027344, val loss 6.640821933746338
iter 90, train loss 13.2833890914917, val loss 6.613358020782471
best loss 6.537145137786865
not here
quantized in 31.143331289291382 seconds
36370 MiB free out of 48676 MiB total
16 mlp.gate_proj
Pruning ...
256
iter 0, train loss 280.55450439453125, val loss 281.8209228515625
iter 10, train loss 297.43975830078125, val loss 321.2806396484375
iter 20, train loss 288.918212890625, val loss 316.9729919433594
iter 30, train loss 289.14764404296875, val loss 311.7037353515625
iter 40, train loss 288.8669128417969, val loss 312.5593566894531
iter 50, train loss 287.920654296875, val loss 310.070556640625
iter 60, train loss 286.9996643066406, val loss 311.6339416503906
iter 70, train loss 286.501708984375, val loss 309.11297607421875
iter 80, train loss 286.13916015625, val loss 309.16607666015625
iter 90, train loss 286.0149841308594, val loss 314.8739318847656
best loss 281.7833557128906
not here
quantized in 86.05917811393738 seconds
36068 MiB free out of 48676 MiB total
16 mlp.up_proj
Pruning ...
256
iter 0, train loss 252.32850646972656, val loss 269.6919860839844
iter 10, train loss 253.80650329589844, val loss 281.08050537109375
iter 20, train loss 253.4237060546875, val loss 277.0262451171875
iter 30, train loss 253.26841735839844, val loss 274.38421630859375
iter 40, train loss 253.1294708251953, val loss 274.9267883300781
iter 50, train loss 253.03294372558594, val loss 275.1314697265625
iter 60, train loss 252.96746826171875, val loss 277.6668395996094
iter 70, train loss 253.0538330078125, val loss 281.79571533203125
iter 80, train loss 252.77914428710938, val loss 279.85394287109375
iter 90, train loss 252.61094665527344, val loss 281.40966796875
best loss 269.6919860839844
not here
quantized in 84.31711792945862 seconds
35874 MiB free out of 48676 MiB total
16 mlp.down_proj
Pruning ...
256
iter 0, train loss 8.63032341003418, val loss 6.8286542892456055
iter 10, train loss 8.639751434326172, val loss 6.881750583648682
iter 20, train loss 8.602928161621094, val loss 6.844784259796143
iter 30, train loss 8.591588020324707, val loss 6.912010669708252
iter 40, train loss 8.581316947937012, val loss 6.958398818969727
iter 50, train loss 8.573801040649414, val loss 6.8898820877075195
iter 60, train loss 8.563240051269531, val loss 6.850725173950195
iter 70, train loss 8.555120468139648, val loss 6.93151330947876
iter 80, train loss 8.554567337036133, val loss 7.040515899658203
iter 90, train loss 8.552111625671387, val loss 7.059277534484863
best loss 6.780457019805908
not here
quantized in 91.26814222335815 seconds
35680 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.codebook', 'self_attn.q_proj.quantizer.norms_1', 'self_attn.q_proj.quantizer.norms_0', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.codebook', 'self_attn.k_proj.quantizer.norms_1', 'self_attn.k_proj.quantizer.norms_0', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.codebook', 'self_attn.v_proj.quantizer.norms_1', 'self_attn.v_proj.quantizer.norms_0', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.codebook', 'self_attn.o_proj.quantizer.norms_1', 'self_attn.o_proj.quantizer.norms_0', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.codebook', 'mlp.gate_proj.quantizer.norms_1', 'mlp.gate_proj.quantizer.norms_0', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.codebook', 'mlp.up_proj.quantizer.norms_1', 'mlp.up_proj.quantizer.norms_0', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.codebook', 'mlp.down_proj.quantizer.norms_1', 'mlp.down_proj.quantizer.norms_0', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135936
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.009867412358289585 val loss: 0.015578546153847128
6652 MiB free out of 48676 MiB total
epoch 1 loss: 0.009087489474040922 val loss: 0.016215271782130003
6652 MiB free out of 48676 MiB total
epoch 2 loss: 0.008871977079252247 val loss: 0.016409398871473968
6652 MiB free out of 48676 MiB total
epoch 3 loss: 0.008765454993408639 val loss: 0.01663185062352568
6652 MiB free out of 48676 MiB total
epoch 4 loss: 0.008691111081134295 val loss: 0.016859123716130853
6652 MiB free out of 48676 MiB total
35680 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
6652 MiB free out of 48676 MiB total
after cast to cpu
37850 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
17 self_attn.q_proj
Pruning ...
256
iter 0, train loss 341.91644287109375, val loss 291.682861328125
iter 10, train loss 364.6453552246094, val loss 362.568359375
iter 20, train loss 352.53863525390625, val loss 352.18701171875
iter 30, train loss 344.32427978515625, val loss 343.4954833984375
iter 40, train loss 342.0875549316406, val loss 341.5216369628906
iter 50, train loss 339.7428894042969, val loss 340.17974853515625
iter 60, train loss 337.2206115722656, val loss 338.12548828125
iter 70, train loss 335.7037658691406, val loss 336.30218505859375
iter 80, train loss 334.94793701171875, val loss 334.26190185546875
iter 90, train loss 335.2110900878906, val loss 333.961669921875
best loss 291.682861328125
not here
quantized in 37.05807089805603 seconds
36422 MiB free out of 48676 MiB total
17 self_attn.k_proj
Pruning ...
256
iter 0, train loss 373.5598449707031, val loss 310.3254699707031
iter 10, train loss 397.2840881347656, val loss 390.45294189453125
iter 20, train loss 390.0281982421875, val loss 388.44586181640625
iter 30, train loss 381.9023132324219, val loss 382.001953125
iter 40, train loss 378.48760986328125, val loss 378.14801025390625
iter 50, train loss 375.333984375, val loss 375.03216552734375
iter 60, train loss 372.59906005859375, val loss 371.6943664550781
iter 70, train loss 370.3294982910156, val loss 370.2380065917969
iter 80, train loss 368.74554443359375, val loss 369.4654541015625
iter 90, train loss 367.68536376953125, val loss 368.29913330078125
best loss 310.3254699707031
not here
quantized in 37.38310623168945 seconds
36412 MiB free out of 48676 MiB total
17 self_attn.v_proj
Pruning ...
256
iter 0, train loss 137.78627014160156, val loss 140.7398681640625
iter 10, train loss 138.2146453857422, val loss 142.49012756347656
iter 20, train loss 137.73550415039062, val loss 142.1601104736328
iter 30, train loss 137.333740234375, val loss 142.5111083984375
iter 40, train loss 137.37767028808594, val loss 142.19007873535156
iter 50, train loss 137.494873046875, val loss 142.39186096191406
iter 60, train loss 137.2917022705078, val loss 142.20953369140625
iter 70, train loss 137.31838989257812, val loss 142.24195861816406
iter 80, train loss 137.35369873046875, val loss 142.12535095214844
iter 90, train loss 137.3899383544922, val loss 142.1119384765625
best loss 140.7398681640625
not here
quantized in 35.075969219207764 seconds
36402 MiB free out of 48676 MiB total
17 self_attn.o_proj
Pruning ...
256
iter 0, train loss 10.343379974365234, val loss 6.901788711547852
iter 10, train loss 10.147080421447754, val loss 7.077773571014404
iter 20, train loss 10.084590911865234, val loss 6.966431617736816
iter 30, train loss 10.040634155273438, val loss 6.978917598724365
iter 40, train loss 9.975960731506348, val loss 7.026578903198242
iter 50, train loss 9.995687484741211, val loss 7.191922664642334
iter 60, train loss 9.978721618652344, val loss 7.021973609924316
iter 70, train loss 9.963187217712402, val loss 6.868098258972168
iter 80, train loss 9.964345932006836, val loss 6.823095321655273
iter 90, train loss 9.960199356079102, val loss 6.810641765594482
best loss 6.626828193664551
not here
quantized in 34.952545404434204 seconds
36402 MiB free out of 48676 MiB total
17 mlp.gate_proj
Pruning ...
256
iter 0, train loss 318.45843505859375, val loss 330.31951904296875
iter 10, train loss 335.65826416015625, val loss 383.8348083496094
iter 20, train loss 331.02838134765625, val loss 367.9817199707031
iter 30, train loss 331.9490051269531, val loss 376.2671203613281
iter 40, train loss 331.41070556640625, val loss 369.8046875
iter 50, train loss 330.52410888671875, val loss 370.0729675292969
iter 60, train loss 329.8658447265625, val loss 365.62249755859375
iter 70, train loss 328.958984375, val loss 361.99493408203125
iter 80, train loss 328.42108154296875, val loss 362.7059631347656
iter 90, train loss 328.3841552734375, val loss 365.85028076171875
best loss 330.31951904296875
not here
quantized in 96.35152173042297 seconds
36100 MiB free out of 48676 MiB total
17 mlp.up_proj
Pruning ...
256
iter 0, train loss 277.3955993652344, val loss 305.03125
iter 10, train loss 278.9104919433594, val loss 311.8591613769531
iter 20, train loss 279.4387512207031, val loss 316.6955261230469
iter 30, train loss 278.99908447265625, val loss 314.05755615234375
iter 40, train loss 278.7833251953125, val loss 312.0793762207031
iter 50, train loss 279.3050537109375, val loss 310.2152099609375
iter 60, train loss 279.45916748046875, val loss 311.0301208496094
iter 70, train loss 279.4530029296875, val loss 314.6428527832031
iter 80, train loss 279.4779052734375, val loss 312.81793212890625
iter 90, train loss 279.525390625, val loss 312.6911315917969
best loss 305.03125
not here
quantized in 116.68729090690613 seconds
35906 MiB free out of 48676 MiB total
17 mlp.down_proj
Pruning ...
256
iter 0, train loss 9.355754852294922, val loss 8.241188049316406
iter 10, train loss 9.36892318725586, val loss 8.312642097473145
iter 20, train loss 9.348819732666016, val loss 8.184469223022461
iter 30, train loss 9.319648742675781, val loss 8.158119201660156
iter 40, train loss 9.311352729797363, val loss 8.220115661621094
iter 50, train loss 9.310490608215332, val loss 8.299144744873047
iter 60, train loss 9.298672676086426, val loss 8.246647834777832
iter 70, train loss 9.290298461914062, val loss 8.277029991149902
iter 80, train loss 9.286412239074707, val loss 8.265484809875488
iter 90, train loss 9.278451919555664, val loss 8.327516555786133
best loss 8.095520973205566
not here
quantized in 93.89912629127502 seconds
35712 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.codebook', 'self_attn.q_proj.quantizer.norms_1', 'self_attn.q_proj.quantizer.norms_0', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.codebook', 'self_attn.k_proj.quantizer.norms_1', 'self_attn.k_proj.quantizer.norms_0', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.codebook', 'self_attn.v_proj.quantizer.norms_1', 'self_attn.v_proj.quantizer.norms_0', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.codebook', 'self_attn.o_proj.quantizer.norms_1', 'self_attn.o_proj.quantizer.norms_0', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.codebook', 'mlp.gate_proj.quantizer.norms_1', 'mlp.gate_proj.quantizer.norms_0', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.codebook', 'mlp.up_proj.quantizer.norms_1', 'mlp.up_proj.quantizer.norms_0', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.codebook', 'mlp.down_proj.quantizer.norms_1', 'mlp.down_proj.quantizer.norms_0', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135936
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.010010053170844913 val loss: 0.01903856312856078
7644 MiB free out of 48676 MiB total
epoch 1 loss: 0.009054111702425871 val loss: 0.019157615024596453
7644 MiB free out of 48676 MiB total
epoch 2 loss: 0.008813035274215508 val loss: 0.019064400694333017
7644 MiB free out of 48676 MiB total
epoch 3 loss: 0.008710444635653403 val loss: 0.019120446988381445
7644 MiB free out of 48676 MiB total
epoch 4 loss: 0.008644177381938789 val loss: 0.019225733471103013
7644 MiB free out of 48676 MiB total
35712 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
7644 MiB free out of 48676 MiB total
after cast to cpu
37850 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
18 self_attn.q_proj
Pruning ...
256
iter 0, train loss 373.396240234375, val loss 311.7970275878906
iter 10, train loss 393.6241760253906, val loss 390.4173583984375
iter 20, train loss 376.8795471191406, val loss 374.48834228515625
iter 30, train loss 367.5993347167969, val loss 366.468994140625
iter 40, train loss 361.171142578125, val loss 359.8698425292969
iter 50, train loss 357.4096984863281, val loss 357.6393127441406
iter 60, train loss 356.4571838378906, val loss 357.26263427734375
iter 70, train loss 355.6738586425781, val loss 357.3428955078125
iter 80, train loss 356.5003662109375, val loss 357.42962646484375
iter 90, train loss 355.8643798828125, val loss 356.5692138671875
best loss 311.7970275878906
not here
quantized in 34.11310696601868 seconds
36422 MiB free out of 48676 MiB total
18 self_attn.k_proj
Pruning ...
256
iter 0, train loss 398.2452087402344, val loss 329.6319274902344
iter 10, train loss 423.6082763671875, val loss 419.1972351074219
iter 20, train loss 412.6003112792969, val loss 410.14251708984375
iter 30, train loss 404.4449157714844, val loss 404.0634765625
iter 40, train loss 395.3570861816406, val loss 395.6277160644531
iter 50, train loss 392.05413818359375, val loss 392.225830078125
iter 60, train loss 386.9798583984375, val loss 387.6173400878906
iter 70, train loss 387.28741455078125, val loss 389.1473083496094
iter 80, train loss 385.7325439453125, val loss 386.7626647949219
iter 90, train loss 384.71087646484375, val loss 386.63641357421875
best loss 329.6319274902344
not here
quantized in 32.97345805168152 seconds
36412 MiB free out of 48676 MiB total
18 self_attn.v_proj
Pruning ...
256
iter 0, train loss 167.4087371826172, val loss 169.8887939453125
iter 10, train loss 167.51644897460938, val loss 171.4353790283203
iter 20, train loss 167.66432189941406, val loss 171.57891845703125
iter 30, train loss 167.33688354492188, val loss 171.39903259277344
iter 40, train loss 166.62496948242188, val loss 172.31988525390625
iter 50, train loss 166.39593505859375, val loss 172.06024169921875
iter 60, train loss 166.35195922851562, val loss 171.55804443359375
iter 70, train loss 166.49879455566406, val loss 171.7939910888672
iter 80, train loss 166.43136596679688, val loss 171.66738891601562
iter 90, train loss 166.51483154296875, val loss 171.57708740234375
best loss 169.8887939453125
not here
quantized in 31.072607040405273 seconds
36402 MiB free out of 48676 MiB total
18 self_attn.o_proj
Pruning ...
256
iter 0, train loss 9.337090492248535, val loss 8.248690605163574
iter 10, train loss 9.231243133544922, val loss 9.000943183898926
iter 20, train loss 9.154040336608887, val loss 8.913597106933594
iter 30, train loss 9.089399337768555, val loss 8.754203796386719
iter 40, train loss 9.054306030273438, val loss 8.68757152557373
iter 50, train loss 9.027389526367188, val loss 8.55941104888916
iter 60, train loss 8.994221687316895, val loss 8.534506797790527
iter 70, train loss 8.96198558807373, val loss 8.298572540283203
iter 80, train loss 8.946941375732422, val loss 8.266042709350586
iter 90, train loss 8.942378044128418, val loss 8.561746597290039
best loss 8.2200288772583
not here
quantized in 31.33085823059082 seconds
36370 MiB free out of 48676 MiB total
18 mlp.gate_proj
Pruning ...
256
iter 0, train loss 359.9468078613281, val loss 373.57464599609375
iter 10, train loss 377.4054260253906, val loss 419.05279541015625
iter 20, train loss 374.9738464355469, val loss 410.70611572265625
iter 30, train loss 376.3329162597656, val loss 420.1699523925781
iter 40, train loss 374.6409912109375, val loss 417.6666564941406
iter 50, train loss 373.35137939453125, val loss 416.7762451171875
iter 60, train loss 372.16900634765625, val loss 418.1979064941406
iter 70, train loss 372.04669189453125, val loss 419.56573486328125
iter 80, train loss 372.3416442871094, val loss 412.2127685546875
iter 90, train loss 372.2566833496094, val loss 415.9617919921875
best loss 370.16192626953125
not here
quantized in 85.76555061340332 seconds
36068 MiB free out of 48676 MiB total
18 mlp.up_proj
Pruning ...
256
iter 0, train loss 305.23822021484375, val loss 331.24639892578125
iter 10, train loss 305.876953125, val loss 348.12884521484375
iter 20, train loss 306.4870300292969, val loss 347.33038330078125
iter 30, train loss 306.73577880859375, val loss 351.31988525390625
iter 40, train loss 306.7034912109375, val loss 352.1896057128906
iter 50, train loss 306.7131042480469, val loss 353.76123046875
iter 60, train loss 306.8133850097656, val loss 350.75732421875
iter 70, train loss 306.6836853027344, val loss 350.5876159667969
iter 80, train loss 306.72698974609375, val loss 345.1722717285156
iter 90, train loss 306.84521484375, val loss 343.9974365234375
best loss 331.24639892578125
not here
quantized in 83.67902374267578 seconds
35874 MiB free out of 48676 MiB total
18 mlp.down_proj
Pruning ...
256
iter 0, train loss 10.744109153747559, val loss 7.4667463302612305
iter 10, train loss 10.753713607788086, val loss 7.518416881561279
iter 20, train loss 10.724634170532227, val loss 7.572390556335449
iter 30, train loss 10.707929611206055, val loss 7.736610412597656
iter 40, train loss 10.712207794189453, val loss 7.740777969360352
iter 50, train loss 10.711670875549316, val loss 7.687170028686523
iter 60, train loss 10.702661514282227, val loss 7.559267520904541
iter 70, train loss 10.702488899230957, val loss 7.471284866333008
iter 80, train loss 10.700116157531738, val loss 7.503520488739014
iter 90, train loss 10.690272331237793, val loss 7.517579078674316
best loss 7.410231113433838
not here
quantized in 91.09714579582214 seconds
35680 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.codebook', 'self_attn.q_proj.quantizer.norms_1', 'self_attn.q_proj.quantizer.norms_0', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.codebook', 'self_attn.k_proj.quantizer.norms_1', 'self_attn.k_proj.quantizer.norms_0', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.codebook', 'self_attn.v_proj.quantizer.norms_1', 'self_attn.v_proj.quantizer.norms_0', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.codebook', 'self_attn.o_proj.quantizer.norms_1', 'self_attn.o_proj.quantizer.norms_0', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.codebook', 'mlp.gate_proj.quantizer.norms_1', 'mlp.gate_proj.quantizer.norms_0', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.codebook', 'mlp.up_proj.quantizer.norms_1', 'mlp.up_proj.quantizer.norms_0', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.codebook', 'mlp.down_proj.quantizer.norms_1', 'mlp.down_proj.quantizer.norms_0', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135936
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.011718247777025681 val loss: 0.018034984124824405
6652 MiB free out of 48676 MiB total
epoch 1 loss: 0.010238787144771777 val loss: 0.018832538626156747
6652 MiB free out of 48676 MiB total
epoch 2 loss: 0.0099091066949768 val loss: 0.019111557980068028
6652 MiB free out of 48676 MiB total
epoch 3 loss: 0.00977413635700941 val loss: 0.019275683560408652
6652 MiB free out of 48676 MiB total
epoch 4 loss: 0.009691311439382844 val loss: 0.019440389471128583
6652 MiB free out of 48676 MiB total
35680 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
6652 MiB free out of 48676 MiB total
after cast to cpu
37850 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
19 self_attn.q_proj
Pruning ...
256
iter 0, train loss 353.5690612792969, val loss 301.5352783203125
iter 10, train loss 375.121337890625, val loss 371.2889099121094
iter 20, train loss 358.5809326171875, val loss 355.95953369140625
iter 30, train loss 351.52728271484375, val loss 350.947265625
iter 40, train loss 350.0116882324219, val loss 350.3800048828125
iter 50, train loss 349.0704345703125, val loss 349.7695007324219
iter 60, train loss 346.2299499511719, val loss 347.24310302734375
iter 70, train loss 344.83624267578125, val loss 346.3974609375
iter 80, train loss 343.8929443359375, val loss 345.60369873046875
iter 90, train loss 342.7322998046875, val loss 344.1044006347656
best loss 301.5352783203125
not here
quantized in 34.07535743713379 seconds
36422 MiB free out of 48676 MiB total
19 self_attn.k_proj
Pruning ...
256
iter 0, train loss 378.9825134277344, val loss 316.95928955078125
iter 10, train loss 400.580810546875, val loss 396.6663818359375
iter 20, train loss 395.4052429199219, val loss 394.5291748046875
iter 30, train loss 388.2940673828125, val loss 389.1039123535156
iter 40, train loss 380.4715576171875, val loss 382.2592468261719
iter 50, train loss 378.165283203125, val loss 379.7501220703125
iter 60, train loss 376.07073974609375, val loss 377.8021240234375
iter 70, train loss 374.1437072753906, val loss 376.1809997558594
iter 80, train loss 372.8609619140625, val loss 375.14593505859375
iter 90, train loss 371.55108642578125, val loss 373.5841979980469
best loss 316.95928955078125
not here
quantized in 32.71420073509216 seconds
36412 MiB free out of 48676 MiB total
19 self_attn.v_proj
Pruning ...
256
iter 0, train loss 169.10601806640625, val loss 173.14590454101562
iter 10, train loss 169.51986694335938, val loss 174.5645751953125
iter 20, train loss 169.165283203125, val loss 174.89976501464844
iter 30, train loss 169.0546417236328, val loss 174.84539794921875
iter 40, train loss 168.96533203125, val loss 174.5072784423828
iter 50, train loss 168.77366638183594, val loss 174.5412139892578
iter 60, train loss 168.68345642089844, val loss 174.40072631835938
iter 70, train loss 168.67787170410156, val loss 174.37738037109375
iter 80, train loss 168.6376495361328, val loss 174.5223388671875
iter 90, train loss 168.38479614257812, val loss 174.85919189453125
best loss 173.14590454101562
not here
quantized in 31.083184719085693 seconds
36402 MiB free out of 48676 MiB total
19 self_attn.o_proj
Pruning ...
256
iter 0, train loss 9.648847579956055, val loss 8.751914978027344
iter 10, train loss 9.300437927246094, val loss 9.467296600341797
iter 20, train loss 9.210603713989258, val loss 9.449920654296875
iter 30, train loss 9.099782943725586, val loss 9.241767883300781
iter 40, train loss 9.066829681396484, val loss 9.120471954345703
iter 50, train loss 9.03211784362793, val loss 9.190408706665039
iter 60, train loss 9.029942512512207, val loss 9.12832260131836
iter 70, train loss 9.044978141784668, val loss 9.232231140136719
iter 80, train loss 9.04456901550293, val loss 9.149328231811523
iter 90, train loss 9.044281005859375, val loss 9.109228134155273
best loss 8.751914978027344
not here
quantized in 31.3273868560791 seconds
36370 MiB free out of 48676 MiB total
19 mlp.gate_proj
Pruning ...
256
iter 0, train loss 381.2589111328125, val loss 390.92095947265625
iter 10, train loss 397.1788330078125, val loss 440.91729736328125
iter 20, train loss 396.7047119140625, val loss 439.833740234375
iter 30, train loss 397.34796142578125, val loss 430.6154479980469
iter 40, train loss 396.0030822753906, val loss 429.01873779296875
iter 50, train loss 396.0492858886719, val loss 432.1473693847656
iter 60, train loss 395.9189147949219, val loss 439.119873046875
iter 70, train loss 395.62457275390625, val loss 440.43603515625
iter 80, train loss 395.8470458984375, val loss 439.839599609375
iter 90, train loss 395.45166015625, val loss 442.84539794921875
best loss 390.92095947265625
not here
quantized in 85.57936096191406 seconds
36068 MiB free out of 48676 MiB total
19 mlp.up_proj
Pruning ...
256
iter 0, train loss 327.3164367675781, val loss 349.1317138671875
iter 10, train loss 327.63067626953125, val loss 358.5364990234375
iter 20, train loss 328.2660827636719, val loss 361.4158935546875
iter 30, train loss 329.3528747558594, val loss 365.07861328125
iter 40, train loss 329.29071044921875, val loss 362.8117980957031
iter 50, train loss 329.3728942871094, val loss 366.4516296386719
iter 60, train loss 329.3275146484375, val loss 368.3328857421875
iter 70, train loss 328.9681091308594, val loss 367.1275939941406
iter 80, train loss 328.7833557128906, val loss 363.189453125
iter 90, train loss 328.9620361328125, val loss 363.79388427734375
best loss 349.1317138671875
not here
quantized in 84.9874336719513 seconds
35874 MiB free out of 48676 MiB total
19 mlp.down_proj
Pruning ...
256
iter 0, train loss 11.853734970092773, val loss 7.705634117126465
iter 10, train loss 11.859586715698242, val loss 7.973936557769775
iter 20, train loss 11.81789779663086, val loss 7.767410755157471
iter 30, train loss 11.806276321411133, val loss 7.881535530090332
iter 40, train loss 11.798467636108398, val loss 7.880702018737793
iter 50, train loss 11.788517951965332, val loss 7.970489501953125
iter 60, train loss 11.779213905334473, val loss 8.088094711303711
iter 70, train loss 11.764599800109863, val loss 7.947780609130859
iter 80, train loss 11.773831367492676, val loss 7.989650726318359
iter 90, train loss 11.766948699951172, val loss 7.940219879150391
best loss 7.705634117126465
not here
quantized in 91.54867100715637 seconds
35680 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.codebook', 'self_attn.q_proj.quantizer.norms_1', 'self_attn.q_proj.quantizer.norms_0', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.codebook', 'self_attn.k_proj.quantizer.norms_1', 'self_attn.k_proj.quantizer.norms_0', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.codebook', 'self_attn.v_proj.quantizer.norms_1', 'self_attn.v_proj.quantizer.norms_0', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.codebook', 'self_attn.o_proj.quantizer.norms_1', 'self_attn.o_proj.quantizer.norms_0', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.codebook', 'mlp.gate_proj.quantizer.norms_1', 'mlp.gate_proj.quantizer.norms_0', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.codebook', 'mlp.up_proj.quantizer.norms_1', 'mlp.up_proj.quantizer.norms_0', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.codebook', 'mlp.down_proj.quantizer.norms_1', 'mlp.down_proj.quantizer.norms_0', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135936
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.012853915271989536 val loss: 0.018516761949285865
6652 MiB free out of 48676 MiB total
epoch 1 loss: 0.011265090040978976 val loss: 0.01922758447472006
6652 MiB free out of 48676 MiB total
epoch 2 loss: 0.010905784540227614 val loss: 0.019327204674482346
6652 MiB free out of 48676 MiB total
epoch 3 loss: 0.010766269930172712 val loss: 0.019380677142180502
6652 MiB free out of 48676 MiB total
epoch 4 loss: 0.010679809165594634 val loss: 0.019441754557192326
6652 MiB free out of 48676 MiB total
35680 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
6652 MiB free out of 48676 MiB total
after cast to cpu
37850 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
20 self_attn.q_proj
Pruning ...
256
iter 0, train loss 366.569091796875, val loss 311.8993835449219
iter 10, train loss 385.4561767578125, val loss 382.906494140625
iter 20, train loss 371.9638366699219, val loss 370.5129089355469
iter 30, train loss 363.7579040527344, val loss 363.6806335449219
iter 40, train loss 359.48895263671875, val loss 359.0805969238281
iter 50, train loss 358.233642578125, val loss 358.0687255859375
iter 60, train loss 356.00946044921875, val loss 355.96630859375
iter 70, train loss 355.8400573730469, val loss 355.1361389160156
iter 80, train loss 354.7218933105469, val loss 354.8759765625
iter 90, train loss 354.09112548828125, val loss 354.1672058105469
best loss 311.8993835449219
not here
quantized in 33.74310278892517 seconds
36422 MiB free out of 48676 MiB total
20 self_attn.k_proj
Pruning ...
256
iter 0, train loss 388.39801025390625, val loss 327.4215087890625
iter 10, train loss 413.19549560546875, val loss 408.8006591796875
iter 20, train loss 403.020751953125, val loss 403.24212646484375
iter 30, train loss 395.81060791015625, val loss 396.4808349609375
iter 40, train loss 390.39300537109375, val loss 391.59869384765625
iter 50, train loss 386.5869445800781, val loss 387.7068176269531
iter 60, train loss 384.1332702636719, val loss 385.01568603515625
iter 70, train loss 383.4363098144531, val loss 383.9158630371094
iter 80, train loss 381.78155517578125, val loss 383.2613525390625
iter 90, train loss 380.9053039550781, val loss 382.88397216796875
best loss 327.4215087890625
not here
quantized in 32.766459465026855 seconds
36412 MiB free out of 48676 MiB total
20 self_attn.v_proj
Pruning ...
256
iter 0, train loss 175.31884765625, val loss 178.69586181640625
iter 10, train loss 175.30426025390625, val loss 180.4688262939453
iter 20, train loss 174.9298553466797, val loss 180.91319274902344
iter 30, train loss 174.51004028320312, val loss 180.96145629882812
iter 40, train loss 174.69485473632812, val loss 179.7111358642578
iter 50, train loss 174.34689331054688, val loss 179.31912231445312
iter 60, train loss 174.21832275390625, val loss 179.75726318359375
iter 70, train loss 174.32028198242188, val loss 180.09140014648438
iter 80, train loss 174.26498413085938, val loss 179.95118713378906
iter 90, train loss 174.1734619140625, val loss 180.07958984375
best loss 178.69586181640625
not here
quantized in 31.107719659805298 seconds
36402 MiB free out of 48676 MiB total
20 self_attn.o_proj
Pruning ...
256
iter 0, train loss 13.732320785522461, val loss 10.748771667480469
iter 10, train loss 12.386758804321289, val loss 11.746773719787598
iter 20, train loss 11.852853775024414, val loss 11.41082763671875
iter 30, train loss 11.459444999694824, val loss 11.39214038848877
iter 40, train loss 11.082500457763672, val loss 11.150659561157227
iter 50, train loss 10.900440216064453, val loss 10.919418334960938
iter 60, train loss 10.762340545654297, val loss 10.684954643249512
iter 70, train loss 10.72557544708252, val loss 10.867341995239258
iter 80, train loss 10.686936378479004, val loss 10.70881462097168
iter 90, train loss 10.672496795654297, val loss 10.73897933959961
best loss 10.3390531539917
not here
quantized in 31.874892711639404 seconds
36402 MiB free out of 48676 MiB total
20 mlp.gate_proj
Pruning ...
256
iter 0, train loss 413.2789611816406, val loss 437.68359375
iter 10, train loss 429.3753967285156, val loss 478.20062255859375
iter 20, train loss 430.0363464355469, val loss 474.62579345703125
iter 30, train loss 429.6503601074219, val loss 480.4121398925781
iter 40, train loss 428.1553649902344, val loss 474.57177734375
iter 50, train loss 428.03680419921875, val loss 467.38189697265625
iter 60, train loss 428.251708984375, val loss 469.58154296875
iter 70, train loss 427.27435302734375, val loss 464.1386413574219
iter 80, train loss 426.87646484375, val loss 469.1749267578125
iter 90, train loss 426.6591796875, val loss 466.215576171875
best loss 437.68359375
not here
quantized in 85.14742541313171 seconds
36100 MiB free out of 48676 MiB total
20 mlp.up_proj
Pruning ...
256
iter 0, train loss 350.1736145019531, val loss 376.88885498046875
iter 10, train loss 351.02545166015625, val loss 391.5403137207031
iter 20, train loss 352.44842529296875, val loss 393.8660888671875
iter 30, train loss 352.6979675292969, val loss 402.2149353027344
iter 40, train loss 353.0826110839844, val loss 394.63323974609375
iter 50, train loss 353.36474609375, val loss 393.6571044921875
iter 60, train loss 353.67620849609375, val loss 396.9931640625
iter 70, train loss 353.82550048828125, val loss 403.51416015625
iter 80, train loss 353.6923828125, val loss 399.3128356933594
iter 90, train loss 353.6224060058594, val loss 400.6927490234375
best loss 376.88885498046875
not here
quantized in 84.46005964279175 seconds
35906 MiB free out of 48676 MiB total
20 mlp.down_proj
Pruning ...
256
iter 0, train loss 14.921157836914062, val loss 10.705666542053223
iter 10, train loss 14.897425651550293, val loss 10.663934707641602
iter 20, train loss 14.766231536865234, val loss 10.696008682250977
iter 30, train loss 14.720303535461426, val loss 10.695260047912598
iter 40, train loss 14.697192192077637, val loss 10.87850284576416
iter 50, train loss 14.690523147583008, val loss 10.713525772094727
iter 60, train loss 14.68692684173584, val loss 10.580738067626953
iter 70, train loss 14.668241500854492, val loss 10.432830810546875
iter 80, train loss 14.655656814575195, val loss 10.468391418457031
iter 90, train loss 14.654932975769043, val loss 10.530750274658203
best loss 10.374818801879883
not here
quantized in 91.55168437957764 seconds
35712 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.codebook', 'self_attn.q_proj.quantizer.norms_1', 'self_attn.q_proj.quantizer.norms_0', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.codebook', 'self_attn.k_proj.quantizer.norms_1', 'self_attn.k_proj.quantizer.norms_0', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.codebook', 'self_attn.v_proj.quantizer.norms_1', 'self_attn.v_proj.quantizer.norms_0', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.codebook', 'self_attn.o_proj.quantizer.norms_1', 'self_attn.o_proj.quantizer.norms_0', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.codebook', 'mlp.gate_proj.quantizer.norms_1', 'mlp.gate_proj.quantizer.norms_0', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.codebook', 'mlp.up_proj.quantizer.norms_1', 'mlp.up_proj.quantizer.norms_0', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.codebook', 'mlp.down_proj.quantizer.norms_1', 'mlp.down_proj.quantizer.norms_0', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135936
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.01590028752980288 val loss: 0.023840730194933712
7644 MiB free out of 48676 MiB total
epoch 1 loss: 0.014048678705876227 val loss: 0.02498225588351488
7644 MiB free out of 48676 MiB total
epoch 2 loss: 0.013509050928405486 val loss: 0.025711373309604824
7644 MiB free out of 48676 MiB total
epoch 3 loss: 0.013275296449137386 val loss: 0.026162530877627432
7644 MiB free out of 48676 MiB total
epoch 4 loss: 0.01313419236248592 val loss: 0.026536795659922063
7644 MiB free out of 48676 MiB total
35712 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
7644 MiB free out of 48676 MiB total
after cast to cpu
37850 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
21 self_attn.q_proj
Pruning ...
256
iter 0, train loss 380.11700439453125, val loss 337.6190490722656
iter 10, train loss 402.0152893066406, val loss 403.87255859375
iter 20, train loss 387.6248474121094, val loss 389.5634460449219
iter 30, train loss 380.94036865234375, val loss 384.6304626464844
iter 40, train loss 381.24957275390625, val loss 386.60308837890625
iter 50, train loss 377.483642578125, val loss 381.59283447265625
iter 60, train loss 376.614013671875, val loss 380.84661865234375
iter 70, train loss 375.51165771484375, val loss 380.28338623046875
iter 80, train loss 373.9694519042969, val loss 379.2238464355469
iter 90, train loss 373.7987365722656, val loss 377.74078369140625
best loss 337.6190490722656
not here
quantized in 33.98556351661682 seconds
36422 MiB free out of 48676 MiB total
21 self_attn.k_proj
Pruning ...
256
iter 0, train loss 399.0972900390625, val loss 348.3679504394531
iter 10, train loss 421.2201232910156, val loss 419.0736083984375
iter 20, train loss 403.771484375, val loss 403.436279296875
iter 30, train loss 397.60833740234375, val loss 398.8393249511719
iter 40, train loss 394.69024658203125, val loss 398.3780517578125
iter 50, train loss 392.42840576171875, val loss 396.3294677734375
iter 60, train loss 391.3984069824219, val loss 396.06988525390625
iter 70, train loss 390.796630859375, val loss 396.2822265625
iter 80, train loss 390.18182373046875, val loss 395.77777099609375
iter 90, train loss 390.3072204589844, val loss 396.1218566894531
best loss 348.3679504394531
not here
quantized in 32.63903999328613 seconds
36412 MiB free out of 48676 MiB total
21 self_attn.v_proj
Pruning ...
256
iter 0, train loss 210.24197387695312, val loss 216.41177368164062
iter 10, train loss 210.0884246826172, val loss 218.22378540039062
iter 20, train loss 209.5337677001953, val loss 218.33950805664062
iter 30, train loss 209.44546508789062, val loss 219.0465087890625
iter 40, train loss 209.33241271972656, val loss 218.56373596191406
iter 50, train loss 209.00906372070312, val loss 217.8552703857422
iter 60, train loss 209.05929565429688, val loss 217.93240356445312
iter 70, train loss 208.99610900878906, val loss 218.74417114257812
iter 80, train loss 208.8371124267578, val loss 218.82339477539062
iter 90, train loss 208.62924194335938, val loss 219.05030822753906
best loss 216.41177368164062
not here
quantized in 31.22598099708557 seconds
36402 MiB free out of 48676 MiB total
21 self_attn.o_proj
Pruning ...
256
iter 0, train loss 10.076169967651367, val loss 12.614686012268066
iter 10, train loss 9.243536949157715, val loss 13.70610237121582
iter 20, train loss 8.95157241821289, val loss 13.601758003234863
iter 30, train loss 8.770912170410156, val loss 13.356490135192871
iter 40, train loss 8.67444896697998, val loss 13.257320404052734
iter 50, train loss 8.626169204711914, val loss 13.283355712890625
iter 60, train loss 8.578418731689453, val loss 13.277196884155273
iter 70, train loss 8.546772003173828, val loss 13.311044692993164
iter 80, train loss 8.541983604431152, val loss 13.128486633300781
iter 90, train loss 8.53237533569336, val loss 13.124942779541016
best loss 12.614686012268066
not here
quantized in 31.87505865097046 seconds
36370 MiB free out of 48676 MiB total
21 mlp.gate_proj
Pruning ...
256
iter 0, train loss 443.26776123046875, val loss 466.36944580078125
iter 10, train loss 461.15643310546875, val loss 521.9273681640625
iter 20, train loss 463.42987060546875, val loss 519.1834716796875
iter 30, train loss 463.7119140625, val loss 518.7391357421875
iter 40, train loss 461.7098388671875, val loss 518.2889404296875
iter 50, train loss 461.98760986328125, val loss 519.780517578125
iter 60, train loss 461.5745849609375, val loss 514.4156494140625
iter 70, train loss 461.39208984375, val loss 498.4051208496094
iter 80, train loss 461.0997314453125, val loss 500.27911376953125
iter 90, train loss 460.849365234375, val loss 503.10284423828125
best loss 466.36944580078125
not here
quantized in 85.74052691459656 seconds
36068 MiB free out of 48676 MiB total
21 mlp.up_proj
Pruning ...
256
iter 0, train loss 371.3258361816406, val loss 396.92645263671875
iter 10, train loss 371.5155029296875, val loss 407.2963562011719
iter 20, train loss 372.8106994628906, val loss 412.6436462402344
iter 30, train loss 373.69049072265625, val loss 421.2469177246094
iter 40, train loss 373.816162109375, val loss 417.8758239746094
iter 50, train loss 374.11053466796875, val loss 422.649169921875
iter 60, train loss 373.805419921875, val loss 419.7948913574219
iter 70, train loss 374.0445556640625, val loss 418.4201354980469
iter 80, train loss 373.95538330078125, val loss 423.7188415527344
iter 90, train loss 374.10595703125, val loss 425.86883544921875
best loss 396.92645263671875
not here
quantized in 84.76225280761719 seconds
35874 MiB free out of 48676 MiB total
21 mlp.down_proj
Pruning ...
256
iter 0, train loss 15.485116958618164, val loss 13.686128616333008
iter 10, train loss 15.475201606750488, val loss 13.694149017333984
iter 20, train loss 15.388873100280762, val loss 13.779251098632812
iter 30, train loss 15.341163635253906, val loss 13.92526626586914
iter 40, train loss 15.298879623413086, val loss 13.67567253112793
iter 50, train loss 15.264162063598633, val loss 13.749135971069336
iter 60, train loss 15.257688522338867, val loss 13.62824821472168
iter 70, train loss 15.244243621826172, val loss 13.952507019042969
iter 80, train loss 15.228848457336426, val loss 13.815780639648438
iter 90, train loss 15.228458404541016, val loss 13.705159187316895
best loss 13.427151679992676
not here
quantized in 91.39548921585083 seconds
35680 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.codebook', 'self_attn.q_proj.quantizer.norms_1', 'self_attn.q_proj.quantizer.norms_0', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.codebook', 'self_attn.k_proj.quantizer.norms_1', 'self_attn.k_proj.quantizer.norms_0', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.codebook', 'self_attn.v_proj.quantizer.norms_1', 'self_attn.v_proj.quantizer.norms_0', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.codebook', 'self_attn.o_proj.quantizer.norms_1', 'self_attn.o_proj.quantizer.norms_0', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.codebook', 'mlp.gate_proj.quantizer.norms_1', 'mlp.gate_proj.quantizer.norms_0', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.codebook', 'mlp.up_proj.quantizer.norms_1', 'mlp.up_proj.quantizer.norms_0', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.codebook', 'mlp.down_proj.quantizer.norms_1', 'mlp.down_proj.quantizer.norms_0', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135936
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.015277317907020915 val loss: 0.031212612171657383
6652 MiB free out of 48676 MiB total
epoch 1 loss: 0.013919597244239412 val loss: 0.032211817684583366
6652 MiB free out of 48676 MiB total
epoch 2 loss: 0.013542711290938314 val loss: 0.03260984888765961
6652 MiB free out of 48676 MiB total
epoch 3 loss: 0.013382511409872677 val loss: 0.0327599763404578
6652 MiB free out of 48676 MiB total
epoch 4 loss: 0.013280376224429347 val loss: 0.03286213625688106
6652 MiB free out of 48676 MiB total
35680 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
6652 MiB free out of 48676 MiB total
after cast to cpu
37850 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
22 self_attn.q_proj
Pruning ...
256
iter 0, train loss 406.37615966796875, val loss 363.13714599609375
iter 10, train loss 434.9996337890625, val loss 431.9434509277344
iter 20, train loss 415.0472717285156, val loss 416.2432556152344
iter 30, train loss 407.856201171875, val loss 412.04583740234375
iter 40, train loss 405.44091796875, val loss 410.07958984375
iter 50, train loss 404.3177185058594, val loss 409.28546142578125
iter 60, train loss 402.7194519042969, val loss 406.8390197753906
iter 70, train loss 403.115234375, val loss 408.6550598144531
iter 80, train loss 401.954833984375, val loss 409.0091857910156
iter 90, train loss 401.5239562988281, val loss 408.05743408203125
best loss 363.13714599609375
not here
quantized in 33.66122651100159 seconds
36422 MiB free out of 48676 MiB total
22 self_attn.k_proj
Pruning ...
256
iter 0, train loss 431.48028564453125, val loss 376.1488037109375
iter 10, train loss 449.86566162109375, val loss 449.61761474609375
iter 20, train loss 437.8352966308594, val loss 442.1484680175781
iter 30, train loss 434.2369689941406, val loss 438.3018493652344
iter 40, train loss 430.5986328125, val loss 434.9947814941406
iter 50, train loss 428.13848876953125, val loss 432.5577087402344
iter 60, train loss 425.22418212890625, val loss 429.10113525390625
iter 70, train loss 424.5256042480469, val loss 429.55548095703125
iter 80, train loss 424.2873840332031, val loss 429.6283874511719
iter 90, train loss 423.6185607910156, val loss 429.7616882324219
best loss 376.1488037109375
not here
quantized in 32.43377709388733 seconds
36412 MiB free out of 48676 MiB total
22 self_attn.v_proj
Pruning ...
256
iter 0, train loss 218.26373291015625, val loss 222.60922241210938
iter 10, train loss 218.40133666992188, val loss 224.90603637695312
iter 20, train loss 217.78753662109375, val loss 225.2078399658203
iter 30, train loss 218.06533813476562, val loss 225.31048583984375
iter 40, train loss 217.7681121826172, val loss 225.81687927246094
iter 50, train loss 217.492919921875, val loss 226.05474853515625
iter 60, train loss 217.3671875, val loss 226.1654052734375
iter 70, train loss 217.45169067382812, val loss 225.578369140625
iter 80, train loss 217.50765991210938, val loss 225.508056640625
iter 90, train loss 217.22317504882812, val loss 225.44102478027344
best loss 222.60922241210938
not here
quantized in 31.1534104347229 seconds
36402 MiB free out of 48676 MiB total
22 self_attn.o_proj
Pruning ...
256
iter 0, train loss 44.46997833251953, val loss 15.904850006103516
iter 10, train loss 34.592735290527344, val loss 18.72362518310547
iter 20, train loss 28.54326629638672, val loss 16.313615798950195
iter 30, train loss 24.53504180908203, val loss 16.191665649414062
iter 40, train loss 22.45848846435547, val loss 16.390270233154297
iter 50, train loss 21.846195220947266, val loss 15.545916557312012
iter 60, train loss 19.713085174560547, val loss 16.736474990844727
iter 70, train loss 18.86521339416504, val loss 16.583393096923828
iter 80, train loss 17.906862258911133, val loss 15.72847843170166
iter 90, train loss 17.860593795776367, val loss 15.763609886169434
best loss 15.318107604980469
not here
quantized in 32.57140231132507 seconds
36402 MiB free out of 48676 MiB total
22 mlp.gate_proj
Pruning ...
256
iter 0, train loss 466.9067687988281, val loss 491.08770751953125
iter 10, train loss 482.95086669921875, val loss 533.20166015625
iter 20, train loss 483.69903564453125, val loss 522.1944580078125
iter 30, train loss 484.3743591308594, val loss 523.3257446289062
iter 40, train loss 482.65826416015625, val loss 532.9461059570312
iter 50, train loss 481.65093994140625, val loss 535.1392211914062
iter 60, train loss 480.78717041015625, val loss 526.35009765625
iter 70, train loss 480.92938232421875, val loss 528.1053466796875
iter 80, train loss 481.0534973144531, val loss 537.3350830078125
iter 90, train loss 480.8591613769531, val loss 530.567138671875
best loss 491.08770751953125
not here
quantized in 85.48161196708679 seconds
36100 MiB free out of 48676 MiB total
22 mlp.up_proj
Pruning ...
256
iter 0, train loss 386.4991455078125, val loss 418.1064758300781
iter 10, train loss 387.3437194824219, val loss 435.958251953125
iter 20, train loss 388.490478515625, val loss 431.74560546875
iter 30, train loss 388.9518737792969, val loss 429.0344543457031
iter 40, train loss 388.9447937011719, val loss 429.2431640625
iter 50, train loss 389.5028381347656, val loss 429.41754150390625
iter 60, train loss 389.6741638183594, val loss 429.85968017578125
iter 70, train loss 390.0628967285156, val loss 434.0400085449219
iter 80, train loss 389.9873352050781, val loss 436.46929931640625
iter 90, train loss 390.03338623046875, val loss 438.1554870605469
best loss 418.1064758300781
not here
quantized in 85.14495468139648 seconds
35906 MiB free out of 48676 MiB total
22 mlp.down_proj
Pruning ...
256
iter 0, train loss 17.015193939208984, val loss 13.477588653564453
iter 10, train loss 17.042110443115234, val loss 13.587629318237305
iter 20, train loss 16.99480438232422, val loss 13.504291534423828
iter 30, train loss 16.984525680541992, val loss 13.297395706176758
iter 40, train loss 16.960811614990234, val loss 13.458805084228516
iter 50, train loss 16.942325592041016, val loss 13.744942665100098
iter 60, train loss 16.937461853027344, val loss 13.535953521728516
iter 70, train loss 16.919448852539062, val loss 13.374136924743652
iter 80, train loss 16.923315048217773, val loss 13.25848388671875
iter 90, train loss 16.920719146728516, val loss 13.386283874511719
best loss 13.173506736755371
not here
quantized in 91.50800895690918 seconds
35712 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.codebook', 'self_attn.q_proj.quantizer.norms_1', 'self_attn.q_proj.quantizer.norms_0', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.codebook', 'self_attn.k_proj.quantizer.norms_1', 'self_attn.k_proj.quantizer.norms_0', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.codebook', 'self_attn.v_proj.quantizer.norms_1', 'self_attn.v_proj.quantizer.norms_0', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.codebook', 'self_attn.o_proj.quantizer.norms_1', 'self_attn.o_proj.quantizer.norms_0', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.codebook', 'mlp.gate_proj.quantizer.norms_1', 'mlp.gate_proj.quantizer.norms_0', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.codebook', 'mlp.up_proj.quantizer.norms_1', 'mlp.up_proj.quantizer.norms_0', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.codebook', 'mlp.down_proj.quantizer.norms_1', 'mlp.down_proj.quantizer.norms_0', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135936
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.022309046355076134 val loss: 0.03076851146761328
7644 MiB free out of 48676 MiB total
epoch 1 loss: 0.0184133076982107 val loss: 0.032105701393447816
7644 MiB free out of 48676 MiB total
epoch 2 loss: 0.017179504880914465 val loss: 0.032648904947564006
7644 MiB free out of 48676 MiB total
epoch 3 loss: 0.016730895615182817 val loss: 0.03281572391279042
7644 MiB free out of 48676 MiB total
epoch 4 loss: 0.016449504750198685 val loss: 0.03297504456713796
7644 MiB free out of 48676 MiB total
35712 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
7644 MiB free out of 48676 MiB total
after cast to cpu
37850 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
23 self_attn.q_proj
Pruning ...
256
iter 0, train loss 443.1966247558594, val loss 409.47918701171875
iter 10, train loss 457.2948913574219, val loss 465.135986328125
iter 20, train loss 441.8704833984375, val loss 451.2575378417969
iter 30, train loss 441.5396423339844, val loss 454.3085632324219
iter 40, train loss 438.73486328125, val loss 452.62274169921875
iter 50, train loss 436.1710205078125, val loss 450.3875732421875
iter 60, train loss 433.277587890625, val loss 446.70904541015625
iter 70, train loss 431.91412353515625, val loss 446.4310302734375
iter 80, train loss 430.5639953613281, val loss 446.6233825683594
iter 90, train loss 430.47967529296875, val loss 446.0624084472656
best loss 409.47918701171875
not here
quantized in 33.54359459877014 seconds
36422 MiB free out of 48676 MiB total
23 self_attn.k_proj
Pruning ...
256
iter 0, train loss 459.0492248535156, val loss 421.6376953125
iter 10, train loss 484.4570617675781, val loss 494.6936340332031
iter 20, train loss 467.8197937011719, val loss 481.40020751953125
iter 30, train loss 456.5037841796875, val loss 473.52252197265625
iter 40, train loss 454.3767395019531, val loss 472.09442138671875
iter 50, train loss 455.5241394042969, val loss 471.2821044921875
iter 60, train loss 454.84283447265625, val loss 469.592041015625
iter 70, train loss 453.9355163574219, val loss 468.2810974121094
iter 80, train loss 452.70648193359375, val loss 468.312744140625
iter 90, train loss 451.60980224609375, val loss 467.56378173828125
best loss 421.6376953125
not here
quantized in 32.41987586021423 seconds
36412 MiB free out of 48676 MiB total
23 self_attn.v_proj
Pruning ...
256
iter 0, train loss 268.7411193847656, val loss 278.3655700683594
iter 10, train loss 268.76934814453125, val loss 280.79296875
iter 20, train loss 268.11688232421875, val loss 281.31884765625
iter 30, train loss 268.19219970703125, val loss 282.2389831542969
iter 40, train loss 267.8939514160156, val loss 281.98583984375
iter 50, train loss 267.6987609863281, val loss 282.67169189453125
iter 60, train loss 267.4444580078125, val loss 282.4419250488281
iter 70, train loss 267.3753662109375, val loss 282.08349609375
iter 80, train loss 267.454833984375, val loss 282.5262451171875
iter 90, train loss 267.40936279296875, val loss 282.3522033691406
best loss 278.3655700683594
not here
quantized in 31.07010817527771 seconds
36402 MiB free out of 48676 MiB total
23 self_attn.o_proj
Pruning ...
256
iter 0, train loss 12.705076217651367, val loss 17.888050079345703
iter 10, train loss 12.51272201538086, val loss 18.957067489624023
iter 20, train loss 12.402728080749512, val loss 18.727781295776367
iter 30, train loss 12.37592601776123, val loss 18.50798225402832
iter 40, train loss 12.348461151123047, val loss 18.857255935668945
iter 50, train loss 12.311474800109863, val loss 18.897178649902344
iter 60, train loss 12.257791519165039, val loss 18.82794952392578
iter 70, train loss 12.23228931427002, val loss 18.780221939086914
iter 80, train loss 12.207950592041016, val loss 19.017559051513672
iter 90, train loss 12.19839859008789, val loss 18.976694107055664
best loss 17.888050079345703
not here
quantized in 31.4426372051239 seconds
36370 MiB free out of 48676 MiB total
23 mlp.gate_proj
Pruning ...
256
iter 0, train loss 505.2078552246094, val loss 559.5682373046875
iter 10, train loss 521.0962524414062, val loss 593.5172119140625
iter 20, train loss 523.9518432617188, val loss 592.7312622070312
iter 30, train loss 522.6380004882812, val loss 594.2666015625
iter 40, train loss 520.9332885742188, val loss 599.1517944335938
iter 50, train loss 520.0750122070312, val loss 597.7220458984375
iter 60, train loss 519.7140502929688, val loss 598.0819091796875
iter 70, train loss 519.873291015625, val loss 606.696533203125
iter 80, train loss 518.7836303710938, val loss 599.93994140625
iter 90, train loss 517.9747314453125, val loss 598.9393310546875
best loss 559.5682373046875
not here
quantized in 107.1631407737732 seconds
36068 MiB free out of 48676 MiB total
23 mlp.up_proj
Pruning ...
256
iter 0, train loss 422.60333251953125, val loss 471.82562255859375
iter 10, train loss 422.8577575683594, val loss 474.7554931640625
iter 20, train loss 423.4372253417969, val loss 485.5238037109375
iter 30, train loss 424.3568115234375, val loss 497.66961669921875
iter 40, train loss 424.70654296875, val loss 500.24444580078125
iter 50, train loss 424.80865478515625, val loss 490.6758728027344
iter 60, train loss 425.01239013671875, val loss 498.27703857421875
iter 70, train loss 424.7146301269531, val loss 501.5115966796875
iter 80, train loss 424.9454345703125, val loss 498.2677307128906
iter 90, train loss 424.83489990234375, val loss 498.1734619140625
best loss 471.54473876953125
not here
quantized in 88.57552862167358 seconds
35874 MiB free out of 48676 MiB total
23 mlp.down_proj
Pruning ...
256
iter 0, train loss 18.863861083984375, val loss 15.848321914672852
iter 10, train loss 18.889728546142578, val loss 15.971271514892578
iter 20, train loss 18.844173431396484, val loss 15.96306037902832
iter 30, train loss 18.834848403930664, val loss 15.8836669921875
iter 40, train loss 18.82767105102539, val loss 15.929252624511719
iter 50, train loss 18.818937301635742, val loss 15.764119148254395
iter 60, train loss 18.814016342163086, val loss 15.992969512939453
iter 70, train loss 18.813302993774414, val loss 15.764095306396484
iter 80, train loss 18.805173873901367, val loss 15.687019348144531
iter 90, train loss 18.80356216430664, val loss 15.853141784667969
best loss 15.660758018493652
not here
quantized in 91.91067385673523 seconds
35680 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.codebook', 'self_attn.q_proj.quantizer.norms_1', 'self_attn.q_proj.quantizer.norms_0', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.codebook', 'self_attn.k_proj.quantizer.norms_1', 'self_attn.k_proj.quantizer.norms_0', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.codebook', 'self_attn.v_proj.quantizer.norms_1', 'self_attn.v_proj.quantizer.norms_0', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.codebook', 'self_attn.o_proj.quantizer.norms_1', 'self_attn.o_proj.quantizer.norms_0', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.codebook', 'mlp.gate_proj.quantizer.norms_1', 'mlp.gate_proj.quantizer.norms_0', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.codebook', 'mlp.up_proj.quantizer.norms_1', 'mlp.up_proj.quantizer.norms_0', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.codebook', 'mlp.down_proj.quantizer.norms_1', 'mlp.down_proj.quantizer.norms_0', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135936
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.019045805471250787 val loss: 0.03496824158355594
6652 MiB free out of 48676 MiB total
epoch 1 loss: 0.017977290342969354 val loss: 0.035186412977054715
6652 MiB free out of 48676 MiB total
epoch 2 loss: 0.017632844705076423 val loss: 0.03524245321750641
6652 MiB free out of 48676 MiB total
epoch 3 loss: 0.017478208304964937 val loss: 0.03525464283302426
6652 MiB free out of 48676 MiB total
epoch 4 loss: 0.017381931262207218 val loss: 0.0352684601675719
6652 MiB free out of 48676 MiB total
35680 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
6652 MiB free out of 48676 MiB total
after cast to cpu
37850 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
24 self_attn.q_proj
Pruning ...
256
iter 0, train loss 408.9219665527344, val loss 373.41473388671875
iter 10, train loss 423.4563903808594, val loss 432.9325256347656
iter 20, train loss 406.60797119140625, val loss 413.3194885253906
iter 30, train loss 404.1849060058594, val loss 412.2547302246094
iter 40, train loss 402.8651428222656, val loss 410.8544921875
iter 50, train loss 402.45953369140625, val loss 411.38995361328125
iter 60, train loss 400.7016296386719, val loss 410.2178955078125
iter 70, train loss 400.2024230957031, val loss 410.5788879394531
iter 80, train loss 399.24871826171875, val loss 410.0091857910156
iter 90, train loss 399.8121337890625, val loss 409.6440734863281
best loss 373.41473388671875
not here
quantized in 33.59713435173035 seconds
36422 MiB free out of 48676 MiB total
24 self_attn.k_proj
Pruning ...
256
iter 0, train loss 425.44451904296875, val loss 382.101318359375
iter 10, train loss 442.69488525390625, val loss 448.12030029296875
iter 20, train loss 432.515869140625, val loss 441.52069091796875
iter 30, train loss 426.96710205078125, val loss 437.3737487792969
iter 40, train loss 422.528076171875, val loss 434.0638427734375
iter 50, train loss 418.68109130859375, val loss 431.4519348144531
iter 60, train loss 416.5099792480469, val loss 428.82843017578125
iter 70, train loss 416.0295104980469, val loss 428.18829345703125
iter 80, train loss 415.63006591796875, val loss 428.4026184082031
iter 90, train loss 415.2159423828125, val loss 428.125244140625
best loss 382.101318359375
not here
quantized in 32.22758078575134 seconds
36412 MiB free out of 48676 MiB total
24 self_attn.v_proj
Pruning ...
256
iter 0, train loss 256.8497009277344, val loss 267.0715637207031
iter 10, train loss 256.206787109375, val loss 267.9532775878906
iter 20, train loss 256.009033203125, val loss 268.7810974121094
iter 30, train loss 255.95050048828125, val loss 267.91302490234375
iter 40, train loss 255.20050048828125, val loss 266.9488220214844
iter 50, train loss 255.1323699951172, val loss 267.87066650390625
iter 60, train loss 254.7157440185547, val loss 267.9205627441406
iter 70, train loss 254.7332000732422, val loss 267.56488037109375
iter 80, train loss 254.76458740234375, val loss 267.8088684082031
iter 90, train loss 254.8384552001953, val loss 267.89447021484375
best loss 266.69610595703125
not here
quantized in 31.173175573349 seconds
36434 MiB free out of 48676 MiB total
24 self_attn.o_proj
Pruning ...
256
iter 0, train loss 25.94623374938965, val loss 17.13353729248047
iter 10, train loss 20.75750732421875, val loss 19.221817016601562
iter 20, train loss 18.658920288085938, val loss 19.383283615112305
iter 30, train loss 17.004798889160156, val loss 19.606712341308594
iter 40, train loss 16.436134338378906, val loss 18.984046936035156
iter 50, train loss 16.074731826782227, val loss 18.839908599853516
iter 60, train loss 15.934981346130371, val loss 18.487186431884766
iter 70, train loss 15.742218017578125, val loss 18.710020065307617
iter 80, train loss 15.591935157775879, val loss 18.613567352294922
iter 90, train loss 15.493045806884766, val loss 18.20462417602539
best loss 16.96261978149414
not here
quantized in 31.97813844680786 seconds
36402 MiB free out of 48676 MiB total
24 mlp.gate_proj
Pruning ...
256
iter 0, train loss 532.637451171875, val loss 582.5950927734375
iter 10, train loss 549.9530639648438, val loss 639.8065185546875
iter 20, train loss 550.560302734375, val loss 621.5226440429688
iter 30, train loss 549.3200073242188, val loss 625.4288330078125
iter 40, train loss 547.591064453125, val loss 623.8972778320312
iter 50, train loss 547.3419189453125, val loss 631.3423461914062
iter 60, train loss 548.0252685546875, val loss 620.9403686523438
iter 70, train loss 547.55517578125, val loss 622.2992553710938
iter 80, train loss 548.0224609375, val loss 631.4476318359375
iter 90, train loss 547.8753662109375, val loss 634.0993041992188
best loss 582.5950927734375
not here
quantized in 84.93922305107117 seconds
36100 MiB free out of 48676 MiB total
24 mlp.up_proj
Pruning ...
256
iter 0, train loss 445.5783996582031, val loss 516.5015869140625
iter 10, train loss 446.3587646484375, val loss 513.6083374023438
iter 20, train loss 447.55987548828125, val loss 527.8768920898438
iter 30, train loss 448.7625732421875, val loss 522.7010498046875
iter 40, train loss 448.994873046875, val loss 526.1287841796875
iter 50, train loss 449.3128662109375, val loss 526.3569946289062
iter 60, train loss 449.8414306640625, val loss 524.443359375
iter 70, train loss 450.18365478515625, val loss 529.4186401367188
iter 80, train loss 450.199462890625, val loss 530.5489501953125
iter 90, train loss 450.05975341796875, val loss 535.0642700195312
best loss 511.83319091796875
not here
quantized in 84.19983696937561 seconds
35906 MiB free out of 48676 MiB total
24 mlp.down_proj
Pruning ...
256
iter 0, train loss 20.141021728515625, val loss 20.381858825683594
iter 10, train loss 20.166337966918945, val loss 20.045516967773438
iter 20, train loss 20.10443115234375, val loss 20.202777862548828
iter 30, train loss 20.078533172607422, val loss 19.911376953125
iter 40, train loss 20.069313049316406, val loss 20.004573822021484
iter 50, train loss 20.059432983398438, val loss 19.847679138183594
iter 60, train loss 20.058059692382812, val loss 20.144760131835938
iter 70, train loss 20.053869247436523, val loss 19.902624130249023
iter 80, train loss 20.044246673583984, val loss 19.736112594604492
iter 90, train loss 20.037757873535156, val loss 19.47970199584961
best loss 19.396211624145508
not here
quantized in 91.58273029327393 seconds
35712 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.codebook', 'self_attn.q_proj.quantizer.norms_1', 'self_attn.q_proj.quantizer.norms_0', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.codebook', 'self_attn.k_proj.quantizer.norms_1', 'self_attn.k_proj.quantizer.norms_0', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.codebook', 'self_attn.v_proj.quantizer.norms_1', 'self_attn.v_proj.quantizer.norms_0', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.codebook', 'self_attn.o_proj.quantizer.norms_1', 'self_attn.o_proj.quantizer.norms_0', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.codebook', 'mlp.gate_proj.quantizer.norms_1', 'mlp.gate_proj.quantizer.norms_0', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.codebook', 'mlp.up_proj.quantizer.norms_1', 'mlp.up_proj.quantizer.norms_0', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.codebook', 'mlp.down_proj.quantizer.norms_1', 'mlp.down_proj.quantizer.norms_0', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135936
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.021852192032383755 val loss: 0.04196386621333659
7644 MiB free out of 48676 MiB total
epoch 1 loss: 0.020434710255358368 val loss: 0.04230706510134041
7644 MiB free out of 48676 MiB total
epoch 2 loss: 0.019930539463530295 val loss: 0.04239709326066077
7644 MiB free out of 48676 MiB total
epoch 3 loss: 0.01968666084576398 val loss: 0.0423749559558928
7644 MiB free out of 48676 MiB total
epoch 4 loss: 0.0195286428934196 val loss: 0.042346875881776214
7644 MiB free out of 48676 MiB total
35712 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
7644 MiB free out of 48676 MiB total
after cast to cpu
37850 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
25 self_attn.q_proj
Pruning ...
256
iter 0, train loss 478.30810546875, val loss 450.04217529296875
iter 10, train loss 492.00433349609375, val loss 509.50244140625
iter 20, train loss 470.4220275878906, val loss 487.482177734375
iter 30, train loss 475.257080078125, val loss 493.2283020019531
iter 40, train loss 475.44482421875, val loss 494.1864013671875
iter 50, train loss 471.1944885253906, val loss 492.71356201171875
iter 60, train loss 469.78240966796875, val loss 491.9378662109375
iter 70, train loss 469.1874084472656, val loss 490.7110595703125
iter 80, train loss 468.34674072265625, val loss 489.0419616699219
iter 90, train loss 467.70843505859375, val loss 489.2114562988281
best loss 450.04217529296875
not here
quantized in 33.40424871444702 seconds
36422 MiB free out of 48676 MiB total
25 self_attn.k_proj
Pruning ...
256
iter 0, train loss 493.15625, val loss 460.05419921875
iter 10, train loss 518.6717529296875, val loss 529.8050537109375
iter 20, train loss 495.2495422363281, val loss 515.7167358398438
iter 30, train loss 487.7672119140625, val loss 508.586181640625
iter 40, train loss 487.41595458984375, val loss 509.6513671875
iter 50, train loss 485.8905944824219, val loss 507.03424072265625
iter 60, train loss 485.77996826171875, val loss 508.6143493652344
iter 70, train loss 483.8280334472656, val loss 506.89239501953125
iter 80, train loss 482.62042236328125, val loss 507.1570129394531
iter 90, train loss 483.0531311035156, val loss 507.1634826660156
best loss 460.05419921875
not here
quantized in 32.28330755233765 seconds
36412 MiB free out of 48676 MiB total
25 self_attn.v_proj
Pruning ...
256
iter 0, train loss 324.23138427734375, val loss 342.6494445800781
iter 10, train loss 324.1773986816406, val loss 345.4072570800781
iter 20, train loss 323.7755126953125, val loss 346.9547119140625
iter 30, train loss 323.6031188964844, val loss 346.645751953125
iter 40, train loss 323.0939025878906, val loss 346.1676940917969
iter 50, train loss 323.185791015625, val loss 346.80322265625
iter 60, train loss 323.15826416015625, val loss 346.80010986328125
iter 70, train loss 323.2010498046875, val loss 346.2168273925781
iter 80, train loss 322.97564697265625, val loss 345.86566162109375
iter 90, train loss 322.8756103515625, val loss 345.86376953125
best loss 342.6494445800781
not here
quantized in 31.335615873336792 seconds
36402 MiB free out of 48676 MiB total
25 self_attn.o_proj
Pruning ...
256
iter 0, train loss 16.240131378173828, val loss 25.421550750732422
iter 10, train loss 16.58472442626953, val loss 27.51825714111328
iter 20, train loss 16.70477867126465, val loss 27.904104232788086
iter 30, train loss 16.441078186035156, val loss 26.386375427246094
iter 40, train loss 16.35144805908203, val loss 26.6182918548584
iter 50, train loss 16.246715545654297, val loss 26.271141052246094
iter 60, train loss 16.245798110961914, val loss 26.637731552124023
iter 70, train loss 16.283437728881836, val loss 27.690349578857422
iter 80, train loss 16.146442413330078, val loss 27.520999908447266
iter 90, train loss 16.10026741027832, val loss 27.567440032958984
best loss 25.202638626098633
not here
quantized in 31.61786198616028 seconds
36370 MiB free out of 48676 MiB total
25 mlp.gate_proj
Pruning ...
256
iter 0, train loss 577.4984130859375, val loss 618.2767333984375
iter 10, train loss 595.38916015625, val loss 669.9713134765625
iter 20, train loss 599.674072265625, val loss 685.458251953125
iter 30, train loss 597.5773315429688, val loss 675.166748046875
iter 40, train loss 595.6943359375, val loss 679.26025390625
iter 50, train loss 595.0040283203125, val loss 673.0830078125
iter 60, train loss 594.4263305664062, val loss 672.9249267578125
iter 70, train loss 594.6717529296875, val loss 671.6553955078125
iter 80, train loss 594.7227172851562, val loss 674.1827392578125
iter 90, train loss 594.2591552734375, val loss 671.9002685546875
best loss 618.2767333984375
not here
quantized in 85.30333590507507 seconds
36068 MiB free out of 48676 MiB total
25 mlp.up_proj
Pruning ...
256
iter 0, train loss 484.4946594238281, val loss 535.8045654296875
iter 10, train loss 485.18878173828125, val loss 545.126708984375
iter 20, train loss 485.6326599121094, val loss 549.33154296875
iter 30, train loss 486.3514404296875, val loss 545.6962280273438
iter 40, train loss 486.8788146972656, val loss 548.422119140625
iter 50, train loss 486.6906433105469, val loss 550.4454345703125
iter 60, train loss 486.7777099609375, val loss 554.9151000976562
iter 70, train loss 486.9529724121094, val loss 563.59912109375
iter 80, train loss 486.8829345703125, val loss 564.1209716796875
iter 90, train loss 486.5689392089844, val loss 566.3256225585938
best loss 535.8045654296875
not here
quantized in 84.47900390625 seconds
35874 MiB free out of 48676 MiB total
25 mlp.down_proj
Pruning ...
256
iter 0, train loss 21.645633697509766, val loss 21.256515502929688
iter 10, train loss 21.683616638183594, val loss 21.33087921142578
iter 20, train loss 21.603748321533203, val loss 21.026573181152344
iter 30, train loss 21.596500396728516, val loss 21.59792137145996
iter 40, train loss 21.57763671875, val loss 21.119747161865234
iter 50, train loss 21.582027435302734, val loss 21.43648338317871
iter 60, train loss 21.565662384033203, val loss 21.369340896606445
iter 70, train loss 21.557437896728516, val loss 21.02326011657715
iter 80, train loss 21.560117721557617, val loss 21.095542907714844
iter 90, train loss 21.548734664916992, val loss 20.978187561035156
best loss 20.82508087158203
not here
quantized in 91.23954129219055 seconds
35680 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.codebook', 'self_attn.q_proj.quantizer.norms_1', 'self_attn.q_proj.quantizer.norms_0', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.codebook', 'self_attn.k_proj.quantizer.norms_1', 'self_attn.k_proj.quantizer.norms_0', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.codebook', 'self_attn.v_proj.quantizer.norms_1', 'self_attn.v_proj.quantizer.norms_0', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.codebook', 'self_attn.o_proj.quantizer.norms_1', 'self_attn.o_proj.quantizer.norms_0', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.codebook', 'mlp.gate_proj.quantizer.norms_1', 'mlp.gate_proj.quantizer.norms_0', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.codebook', 'mlp.up_proj.quantizer.norms_1', 'mlp.up_proj.quantizer.norms_0', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.codebook', 'mlp.down_proj.quantizer.norms_1', 'mlp.down_proj.quantizer.norms_0', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135936
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.024627545222756453 val loss: 0.047949393978342414
6652 MiB free out of 48676 MiB total
epoch 1 loss: 0.02269216721470002 val loss: 0.04799533216282725
6652 MiB free out of 48676 MiB total
epoch 2 loss: 0.021991440298734233 val loss: 0.048035801853984594
6652 MiB free out of 48676 MiB total
epoch 3 loss: 0.021637551035382785 val loss: 0.04802154377102852
6652 MiB free out of 48676 MiB total
epoch 4 loss: 0.021416979594505392 val loss: 0.04798057908192277
6652 MiB free out of 48676 MiB total
35680 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
6652 MiB free out of 48676 MiB total
after cast to cpu
37850 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
26 self_attn.q_proj
Pruning ...
256
iter 0, train loss 451.37359619140625, val loss 409.80059814453125
iter 10, train loss 476.6145935058594, val loss 478.463134765625
iter 20, train loss 459.3523254394531, val loss 466.76690673828125
iter 30, train loss 451.12255859375, val loss 460.6905822753906
iter 40, train loss 443.985595703125, val loss 452.84893798828125
iter 50, train loss 441.28851318359375, val loss 451.5676574707031
iter 60, train loss 442.8177490234375, val loss 452.8473815917969
iter 70, train loss 442.3097839355469, val loss 453.6922912597656
iter 80, train loss 440.8710021972656, val loss 452.412353515625
iter 90, train loss 439.86578369140625, val loss 451.2582702636719
best loss 409.80059814453125
not here
quantized in 33.57728099822998 seconds
36422 MiB free out of 48676 MiB total
26 self_attn.k_proj
Pruning ...
256
iter 0, train loss 478.29815673828125, val loss 422.8655090332031
iter 10, train loss 504.40704345703125, val loss 505.80047607421875
iter 20, train loss 484.911865234375, val loss 493.8851318359375
iter 30, train loss 477.3961181640625, val loss 487.9473876953125
iter 40, train loss 471.2435607910156, val loss 481.6715393066406
iter 50, train loss 468.71636962890625, val loss 478.0572509765625
iter 60, train loss 466.15252685546875, val loss 477.23199462890625
iter 70, train loss 464.9391784667969, val loss 477.96502685546875
iter 80, train loss 463.859130859375, val loss 477.1756286621094
iter 90, train loss 462.6209411621094, val loss 476.6409912109375
best loss 422.8655090332031
not here
quantized in 32.719858169555664 seconds
36412 MiB free out of 48676 MiB total
26 self_attn.v_proj
Pruning ...
256
iter 0, train loss 318.3269958496094, val loss 328.46820068359375
iter 10, train loss 318.1575622558594, val loss 330.9598083496094
iter 20, train loss 317.9053955078125, val loss 330.7480163574219
iter 30, train loss 317.7972106933594, val loss 331.91241455078125
iter 40, train loss 317.71783447265625, val loss 331.33868408203125
iter 50, train loss 317.36236572265625, val loss 332.0254821777344
iter 60, train loss 316.9907531738281, val loss 331.1971435546875
iter 70, train loss 316.56591796875, val loss 330.838623046875
iter 80, train loss 316.66802978515625, val loss 330.24713134765625
iter 90, train loss 316.66650390625, val loss 330.7294616699219
best loss 328.46820068359375
not here
quantized in 31.213799238204956 seconds
36402 MiB free out of 48676 MiB total
26 self_attn.o_proj
Pruning ...
256
iter 0, train loss 31.605730056762695, val loss 25.964950561523438
iter 10, train loss 23.78213119506836, val loss 29.900686264038086
iter 20, train loss 22.156700134277344, val loss 28.905044555664062
iter 30, train loss 20.502927780151367, val loss 28.3072509765625
iter 40, train loss 20.588911056518555, val loss 28.91973304748535
iter 50, train loss 20.244129180908203, val loss 28.247966766357422
iter 60, train loss 19.896224975585938, val loss 27.9274845123291
iter 70, train loss 19.775798797607422, val loss 28.272567749023438
iter 80, train loss 19.642993927001953, val loss 27.92131805419922
iter 90, train loss 19.602951049804688, val loss 27.66448211669922
best loss 25.964950561523438
not here
quantized in 31.948827028274536 seconds
36370 MiB free out of 48676 MiB total
26 mlp.gate_proj
Pruning ...
256
iter 0, train loss 609.00048828125, val loss 655.5263061523438
iter 10, train loss 632.4888916015625, val loss 718.3206787109375
iter 20, train loss 642.44775390625, val loss 743.66162109375
iter 30, train loss 640.1343994140625, val loss 741.226806640625
iter 40, train loss 639.1561889648438, val loss 735.875732421875
iter 50, train loss 637.62158203125, val loss 719.4462890625
iter 60, train loss 636.1646728515625, val loss 723.1268310546875
iter 70, train loss 635.040771484375, val loss 723.6688232421875
iter 80, train loss 633.5228881835938, val loss 720.963623046875
iter 90, train loss 633.01025390625, val loss 724.8282470703125
best loss 655.5263061523438
not here
quantized in 85.46012711524963 seconds
36068 MiB free out of 48676 MiB total
26 mlp.up_proj
Pruning ...
256
iter 0, train loss 512.7593994140625, val loss 573.175048828125
iter 10, train loss 513.8388671875, val loss 580.8370361328125
iter 20, train loss 515.0357666015625, val loss 587.4003295898438
iter 30, train loss 515.3884887695312, val loss 588.0009765625
iter 40, train loss 515.71435546875, val loss 596.9632568359375
iter 50, train loss 515.819091796875, val loss 598.7908325195312
iter 60, train loss 516.1538696289062, val loss 597.52099609375
iter 70, train loss 516.2899169921875, val loss 595.187255859375
iter 80, train loss 515.8773193359375, val loss 595.5692138671875
iter 90, train loss 515.938720703125, val loss 599.3201904296875
best loss 573.175048828125
not here
quantized in 85.55715656280518 seconds
35874 MiB free out of 48676 MiB total
26 mlp.down_proj
Pruning ...
256
iter 0, train loss 23.406007766723633, val loss 24.361318588256836
iter 10, train loss 23.420387268066406, val loss 24.5366153717041
iter 20, train loss 23.35713005065918, val loss 24.164321899414062
iter 30, train loss 23.33194923400879, val loss 24.58050537109375
iter 40, train loss 23.29761505126953, val loss 24.716930389404297
iter 50, train loss 23.290246963500977, val loss 24.410287857055664
iter 60, train loss 23.305097579956055, val loss 23.948410034179688
iter 70, train loss 23.281034469604492, val loss 24.09200668334961
iter 80, train loss 23.280414581298828, val loss 24.225440979003906
iter 90, train loss 23.277694702148438, val loss 24.149856567382812
best loss 23.79680633544922
not here
quantized in 91.8332257270813 seconds
35680 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.codebook', 'self_attn.q_proj.quantizer.norms_1', 'self_attn.q_proj.quantizer.norms_0', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.codebook', 'self_attn.k_proj.quantizer.norms_1', 'self_attn.k_proj.quantizer.norms_0', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.codebook', 'self_attn.v_proj.quantizer.norms_1', 'self_attn.v_proj.quantizer.norms_0', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.codebook', 'self_attn.o_proj.quantizer.norms_1', 'self_attn.o_proj.quantizer.norms_0', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.codebook', 'mlp.gate_proj.quantizer.norms_1', 'mlp.gate_proj.quantizer.norms_0', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.codebook', 'mlp.up_proj.quantizer.norms_1', 'mlp.up_proj.quantizer.norms_0', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.codebook', 'mlp.down_proj.quantizer.norms_1', 'mlp.down_proj.quantizer.norms_0', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135936
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.02858113290858455 val loss: 0.05475659086368978
6652 MiB free out of 48676 MiB total
epoch 1 loss: 0.025852110789855942 val loss: 0.056130012730136514
6652 MiB free out of 48676 MiB total
epoch 2 loss: 0.02487082575680688 val loss: 0.056649538455531
6652 MiB free out of 48676 MiB total
epoch 3 loss: 0.024453535661450587 val loss: 0.056709669064730406
6652 MiB free out of 48676 MiB total
epoch 4 loss: 0.024186811759136617 val loss: 0.0567121731583029
6652 MiB free out of 48676 MiB total
35680 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
6652 MiB free out of 48676 MiB total
after cast to cpu
37850 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
27 self_attn.q_proj
Pruning ...
256
iter 0, train loss 497.9011535644531, val loss 471.8323974609375
iter 10, train loss 533.4124145507812, val loss 551.0872802734375
iter 20, train loss 506.5101013183594, val loss 532.859619140625
iter 30, train loss 496.30731201171875, val loss 525.564208984375
iter 40, train loss 492.52337646484375, val loss 520.8373413085938
iter 50, train loss 490.47314453125, val loss 518.6547241210938
iter 60, train loss 490.9195861816406, val loss 519.3262939453125
iter 70, train loss 489.41278076171875, val loss 517.717529296875
iter 80, train loss 489.2143859863281, val loss 518.4872436523438
iter 90, train loss 489.7081298828125, val loss 518.5877685546875
best loss 471.8323974609375
not here
quantized in 33.64155459403992 seconds
36422 MiB free out of 48676 MiB total
27 self_attn.k_proj
Pruning ...
256
iter 0, train loss 540.833740234375, val loss 485.5146789550781
iter 10, train loss 569.805908203125, val loss 596.010009765625
iter 20, train loss 540.286376953125, val loss 567.8330688476562
iter 30, train loss 524.7779541015625, val loss 552.3187255859375
iter 40, train loss 525.0480346679688, val loss 553.3863525390625
iter 50, train loss 524.8525390625, val loss 552.7909545898438
iter 60, train loss 523.40576171875, val loss 551.837158203125
iter 70, train loss 520.392333984375, val loss 550.1202392578125
iter 80, train loss 518.808349609375, val loss 546.8260498046875
iter 90, train loss 515.9973754882812, val loss 544.1334228515625
best loss 485.5146789550781
not here
quantized in 32.513657331466675 seconds
36412 MiB free out of 48676 MiB total
27 self_attn.v_proj
Pruning ...
256
iter 0, train loss 336.8384704589844, val loss 351.7866516113281
iter 10, train loss 337.475341796875, val loss 357.33245849609375
iter 20, train loss 337.4122314453125, val loss 358.75677490234375
iter 30, train loss 336.62994384765625, val loss 357.3765869140625
iter 40, train loss 335.7630920410156, val loss 357.99761962890625
iter 50, train loss 335.72216796875, val loss 358.3486328125
iter 60, train loss 335.81195068359375, val loss 359.2886962890625
iter 70, train loss 335.0989685058594, val loss 358.29864501953125
iter 80, train loss 335.2158203125, val loss 358.5506591796875
iter 90, train loss 335.0597229003906, val loss 358.4779052734375
best loss 351.7866516113281
not here
quantized in 31.237299919128418 seconds
36402 MiB free out of 48676 MiB total
27 self_attn.o_proj
Pruning ...
256
iter 0, train loss 20.920434951782227, val loss 27.47446060180664
iter 10, train loss 19.597747802734375, val loss 29.759483337402344
iter 20, train loss 18.991832733154297, val loss 30.267414093017578
iter 30, train loss 18.614545822143555, val loss 30.454429626464844
iter 40, train loss 18.435161590576172, val loss 29.923076629638672
iter 50, train loss 18.272336959838867, val loss 29.176584243774414
iter 60, train loss 18.17066192626953, val loss 29.272579193115234
iter 70, train loss 18.044219970703125, val loss 29.551132202148438
iter 80, train loss 17.9881591796875, val loss 29.502086639404297
iter 90, train loss 17.9451904296875, val loss 29.032772064208984
best loss 27.47446060180664
not here
quantized in 31.699301958084106 seconds
36370 MiB free out of 48676 MiB total
27 mlp.gate_proj
Pruning ...
256
iter 0, train loss 660.4288330078125, val loss 734.8939819335938
iter 10, train loss 687.8931884765625, val loss 800.4419555664062
iter 20, train loss 687.7484741210938, val loss 803.3020629882812
iter 30, train loss 688.7152709960938, val loss 794.4625244140625
iter 40, train loss 691.0003051757812, val loss 789.4077758789062
iter 50, train loss 688.9451904296875, val loss 794.0894165039062
iter 60, train loss 687.751708984375, val loss 788.989013671875
iter 70, train loss 685.6943969726562, val loss 794.9031982421875
iter 80, train loss 684.587158203125, val loss 796.3703002929688
iter 90, train loss 684.008544921875, val loss 799.9594116210938
best loss 732.78662109375
not here
quantized in 86.31367802619934 seconds
36068 MiB free out of 48676 MiB total
27 mlp.up_proj
Pruning ...
256
iter 0, train loss 560.8505249023438, val loss 629.4555053710938
iter 10, train loss 561.5269775390625, val loss 644.470703125
iter 20, train loss 562.60595703125, val loss 653.5680541992188
iter 30, train loss 562.774169921875, val loss 658.1593017578125
iter 40, train loss 563.6844482421875, val loss 670.1338500976562
iter 50, train loss 564.137939453125, val loss 671.0037231445312
iter 60, train loss 563.9678955078125, val loss 674.9915771484375
iter 70, train loss 563.4892578125, val loss 668.1651611328125
iter 80, train loss 563.5206298828125, val loss 673.5344848632812
iter 90, train loss 563.8317260742188, val loss 672.446044921875
best loss 623.8382568359375
not here
quantized in 85.50823640823364 seconds
35874 MiB free out of 48676 MiB total
27 mlp.down_proj
Pruning ...
256
iter 0, train loss 27.54241371154785, val loss 33.11737823486328
iter 10, train loss 27.559995651245117, val loss 33.209110260009766
iter 20, train loss 27.487403869628906, val loss 33.18560028076172
iter 30, train loss 27.518310546875, val loss 32.95363235473633
iter 40, train loss 27.511817932128906, val loss 33.89622497558594
iter 50, train loss 27.501543045043945, val loss 33.35008239746094
iter 60, train loss 27.507957458496094, val loss 34.07741928100586
iter 70, train loss 27.494625091552734, val loss 33.963565826416016
iter 80, train loss 27.474319458007812, val loss 33.38471221923828
iter 90, train loss 27.454708099365234, val loss 33.711177825927734
best loss 32.95363235473633
not here
quantized in 91.9263424873352 seconds
35680 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.codebook', 'self_attn.q_proj.quantizer.norms_1', 'self_attn.q_proj.quantizer.norms_0', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.codebook', 'self_attn.k_proj.quantizer.norms_1', 'self_attn.k_proj.quantizer.norms_0', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.codebook', 'self_attn.v_proj.quantizer.norms_1', 'self_attn.v_proj.quantizer.norms_0', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.codebook', 'self_attn.o_proj.quantizer.norms_1', 'self_attn.o_proj.quantizer.norms_0', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.codebook', 'mlp.gate_proj.quantizer.norms_1', 'mlp.gate_proj.quantizer.norms_0', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.codebook', 'mlp.up_proj.quantizer.norms_1', 'mlp.up_proj.quantizer.norms_0', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.codebook', 'mlp.down_proj.quantizer.norms_1', 'mlp.down_proj.quantizer.norms_0', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135936
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.029793698427965865 val loss: 0.0727021899074316
6652 MiB free out of 48676 MiB total
epoch 1 loss: 0.027761716846725903 val loss: 0.07328238105401397
6652 MiB free out of 48676 MiB total
epoch 2 loss: 0.027169924695044756 val loss: 0.07336872955784202
6652 MiB free out of 48676 MiB total
epoch 3 loss: 0.02686676381563302 val loss: 0.07330297119915485
6652 MiB free out of 48676 MiB total
epoch 4 loss: 0.02665773253829684 val loss: 0.07324868300929666
6652 MiB free out of 48676 MiB total
35680 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
6652 MiB free out of 48676 MiB total
after cast to cpu
37850 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
28 self_attn.q_proj
Pruning ...
256
iter 0, train loss 496.980712890625, val loss 459.393798828125
iter 10, train loss 529.7301025390625, val loss 546.8365478515625
iter 20, train loss 506.6412048339844, val loss 523.37744140625
iter 30, train loss 496.33209228515625, val loss 517.1541748046875
iter 40, train loss 492.8371887207031, val loss 513.4957275390625
iter 50, train loss 493.2493896484375, val loss 514.4489135742188
iter 60, train loss 489.98895263671875, val loss 512.0421752929688
iter 70, train loss 489.00701904296875, val loss 510.60528564453125
iter 80, train loss 487.4078369140625, val loss 510.1159973144531
iter 90, train loss 487.13165283203125, val loss 510.7166748046875
best loss 459.393798828125
not here
quantized in 33.84863305091858 seconds
36422 MiB free out of 48676 MiB total
28 self_attn.k_proj
Pruning ...
256
iter 0, train loss 535.8515014648438, val loss 470.5982360839844
iter 10, train loss 571.0340576171875, val loss 590.1007690429688
iter 20, train loss 545.6726684570312, val loss 568.5386962890625
iter 30, train loss 534.2200317382812, val loss 556.27197265625
iter 40, train loss 525.4208984375, val loss 546.7176513671875
iter 50, train loss 524.4586181640625, val loss 546.21044921875
iter 60, train loss 522.510009765625, val loss 543.331787109375
iter 70, train loss 519.4974975585938, val loss 540.7423095703125
iter 80, train loss 517.0316772460938, val loss 537.809814453125
iter 90, train loss 514.9608764648438, val loss 535.7537841796875
best loss 470.5982360839844
not here
quantized in 33.328431606292725 seconds
36412 MiB free out of 48676 MiB total
28 self_attn.v_proj
Pruning ...
256
iter 0, train loss 370.93963623046875, val loss 386.12359619140625
iter 10, train loss 371.9602355957031, val loss 390.95404052734375
iter 20, train loss 371.5239562988281, val loss 392.64013671875
iter 30, train loss 371.2741394042969, val loss 392.49139404296875
iter 40, train loss 371.15399169921875, val loss 392.27740478515625
iter 50, train loss 371.29498291015625, val loss 392.196044921875
iter 60, train loss 370.187744140625, val loss 392.0164794921875
iter 70, train loss 369.5509033203125, val loss 392.2080383300781
iter 80, train loss 369.417724609375, val loss 392.6033630371094
iter 90, train loss 369.26995849609375, val loss 392.60614013671875
best loss 386.12359619140625
not here
quantized in 31.434101104736328 seconds
36402 MiB free out of 48676 MiB total
28 self_attn.o_proj
Pruning ...
256
iter 0, train loss 29.062646865844727, val loss 35.329620361328125
iter 10, train loss 28.214818954467773, val loss 37.925453186035156
iter 20, train loss 28.086055755615234, val loss 37.94541549682617
iter 30, train loss 27.951168060302734, val loss 37.42524719238281
iter 40, train loss 27.73532485961914, val loss 37.97978210449219
iter 50, train loss 27.556686401367188, val loss 37.99267578125
iter 60, train loss 27.402896881103516, val loss 38.710792541503906
iter 70, train loss 27.353256225585938, val loss 38.15991973876953
iter 80, train loss 27.361793518066406, val loss 37.91844940185547
iter 90, train loss 27.24175262451172, val loss 37.8162727355957
best loss 35.329620361328125
not here
quantized in 31.561452865600586 seconds
36370 MiB free out of 48676 MiB total
28 mlp.gate_proj
Pruning ...
256
iter 0, train loss 694.4447631835938, val loss 752.1550903320312
iter 10, train loss 722.2637939453125, val loss 820.3849487304688
iter 20, train loss 722.8070068359375, val loss 825.3387451171875
iter 30, train loss 719.552001953125, val loss 812.7734375
iter 40, train loss 716.5982055664062, val loss 806.79150390625
iter 50, train loss 715.5303955078125, val loss 816.3497314453125
iter 60, train loss 715.6204833984375, val loss 809.303466796875
iter 70, train loss 713.6124877929688, val loss 810.1217041015625
iter 80, train loss 712.7798461914062, val loss 808.8291015625
iter 90, train loss 712.96435546875, val loss 808.357177734375
best loss 752.1550903320312
not here
quantized in 85.27769589424133 seconds
36068 MiB free out of 48676 MiB total
28 mlp.up_proj
Pruning ...
256
iter 0, train loss 616.12841796875, val loss 677.2150268554688
iter 10, train loss 623.686279296875, val loss 726.1898193359375
iter 20, train loss 617.2618408203125, val loss 716.9047241210938
iter 30, train loss 615.3901977539062, val loss 719.7258911132812
iter 40, train loss 615.458984375, val loss 717.7301025390625
iter 50, train loss 616.0255126953125, val loss 709.5350952148438
iter 60, train loss 615.458984375, val loss 713.0694580078125
iter 70, train loss 616.3983154296875, val loss 710.174072265625
iter 80, train loss 616.94580078125, val loss 712.8668212890625
iter 90, train loss 616.5545654296875, val loss 719.862548828125
best loss 677.2150268554688
not here
quantized in 86.45746874809265 seconds
35874 MiB free out of 48676 MiB total
28 mlp.down_proj
Pruning ...
256
iter 0, train loss 33.11640167236328, val loss 38.803306579589844
iter 10, train loss 33.2181396484375, val loss 39.361968994140625
iter 20, train loss 33.12177658081055, val loss 39.58213424682617
iter 30, train loss 33.048343658447266, val loss 38.94795227050781
iter 40, train loss 33.001068115234375, val loss 39.27532958984375
iter 50, train loss 32.94025421142578, val loss 39.15096664428711
iter 60, train loss 32.93660354614258, val loss 39.369667053222656
iter 70, train loss 32.90062713623047, val loss 38.43146514892578
iter 80, train loss 32.86426544189453, val loss 38.63064193725586
iter 90, train loss 32.814125061035156, val loss 38.80204772949219
best loss 38.27724838256836
not here
quantized in 92.04272699356079 seconds
35680 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.codebook', 'self_attn.q_proj.quantizer.norms_1', 'self_attn.q_proj.quantizer.norms_0', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.codebook', 'self_attn.k_proj.quantizer.norms_1', 'self_attn.k_proj.quantizer.norms_0', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.codebook', 'self_attn.v_proj.quantizer.norms_1', 'self_attn.v_proj.quantizer.norms_0', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.codebook', 'self_attn.o_proj.quantizer.norms_1', 'self_attn.o_proj.quantizer.norms_0', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.codebook', 'mlp.gate_proj.quantizer.norms_1', 'mlp.gate_proj.quantizer.norms_0', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.codebook', 'mlp.up_proj.quantizer.norms_1', 'mlp.up_proj.quantizer.norms_0', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.codebook', 'mlp.down_proj.quantizer.norms_1', 'mlp.down_proj.quantizer.norms_0', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135936
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.037250270892400295 val loss: 0.08150034537538886
6652 MiB free out of 48676 MiB total
epoch 1 loss: 0.034785816533258185 val loss: 0.0813735700212419
6652 MiB free out of 48676 MiB total
epoch 2 loss: 0.03409640445897821 val loss: 0.08150918688625097
6652 MiB free out of 48676 MiB total
epoch 3 loss: 0.03372490515175741 val loss: 0.08166729751974344
6652 MiB free out of 48676 MiB total
epoch 4 loss: 0.03345572583202738 val loss: 0.08181250328198075
6652 MiB free out of 48676 MiB total
35680 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
6652 MiB free out of 48676 MiB total
after cast to cpu
37850 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
29 self_attn.q_proj
Pruning ...
256
iter 0, train loss 450.30316162109375, val loss 405.4249572753906
iter 10, train loss 483.1842041015625, val loss 504.62408447265625
iter 20, train loss 463.491455078125, val loss 484.2964782714844
iter 30, train loss 455.10418701171875, val loss 477.92633056640625
iter 40, train loss 447.6352233886719, val loss 471.140869140625
iter 50, train loss 446.53411865234375, val loss 471.0901794433594
iter 60, train loss 444.36492919921875, val loss 467.3398132324219
iter 70, train loss 441.66766357421875, val loss 463.80078125
iter 80, train loss 441.34405517578125, val loss 463.3906555175781
iter 90, train loss 440.61688232421875, val loss 463.39013671875
best loss 405.4249572753906
not here
quantized in 34.270583152770996 seconds
36422 MiB free out of 48676 MiB total
29 self_attn.k_proj
Pruning ...
256
iter 0, train loss 485.9217529296875, val loss 424.3141784667969
iter 10, train loss 515.04541015625, val loss 542.104248046875
iter 20, train loss 499.26812744140625, val loss 524.6166381835938
iter 30, train loss 483.24188232421875, val loss 509.21722412109375
iter 40, train loss 476.3890686035156, val loss 501.6949462890625
iter 50, train loss 472.0733947753906, val loss 499.3396911621094
iter 60, train loss 469.1391296386719, val loss 495.410888671875
iter 70, train loss 468.0181884765625, val loss 494.4761657714844
iter 80, train loss 466.91021728515625, val loss 493.6642761230469
iter 90, train loss 466.1606750488281, val loss 492.19012451171875
best loss 424.3141784667969
not here
quantized in 33.52564811706543 seconds
36412 MiB free out of 48676 MiB total
29 self_attn.v_proj
Pruning ...
256
iter 0, train loss 347.60235595703125, val loss 371.1342468261719
iter 10, train loss 346.70037841796875, val loss 373.08624267578125
iter 20, train loss 346.8013610839844, val loss 373.9515380859375
iter 30, train loss 346.6370849609375, val loss 375.1552734375
iter 40, train loss 347.0230712890625, val loss 373.7126770019531
iter 50, train loss 347.0129089355469, val loss 373.530029296875
iter 60, train loss 346.7201843261719, val loss 373.4067077636719
iter 70, train loss 346.9295959472656, val loss 373.55938720703125
iter 80, train loss 346.68951416015625, val loss 373.6752014160156
iter 90, train loss 346.5074462890625, val loss 373.48260498046875
best loss 371.1342468261719
not here
quantized in 31.294907808303833 seconds
36402 MiB free out of 48676 MiB total
29 self_attn.o_proj
Pruning ...
256
iter 0, train loss 37.773372650146484, val loss 37.2401008605957
iter 10, train loss 29.901826858520508, val loss 42.27134704589844
iter 20, train loss 28.643680572509766, val loss 42.31654357910156
iter 30, train loss 27.938085556030273, val loss 41.078895568847656
iter 40, train loss 27.14431381225586, val loss 40.68754577636719
iter 50, train loss 26.752017974853516, val loss 39.66328430175781
iter 60, train loss 26.712400436401367, val loss 39.59347915649414
iter 70, train loss 26.494382858276367, val loss 39.726402282714844
iter 80, train loss 26.266525268554688, val loss 39.87702941894531
iter 90, train loss 26.165565490722656, val loss 40.0867805480957
best loss 37.2401008605957
not here
quantized in 32.31242847442627 seconds
36370 MiB free out of 48676 MiB total
29 mlp.gate_proj
Pruning ...
256
iter 0, train loss 720.5355834960938, val loss 789.8472290039062
iter 10, train loss 751.7332763671875, val loss 900.9349365234375
iter 20, train loss 745.1590576171875, val loss 887.2953491210938
iter 30, train loss 747.86767578125, val loss 899.9583740234375
iter 40, train loss 744.5489501953125, val loss 875.4835205078125
iter 50, train loss 742.064208984375, val loss 875.1807250976562
iter 60, train loss 740.9617309570312, val loss 875.2874755859375
iter 70, train loss 738.9960327148438, val loss 875.2799682617188
iter 80, train loss 738.7698364257812, val loss 879.58447265625
iter 90, train loss 740.5203247070312, val loss 874.632568359375
best loss 789.8472290039062
not here
quantized in 86.38322877883911 seconds
36068 MiB free out of 48676 MiB total
29 mlp.up_proj
Pruning ...
256
iter 0, train loss 648.321044921875, val loss 701.9095458984375
iter 10, train loss 669.7693481445312, val loss 801.6514892578125
iter 20, train loss 644.023193359375, val loss 764.8961791992188
iter 30, train loss 642.43017578125, val loss 774.27099609375
iter 40, train loss 639.5089721679688, val loss 765.835693359375
iter 50, train loss 640.9423828125, val loss 777.825927734375
iter 60, train loss 639.2720947265625, val loss 767.8917846679688
iter 70, train loss 638.421630859375, val loss 757.6356811523438
iter 80, train loss 637.85595703125, val loss 762.9881591796875
iter 90, train loss 638.0952758789062, val loss 761.5621948242188
best loss 701.9095458984375
not here
quantized in 86.50143647193909 seconds
35874 MiB free out of 48676 MiB total
29 mlp.down_proj
Pruning ...
256
iter 0, train loss 38.093814849853516, val loss 118.35188293457031
iter 10, train loss 38.32442855834961, val loss 111.89311218261719
iter 20, train loss 38.33144760131836, val loss 104.44990539550781
iter 30, train loss 38.42393493652344, val loss 97.73676300048828
iter 40, train loss 38.331085205078125, val loss 94.20823669433594
iter 50, train loss 38.265960693359375, val loss 89.4075698852539
iter 60, train loss 38.26025390625, val loss 85.83103942871094
iter 70, train loss 38.12303161621094, val loss 80.3543701171875
iter 80, train loss 38.104698181152344, val loss 77.6576919555664
iter 90, train loss 38.021514892578125, val loss 78.27623748779297
best loss 77.22015380859375
not here
quantized in 91.6171362400055 seconds
35680 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.codebook', 'self_attn.q_proj.quantizer.norms_1', 'self_attn.q_proj.quantizer.norms_0', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.codebook', 'self_attn.k_proj.quantizer.norms_1', 'self_attn.k_proj.quantizer.norms_0', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.codebook', 'self_attn.v_proj.quantizer.norms_1', 'self_attn.v_proj.quantizer.norms_0', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.codebook', 'self_attn.o_proj.quantizer.norms_1', 'self_attn.o_proj.quantizer.norms_0', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.codebook', 'mlp.gate_proj.quantizer.norms_1', 'mlp.gate_proj.quantizer.norms_0', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.codebook', 'mlp.up_proj.quantizer.norms_1', 'mlp.up_proj.quantizer.norms_0', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.codebook', 'mlp.down_proj.quantizer.norms_1', 'mlp.down_proj.quantizer.norms_0', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135936
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.04306013192399405 val loss: 0.14885886944830418
6652 MiB free out of 48676 MiB total
epoch 1 loss: 0.0397448874427937 val loss: 0.1567154978401959
6652 MiB free out of 48676 MiB total
epoch 2 loss: 0.03858467683312483 val loss: 0.17223157407715917
6652 MiB free out of 48676 MiB total
epoch 3 loss: 0.03793372254585847 val loss: 0.19111676281318069
6652 MiB free out of 48676 MiB total
epoch 4 loss: 0.03745835102745332 val loss: 0.211249440908432
6652 MiB free out of 48676 MiB total
35680 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
6652 MiB free out of 48676 MiB total
after cast to cpu
37850 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
30 self_attn.q_proj
Pruning ...
256
iter 0, train loss 484.85516357421875, val loss 452.68682861328125
iter 10, train loss 517.0068969726562, val loss 557.2791748046875
iter 20, train loss 492.6265869140625, val loss 530.804931640625
iter 30, train loss 477.2965087890625, val loss 518.5540771484375
iter 40, train loss 473.42523193359375, val loss 517.467041015625
iter 50, train loss 471.5323791503906, val loss 512.8212890625
iter 60, train loss 467.32330322265625, val loss 509.1428527832031
iter 70, train loss 466.94232177734375, val loss 507.94952392578125
iter 80, train loss 466.56884765625, val loss 509.019775390625
iter 90, train loss 466.06842041015625, val loss 509.6820068359375
best loss 452.68682861328125
not here
quantized in 33.80207133293152 seconds
36422 MiB free out of 48676 MiB total
30 self_attn.k_proj
Pruning ...
256
iter 0, train loss 521.6453857421875, val loss 467.9324035644531
iter 10, train loss 556.0284423828125, val loss 600.491943359375
iter 20, train loss 532.4666748046875, val loss 579.2774658203125
iter 30, train loss 515.4586791992188, val loss 562.8258056640625
iter 40, train loss 502.212890625, val loss 548.5787353515625
iter 50, train loss 498.88067626953125, val loss 546.412353515625
iter 60, train loss 496.8463134765625, val loss 541.758544921875
iter 70, train loss 494.10626220703125, val loss 538.908935546875
iter 80, train loss 492.3563232421875, val loss 539.51025390625
iter 90, train loss 491.93951416015625, val loss 540.5460815429688
best loss 467.9324035644531
not here
quantized in 32.75321555137634 seconds
36412 MiB free out of 48676 MiB total
30 self_attn.v_proj
Pruning ...
256
iter 0, train loss 393.2026062011719, val loss 419.36602783203125
iter 10, train loss 394.878173828125, val loss 428.6705627441406
iter 20, train loss 392.93707275390625, val loss 430.84759521484375
iter 30, train loss 394.51043701171875, val loss 432.3670959472656
iter 40, train loss 395.3821105957031, val loss 433.3056640625
iter 50, train loss 395.24053955078125, val loss 433.10400390625
iter 60, train loss 394.13970947265625, val loss 432.4886169433594
iter 70, train loss 393.332275390625, val loss 433.7762756347656
iter 80, train loss 392.99237060546875, val loss 434.156982421875
iter 90, train loss 392.81072998046875, val loss 434.242431640625
best loss 419.36602783203125
not here
quantized in 31.21204900741577 seconds
36402 MiB free out of 48676 MiB total
30 self_attn.o_proj
Pruning ...
256
iter 0, train loss 38.753318786621094, val loss 35.21000289916992
iter 10, train loss 32.89051055908203, val loss 41.73891067504883
iter 20, train loss 31.250551223754883, val loss 41.83650588989258
iter 30, train loss 30.398788452148438, val loss 41.50476837158203
iter 40, train loss 29.949810028076172, val loss 40.64379119873047
iter 50, train loss 29.500057220458984, val loss 39.751487731933594
iter 60, train loss 29.136333465576172, val loss 39.751827239990234
iter 70, train loss 29.040611267089844, val loss 39.33791732788086
iter 80, train loss 28.983976364135742, val loss 38.83827209472656
iter 90, train loss 28.781352996826172, val loss 38.63597106933594
best loss 35.21000289916992
not here
quantized in 31.930619955062866 seconds
36370 MiB free out of 48676 MiB total
30 mlp.gate_proj
Pruning ...
256
iter 0, train loss 815.132080078125, val loss 851.104736328125
iter 10, train loss 852.4714965820312, val loss 1023.0455322265625
iter 20, train loss 824.11669921875, val loss 978.83251953125
iter 30, train loss 820.5093994140625, val loss 991.9605102539062
iter 40, train loss 814.479248046875, val loss 1005.7691650390625
iter 50, train loss 813.7906494140625, val loss 992.70263671875
iter 60, train loss 807.9464721679688, val loss 984.7833862304688
iter 70, train loss 807.90478515625, val loss 985.65625
iter 80, train loss 803.9136962890625, val loss 974.2066040039062
iter 90, train loss 803.9273071289062, val loss 988.9282836914062
best loss 851.104736328125
not here
quantized in 87.35097074508667 seconds
36068 MiB free out of 48676 MiB total
30 mlp.up_proj
Pruning ...
256
iter 0, train loss 722.7654418945312, val loss 794.3818359375
iter 10, train loss 757.4361572265625, val loss 930.185791015625
iter 20, train loss 715.6859741210938, val loss 876.8046875
iter 30, train loss 692.6396484375, val loss 894.8441772460938
iter 40, train loss 688.2465209960938, val loss 838.0240478515625
iter 50, train loss 683.4766845703125, val loss 839.0342407226562
iter 60, train loss 678.8041381835938, val loss 831.6572265625
iter 70, train loss 677.4044799804688, val loss 849.5478515625
iter 80, train loss 676.5684814453125, val loss 844.112548828125
iter 90, train loss 675.9146728515625, val loss 846.90869140625
best loss 794.3818359375
not here
quantized in 87.24754595756531 seconds
35874 MiB free out of 48676 MiB total
30 mlp.down_proj
Pruning ...
256
iter 0, train loss 53.41288757324219, val loss 73.05254364013672
iter 10, train loss 54.10218811035156, val loss 80.5956802368164
iter 20, train loss 53.60331726074219, val loss 78.55139923095703
iter 30, train loss 53.556922912597656, val loss 79.1549072265625
iter 40, train loss 53.20848083496094, val loss 77.18738555908203
iter 50, train loss 53.11954879760742, val loss 78.37947082519531
iter 60, train loss 52.90715789794922, val loss 77.29046630859375
iter 70, train loss 52.77919006347656, val loss 76.72126770019531
iter 80, train loss 52.54728698730469, val loss 76.88493347167969
iter 90, train loss 52.548606872558594, val loss 76.9845962524414
best loss 73.05254364013672
not here
quantized in 92.47209000587463 seconds
35680 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.codebook', 'self_attn.q_proj.quantizer.norms_1', 'self_attn.q_proj.quantizer.norms_0', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.codebook', 'self_attn.k_proj.quantizer.norms_1', 'self_attn.k_proj.quantizer.norms_0', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.codebook', 'self_attn.v_proj.quantizer.norms_1', 'self_attn.v_proj.quantizer.norms_0', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.codebook', 'self_attn.o_proj.quantizer.norms_1', 'self_attn.o_proj.quantizer.norms_0', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.codebook', 'mlp.gate_proj.quantizer.norms_1', 'mlp.gate_proj.quantizer.norms_0', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.codebook', 'mlp.up_proj.quantizer.norms_1', 'mlp.up_proj.quantizer.norms_0', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.codebook', 'mlp.down_proj.quantizer.norms_1', 'mlp.down_proj.quantizer.norms_0', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135936
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.06599156436277553 val loss: 12.458714351058006
6652 MiB free out of 48676 MiB total
epoch 1 loss: 0.05542883262387477 val loss: 24.799030497670174
6652 MiB free out of 48676 MiB total
epoch 2 loss: 0.05326363618951291 val loss: 39.895107604563236
6652 MiB free out of 48676 MiB total
epoch 3 loss: 0.052131282864138484 val loss: 60.350684724748135
6652 MiB free out of 48676 MiB total
epoch 4 loss: 0.05132805637549609 val loss: 86.24488657712936
6652 MiB free out of 48676 MiB total
35680 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
6652 MiB free out of 48676 MiB total
after cast to cpu
37850 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
31 self_attn.q_proj
Pruning ...
256
iter 0, train loss 364.9535217285156, val loss 311.7123107910156
iter 10, train loss 381.5442199707031, val loss 404.4276123046875
iter 20, train loss 367.8077392578125, val loss 390.98809814453125
iter 30, train loss 355.2562561035156, val loss 377.04620361328125
iter 40, train loss 349.7510070800781, val loss 370.48284912109375
iter 50, train loss 348.4576416015625, val loss 369.11846923828125
iter 60, train loss 346.7993469238281, val loss 367.7215576171875
iter 70, train loss 344.4494934082031, val loss 365.89654541015625
iter 80, train loss 342.7919921875, val loss 364.0343322753906
iter 90, train loss 343.5956115722656, val loss 364.27630615234375
best loss 311.7123107910156
not here
quantized in 34.575385332107544 seconds
36422 MiB free out of 48676 MiB total
31 self_attn.k_proj
Pruning ...
256
iter 0, train loss 424.114013671875, val loss 343.07037353515625
iter 10, train loss 433.75946044921875, val loss 463.79052734375
iter 20, train loss 424.53289794921875, val loss 456.1827697753906
iter 30, train loss 407.6059875488281, val loss 440.60870361328125
iter 40, train loss 397.16357421875, val loss 427.3896484375
iter 50, train loss 392.4976501464844, val loss 423.73736572265625
iter 60, train loss 389.00018310546875, val loss 419.32818603515625
iter 70, train loss 388.10247802734375, val loss 418.8228454589844
iter 80, train loss 387.17529296875, val loss 417.30877685546875
iter 90, train loss 387.274658203125, val loss 417.81219482421875
best loss 343.07037353515625
not here
quantized in 33.94253158569336 seconds
36412 MiB free out of 48676 MiB total
31 self_attn.v_proj
Pruning ...
256
iter 0, train loss 223.30419921875, val loss 238.97695922851562
iter 10, train loss 223.86160278320312, val loss 243.06886291503906
iter 20, train loss 223.22865295410156, val loss 243.16561889648438
iter 30, train loss 222.8916778564453, val loss 243.43130493164062
iter 40, train loss 222.91287231445312, val loss 243.49989318847656
iter 50, train loss 222.399658203125, val loss 243.61904907226562
iter 60, train loss 222.032470703125, val loss 243.11300659179688
iter 70, train loss 221.6132049560547, val loss 242.94537353515625
iter 80, train loss 221.452880859375, val loss 242.57452392578125
iter 90, train loss 221.618896484375, val loss 242.17672729492188
best loss 238.97695922851562
not here
quantized in 31.151111364364624 seconds
36402 MiB free out of 48676 MiB total
31 self_attn.o_proj
Pruning ...
256
iter 0, train loss 176.2386474609375, val loss 29.87263298034668
iter 10, train loss 137.8111114501953, val loss 32.04683303833008
iter 20, train loss 122.138916015625, val loss 30.296222686767578
iter 30, train loss 115.19153594970703, val loss 30.882484436035156
iter 40, train loss 105.61016845703125, val loss 30.636993408203125
iter 50, train loss 100.99150848388672, val loss 29.693157196044922
iter 60, train loss 97.72105407714844, val loss 30.228534698486328
iter 70, train loss 95.2562484741211, val loss 29.7130126953125
iter 80, train loss 93.16116333007812, val loss 29.715076446533203
iter 90, train loss 91.76136779785156, val loss 29.231868743896484
best loss 28.199172973632812
not here
quantized in 32.53114414215088 seconds
36402 MiB free out of 48676 MiB total
31 mlp.gate_proj
Pruning ...
256
iter 0, train loss 745.6669921875, val loss 702.364501953125
iter 10, train loss 766.6681518554688, val loss 855.7298583984375
iter 20, train loss 721.8424682617188, val loss 799.291015625
iter 30, train loss 701.2047119140625, val loss 773.7759399414062
iter 40, train loss 701.8480224609375, val loss 772.35986328125
iter 50, train loss 701.3493041992188, val loss 777.86767578125
iter 60, train loss 701.8056640625, val loss 773.2052612304688
iter 70, train loss 701.6110229492188, val loss 777.3405151367188
iter 80, train loss 703.5582275390625, val loss 777.1710815429688
iter 90, train loss 702.07373046875, val loss 780.3896484375
best loss 701.7000732421875
not here
quantized in 88.31278610229492 seconds
36100 MiB free out of 48676 MiB total
31 mlp.up_proj
Pruning ...
256
iter 0, train loss 708.2614135742188, val loss 661.500244140625
iter 10, train loss 718.8033447265625, val loss 787.5997314453125
iter 20, train loss 687.230224609375, val loss 753.8751220703125
iter 30, train loss 658.156494140625, val loss 736.7420043945312
iter 40, train loss 642.5718994140625, val loss 724.75390625
iter 50, train loss 639.5345458984375, val loss 714.4967041015625
iter 60, train loss 635.9605102539062, val loss 706.8809814453125
iter 70, train loss 633.9485473632812, val loss 708.7984619140625
iter 80, train loss 633.13671875, val loss 705.8270874023438
iter 90, train loss 632.1746826171875, val loss 703.4519653320312
best loss 639.6174926757812
not here
quantized in 90.59148621559143 seconds
35906 MiB free out of 48676 MiB total
31 mlp.down_proj
Pruning ...
256
iter 0, train loss 100.45748901367188, val loss 1505.760498046875
iter 10, train loss 110.59503173828125, val loss 915.4982299804688
iter 20, train loss 103.79766845703125, val loss 432.1390380859375
iter 30, train loss 97.73924255371094, val loss 333.0171203613281
iter 40, train loss 94.74374389648438, val loss 268.588134765625
iter 50, train loss 92.57020568847656, val loss 272.4853515625
iter 60, train loss 91.14878845214844, val loss 279.3919677734375
iter 70, train loss 89.64282989501953, val loss 263.59442138671875
iter 80, train loss 88.29144287109375, val loss 231.59652709960938
iter 90, train loss 87.95721435546875, val loss 246.8341064453125
best loss 231.59652709960938
not here
quantized in 95.29581069946289 seconds
35712 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.codebook', 'self_attn.q_proj.quantizer.norms_1', 'self_attn.q_proj.quantizer.norms_0', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.codebook', 'self_attn.k_proj.quantizer.norms_1', 'self_attn.k_proj.quantizer.norms_0', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.codebook', 'self_attn.v_proj.quantizer.norms_1', 'self_attn.v_proj.quantizer.norms_0', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.codebook', 'self_attn.o_proj.quantizer.norms_1', 'self_attn.o_proj.quantizer.norms_0', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.codebook', 'mlp.gate_proj.quantizer.norms_1', 'mlp.gate_proj.quantizer.norms_0', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.codebook', 'mlp.up_proj.quantizer.norms_1', 'mlp.up_proj.quantizer.norms_0', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.codebook', 'mlp.down_proj.quantizer.norms_1', 'mlp.down_proj.quantizer.norms_0', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135936
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.18000518390908837 val loss: 3.6741872876882553
7644 MiB free out of 48676 MiB total
epoch 1 loss: 0.14795169094577432 val loss: 3.3016689270734787
7644 MiB free out of 48676 MiB total
epoch 2 loss: 0.13844083808362484 val loss: 3.2319173216819763
7644 MiB free out of 48676 MiB total
epoch 3 loss: 0.13332385988906026 val loss: 3.2097692787647247
7644 MiB free out of 48676 MiB total
epoch 4 loss: 0.12959146552020684 val loss: 3.184331953525543
7644 MiB free out of 48676 MiB total
35712 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
7644 MiB free out of 48676 MiB total
after cast to cpu
37850 MiB free out of 48676 MiB total
Total bits: 13017415680.0 Total params: 6476005376
average bits per value: 2.0100995790155443
total time taken: 21428.413144350052
Loading tokenizer for meta-llama/Llama-2-7b-hf
/home/lliu/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Dataset: wikitext2
Evaluating ...
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
Perplexity: 7.467979
