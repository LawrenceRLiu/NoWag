{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "import cvxpy as cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Quantizer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 initial_codebook:torch.Tensor,\n",
    "                 H: torch.Tensor,\n",
    "                 Weights: torch.Tensor,\n",
    "                 ):\n",
    "        \"\"\"Vector Quantizer\n",
    "\n",
    "        Args:\n",
    "            initial_codebook (torch.Tensor): the initial codebook (n,k)\n",
    "            H (torch.Tensor): the Hessian of shape (n,n)\n",
    "            Weights (torch.Tensor): weights of shape (n,n)\n",
    "        \"\"\"\n",
    "        super(Quantizer, self).__init__()\n",
    "\n",
    "        # self.codebook = nn.Parameter(initial_codebook, requires_grad=True)\n",
    "        self.codebook = nn.Parameter(initial_codebook)\n",
    "        self.H = H\n",
    "        self.Weights = Weights\n",
    "        n,k = initial_codebook.shape\n",
    "        #randomly initialize the initial assignments through xavier uniform initialization\n",
    "        #a matrix of shape (k,n)\n",
    "\n",
    "        #logistic initialization\n",
    "        initial_assignments = torch.empty(k,n,device = initial_codebook.device,\n",
    "                                                            dtype = initial_codebook.dtype\n",
    "                                                            ).uniform_(0,1)\n",
    "        \n",
    "\n",
    "        self.initial_assignments = nn.Parameter(#take the logistic of the initial assignments\n",
    "                                                torch.log(initial_assignments / (1 - initial_assignments)),\n",
    "                                                requires_grad = True\n",
    "                                                )\n",
    "\n",
    "        self.activation = nn.PReLU().to(initial_codebook.device)\n",
    "        # self.initial_assignments = torch.empty(n,k,device = initial_codebook.device,\n",
    "        #                                                     dtype = initial_codebook.dtype\n",
    "        #                                                     ).uniform_(-1,1)\n",
    "\n",
    "        print(self.H.dtype,self.Weights.dtype,self.codebook.dtype,self.initial_assignments.dtype)\n",
    "\n",
    "\n",
    "    def sparse_penalty(self, assignments: torch.Tensor, target_sparsity:float, \n",
    "                       greater_penalty:float):\n",
    "        \"\"\"Penalty function for the assignments\n",
    "\n",
    "        Args:\n",
    "            assignments (torch.Tensor): the assignments of shape (k,n)\n",
    "                assumed to be past through \n",
    "            target_sparsity (float): the target sparsity of the assignments, sparsity is column wise\n",
    "            greater_penalty (float): the penalty for greater sparsity\n",
    "            lesser_penalty (float): the penalty for lesser sparsity\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: the penalty\n",
    "        \"\"\"\n",
    "        sparsity = torch.sum(assignments, dim = 0) #shape (n,)\n",
    "        sparsity = sparsity / assignments.shape[0]\n",
    "        penalty = torch.relu(sparsity - target_sparsity)\n",
    "        return torch.sum(greater_penalty * penalty), torch.sum(assignments)\n",
    "\n",
    "    def penalty(self, assignments, beta, penalty_1_weight,\n",
    "                penalty_2_weight):\n",
    "        \"\"\"Penalty function for the assignments\n",
    "\n",
    "        Args:\n",
    "            assignments (torch.Tensor): the assignments of shape (k,n)\n",
    "                assumed to be past through \n",
    "            beta (float): a hyperparameter for the first penalty, this penalty\n",
    "            is 1- |2*a_ij - 1|**(beta)\n",
    "\n",
    "            penalty_1_weight (float): the weight of the first penalty\n",
    "            penalty_2_weight (float): the weight of the second penalty\n",
    "            this penalty is the sum of the assignments, therefore enforceing \n",
    "            sparsity\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: the penalty\n",
    "        \"\"\"\n",
    "        # assert torch.isfinite(torch.abs(2*assignments - 1)).all()\n",
    "        penalty_1 = torch.sum(1 - torch.abs(2*assignments - 1)**beta)\n",
    "        # print(penalty_1)\n",
    "        # penalty_2, sparsity = self.sparse_penalty(assignments, 0.5, 1)\n",
    "        # return penalty_1_weight * penalty_1 + penalty_2_weight * penalty_2, penalty_1, sparsity\n",
    "        penalty_2 = torch.sum(assignments)\n",
    "        # print(penalty_2)\n",
    "        return penalty_1_weight * penalty_1 + penalty_2_weight * penalty_2, penalty_1, penalty_2\n",
    "    \n",
    "    def forward(self,beta,penalty_1_weight,\n",
    "                penalty_2_weight):\n",
    "        \"\"\"Forward pass of the quantizer\n",
    "\n",
    "        Args:\n",
    "            beta (float): a hyperparameter for the first penalty, this penalty\n",
    "            is 1- |2*a_ij - 1|**(beta)\n",
    "\n",
    "            penalty_1_weight (float): the weight of the first penalty\n",
    "            penalty_2_weight (float): the weight of the second penalty\n",
    "            this penalty is the sum of the assignments, therefore enforceing \n",
    "            sparsity\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: the quantized codebook\n",
    "        \"\"\"\n",
    "        #get the assignments\n",
    "        assignments = torch.sigmoid(self.initial_assignments)*1.2 - 0.1\n",
    "        # print(assignments)\n",
    "        # assert torch.isfinite(assignments).all()\n",
    "        assignments = torch.clip(assignments,0,1)\n",
    "        # assert torch.isfinite(assignments).all()\n",
    "\n",
    "        #get the quantized_weights\n",
    "        quantized_weights = self.codebook @ assignments\n",
    "        assert torch.isfinite(quantized_weights).all()\n",
    "        #get the difference between the quantized weights and the original weights\n",
    "        diff = quantized_weights - self.Weights\n",
    "\n",
    "        #get the loss\n",
    "        loss = torch.einsum('ik,kl,il->', diff, self.H, diff)\n",
    "        # assert torch.isclose(loss,torch.sum(diff**2))\n",
    "        # loss = torch.sum(diff**2)\n",
    "        #get the penalty\n",
    "        penalty,p1,p2 = self.penalty(assignments, beta, penalty_1_weight, penalty_2_weight)\n",
    "        # assert torch.isfinite(penalty)\n",
    "        return loss + penalty, loss, penalty,p1,p2\n",
    "    \n",
    "    def get_rounded_error(self):\n",
    "\n",
    "        #get the assignments\n",
    "        assignments = torch.sigmoid(self.initial_assignments)*1.2 - 0.1\n",
    "        assignments = torch.clip(assignments,0,1)\n",
    "        assignments = torch.round(assignments)  \n",
    "        #get the quantized_weights\n",
    "        quantized_weights = self.codebook @ assignments\n",
    "\n",
    "        #get the difference between the quantized weights and the original weights\n",
    "        diff = quantized_weights - self.Weights\n",
    "\n",
    "        #get the loss\n",
    "        # loss = torch.einsum('ik,kl,il->', diff, self.H, diff)\n",
    "        # assert torch.isclose(loss,torch.sum(diff**2))\n",
    "        # loss = torch.sum(diff**2)\n",
    "        loss = torch.einsum('ik,kl,il->', diff, self.H, diff)\n",
    "        return loss\n",
    "    \n",
    "    def update_codebook(self):\n",
    "        raise NotImplementedError\n",
    "        with torch.no_grad():\n",
    "            assignments = torch.sigmoid(self.initial_assignments)\n",
    "\n",
    "            #cast to numpy\n",
    "            assignments = assignments.cpu().numpy()\n",
    "            weights = self.Weights.cpu().numpy()\n",
    "            H = self.H.cpu().numpy()\n",
    "\n",
    "            #use cvxpy to solve the optimization problem\n",
    "            k,n = assignments.shape\n",
    "            codebook = cp.Variable(self.codebook.shape)\n",
    "            objective = cp.Minimize(cp.trace((weights - codebook @ assignments).T @ H @ (weights - codebook @ assignments)))\n",
    "            constraints = []\n",
    "            \n",
    "            #no constraints\n",
    "            prob = cp.Problem(objective, constraints)\n",
    "            prob.solve()\n",
    "            self.codebook = torch.tensor(codebook.value,device = self.codebook.device,dtype = self.codebook.dtype)\n",
    "            \n",
    "        return self.codebook\n",
    "\n",
    "    def update_assignments(self):\n",
    "        raise NotImplementedError\n",
    "        #closed form update \n",
    "        with torch.no_grad():\n",
    "            # assignments = self.initial_assignments\n",
    "            # print(assignments.shape)\n",
    "            #update formula for the assignments is\n",
    "            #A = W^T C (C^T C)^-1\n",
    "\n",
    "            codebook_inner_product = self.codebook.T @ self.H @ self.codebook\n",
    "            inverse = torch.inverse(codebook_inner_product)\n",
    "\n",
    "            self.initial_assignments = self.Weights @ self.H @ self.codebook @ inverse\n",
    "            print(self.initial_assignments)\n",
    "            assert torch.all(torch.isfinite(self.initial_assignments))\n",
    "        \n",
    "        #take the mean of the assignments\n",
    "        mean = torch.mean(torch.abs(self.initial_assignments))\n",
    "        #rescale the assignments\n",
    "        self.initial_assignments = self.initial_assignments / mean\n",
    "        #scale up the codebook\n",
    "        self.codebook = self.codebook * mean\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.load(\"/home/lliu/huffman/test/original_weights.pt\")\n",
    "weights = data['weights'].float().to(torch.device(\"cuda:6\"))\n",
    "H = data['H'].float().to(torch.device(\"cuda:6\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def BetaScheduler_(i, T, scale, bias):\n",
    "\n",
    "#     signal = (np.sin(((i%T)/T-0.5)*np.pi)+1)*0.5\n",
    "\n",
    "#     return scale * signal + bias\n",
    "\n",
    "# def BetaScheduler_(i, T, scale, bias):\n",
    "\n",
    "#     signal = (np.sin(((i)/T)*np.pi)+1) - 1\n",
    "\n",
    "#     return scale * signal + bias\n",
    "\n",
    "def BetaScheduler_(i, T, scale, bias):\n",
    "\n",
    "    # signal = (np.sin(((i%T)/T-0.5)*np.pi)+1)*0.5\n",
    "    signal = (T-i)/T\n",
    "    # signal = (np.cos(((i)/T)*np.pi*2))*0.5+0.5\n",
    "    return scale * signal + bias\n",
    "\n",
    "# def BetaScheduler_(i, T, scale, bias):\n",
    "\n",
    "#     # signal = (np.sin(((i%T)/T-0.5)*np.pi)+1)*0.5\n",
    "#     # signal = abs((i-T/2)*2/T)**5\n",
    "#     signal = (i/T)**5\n",
    "#     return scale * signal + bias\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "k = 256\n",
    "\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "#try to initalize the codebook from the eigenvectors of the weights\n",
    "eigenvalues, eigenvectors = torch.linalg.eigh(weights)\n",
    "print(eigenvalues.shape, eigenvectors.shape)\n",
    "# eigenvectors = eigenvalues.vectors\n",
    "indexs = torch.argsort(eigenvalues,descending=True)[:k]\n",
    "#renormalize the eigenvectors\n",
    "initial_codebook = eigenvectors[indexs,:].T / k\n",
    "print(initial_codebook.shape)\n",
    "quantizer = Quantizer(\n",
    "                        # weights[indexs,:].T/k\n",
    "                        initial_codebook\n",
    "                    #   , torch.eye(H.shape[0],device = H.device,dtype = H.dtype)\n",
    "                    , H\n",
    "                      , weights)\n",
    "\n",
    "\n",
    "n_iters = 10000\n",
    "n_subiters = 10000\n",
    "n_subiters_multiple = 1.1\n",
    "\n",
    "beta_min, beta_max = 1, 4\n",
    "\n",
    "penalty_1_weight = 1\n",
    "penalty_2_weight = 1\n",
    "penalty_2_multiple = 1 \n",
    " \n",
    "beta = 5 \n",
    "penalty_2_weight = 0.01\n",
    "\n",
    "losses = []\n",
    "rounded_errors = []\n",
    "i_s = []\n",
    "\n",
    "errors = []\n",
    "penalties = []\n",
    "binary_penalty = []\n",
    "sparsity_penalty = []\n",
    "betas = []\n",
    "penalty_2_weights = []\n",
    "lrs = []\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam([{'params': quantizer.codebook, 'lr': 5e-4},\n",
    "                                {'params': quantizer.initial_assignments, 'lr': 1e-3}])\n",
    "# scheduler_up = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.95)\n",
    "# scheduler_down = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=1.005)\n",
    "\n",
    "j = 0\n",
    "prev_loss = float('inf')\n",
    "patience_up = 25\n",
    "patience_down = 25\n",
    "impatience_1 = 0\n",
    "impatience_2 = 0\n",
    "\n",
    "\n",
    "for i in tqdm.tqdm(range(n_iters)):\n",
    "    beta = BetaScheduler_(j, n_subiters, beta_max - beta_min, beta_min)\n",
    "    # penalty_2_weight = BetaScheduler_(j, n_subiters*penalty_2_multiple, 0.2, 0.0)\n",
    "    # print(beta)\n",
    "    # print(beta)\n",
    "    loss, loss1, loss2, p1, p2 = quantizer(beta, penalty_1_weight, penalty_2_weight)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # print(loss)\n",
    "\n",
    "    assert torch.all(torch.isfinite(loss))\n",
    "\n",
    "    losses.append(loss.item())\n",
    "    errors.append(loss1.item())\n",
    "    penalties.append(loss2.item())\n",
    "    binary_penalty.append(p1.item())\n",
    "    sparsity_penalty.append(p2.item())\n",
    "    betas.append(beta)\n",
    "    penalty_2_weights.append(penalty_2_weight)\n",
    "    j+=1\n",
    "    if i % 1000 == 0:\n",
    "        # print(f\"Loss: {loss.item()}, Error: {loss1.item()/torch.sum(torch.abs(weights**2))}, Loss2: {loss2.item()}, P1: {p1.item()}, sparsity: {p2.item()/(H.shape[0]*k)}, beta: {beta}, penalty_2_weight: {penalty_2_weight}\")\n",
    "        print(f\"Loss: {loss.item()}, Error: {loss1.item()/H.shape[0]}, Loss2: {loss2.item()}, P1: {p1.item()}, sparsity: {p2.item()/(H.shape[0]*k)}, beta: {beta}, penalty_2_weight: {penalty_2_weight}\")\n",
    "        # wandb.log({\"Loss\": loss.item(), \"Error\": loss1.item()/H.shape[0], \"Penalty\": loss2.item(), \"BinaryPenalty\": p1.item(), \"Sparsity\": p2.item()/(H.shape[0]**2),\n",
    "        #            \"penalty_2_weight\": penalty_2_weight, \"beta\": beta})\n",
    "        fig = plt.figure()\n",
    "        plt.hist(np.clip(torch.sigmoid(quantizer.initial_assignments).detach().cpu().numpy().flatten()*1.2-0.1,0,1), bins = 100,density = True)\n",
    "        plt.xlim(0,1)\n",
    "        plt.ylim(1e-5,1)\n",
    "        plt.yscale('log')\n",
    "        plt.title(\"Assignments\")\n",
    "        plt.savefig(f\"test/assignments{i}.png\")  \n",
    "        plt.close(fig)  \n",
    "\n",
    "        rounded_error = quantizer.get_rounded_error()\n",
    "        rounded_errors.append(rounded_error.item())\n",
    "        i_s.append(i)\n",
    "        # print(f\"Rounded Error: {rounded_error.item()/torch.sum(torch.abs(weights**2))}\")\n",
    "        print(f\"Rounded Error: {rounded_error.item()/H.shape[0]}\")\n",
    "    if j % n_subiters == n_subiters - 1:\n",
    "        n_subiters = int(n_subiters * n_subiters_multiple)\n",
    "        j = 0\n",
    "        impatience_1 = 0\n",
    "\n",
    "    # if loss1.item() > prev_loss:\n",
    "    #     impatience_1 += 1\n",
    "    #     if impatience_1 > patience_up:\n",
    "    #         scheduler_up.step()\n",
    "    #         # print(\"Decreasing learning rate to \", scheduler_up.get_last_lr())\n",
    "    #         impatience_1 = 0\n",
    "    # if loss1.item() < prev_loss:\n",
    "    #     impatience_2 += 1\n",
    "    #     if impatience_2 > patience_down:\n",
    "    #         scheduler_down.step()\n",
    "    #         # print(\"Increasing learning rate to \", scheduler_down.get_last_lr())\n",
    "    #         impatience_2 = 0 \n",
    "\n",
    "    prev_loss = loss1.item()\n",
    "\n",
    "    lrs.append(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "    \n",
    "\n",
    "print(loss1.item()/H.shape[0], loss2.item(), p1.item(), p2.item()/(H.shape[0]**2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1/1.0001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(errors[100:])\n",
    "plt.plot(i_s[1:],rounded_errors[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(np.array(losses)/losses[0], label = \"total loss\")\n",
    "plt.plot(np.array(errors)/errors[0], label = \"error\")\n",
    "plt.plot(np.array(penalties)/penalties[0], label = \"penalty\")\n",
    "plt.plot(np.array(binary_penalty)/binary_penalty[0], label = \"binary penalty\")\n",
    "plt.plot(np.array(sparsity_penalty)/sparsity_penalty[0], label = \"sparsity penalty\")\n",
    "plt.legend()\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array(sparsity_penalty)/(H.shape[0]*k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array(betas)/np.max(betas), label = \"beta\")\n",
    "plt.plot(np.array(pe|nalty_2_weights)/np.max(penalty_2_weights), label = \"penalty_2_weight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.arange(1000)\n",
    "T = 1000\n",
    "plt.plot((np.cos(((i)/T)*np.pi*2))*0.5+0.5)\n",
    "# (np.sin(((i)/T)*np.pi)+1)*0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(errors)/weights.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the assignments\n",
    "assignments = torch.sigmoid(quantizer.initial_assignments)*1.2 - 0.1\n",
    "assignments = torch.clip(assignments,0,1)\n",
    "# assignments = torch.round(assignments)\n",
    "print(assignments)\n",
    "#round the assignments\n",
    "# assignments = torch.round(assignments)\n",
    "print(assignments)\n",
    "#get the quantized_weights\n",
    "quantized_weights = quantizer.codebook @ assignments\n",
    "print(quantized_weights)\n",
    "#get the difference between the quantized weights and the original weights\n",
    "diff = quantized_weights - quantizer.Weights\n",
    "print(quantizer.Weights)\n",
    "print(torch.mean(torch.abs(diff**2))/torch.mean(torch.abs(quantizer.Weights**2)))\n",
    "# print(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.einsum('ik,kl,il->', diff, quantizer.H, diff)/(quantizer.Weights.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(torch.round(torch.sigmoid(quantizer.initial_assignments)).detach().cpu().numpy(),interpolation='nearest', aspect='auto')\n",
    "#scale it to be a square image\n",
    "#add a colorbar\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(torch.sigmoid(quantizer.initial_assignments).detach().cpu().numpy()*1.2-0.1,interpolation='nearest', aspect='auto')\n",
    "#scale it to be a square image\n",
    "#add a colorbar\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sum(assignments,dim = 0)/k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(assignments.detach().cpu().numpy().flatten(),bins=100)\n",
    "# plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = assignments.detach().cpu().numpy().flatten()\n",
    "\n",
    "plt.plot(np.sort(a), np.linspace(0,1,len(a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "\n",
    "#create a gif from soem figures\n",
    "\n",
    "import imageio\n",
    "\n",
    "images = []\n",
    "for fn in sorted(glob.glob(\"test/*.png\")):\n",
    "    images.append(imageio.imread(fn))\n",
    "print(len(images))\n",
    "imageio.mimsave('test.gif', images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(quantizer.Weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2198241/2399002648.py:49: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:489.)\n",
      "  output_part1_grad = output_part1.grad.to(device1)  # Transfer gradients to GPU 0\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 49\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[1;32m     48\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()                               \u001b[38;5;66;03m# Backpropagate from GPU 1\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m output_part1_grad \u001b[38;5;241m=\u001b[39m output_part1\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mto(device1)  \u001b[38;5;66;03m# Transfer gradients to GPU 0\u001b[39;00m\n\u001b[1;32m     50\u001b[0m optimizer2\u001b[38;5;241m.\u001b[39mstep()                             \u001b[38;5;66;03m# Update parameters for part2\u001b[39;00m\n\u001b[1;32m     51\u001b[0m optimizer2\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "\n",
    "# Example large model split into two parts\n",
    "class LargeModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.part1 = nn.Sequential(nn.Linear(1000, 1000), nn.ReLU(), nn.Linear(1000, 1000))\n",
    "        self.part2 = nn.Sequential(nn.Linear(1000, 1000), nn.ReLU(), nn.Linear(1000, 10))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.part1(x)\n",
    "        x = self.part2(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model = LargeModel()\n",
    "\n",
    "# Assign parts of the model to different GPUs\n",
    "device1 = torch.device(\"cuda:0\")\n",
    "device2 = torch.device(\"cuda:1\")\n",
    "\n",
    "model.part1.to(device1)\n",
    "model.part2.to(device2)\n",
    "\n",
    "# Optimizers for each part of the model\n",
    "optimizer1 = optim.SGD(model.part1.parameters(), lr=0.01)\n",
    "optimizer2 = optim.SGD(model.part2.parameters(), lr=0.01)\n",
    "\n",
    "# Example input and target\n",
    "input_data = torch.randn(64, 1000).to(device1)\n",
    "target = input_data[:,:10]**2\n",
    "target = target.to(device2)\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Forward pass\n",
    "output_part1 = model.part1(input_data)        # Compute part1 on GPU 0\n",
    "output_part1 = output_part1.to(device2)       # Transfer activations to GPU 1\n",
    "# output_part1.retain_grad()                    # Retain gradients for backpropagation\n",
    "output = model.part2(output_part1)            # Compute part2 on GPU 1\n",
    "\n",
    "# Compute loss\n",
    "# print(output, target)\n",
    "loss = criterion(output, target)\n",
    "\n",
    "# Backward pass\n",
    "loss.backward()                               # Backpropagate from GPU 1\n",
    "output_part1_grad = output_part1.grad.to(device1)  # Transfer gradients to GPU 0\n",
    "optimizer2.step()                             # Update parameters for part2\n",
    "optimizer2.zero_grad()\n",
    "print(output_part1_grad)\n",
    "# Continue backward pass on GPU 0\n",
    "output_part1.backward(output_part1_grad)      # Backpropagate through part1\n",
    "optimizer1.step()                             # Update parameters for part1\n",
    "optimizer1.zero_grad()\n",
    "\n",
    "# Clear GPU memory\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.32434219121932983\n",
      "Loss: 1.5600860118865967\n",
      "Loss: 0.25419846177101135\n",
      "Loss: 0.16643700003623962\n",
      "Loss: 0.45280444622039795\n",
      "Loss: 0.39094817638397217\n",
      "Loss: 0.17928974330425262\n",
      "Loss: 0.03611570596694946\n",
      "Loss: 0.020241662859916687\n",
      "Loss: 0.07807030528783798\n",
      "Loss: 0.12591275572776794\n",
      "Loss: 0.12145406752824783\n",
      "Loss: 0.07714996486902237\n",
      "Loss: 0.029748214408755302\n",
      "Loss: 0.008412993513047695\n",
      "Loss: 0.017759351059794426\n",
      "Loss: 0.039502762258052826\n",
      "Loss: 0.05200333893299103\n",
      "Loss: 0.04808347672224045\n",
      "Loss: 0.03281133994460106\n",
      "Loss: 0.015831150114536285\n",
      "Loss: 0.0050367009826004505\n",
      "Loss: 0.003289426676928997\n",
      "Loss: 0.0081290602684021\n",
      "Loss: 0.014550039544701576\n",
      "Loss: 0.01823667623102665\n",
      "Loss: 0.017323782667517662\n",
      "Loss: 0.012575904838740826\n",
      "Loss: 0.006577337626367807\n",
      "Loss: 0.0024120279122143984\n",
      "Loss: 0.0017641001613810658\n",
      "Loss: 0.003943785559386015\n",
      "Loss: 0.006777748465538025\n",
      "Loss: 0.00833420641720295\n",
      "Loss: 0.007915263064205647\n",
      "Loss: 0.005928944796323776\n",
      "Loss: 0.0033806769642978907\n",
      "Loss: 0.0014284104108810425\n",
      "Loss: 0.0008621363085694611\n",
      "Loss: 0.001651451806537807\n",
      "Loss: 0.0029351525008678436\n",
      "Loss: 0.0036745539400726557\n",
      "Loss: 0.003411480225622654\n",
      "Loss: 0.002438134513795376\n",
      "Loss: 0.0013201512629166245\n",
      "Loss: 0.0005723177455365658\n",
      "Loss: 0.00048143244930543005\n",
      "Loss: 0.0009576766169629991\n",
      "Loss: 0.0015522342873737216\n",
      "Loss: 0.0017943804850801826\n",
      "Loss: 0.0015559160383418202\n",
      "Loss: 0.0010504410602152348\n",
      "Loss: 0.0005757817416451871\n",
      "Loss: 0.00033948069903999567\n",
      "Loss: 0.0004127607389818877\n",
      "Loss: 0.000687631603796035\n",
      "Loss: 0.0009161535417661071\n",
      "Loss: 0.000909558730199933\n",
      "Loss: 0.000692286528646946\n",
      "Loss: 0.00042975798714905977\n",
      "Loss: 0.000266934308456257\n",
      "Loss: 0.00025840153102762997\n",
      "Loss: 0.00037199383950792253\n",
      "Loss: 0.0005012333858758211\n",
      "Loss: 0.0005310503183864057\n",
      "Loss: 0.00043887970969080925\n",
      "Loss: 0.00030273332959041\n",
      "Loss: 0.00021067365014459938\n",
      "Loss: 0.0001968655560631305\n",
      "Loss: 0.0002464350836817175\n",
      "Loss: 0.00030979610164649785\n",
      "Loss: 0.0003276753704994917\n",
      "Loss: 0.00028121835202910006\n",
      "Loss: 0.0002092472859658301\n",
      "Loss: 0.0001623594871489331\n",
      "Loss: 0.00015811728371772915\n",
      "Loss: 0.00018321882816962898\n",
      "Loss: 0.00021047890186309814\n",
      "Loss: 0.00021262992231640965\n",
      "Loss: 0.000183345444384031\n",
      "Loss: 0.00014548983017448336\n",
      "Loss: 0.00012546157813630998\n",
      "Loss: 0.00012851387145929039\n",
      "Loss: 0.00014313157589640468\n",
      "Loss: 0.00015387998428195715\n",
      "Loss: 0.00014911375183146447\n",
      "Loss: 0.00013025678345002234\n",
      "Loss: 0.00011204449401702732\n",
      "Loss: 0.0001062019873643294\n",
      "Loss: 0.0001109592049033381\n",
      "Loss: 0.00011779904161812738\n",
      "Loss: 0.00011913981870748103\n",
      "Loss: 0.0001118221043725498\n",
      "Loss: 0.00010027890675701201\n",
      "Loss: 9.28799418034032e-05\n",
      "Loss: 9.264020627597347e-05\n",
      "Loss: 9.57048760028556e-05\n",
      "Loss: 9.715046326164156e-05\n",
      "Loss: 9.436521213501692e-05\n",
      "Loss: 8.811897714622319e-05\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "\n",
    "# Define a large model with two partitions\n",
    "class LargeModel(nn.Module):\n",
    "    def __init__(self,input_size, output_size): \n",
    "        super().__init__()\n",
    "        self.part1 = nn.Sequential(nn.Linear(input_size, 1000), nn.ReLU(), nn.Linear(1000, 1000))\n",
    "        self.part2 = nn.Sequential(nn.ReLU(), nn.Linear(1000, output_size))\n",
    "    \n",
    "    def forward_part1(self, x):\n",
    "        return self.part1(x)\n",
    "    \n",
    "    def forward_part2(self, x):\n",
    "        return self.part2(x)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.forward_part1(x)\n",
    "        x = self.forward_part2(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model = LargeModel(1,1)\n",
    "device = torch.device(\"cuda:7\")  # Single GPU\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Manual Partitioning Workflow\n",
    "def forward_backward_manual(input_data, target, criterion):\n",
    "    # Move part1 to GPU\n",
    "    model.part1.to(device)\n",
    "    input_data = input_data.to(device)\n",
    "    \n",
    "    # Forward pass through part1\n",
    "    intermediate = model.forward_part1(input_data)\n",
    "    \n",
    "    # Move part1 back to CPU to free memory\n",
    "    model.part1.to(\"cpu\")\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Move part2 to GPU\n",
    "    model.part2.to(device)\n",
    "    intermediate_values = intermediate.clone().detach().requires_grad_(True)\n",
    "    intermediate.to(\"cpu\")\n",
    "    \n",
    "    # Forward pass through part2\n",
    "    output = model.forward_part2(intermediate_values)\n",
    "    \n",
    "    # Compute loss\n",
    "    target = target.to(device)\n",
    "    loss = criterion(output, target)\n",
    "    \n",
    "    # Backward pass through part2\n",
    "    loss.backward()\n",
    "\n",
    "    # print(intermediate_values.grad)\n",
    "    # raise NotImplementedError\n",
    "    \n",
    "    # Move part2 back to CPU to free memory\n",
    "    model.part2.to(\"cpu\")\n",
    "    grad_keep = intermediate_values.grad.clone()\n",
    "    intermediate_values.to(\"cpu\")\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "\n",
    "    # Move part1 back to GPU for backward pass\n",
    "    # print(intermediate.grad)\n",
    "    model.part1.to(device)\n",
    "    intermediate.to(device)\n",
    "    intermediate.backward(grad_keep)  # Backward through part1\n",
    "\n",
    "    # Update parameters\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "# Example usage\n",
    "input_data = torch.randn(100, 1)\n",
    "fn = lambda x: torch.sin(x)\n",
    "target = fn(input_data)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Training step\n",
    "\n",
    "for _ in range(100):\n",
    "    loss = forward_backward_manual(input_data, target, criterion)\n",
    "    print(f\"Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7d0ea1508950>]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(input_data.detach().cpu().numpy(), target.detach().cpu().numpy(), 'o')\n",
    "\n",
    "x = torch.linspace(torch.min(input_data),torch.max(input_data),100).reshape(-1,1)\n",
    "model.to(\"cpu\")\n",
    "y_target = fn(x)\n",
    "y = model(x).detach().cpu().numpy()\n",
    "plt.plot(x.cpu().numpy(), y)\n",
    "plt.plot(x.cpu().numpy(), y_target.cpu().numpy(), 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8958933353424072\n",
      "2.900325059890747\n",
      "1.4935258626937866\n",
      "0.4889121949672699\n",
      "0.40388885140419006\n",
      "0.5391551852226257\n",
      "0.49618303775787354\n",
      "0.2941880226135254\n",
      "0.09974351525306702\n",
      "0.02735009416937828\n",
      "0.0840713232755661\n",
      "0.17726299166679382\n",
      "0.20794624090194702\n",
      "0.1591680496931076\n",
      "0.0827343761920929\n",
      "0.03432105481624603\n",
      "0.03185631334781647\n",
      "0.05750611424446106\n",
      "0.08222845941781998\n",
      "0.08736351132392883\n",
      "0.07153196632862091\n",
      "0.04581703618168831\n",
      "0.024716360494494438\n",
      "0.016855599358677864\n",
      "0.0211232490837574\n",
      "0.029118327423930168\n",
      "0.03256360813975334\n",
      "0.02894776500761509\n",
      "0.021734628826379776\n",
      "0.016213271766901016\n",
      "0.01520300842821598\n",
      "0.017678266391158104\n",
      "0.02038605511188507\n",
      "0.02042566053569317\n",
      "0.01710822992026806\n",
      "0.012089740484952927\n",
      "0.008116073906421661\n",
      "0.007117605302482843\n",
      "0.008852270431816578\n",
      "0.011157728731632233\n",
      "0.011645324528217316\n",
      "0.009535453282296658\n",
      "0.006106610409915447\n",
      "0.0034924643114209175\n",
      "0.003011173103004694\n",
      "0.004348323214799166\n",
      "0.00605732062831521\n",
      "0.0067146853543818\n",
      "0.005841369740664959\n",
      "0.004057219717651606\n",
      "0.002493997337296605\n",
      "0.001947397249750793\n",
      "0.0023813117295503616\n",
      "0.0030880796257406473\n",
      "0.0033224259968847036\n",
      "0.0028634103946387768\n",
      "0.002072317525744438\n",
      "0.0014904487179592252\n",
      "0.0013790896628051996\n",
      "0.0015927128260955215\n",
      "0.0017908557783812284\n",
      "0.0017471392638981342\n",
      "0.0014941788977012038\n",
      "0.0012324467534199357\n",
      "0.0011225832859054208\n",
      "0.001152037875726819\n",
      "0.0011774051235988736\n",
      "0.0010819939197972417\n",
      "0.0008871128666214645\n",
      "0.000717150280252099\n",
      "0.0006720078526996076\n",
      "0.0007335513946600258\n",
      "0.000792140606790781\n",
      "0.0007544498075731099\n",
      "0.0006258980138227344\n",
      "0.0004961788072250783\n",
      "0.0004502263036556542\n",
      "0.0004921892541460693\n",
      "0.0005498258979059756\n",
      "0.000547455158084631\n",
      "0.0004735903348773718\n",
      "0.00038180622505024076\n",
      "0.00033342582173645496\n",
      "0.00034289341419935226\n",
      "0.00037412188248708844\n",
      "0.0003822625440079719\n",
      "0.00035446605761535466\n",
      "0.0003136166196782142\n",
      "0.0002892035117838532\n",
      "0.0002886048750951886\n",
      "0.00029547029407694936\n",
      "0.0002909788745455444\n",
      "0.0002728282706812024\n",
      "0.00025331860524602234\n",
      "0.0002435225178487599\n",
      "0.00024186132941395044\n",
      "0.00023859298380557448\n",
      "0.00022824833285994828\n",
      "0.00021519824804272503\n",
      "0.0002075147640425712\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "model = LargeModel(1,1)\n",
    "device = torch.device(\"cuda:7\")  # Single GPU\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for _ in range(100):\n",
    "    output = model(input_data)\n",
    "    loss = criterion(output, target)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.],\n",
      "        [1.]])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "x = torch.ones(2,1).requires_grad_()\n",
    "y = F.linear(x, torch.eye(1))   \n",
    "\n",
    "(y[0]**2 + y[1]).backward()\n",
    "print(x.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '2', '3']\n",
      "['3', '2', '1']\n"
     ]
    }
   ],
   "source": [
    "class a:\n",
    "    def __init__(self):\n",
    "        self.a = [\"1\",\"2\",\"3\"]\n",
    "\n",
    "a = a()\n",
    "b = a.a \n",
    "\n",
    "b = b[::-1]\n",
    "print(a.a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "state_dict = torch.load(\"../quantized_models/llama-2-7b/2bpv/quantized/layer_0.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self_attn.q_proj.original_bias 4096\n",
      "self_attn.q_proj.quantizer.norms_1 4096\n",
      "self_attn.q_proj.quantizer.norms_0 4096\n",
      "self_attn.q_proj.quantizer.codebook 1024\n",
      "self_attn.q_proj.quantizer.codes 4194304\n",
      "self_attn.k_proj.original_bias 4096\n",
      "self_attn.k_proj.quantizer.norms_1 4096\n",
      "self_attn.k_proj.quantizer.norms_0 4096\n",
      "self_attn.k_proj.quantizer.codebook 1024\n",
      "self_attn.k_proj.quantizer.codes 4194304\n",
      "self_attn.v_proj.original_bias 4096\n",
      "self_attn.v_proj.quantizer.norms_1 4096\n",
      "self_attn.v_proj.quantizer.norms_0 4096\n",
      "self_attn.v_proj.quantizer.codebook 1024\n",
      "self_attn.v_proj.quantizer.codes 4194304\n",
      "self_attn.o_proj.original_bias 4096\n",
      "self_attn.o_proj.quantizer.norms_1 4096\n",
      "self_attn.o_proj.quantizer.norms_0 4096\n",
      "self_attn.o_proj.quantizer.codebook 1024\n",
      "self_attn.o_proj.quantizer.codes 4194304\n",
      "mlp.gate_proj.original_bias 11008\n",
      "mlp.gate_proj.quantizer.norms_1 11008\n",
      "mlp.gate_proj.quantizer.norms_0 4096\n",
      "mlp.gate_proj.quantizer.codebook 1024\n",
      "mlp.gate_proj.quantizer.codes 11272192\n",
      "mlp.up_proj.original_bias 11008\n",
      "mlp.up_proj.quantizer.norms_1 11008\n",
      "mlp.up_proj.quantizer.norms_0 4096\n",
      "mlp.up_proj.quantizer.codebook 1024\n",
      "mlp.up_proj.quantizer.codes 11272192\n",
      "mlp.down_proj.original_bias 4096\n",
      "mlp.down_proj.quantizer.norms_1 4096\n",
      "mlp.down_proj.quantizer.norms_0 11008\n",
      "mlp.down_proj.quantizer.codebook 1024\n",
      "mlp.down_proj.quantizer.codes 11272192\n",
      "input_layernorm.weight 4096\n",
      "post_attention_layernorm.weight 4096\n",
      "50729728\n"
     ]
    }
   ],
   "source": [
    "n_params = 0\n",
    "for k,v in state_dict.items():\n",
    "    if isinstance(v, torch.FloatTensor):\n",
    "        n_params += v.numel()\n",
    "        print(k,v.numel())\n",
    "print(n_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'codes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_584331/3536948094.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'codes' is not defined"
     ]
    }
   ],
   "source": [
    "codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
