{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e1d005e73ae465a9f65e4a7efbcfcc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def get_llama(model):\n",
    "    import torch\n",
    "    def skip(*args, **kwargs):\n",
    "        pass\n",
    "    torch.nn.init.kaiming_uniform_ = skip\n",
    "    torch.nn.init.uniform_ = skip\n",
    "    torch.nn.init.normal_ = skip\n",
    "    from transformers import LlamaForCausalLM\n",
    "    model = LlamaForCausalLM.from_pretrained(model, torch_dtype='auto')\n",
    "    model.seqlen = 2048\n",
    "    return model\n",
    "\n",
    "model = get_llama(\"huggyllama/llama-7b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[-0.0096, -0.0301,  0.0085,  ...,  0.0178, -0.0052, -0.0365],\n",
       "         [-0.0029, -0.0101,  0.0100,  ...,  0.0147,  0.0040, -0.0104],\n",
       "         [-0.0004,  0.0139, -0.0074,  ..., -0.0083, -0.0070,  0.0146],\n",
       "         ...,\n",
       "         [-0.0107, -0.0061,  0.0310,  ..., -0.0052, -0.0143,  0.0236],\n",
       "         [-0.0104, -0.0213, -0.0129,  ..., -0.0199, -0.0143, -0.0103],\n",
       "         [ 0.0184,  0.0119,  0.0195,  ...,  0.0343, -0.0327, -0.0355]],\n",
       "        dtype=torch.float16, grad_fn=<CloneBackward0>),\n",
       " tensor([[-0.0316,  0.0256, -0.0027,  ...,  0.0219, -0.0161,  0.0289],\n",
       "         [-0.0054, -0.0039,  0.0031,  ..., -0.0019, -0.0096,  0.0147],\n",
       "         [ 0.0014,  0.0120, -0.0139,  ..., -0.0133,  0.0065, -0.0110],\n",
       "         ...,\n",
       "         [-0.0311,  0.0378, -0.0543,  ...,  0.0161, -0.0359, -0.0412],\n",
       "         [-0.0378,  0.0015, -0.0431,  ...,  0.0348, -0.0481, -0.0129],\n",
       "         [ 0.0179,  0.0389,  0.0396,  ..., -0.0151,  0.0100, -0.0242]],\n",
       "        dtype=torch.float16, grad_fn=<CloneBackward0>),\n",
       " tensor([[ 0.0061,  0.0045,  0.0020,  ..., -0.0079,  0.0114,  0.0150],\n",
       "         [-0.0030, -0.0101, -0.0003,  ...,  0.0090, -0.0078,  0.0049],\n",
       "         [-0.0076, -0.0062, -0.0069,  ...,  0.0076, -0.0008,  0.0050],\n",
       "         ...,\n",
       "         [ 0.0006, -0.0003,  0.0183,  ..., -0.0220, -0.0165, -0.0345],\n",
       "         [-0.0104,  0.0019, -0.0041,  ..., -0.0084, -0.0054, -0.0218],\n",
       "         [ 0.0099, -0.0020, -0.0023,  ..., -0.0068,  0.0156,  0.0027]],\n",
       "        dtype=torch.float16, grad_fn=<CloneBackward0>),\n",
       " tensor([[ 5.3520e-03,  6.9389e-03,  4.0398e-03,  ...,  5.8174e-03,\n",
       "           1.5020e-03, -6.0997e-03],\n",
       "         [ 7.3662e-03,  3.4180e-03,  8.2550e-03,  ..., -3.8600e-04,\n",
       "           1.1452e-02,  5.6458e-03],\n",
       "         [-7.2212e-03,  4.6158e-03,  9.6846e-04,  ..., -1.8005e-02,\n",
       "          -1.3214e-02,  6.8550e-03],\n",
       "         ...,\n",
       "         [-2.3689e-03,  2.7714e-03, -1.4849e-03,  ...,  1.8066e-02,\n",
       "           5.3482e-03, -2.8515e-03],\n",
       "         [-5.5909e-05, -6.5727e-03,  8.5144e-03,  ..., -1.8204e-02,\n",
       "          -7.1907e-03,  2.9411e-03],\n",
       "         [ 6.8817e-03,  1.6747e-03,  5.0621e-03,  ..., -3.0548e-02,\n",
       "           1.4372e-03,  3.9101e-03]], dtype=torch.float16,\n",
       "        grad_fn=<CloneBackward0>)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = []\n",
    "layer = model.model.layers[0]\n",
    "weights.append(layer.self_attn.q_proj.weight.clone())\n",
    "weights.append(layer.self_attn.k_proj.weight.clone())\n",
    "weights.append(layer.self_attn.v_proj.weight.clone())\n",
    "weights.append(layer.self_attn.o_proj.weight.clone())\n",
    "\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16384, 4096])\n"
     ]
    }
   ],
   "source": [
    "total_weights = torch.concatenate(weights, dim=0)\n",
    "print(total_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "\n",
    "@torch.jit.script\n",
    "def assigment_step(W:torch.Tensor, H:torch.Tensor, quantized_vectors:torch.Tensor):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        W (torch.tensor): weights of shape (m,n))\n",
    "        H (torch.tensor): matrix of shape (n,n)\n",
    "        quantized_vectors (torch.tensor): quantized vectors of shape (n,k)\n",
    "            where k is the number of quantized vectors\n",
    "    \n",
    "    Returns:\n",
    "        torch.tensor: updated assignments\n",
    "        torch.tensor: updated errors\n",
    "    \"\"\"\n",
    "\n",
    "    #create a tensor of shape (m,k,n)\n",
    "    #where the slice [i,k,:] consists of \n",
    "    # W[i] - quantized_vectors[k]   \n",
    "    #minus the k quantized vectors\n",
    "    # print(W.dtype)\n",
    "    # print(torch.max(H))\n",
    "    # print(quantized_vectors.shape)\n",
    "    assert torch.all(torch.isfinite(W)), f\"W is not finite, {W}, {W[~torch.isfinite(W)]}\"\n",
    "    assert torch.all(torch.isfinite(quantized_vectors)), f\"quantized_vectors is not finite, {quantized_vectors}, {quantized_vectors[~torch.isfinite(quantized_vectors)]}\"\n",
    "    assignments = torch.zeros(W.shape[0], dtype=torch.long)\n",
    "    error = 0.0\n",
    "    # print(W.shape)\n",
    "    for i in range(W.shape[0]):\n",
    "        diff = W[i].unsqueeze(0) - quantized_vectors.T\n",
    "        #shape (k,n)\n",
    "        # print(diff.shape)\n",
    "        errors = torch.einsum('jk,kl,jl->j', diff, H, diff)\n",
    "        assignments[i] = torch.argmin(errors)\n",
    "        # print(errors)\n",
    "        error += (errors[assignments[i]]).item()\n",
    "        # print(error)\n",
    "    return assignments, error\n",
    "\n",
    "@torch.jit.script\n",
    "def update_step(W:torch.Tensor, prev_quantized:torch.Tensor, assignments:torch.Tensor)->torch.Tensor:\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        W (torch.tensor): weights of shape (n,n)\n",
    "        assignments (torch.tensor): assignments of shape (n,)\n",
    "    \n",
    "    Returns:\n",
    "        quantized_vectors (torch.tensor): quantized vectors of shape (n,k)\n",
    "    \"\"\"\n",
    "\n",
    "    #initialize the updated quantized vectors\n",
    "    updated_quantized_vectors = torch.zeros_like(prev_quantized)\n",
    "\n",
    "    #the quantized vectors are just the mean of the weights\n",
    "    #that are assigned to the same cluster\n",
    "    for i in range(updated_quantized_vectors.shape[0]):\n",
    "        # assert torch.all(torch.isfinite(W[assignments == i])), f\"W[assigments == i] is not finite, {W[assignments == i]}, {W[assignments == i][~torch.isfinite(W[assignments == i])]}\"\n",
    "        if torch.any(assignments == i):\n",
    "            updated_quantized_vectors[:,i] = W[assignments == i].mean(dim=0)\n",
    "    \n",
    "    return updated_quantized_vectors\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def vector_quantize(W, H, k, max_iters = 1000, \n",
    "                    max_init_iters = 1,\n",
    "                    convergence_threshold = 1e-3):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        W (torch.tensor): weights of shape (n,n)\n",
    "        H (torch.tensor): matrix of shape (n,n)\n",
    "        k (int): number of quantized vectors\n",
    "    \n",
    "    Returns:\n",
    "        torch.tensor: quantized vectors of shape (n,k)\n",
    "    \"\"\"\n",
    "    assert torch.all(torch.isfinite(H)), f\"H is not finite, {H}, {H[~torch.isfinite(H)]}\"\n",
    "    min_error = float('inf')\n",
    "    bar = tqdm.tqdm(total=max_init_iters*max_iters)\n",
    "    for i in range(max_init_iters):\n",
    "        #initialize the quantized vectors\n",
    "        indexs = torch.randperm(W.shape[0])[:k]\n",
    "        quantized_vectors = W[indexs,:].T\n",
    "        converged = False\n",
    "        for i in range(max_iters):\n",
    "            bar.update(1)\n",
    "            updated_assignments,error = assigment_step(W, H, quantized_vectors)\n",
    "            print(\"error\", error)\n",
    "            quantized_vectors = update_step(W, quantized_vectors, updated_assignments)\n",
    "            # print(updated_quantized_vectors.shape)\n",
    "            if i != 0:\n",
    "                if torch.allclose(assignments, updated_assignments, atol=convergence_threshold):\n",
    "                    print(f\"Converged after {i} iterations, error {error}\")\n",
    "                    converged = True\n",
    "                    bar.update(max_iters - i - 1)\n",
    "                    break\n",
    "            assignments = updated_assignments\n",
    "        if not converged:\n",
    "            print(\"warning: did not converge\")\n",
    "        if error < min_error:\n",
    "            min_error = error\n",
    "            best_quantized_vectors = quantized_vectors\n",
    "            best_assignments = assignments\n",
    "    print(\"quantized with best error\", min_error)\n",
    "    return best_quantized_vectors, best_assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.load('/home/lliu/huffman/test/original_weights.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = data[\"H\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:7\")\n",
    "# weights = data[\"weights\"].to(device)\n",
    "weights = total_weights.to(device) \n",
    "H = H.to(device).to(weights.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16384, 4096])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [08:34<00:00,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 24 iterations, error 0\n",
      "quantized with best error 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "quantized_vectors, assignments = vector_quantize(weights, H/H.shape[0], 3072)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.load(\"/home/lliu/huffman/test/original_weights2.pt\")\n",
    "\n",
    "device = torch.device(\"cuda:7\")\n",
    "\n",
    "for key in data.keys():\n",
    "    # if isinstance(data[key], torch.Tensor):\n",
    "    #     data[key] = data[key].to(device)\n",
    "    if isinstance(data[key], list):\n",
    "        for i in range(len(data[key])):\n",
    "            data[key][i] = data[key][i].to(device)\n",
    "\n",
    "for key in data.keys():\n",
    "    if isinstance(data[key], list):\n",
    "        data[key] = torch.stack(data[key], dim=0)   \n",
    "\n",
    "x = data[\"Input\"].reshape(-1, H.shape[0]).to(H.dtype)\n",
    "y = data[\"Output\"].reshape(-1, H.shape[0]).to(H.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5557, device='cuda:7', dtype=torch.float16, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantized_weights = quantized_vectors[:,assignments[:4096]].T\n",
    "torch.sum((quantized_weights - weights[:4096])**2)/torch.sum(weights[:4096]**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.4717,  0.7559, -0.5723,  ...,  0.7559,  0.6211,  0.0622],\n",
      "        [ 1.4893,  0.7520, -0.5386,  ...,  0.7520,  0.5962, -0.0842],\n",
      "        [ 1.5059,  0.7505, -0.5044,  ...,  0.7505,  0.5630, -0.0619],\n",
      "        ...,\n",
      "        [ 1.4482,  0.7217, -0.4734,  ...,  0.7217,  0.6177, -0.0407],\n",
      "        [ 1.4570,  0.6582, -0.5508,  ...,  0.6582,  0.7993, -0.3870],\n",
      "        [ 1.3516,  0.7266, -0.5947,  ...,  0.7266,  0.6470,  0.0765]],\n",
      "       device='cuda:7', dtype=torch.float16, grad_fn=<MmBackward0>)\n",
      "0.04073159784560144\n"
     ]
    }
   ],
   "source": [
    "y_hat = x @ quantized_weights.T\n",
    "print(y_hat)\n",
    "print(torch.mean((y_hat - y)**2).item()/torch.mean(y**2).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 3072 artists>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot out the distribution for each cluster\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "unique, counts = np.unique(assignments.cpu().numpy(), return_counts=True)\n",
    "plt.bar(unique, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16384"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16384])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assignments.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
