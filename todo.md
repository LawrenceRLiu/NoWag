# To-Do List

## Refactoring
- [x] Make a parent class for all compression algorithms
- [x] Move Sparse and Quantization to their own files
- [x] Fix the layer by layer code (fixed for quantization only)
- [x] Fix the Sparse code
- [ ] Make a joint class that works

## Zero-shot Compression
- [ ] Low Rank to work
- [ ] Structured FFN to work
- [ ] non-determnistic pruning to work
- [ ] Convert hessian generation to importance generation (diagonal only)
- [ ] 70B model 7d quantization, talk to GT to split up the weights
- [ ] Trellis Quantization

## 

## Fine-tuning
- [ ] Get finetuning script to work
- [ ] LoRA finetuning for 
- [ ] Pruned finetuning 

## Overall
- [ ] Compress Deepseek
- [ ] Llama 3.1
- [ ] Mistral?

