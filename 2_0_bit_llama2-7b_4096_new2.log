/home/lliu/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.24s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.29it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.18it/s]
Model loaded. LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
4096
Loading tokenizer for meta-llama/Llama-2-7b-hf
Starting...
39708 MiB free out of 48676 MiB total
attention_mask torch.Size([1, 1, 4096, 4096]) cuda:5 torch.float16
position_ids torch.Size([1, 4096]) cuda:5 torch.int64
position_ids torch.Size([1, 4096])
attention_mask torch.Size([1, 1, 4096, 4096])
Ready.
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj'], ['self_attn.o_proj'], ['mlp.up_proj', 'mlp.gate_proj'], ['mlp.down_proj']]
finished adding batches
subset {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
0 self_attn.k_proj
Pruning ...
realigning
initial loss 1.8915133476257324
final loss 1.5419642925262451
quantized
not here
quantized in 22.248396158218384 seconds
37920 MiB free out of 48676 MiB total
0 self_attn.v_proj
Pruning ...
realigning
initial loss 0.11391407996416092
final loss 0.09255972504615784
quantized
not here
quantized in 19.36563277244568 seconds
37984 MiB free out of 48676 MiB total
0 self_attn.q_proj
Pruning ...
realigning
initial loss 1.56924569606781
final loss 1.1642513275146484
quantized
not here
quantized in 20.534560918807983 seconds
37984 MiB free out of 48676 MiB total
finished: {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 27648
optimizing the following parameters: dict_keys(['self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 152051712
n accumulation steps: 1
n batches: 128
epoch 0 loss: 6.124211113789357e-07
epoch 1 loss: 2.9747138197144807e-07
epoch 2 loss: 2.2992374648644898e-07
epoch 3 loss: 2.007969511774732e-07
epoch 4 loss: 1.851115805084902e-07
finished adding batches
subset {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
0 self_attn.o_proj
Pruning ...
realigning
initial loss 0.016807274892926216
final loss 0.013278797268867493
quantized
not here
quantized in 21.51044249534607 seconds
38166 MiB free out of 48676 MiB total
finished: {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 36864
optimizing the following parameters: dict_keys(['mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135274496
n accumulation steps: 1
n batches: 128
epoch 0 loss: 1.6526248556303358e-06
epoch 1 loss: 1.4523200029259442e-06
epoch 2 loss: 1.330203279081843e-06
epoch 3 loss: 1.241809218122114e-06
epoch 4 loss: 1.1745293555520675e-06
finished adding batches
subset {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
0 mlp.up_proj
Pruning ...
realigning
initial loss 3.701059341430664
final loss 3.4530630111694336
quantized
not here
quantized in 54.63915252685547 seconds
36970 MiB free out of 48676 MiB total
0 mlp.gate_proj
Pruning ...
realigning
initial loss 5.41966438293457
final loss 4.636141300201416
quantized
not here
quantized in 54.504339933395386 seconds
36862 MiB free out of 48676 MiB total
finished: {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 69120
optimizing the following parameters: dict_keys(['mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 45096960
n accumulation steps: 1
n batches: 128
epoch 0 loss: 2.201201058760205e-06
epoch 1 loss: 1.8086624713475885e-06
epoch 2 loss: 1.6713625212716465e-06
epoch 3 loss: 1.6033232093093375e-06
epoch 4 loss: 1.558539517887425e-06
finished adding batches
subset {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
0 mlp.down_proj
Pruning ...
realigning
initial loss 0.0175836980342865
final loss 0.014217210933566093
quantized
not here
quantized in 53.37568521499634 seconds
35924 MiB free out of 48676 MiB total
finished: {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 3.831954686006611e-06
epoch 1 loss: 3.506268729580597e-06
epoch 2 loss: 3.4074282648788312e-06
epoch 3 loss: 3.3467681781473857e-06
epoch 4 loss: 3.3052716332804266e-06
9974 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
9974 MiB free out of 48676 MiB total
after cast to cpu
37506 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj'], ['self_attn.o_proj'], ['mlp.up_proj', 'mlp.gate_proj'], ['mlp.down_proj']]
finished adding batches
subset {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
1 self_attn.k_proj
Pruning ...
realigning
initial loss 24.864742279052734
final loss 19.714046478271484
quantized
not here
quantized in 22.06756091117859 seconds
37568 MiB free out of 48676 MiB total
1 self_attn.v_proj
Pruning ...
realigning
initial loss 1.0546290874481201
final loss 0.9229445457458496
quantized
not here
quantized in 20.100680589675903 seconds
37568 MiB free out of 48676 MiB total
1 self_attn.q_proj
Pruning ...
realigning
initial loss 23.2281551361084
final loss 17.99720001220703
quantized
not here
quantized in 21.35391402244568 seconds
37568 MiB free out of 48676 MiB total
finished: {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 27648
optimizing the following parameters: dict_keys(['self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 152051712
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0006613294802022551
epoch 1 loss: 0.0002002435611814235
epoch 2 loss: 0.00018462387239281952
epoch 3 loss: 0.00023962715251713007
epoch 4 loss: 0.000181687645984141
finished adding batches
subset {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
1 self_attn.o_proj
Pruning ...
realigning
initial loss 0.08045915514230728
final loss 0.0749443769454956
quantized
not here
quantized in 22.23232126235962 seconds
37570 MiB free out of 48676 MiB total
finished: {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 36864
optimizing the following parameters: dict_keys(['mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135274496
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.00023155757158122015
epoch 1 loss: 0.00012615846698338373
epoch 2 loss: 9.885386172214794e-05
epoch 3 loss: 7.774966105245085e-05
epoch 4 loss: 6.282065204743503e-05
finished adding batches
subset {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
1 mlp.up_proj
Pruning ...
realigning
initial loss 13.815547943115234
final loss 13.2343111038208
quantized
not here
quantized in 56.78420400619507 seconds
36546 MiB free out of 48676 MiB total
1 mlp.gate_proj
Pruning ...
realigning
initial loss 21.96585464477539
final loss 17.89333724975586
quantized
not here
quantized in 55.40020418167114 seconds
36202 MiB free out of 48676 MiB total
finished: {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 69120
optimizing the following parameters: dict_keys(['mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 45096960
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.15227855305420235
epoch 1 loss: 0.14265254337806255
epoch 2 loss: 0.13329138210974634
epoch 3 loss: 0.12411796260857955
epoch 4 loss: 0.11517708467727061
finished adding batches
subset {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
1 mlp.down_proj
Pruning ...
realigning
initial loss 0.08815260231494904
final loss 0.08815260231494904
quantized
not here
quantized in 53.707571506500244 seconds
35178 MiB free out of 48676 MiB total
finished: {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0001903567245449267
epoch 1 loss: 2.4973001359285263e-05
epoch 2 loss: 2.4231857992162986e-05
epoch 3 loss: 2.418580466212461e-05
epoch 4 loss: 2.4077012866996483e-05
10596 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
10596 MiB free out of 48676 MiB total
after cast to cpu
36546 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj'], ['self_attn.o_proj'], ['mlp.up_proj', 'mlp.gate_proj'], ['mlp.down_proj']]
finished adding batches
subset {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
2 self_attn.k_proj
Pruning ...
realigning
initial loss 90.03482055664062
final loss 71.28511047363281
quantized
not here
quantized in 23.96126437187195 seconds
36544 MiB free out of 48676 MiB total
2 self_attn.v_proj
Pruning ...
realigning
initial loss 14.029500007629395
final loss 13.418258666992188
quantized
not here
quantized in 19.944021463394165 seconds
36544 MiB free out of 48676 MiB total
2 self_attn.q_proj
Pruning ...
realigning
initial loss 74.0782241821289
final loss 62.59196472167969
quantized
not here
quantized in 22.169654846191406 seconds
36544 MiB free out of 48676 MiB total
finished: {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 27648
optimizing the following parameters: dict_keys(['self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 152051712
n accumulation steps: 1
n batches: 128
epoch 0 loss: 2.1920975477485172e-05
epoch 1 loss: 1.3040519682760987e-05
epoch 2 loss: 1.1146037039111434e-05
epoch 3 loss: 1.0220248732650816e-05
epoch 4 loss: 9.641817136696318e-06
finished adding batches
subset {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
2 self_attn.o_proj
Pruning ...
realigning
initial loss 0.06671703606843948
final loss 0.06432859599590302
quantized
not here
quantized in 22.24366521835327 seconds
37570 MiB free out of 48676 MiB total
finished: {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 36864
optimizing the following parameters: dict_keys(['mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135274496
n accumulation steps: 1
n batches: 128
epoch 0 loss: 1.0826760366455801e-05
epoch 1 loss: 9.654401075920305e-06
epoch 2 loss: 9.03192577794698e-06
epoch 3 loss: 8.63766024394863e-06
epoch 4 loss: 8.363346790218884e-06
finished adding batches
subset {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
2 mlp.up_proj
Pruning ...
realigning
initial loss 29.359020233154297
final loss 29.05937957763672
quantized
not here
quantized in 54.45400953292847 seconds
36546 MiB free out of 48676 MiB total
2 mlp.gate_proj
Pruning ...
realigning
initial loss 37.66706466674805
final loss 36.66929626464844
quantized
not here
quantized in 55.030794858932495 seconds
36202 MiB free out of 48676 MiB total
finished: {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 69120
optimizing the following parameters: dict_keys(['mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 45096960
n accumulation steps: 1
n batches: 128
epoch 0 loss: 6.18554828406559e-05
epoch 1 loss: 5.9360958857723745e-05
epoch 2 loss: 5.762688118693404e-05
epoch 3 loss: 5.636732751668205e-05
epoch 4 loss: 5.541748907944566e-05
finished adding batches
subset {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
2 mlp.down_proj
Pruning ...
realigning
initial loss 0.1379254162311554
final loss 0.1379254162311554
quantized
not here
quantized in 52.47152233123779 seconds
36202 MiB free out of 48676 MiB total
finished: {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 4.413330128727466e-05
epoch 1 loss: 4.352287157871615e-05
epoch 2 loss: 4.341707082744506e-05
epoch 3 loss: 4.3349684290205914e-05
epoch 4 loss: 4.329743339326342e-05
11382 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
11382 MiB free out of 48676 MiB total
after cast to cpu
36546 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj'], ['self_attn.o_proj'], ['mlp.up_proj', 'mlp.gate_proj'], ['mlp.down_proj']]
finished adding batches
subset {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
3 self_attn.k_proj
Pruning ...
realigning
initial loss 205.14120483398438
final loss 173.57424926757812
quantized
not here
quantized in 23.15638017654419 seconds
36544 MiB free out of 48676 MiB total
3 self_attn.v_proj
Pruning ...
realigning
initial loss 37.44446563720703
final loss 36.62053298950195
quantized
not here
quantized in 20.19514751434326 seconds
36544 MiB free out of 48676 MiB total
3 self_attn.q_proj
Pruning ...
realigning
initial loss 179.91513061523438
final loss 161.4221649169922
quantized
not here
quantized in 21.865843772888184 seconds
36544 MiB free out of 48676 MiB total
finished: {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 27648
optimizing the following parameters: dict_keys(['self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 152051712
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.00012212165546543474
epoch 1 loss: 9.01187204362941e-05
epoch 2 loss: 7.89710887261208e-05
epoch 3 loss: 7.230285660853042e-05
epoch 4 loss: 6.777630903798126e-05
finished adding batches
subset {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
3 self_attn.o_proj
Pruning ...
realigning
initial loss 0.9192160367965698
final loss 0.546825647354126
quantized
not here
quantized in 22.86370062828064 seconds
37961 MiB free out of 48676 MiB total
finished: {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 36864
optimizing the following parameters: dict_keys(['mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135274496
n accumulation steps: 1
n batches: 128
epoch 0 loss: 6.440620884973214e-05
epoch 1 loss: 5.421899348334591e-05
epoch 2 loss: 5.058084246911676e-05
epoch 3 loss: 4.831796314874737e-05
epoch 4 loss: 4.666280739229478e-05
finished adding batches
subset {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
3 mlp.up_proj
Pruning ...
realigning
initial loss 41.92567443847656
final loss 41.57143783569336
quantized
not here
quantized in 54.19958758354187 seconds
36937 MiB free out of 48676 MiB total
3 mlp.gate_proj
Pruning ...
realigning
initial loss 56.13510513305664
final loss 52.81269836425781
quantized
not here
quantized in 53.601765155792236 seconds
36593 MiB free out of 48676 MiB total
finished: {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 69120
optimizing the following parameters: dict_keys(['mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 45096960
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.00011148825353757275
epoch 1 loss: 0.00010523936640538523
epoch 2 loss: 0.00010209692027274286
epoch 3 loss: 9.992793980018178e-05
epoch 4 loss: 9.824901627553118e-05
finished adding batches
subset {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
3 mlp.down_proj
Pruning ...
realigning
initial loss 0.328909695148468
final loss 0.30780163407325745
quantized
not here
quantized in 55.53420066833496 seconds
36593 MiB free out of 48676 MiB total
finished: {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 8.853706543732187e-05
epoch 1 loss: 8.534690874739681e-05
epoch 2 loss: 8.472653843227818e-05
epoch 3 loss: 8.43578478679774e-05
epoch 4 loss: 8.408017458805261e-05
10749 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
10749 MiB free out of 48676 MiB total
after cast to cpu
36937 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj'], ['self_attn.o_proj'], ['mlp.up_proj', 'mlp.gate_proj'], ['mlp.down_proj']]
finished adding batches
subset {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
4 self_attn.k_proj
Pruning ...
realigning
initial loss 178.1652374267578
final loss 149.0334930419922
quantized
not here
quantized in 22.5004940032959 seconds
36935 MiB free out of 48676 MiB total
4 self_attn.v_proj
Pruning ...
realigning
initial loss 34.61722946166992
final loss 34.04546356201172
quantized
not here
quantized in 20.02489447593689 seconds
37959 MiB free out of 48676 MiB total
4 self_attn.q_proj
Pruning ...
realigning
initial loss 152.99147033691406
final loss 137.5843963623047
quantized
not here
quantized in 21.585777282714844 seconds
37959 MiB free out of 48676 MiB total
finished: {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 27648
optimizing the following parameters: dict_keys(['self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 152051712
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0001221202313104186
epoch 1 loss: 0.00010121643344973563
epoch 2 loss: 9.36660468937589e-05
epoch 3 loss: 8.890729094446215e-05
epoch 4 loss: 8.549077671204941e-05
finished adding batches
subset {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
4 self_attn.o_proj
Pruning ...
realigning
initial loss 0.9114055633544922
final loss 0.8576030135154724
quantized
not here
quantized in 22.94412636756897 seconds
37961 MiB free out of 48676 MiB total
finished: {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 36864
optimizing the following parameters: dict_keys(['mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135274496
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0001090551609195245
epoch 1 loss: 9.72802024534758e-05
epoch 2 loss: 9.234385339595974e-05
epoch 3 loss: 8.90352047235865e-05
epoch 4 loss: 8.64893392531485e-05
finished adding batches
subset {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
4 mlp.up_proj
Pruning ...
realigning
initial loss 64.46638488769531
final loss 63.96978759765625
quantized
not here
quantized in 54.32185888290405 seconds
36937 MiB free out of 48676 MiB total
4 mlp.gate_proj
Pruning ...
realigning
initial loss 91.34458923339844
final loss 86.7093505859375
quantized
not here
quantized in 54.27393817901611 seconds
36593 MiB free out of 48676 MiB total
finished: {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 69120
optimizing the following parameters: dict_keys(['mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 45096960
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0002538200906201382
epoch 1 loss: 0.00024067082972578646
epoch 2 loss: 0.0002326663882286084
epoch 3 loss: 0.00022700036845435534
epoch 4 loss: 0.00022262893435254227
finished adding batches
subset {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
4 mlp.down_proj
Pruning ...
realigning
initial loss 0.5977031588554382
final loss 0.5872515439987183
quantized
not here
quantized in 54.37868690490723 seconds
36593 MiB free out of 48676 MiB total
finished: {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0001827143382797658
epoch 1 loss: 0.00017796727672703128
epoch 2 loss: 0.00017676842264791048
epoch 3 loss: 0.00017608765233489976
epoch 4 loss: 0.0001755880648488528
10749 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
10749 MiB free out of 48676 MiB total
after cast to cpu
36937 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj'], ['self_attn.o_proj'], ['mlp.up_proj', 'mlp.gate_proj'], ['mlp.down_proj']]
finished adding batches
subset {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
5 self_attn.k_proj
Pruning ...
realigning
initial loss 218.6624298095703
final loss 179.73007202148438
quantized
not here
quantized in 22.488376140594482 seconds
36935 MiB free out of 48676 MiB total
5 self_attn.v_proj
Pruning ...
realigning
initial loss 41.03158187866211
final loss 40.588951110839844
quantized
not here
quantized in 19.89470934867859 seconds
36935 MiB free out of 48676 MiB total
5 self_attn.q_proj
Pruning ...
realigning
initial loss 176.1708526611328
final loss 157.336181640625
quantized
not here
quantized in 21.6757972240448 seconds
36935 MiB free out of 48676 MiB total
finished: {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 27648
optimizing the following parameters: dict_keys(['self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 152051712
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.00015466275544895325
epoch 1 loss: 0.00012476792034021855
epoch 2 loss: 0.00011554102115951537
epoch 3 loss: 0.00010960271674775868
epoch 4 loss: 0.00010519638402683995
finished adding batches
subset {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
5 self_attn.o_proj
Pruning ...
realigning
initial loss 1.2516226768493652
final loss 1.0173957347869873
quantized
not here
quantized in 23.367711305618286 seconds
36937 MiB free out of 48676 MiB total
finished: {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 36864
optimizing the following parameters: dict_keys(['mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135274496
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.00011859934869562494
epoch 1 loss: 0.00010449342784113469
epoch 2 loss: 9.823776889561486e-05
epoch 3 loss: 9.393989080308529e-05
epoch 4 loss: 9.062474634902173e-05
finished adding batches
subset {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
5 mlp.up_proj
Pruning ...
realigning
initial loss 76.37278747558594
final loss 75.98175811767578
quantized
not here
quantized in 53.48487567901611 seconds
36937 MiB free out of 48676 MiB total
5 mlp.gate_proj
Pruning ...
realigning
initial loss 106.30545043945312
final loss 102.91361999511719
quantized
not here
quantized in 52.97806000709534 seconds
36593 MiB free out of 48676 MiB total
finished: {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 69120
optimizing the following parameters: dict_keys(['mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 45096960
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.00033869820504150994
epoch 1 loss: 0.0003243671490054112
epoch 2 loss: 0.0003159764225983963
epoch 3 loss: 0.0003098616962233791
epoch 4 loss: 0.00030510297256114427
finished adding batches
subset {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
5 mlp.down_proj
Pruning ...
realigning
initial loss 0.7950983643531799
final loss 0.7921397089958191
quantized
not here
quantized in 52.41365957260132 seconds
35569 MiB free out of 48676 MiB total
finished: {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.00025069585262826877
epoch 1 loss: 0.00024678254044374626
epoch 2 loss: 0.00024577907254297315
epoch 3 loss: 0.00024523023546407785
epoch 4 loss: 0.00024483284914822434
10987 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
10987 MiB free out of 48676 MiB total
after cast to cpu
36937 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj'], ['self_attn.o_proj'], ['mlp.up_proj', 'mlp.gate_proj'], ['mlp.down_proj']]
finished adding batches
subset {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
6 self_attn.k_proj
Pruning ...
realigning
initial loss 307.3803405761719
final loss 252.41415405273438
quantized
not here
quantized in 22.363642692565918 seconds
36935 MiB free out of 48676 MiB total
6 self_attn.v_proj
Pruning ...
realigning
initial loss 58.44287872314453
final loss 57.567649841308594
quantized
not here
quantized in 20.069406032562256 seconds
36935 MiB free out of 48676 MiB total
6 self_attn.q_proj
Pruning ...
realigning
initial loss 279.3810119628906
final loss 246.20272827148438
quantized
not here
quantized in 21.084649562835693 seconds
36935 MiB free out of 48676 MiB total
finished: {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 27648
optimizing the following parameters: dict_keys(['self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 152051712
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.00037269595077304984
epoch 1 loss: 0.0002961114191748493
epoch 2 loss: 0.0002651489647860217
epoch 3 loss: 0.0002449490340268312
epoch 4 loss: 0.00022989277897522697
finished adding batches
subset {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
6 self_attn.o_proj
Pruning ...
realigning
initial loss 1.566681146621704
final loss 1.3268206119537354
quantized
not here
quantized in 23.362348318099976 seconds
36937 MiB free out of 48676 MiB total
finished: {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 36864
optimizing the following parameters: dict_keys(['mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135274496
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0001412348453300183
epoch 1 loss: 0.00012223403217603845
epoch 2 loss: 0.00011401110310771401
epoch 3 loss: 0.00010851376987375261
epoch 4 loss: 0.00010438649729849203
finished adding batches
subset {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
6 mlp.up_proj
Pruning ...
realigning
initial loss 90.36874389648438
final loss 89.75182342529297
quantized
not here
quantized in 54.23512673377991 seconds
36937 MiB free out of 48676 MiB total
6 mlp.gate_proj
Pruning ...
realigning
initial loss 132.8593292236328
final loss 127.56635284423828
quantized
not here
quantized in 52.84857964515686 seconds
36765 MiB free out of 48676 MiB total
finished: {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 69120
optimizing the following parameters: dict_keys(['mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 45096960
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0004979114298748755
epoch 1 loss: 0.00047299192397076695
epoch 2 loss: 0.0004583168952194683
epoch 3 loss: 0.0004476024068935658
epoch 4 loss: 0.0004393352990064159
finished adding batches
subset {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
6 mlp.down_proj
Pruning ...
realigning
initial loss 1.2085912227630615
final loss 1.201511025428772
quantized
not here
quantized in 52.212703466415405 seconds
36765 MiB free out of 48676 MiB total
finished: {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.00037845920883228246
epoch 1 loss: 0.00037076720536788343
epoch 2 loss: 0.0003685918543396838
epoch 3 loss: 0.00036746511432284024
epoch 4 loss: 0.00036668362349701056
10665 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
10665 MiB free out of 48676 MiB total
after cast to cpu
36937 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj'], ['self_attn.o_proj'], ['mlp.up_proj', 'mlp.gate_proj'], ['mlp.down_proj']]
finished adding batches
subset {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
7 self_attn.k_proj
Pruning ...
realigning
initial loss 313.3668212890625
final loss 263.4111328125
quantized
not here
quantized in 22.153874397277832 seconds
37959 MiB free out of 48676 MiB total
7 self_attn.v_proj
Pruning ...
realigning
initial loss 65.1014633178711
final loss 64.15679931640625
quantized
not here
quantized in 20.038289546966553 seconds
37959 MiB free out of 48676 MiB total
7 self_attn.q_proj
Pruning ...
realigning
initial loss 312.963134765625
final loss 271.0570068359375
quantized
not here
quantized in 21.186180591583252 seconds
37959 MiB free out of 48676 MiB total
finished: {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 27648
optimizing the following parameters: dict_keys(['self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 152051712
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0004843289339078183
epoch 1 loss: 0.00039183552576105285
epoch 2 loss: 0.00035472039417072665
epoch 3 loss: 0.0003299480117675557
epoch 4 loss: 0.0003113799784841831
finished adding batches
subset {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
7 self_attn.o_proj
Pruning ...
realigning
initial loss 2.283613681793213
final loss 1.9006531238555908
quantized
not here
quantized in 23.32598900794983 seconds
37961 MiB free out of 48676 MiB total
finished: {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 36864
optimizing the following parameters: dict_keys(['mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135274496
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.00019169006930042087
epoch 1 loss: 0.00016433116957159655
epoch 2 loss: 0.00015224086030229955
epoch 3 loss: 0.00014416866304145515
epoch 4 loss: 0.00013815178454024135
finished adding batches
subset {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
7 mlp.up_proj
Pruning ...
realigning
initial loss 103.78662109375
final loss 103.10093688964844
quantized
not here
quantized in 54.134037256240845 seconds
36937 MiB free out of 48676 MiB total
7 mlp.gate_proj
Pruning ...
realigning
initial loss 151.13880920410156
final loss 145.44578552246094
quantized
not here
quantized in 52.94068217277527 seconds
36593 MiB free out of 48676 MiB total
finished: {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 69120
optimizing the following parameters: dict_keys(['mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 45096960
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0006847777794973808
epoch 1 loss: 0.0006437261881728773
epoch 2 loss: 0.0006206405728335085
epoch 3 loss: 0.0006041453143552644
epoch 4 loss: 0.00059155948474654
finished adding batches
subset {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
7 mlp.down_proj
Pruning ...
realigning
initial loss 1.7461464405059814
final loss 1.7379908561706543
quantized
not here
quantized in 51.9316611289978 seconds
35569 MiB free out of 48676 MiB total
finished: {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0005493633784681151
epoch 1 loss: 0.0005381766836762836
epoch 2 loss: 0.0005348462195797765
epoch 3 loss: 0.0005330865938049101
epoch 4 loss: 0.0005318507842275721
10987 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
10987 MiB free out of 48676 MiB total
after cast to cpu
36937 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj'], ['self_attn.o_proj'], ['mlp.up_proj', 'mlp.gate_proj'], ['mlp.down_proj']]
finished adding batches
subset {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
8 self_attn.k_proj
Pruning ...
realigning
initial loss 289.807373046875
final loss 256.5369567871094
quantized
not here
quantized in 22.003870487213135 seconds
37959 MiB free out of 48676 MiB total
8 self_attn.v_proj
Pruning ...
realigning
initial loss 65.39326477050781
final loss 64.38316345214844
quantized
not here
quantized in 19.939637422561646 seconds
37959 MiB free out of 48676 MiB total
8 self_attn.q_proj
Pruning ...
realigning
initial loss 298.24786376953125
final loss 269.5333251953125
quantized
not here
quantized in 21.274606704711914 seconds
37959 MiB free out of 48676 MiB total
finished: {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 27648
optimizing the following parameters: dict_keys(['self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 152051712
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0006261013309085683
epoch 1 loss: 0.0005126459147959395
epoch 2 loss: 0.0004674108797644294
epoch 3 loss: 0.0004373662541183876
epoch 4 loss: 0.00041478518960502697
finished adding batches
subset {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
8 self_attn.o_proj
Pruning ...
realigning
initial loss 3.7576982975006104
final loss 3.64044189453125
quantized
not here
quantized in 23.04498291015625 seconds
37961 MiB free out of 48676 MiB total
finished: {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 36864
optimizing the following parameters: dict_keys(['mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135274496
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0003091485045843001
epoch 1 loss: 0.0002580453289056095
epoch 2 loss: 0.00023746684269099205
epoch 3 loss: 0.00022405567608529964
epoch 4 loss: 0.0002141632086249956
finished adding batches
subset {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
8 mlp.up_proj
Pruning ...
realigning
initial loss 114.18800354003906
final loss 113.1070556640625
quantized
not here
quantized in 53.62717843055725 seconds
36937 MiB free out of 48676 MiB total
8 mlp.gate_proj
Pruning ...
realigning
initial loss 160.0994873046875
final loss 152.07354736328125
quantized
not here
quantized in 54.19415521621704 seconds
36593 MiB free out of 48676 MiB total
finished: {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 69120
optimizing the following parameters: dict_keys(['mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 45096960
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0008432777540292591
epoch 1 loss: 0.0007908720508567058
epoch 2 loss: 0.0007617425812895817
epoch 3 loss: 0.0007409249756165082
epoch 4 loss: 0.000724792567325494
finished adding batches
subset {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
8 mlp.down_proj
Pruning ...
realigning
initial loss 2.17795467376709
final loss 2.1689634323120117
quantized
not here
quantized in 52.05674076080322 seconds
36593 MiB free out of 48676 MiB total
finished: {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0006859086001895776
epoch 1 loss: 0.000671557998884964
epoch 2 loss: 0.000666651316350908
epoch 3 loss: 0.0006640862256972468
epoch 4 loss: 0.0006624066600124934
10749 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
10749 MiB free out of 48676 MiB total
after cast to cpu
36937 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj'], ['self_attn.o_proj'], ['mlp.up_proj', 'mlp.gate_proj'], ['mlp.down_proj']]
finished adding batches
subset {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
9 self_attn.k_proj
Pruning ...
realigning
initial loss 311.17138671875
final loss 273.43658447265625
quantized
not here
quantized in 21.993760347366333 seconds
36935 MiB free out of 48676 MiB total
9 self_attn.v_proj
Pruning ...
realigning
initial loss 71.4471435546875
final loss 70.62506103515625
quantized
not here
quantized in 19.775580883026123 seconds
36935 MiB free out of 48676 MiB total
9 self_attn.q_proj
Pruning ...
realigning
initial loss 279.09521484375
final loss 254.77362060546875
quantized
not here
quantized in 21.01494288444519 seconds
36935 MiB free out of 48676 MiB total
finished: {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 27648
optimizing the following parameters: dict_keys(['self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 152051712
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0006302314159256639
epoch 1 loss: 0.0005183276855404984
epoch 2 loss: 0.00047460319501624326
epoch 3 loss: 0.00044609454062083387
epoch 4 loss: 0.00042515645804996893
finished adding batches
subset {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
9 self_attn.o_proj
Pruning ...
realigning
initial loss 5.394854545593262
final loss 5.115607261657715
quantized
not here
quantized in 23.388155221939087 seconds
36937 MiB free out of 48676 MiB total
finished: {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 36864
optimizing the following parameters: dict_keys(['mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135274496
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0004240225216562976
epoch 1 loss: 0.0003538175737958227
epoch 2 loss: 0.0003264942311034247
epoch 3 loss: 0.00030846662571093475
epoch 4 loss: 0.0002950713792415627
finished adding batches
subset {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
9 mlp.up_proj
Pruning ...
realigning
initial loss 124.67083740234375
final loss 123.52789306640625
quantized
not here
quantized in 54.08422231674194 seconds
36937 MiB free out of 48676 MiB total
9 mlp.gate_proj
Pruning ...
realigning
initial loss 168.01023864746094
final loss 160.98590087890625
quantized
not here
quantized in 53.71968460083008 seconds
36593 MiB free out of 48676 MiB total
finished: {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 69120
optimizing the following parameters: dict_keys(['mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 45096960
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0009644472170293739
epoch 1 loss: 0.0009073905171135266
epoch 2 loss: 0.0008760594705563562
epoch 3 loss: 0.0008539741420463542
epoch 4 loss: 0.0008369504625989066
finished adding batches
subset {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
9 mlp.down_proj
Pruning ...
realigning
initial loss 2.3199143409729004
final loss 2.317168712615967
quantized
not here
quantized in 51.90428376197815 seconds
36593 MiB free out of 48676 MiB total
finished: {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0007520820108766202
epoch 1 loss: 0.0007457891210833623
epoch 2 loss: 0.0007434201343130553
epoch 3 loss: 0.0007422084049721889
epoch 4 loss: 0.0007413997818730422
11773 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
11773 MiB free out of 48676 MiB total
after cast to cpu
36937 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj'], ['self_attn.o_proj'], ['mlp.up_proj', 'mlp.gate_proj'], ['mlp.down_proj']]
finished adding batches
subset {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
10 self_attn.k_proj
Pruning ...
realigning
initial loss 320.94952392578125
final loss 287.00250244140625
quantized
not here
quantized in 22.17492413520813 seconds
36935 MiB free out of 48676 MiB total
10 self_attn.v_proj
Pruning ...
realigning
initial loss 71.25209045410156
final loss 70.23687744140625
quantized
not here
quantized in 19.983306884765625 seconds
37959 MiB free out of 48676 MiB total
10 self_attn.q_proj
Pruning ...
realigning
initial loss 288.4970703125
final loss 261.9779968261719
quantized
not here
quantized in 21.343141794204712 seconds
37959 MiB free out of 48676 MiB total
finished: {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 27648
optimizing the following parameters: dict_keys(['self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 152051712
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0007207963794826355
epoch 1 loss: 0.0005972661956548109
epoch 2 loss: 0.0005459873632389645
epoch 3 loss: 0.0005120975642967096
epoch 4 loss: 0.0004870026612024958
finished adding batches
subset {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
10 self_attn.o_proj
Pruning ...
realigning
initial loss 7.755082130432129
final loss 7.277129650115967
quantized
not here
quantized in 23.09764790534973 seconds
37961 MiB free out of 48676 MiB total
finished: {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 36864
optimizing the following parameters: dict_keys(['mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135274496
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0005823220344609581
epoch 1 loss: 0.00047993723751460493
epoch 2 loss: 0.00043782812713288877
epoch 3 loss: 0.0004102352675090515
epoch 4 loss: 0.00038991004430499743
finished adding batches
subset {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
10 mlp.up_proj
Pruning ...
realigning
initial loss 133.61935424804688
final loss 132.19662475585938
quantized
not here
quantized in 53.828245639801025 seconds
36937 MiB free out of 48676 MiB total
10 mlp.gate_proj
Pruning ...
realigning
initial loss 181.05516052246094
final loss 172.00733947753906
quantized
not here
quantized in 53.26865887641907 seconds
36593 MiB free out of 48676 MiB total
finished: {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 69120
optimizing the following parameters: dict_keys(['mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 45096960
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0011053306398025597
epoch 1 loss: 0.001042274087012629
epoch 2 loss: 0.001005687036922609
epoch 3 loss: 0.0009794210877771548
epoch 4 loss: 0.0009590089425728365
finished adding batches
subset {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
10 mlp.down_proj
Pruning ...
realigning
initial loss 2.890094041824341
final loss 2.868124485015869
quantized
not here
quantized in 54.93786120414734 seconds
36593 MiB free out of 48676 MiB total
finished: {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0008994706504381611
epoch 1 loss: 0.0008737693801776913
epoch 2 loss: 0.0008636474644845293
epoch 3 loss: 0.0008581195620536164
epoch 4 loss: 0.0008547425941287656
10749 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
10749 MiB free out of 48676 MiB total
after cast to cpu
36937 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj'], ['self_attn.o_proj'], ['mlp.up_proj', 'mlp.gate_proj'], ['mlp.down_proj']]
finished adding batches
subset {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
11 self_attn.k_proj
Pruning ...
realigning
initial loss 350.35205078125
final loss 313.03375244140625
quantized
not here
quantized in 22.347286701202393 seconds
36935 MiB free out of 48676 MiB total
11 self_attn.v_proj
Pruning ...
realigning
initial loss 97.8787841796875
final loss 96.72805786132812
quantized
not here
quantized in 19.73700189590454 seconds
36935 MiB free out of 48676 MiB total
11 self_attn.q_proj
Pruning ...
realigning
initial loss 361.9478759765625
final loss 327.2144470214844
quantized
not here
quantized in 21.26967430114746 seconds
36935 MiB free out of 48676 MiB total
finished: {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 27648
optimizing the following parameters: dict_keys(['self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 152051712
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0009217462061315018
epoch 1 loss: 0.000772191957821633
epoch 2 loss: 0.0007075245348460157
epoch 3 loss: 0.0006639902690039889
epoch 4 loss: 0.000631244990927371
finished adding batches
subset {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
11 self_attn.o_proj
Pruning ...
realigning
initial loss 7.108124732971191
final loss 6.594928741455078
quantized
not here
quantized in 23.278404235839844 seconds
37961 MiB free out of 48676 MiB total
finished: {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 36864
optimizing the following parameters: dict_keys(['mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135274496
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0005253707468000357
epoch 1 loss: 0.0004362924071301677
epoch 2 loss: 0.0003996201382960862
epoch 3 loss: 0.00037561126691798563
epoch 4 loss: 0.00035793841448139574
finished adding batches
subset {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
11 mlp.up_proj
Pruning ...
realigning
initial loss 146.8174285888672
final loss 145.5152587890625
quantized
not here
quantized in 54.55716133117676 seconds
36937 MiB free out of 48676 MiB total
11 mlp.gate_proj
Pruning ...
realigning
initial loss 193.5806121826172
final loss 182.81980895996094
quantized
not here
quantized in 55.06866812705994 seconds
36593 MiB free out of 48676 MiB total
finished: {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 69120
optimizing the following parameters: dict_keys(['mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 45096960
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0012688186561717885
epoch 1 loss: 0.0011946498098041047
epoch 2 loss: 0.0011519424406287726
epoch 3 loss: 0.0011212758172405302
epoch 4 loss: 0.0010974369279210805
finished adding batches
subset {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
11 mlp.down_proj
Pruning ...
realigning
initial loss 3.2165260314941406
final loss 3.2088775634765625
quantized
not here
quantized in 54.796247482299805 seconds
35569 MiB free out of 48676 MiB total
finished: {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0010300391136297549
epoch 1 loss: 0.001013817892271618
epoch 2 loss: 0.001007508180464356
epoch 3 loss: 0.0010039669436991971
epoch 4 loss: 0.0010015785856012371
10987 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
10987 MiB free out of 48676 MiB total
after cast to cpu
36937 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj'], ['self_attn.o_proj'], ['mlp.up_proj', 'mlp.gate_proj'], ['mlp.down_proj']]
finished adding batches
subset {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
12 self_attn.k_proj
Pruning ...
realigning
initial loss 378.8927917480469
final loss 336.2227783203125
quantized
not here
quantized in 22.409948587417603 seconds
36935 MiB free out of 48676 MiB total
12 self_attn.v_proj
Pruning ...
realigning
initial loss 94.97642517089844
final loss 94.06954193115234
quantized
not here
quantized in 20.109166383743286 seconds
36935 MiB free out of 48676 MiB total
12 self_attn.q_proj
Pruning ...
realigning
initial loss 344.45123291015625
final loss 315.3419494628906
quantized
not here
quantized in 21.597090005874634 seconds
36935 MiB free out of 48676 MiB total
finished: {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 27648
optimizing the following parameters: dict_keys(['self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 152051712
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0009649428939155769
epoch 1 loss: 0.0008140048453242343
epoch 2 loss: 0.0007488338605980971
epoch 3 loss: 0.0007052226810628781
epoch 4 loss: 0.0006726208239342668
finished adding batches
subset {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
12 self_attn.o_proj
Pruning ...
realigning
initial loss 8.65662670135498
final loss 8.305002212524414
quantized
not here
quantized in 23.37953472137451 seconds
36937 MiB free out of 48676 MiB total
finished: {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 36864
optimizing the following parameters: dict_keys(['mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135274496
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.000668934792656728
epoch 1 loss: 0.0005554849451527843
epoch 2 loss: 0.0005088434447770851
epoch 3 loss: 0.000478289750390104
epoch 4 loss: 0.0004557697375275893
finished adding batches
subset {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
12 mlp.up_proj
Pruning ...
realigning
initial loss 159.26351928710938
final loss 157.74642944335938
quantized
not here
quantized in 55.48242449760437 seconds
36937 MiB free out of 48676 MiB total
12 mlp.gate_proj
Pruning ...
realigning
initial loss 200.34408569335938
final loss 190.7201385498047
quantized
not here
quantized in 54.37443423271179 seconds
36593 MiB free out of 48676 MiB total
finished: {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 69120
optimizing the following parameters: dict_keys(['mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 45096960
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0014623112911067437
epoch 1 loss: 0.0013761145046373713
epoch 2 loss: 0.0013261182903079316
epoch 3 loss: 0.001290357044126722
epoch 4 loss: 0.0012626327343241428
finished adding batches
subset {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
12 mlp.down_proj
Pruning ...
realigning
initial loss 3.492598056793213
final loss 3.488799810409546
quantized
not here
quantized in 55.023993492126465 seconds
36593 MiB free out of 48676 MiB total
finished: {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0011297598366581951
epoch 1 loss: 0.0011184595023223665
epoch 2 loss: 0.001113492100557778
epoch 3 loss: 0.0011109308707091259
epoch 4 loss: 0.0011093917810285348
10749 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
10749 MiB free out of 48676 MiB total
after cast to cpu
36937 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj'], ['self_attn.o_proj'], ['mlp.up_proj', 'mlp.gate_proj'], ['mlp.down_proj']]
finished adding batches
subset {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
13 self_attn.k_proj
Pruning ...
realigning
initial loss 377.2494812011719
final loss 329.22100830078125
quantized
not here
quantized in 22.300180673599243 seconds
36935 MiB free out of 48676 MiB total
13 self_attn.v_proj
Pruning ...
realigning
initial loss 104.75308990478516
final loss 104.05255889892578
quantized
not here
quantized in 19.978177547454834 seconds
36935 MiB free out of 48676 MiB total
13 self_attn.q_proj
Pruning ...
realigning
initial loss 345.24700927734375
final loss 312.35430908203125
quantized
not here
quantized in 21.254051446914673 seconds
36935 MiB free out of 48676 MiB total
finished: {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 27648
optimizing the following parameters: dict_keys(['self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 152051712
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0009452867329855508
epoch 1 loss: 0.0007966922867126414
epoch 2 loss: 0.000734343933800119
epoch 3 loss: 0.0006929815858711663
epoch 4 loss: 0.0006622099913329293
finished adding batches
subset {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
13 self_attn.o_proj
Pruning ...
realigning
initial loss 8.69823169708252
final loss 8.21562385559082
quantized
not here
quantized in 23.399553060531616 seconds
36937 MiB free out of 48676 MiB total
finished: {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 36864
optimizing the following parameters: dict_keys(['mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135274496
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0006911820773893851
epoch 1 loss: 0.0005723853378185595
epoch 2 loss: 0.0005222340598720621
epoch 3 loss: 0.0004895132378806011
epoch 4 loss: 0.00046552966568924603
finished adding batches
subset {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
13 mlp.up_proj
Pruning ...
realigning
initial loss 173.17984008789062
final loss 171.67138671875
quantized
not here
quantized in 56.29320788383484 seconds
36937 MiB free out of 48676 MiB total
13 mlp.gate_proj
Pruning ...
realigning
initial loss 211.44091796875
final loss 201.015625
quantized
not here
quantized in 54.3817183971405 seconds
36593 MiB free out of 48676 MiB total
finished: {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 69120
optimizing the following parameters: dict_keys(['mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 45096960
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0017562769207870588
epoch 1 loss: 0.0016406900504080113
epoch 2 loss: 0.0015737057128717424
epoch 3 loss: 0.0015264056655723834
epoch 4 loss: 0.0014903643823345192
finished adding batches
subset {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
13 mlp.down_proj
Pruning ...
realigning
initial loss 4.336377143859863
final loss 4.32719612121582
quantized
not here
quantized in 56.90234351158142 seconds
36593 MiB free out of 48676 MiB total
finished: {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.001398216431880428
epoch 1 loss: 0.0013798451745969942
epoch 2 loss: 0.0013706026829822804
epoch 3 loss: 0.0013650398013851373
epoch 4 loss: 0.001361291929242725
10749 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
10749 MiB free out of 48676 MiB total
after cast to cpu
36937 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj'], ['self_attn.o_proj'], ['mlp.up_proj', 'mlp.gate_proj'], ['mlp.down_proj']]
finished adding batches
subset {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
14 self_attn.k_proj
Pruning ...
realigning
initial loss 401.8953552246094
final loss 349.8493347167969
quantized
not here
quantized in 22.123149156570435 seconds
37959 MiB free out of 48676 MiB total
14 self_attn.v_proj
Pruning ...
realigning
initial loss 104.755126953125
final loss 104.05221557617188
quantized
not here
quantized in 19.932148933410645 seconds
37959 MiB free out of 48676 MiB total
14 self_attn.q_proj
Pruning ...
realigning
initial loss 367.85626220703125
final loss 326.7926940917969
quantized
not here
quantized in 21.171270847320557 seconds
37959 MiB free out of 48676 MiB total
finished: {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 27648
optimizing the following parameters: dict_keys(['self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 152051712
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.001209413093420153
epoch 1 loss: 0.00100868614981664
epoch 2 loss: 0.0009210867569890979
epoch 3 loss: 0.0008631313294245047
epoch 4 loss: 0.000820512254449568
finished adding batches
subset {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
14 self_attn.o_proj
Pruning ...
realigning
initial loss 11.172539710998535
final loss 11.09360122680664
quantized
not here
quantized in 22.33870792388916 seconds
37961 MiB free out of 48676 MiB total
finished: {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 36864
optimizing the following parameters: dict_keys(['mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135274496
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.00090921068795069
epoch 1 loss: 0.0007377466863545123
epoch 2 loss: 0.0006668425749012385
epoch 3 loss: 0.0006214055433702015
epoch 4 loss: 0.0005884573438379448
finished adding batches
subset {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
14 mlp.up_proj
Pruning ...
realigning
initial loss 188.817626953125
final loss 186.85752868652344
quantized
not here
quantized in 55.377206325531006 seconds
36937 MiB free out of 48676 MiB total
14 mlp.gate_proj
Pruning ...
realigning
initial loss 236.68775939941406
final loss 220.6995391845703
quantized
not here
quantized in 55.074812173843384 seconds
36593 MiB free out of 48676 MiB total
finished: {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 69120
optimizing the following parameters: dict_keys(['mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 45096960
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.002039171451542643
epoch 1 loss: 0.0019060168633586727
epoch 2 loss: 0.0018287204266016488
epoch 3 loss: 0.0017736907620928832
epoch 4 loss: 0.0017313492026005406
finished adding batches
subset {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
14 mlp.down_proj
Pruning ...
realigning
initial loss 4.995160102844238
final loss 4.991045951843262
quantized
not here
quantized in 54.560035705566406 seconds
35569 MiB free out of 48676 MiB total
finished: {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0016198647736018756
epoch 1 loss: 0.0016031409249990247
epoch 2 loss: 0.0015945997120070388
epoch 3 loss: 0.0015899659065325977
epoch 4 loss: 0.0015872800449869828
10987 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
10987 MiB free out of 48676 MiB total
after cast to cpu
36937 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj'], ['self_attn.o_proj'], ['mlp.up_proj', 'mlp.gate_proj'], ['mlp.down_proj']]
finished adding batches
subset {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
15 self_attn.k_proj
Pruning ...
realigning
initial loss 387.02978515625
final loss 329.43365478515625
quantized
not here
quantized in 22.014939546585083 seconds
36935 MiB free out of 48676 MiB total
15 self_attn.v_proj
Pruning ...
realigning
initial loss 108.19496154785156
final loss 107.42464447021484
quantized
not here
quantized in 19.841434478759766 seconds
36935 MiB free out of 48676 MiB total
15 self_attn.q_proj
Pruning ...
realigning
initial loss 342.063232421875
final loss 298.8235778808594
quantized
not here
quantized in 21.379036903381348 seconds
36935 MiB free out of 48676 MiB total
finished: {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 27648
optimizing the following parameters: dict_keys(['self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 152051712
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0012272176118131028
epoch 1 loss: 0.0010263574145028542
epoch 2 loss: 0.0009399273949384224
epoch 3 loss: 0.0008824940809972759
epoch 4 loss: 0.0008402129669775604
finished adding batches
subset {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
15 self_attn.o_proj
Pruning ...
realigning
initial loss 10.231508255004883
final loss 9.950057983398438
quantized
not here
quantized in 23.32412314414978 seconds
36937 MiB free out of 48676 MiB total
finished: {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 36864
optimizing the following parameters: dict_keys(['mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135274496
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0008998626394713938
epoch 1 loss: 0.0007524680427195563
epoch 2 loss: 0.000686107568526495
epoch 3 loss: 0.0006426381596611463
epoch 4 loss: 0.0006108359475547331
finished adding batches
subset {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
15 mlp.up_proj
Pruning ...
realigning
initial loss 208.89242553710938
final loss 206.7833251953125
quantized
not here
quantized in 56.254172801971436 seconds
36937 MiB free out of 48676 MiB total
15 mlp.gate_proj
Pruning ...
realigning
initial loss 259.3468017578125
final loss 243.01605224609375
quantized
not here
quantized in 54.648335456848145 seconds
36593 MiB free out of 48676 MiB total
finished: {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 69120
optimizing the following parameters: dict_keys(['mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 45096960
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.002489899459760636
epoch 1 loss: 0.0023135281444410793
epoch 2 loss: 0.0022159101063152775
epoch 3 loss: 0.002147843570128316
epoch 4 loss: 0.002095911400829209
finished adding batches
subset {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
15 mlp.down_proj
Pruning ...
realigning
initial loss 6.2652587890625
final loss 6.256242752075195
quantized
not here
quantized in 54.91162586212158 seconds
35569 MiB free out of 48676 MiB total
finished: {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.002028834714110417
epoch 1 loss: 0.0020069607153345714
epoch 2 loss: 0.0019957471358793555
epoch 3 loss: 0.0019893134776793886
epoch 4 loss: 0.0019853262447213638
10987 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
10987 MiB free out of 48676 MiB total
after cast to cpu
36937 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj'], ['self_attn.o_proj'], ['mlp.up_proj', 'mlp.gate_proj'], ['mlp.down_proj']]
finished adding batches
subset {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
16 self_attn.k_proj
Pruning ...
realigning
initial loss 410.1724853515625
final loss 344.843505859375
quantized
not here
quantized in 22.24986243247986 seconds
36935 MiB free out of 48676 MiB total
16 self_attn.v_proj
Pruning ...
realigning
initial loss 124.30447387695312
final loss 123.8741226196289
quantized
not here
quantized in 19.988597631454468 seconds
36935 MiB free out of 48676 MiB total
16 self_attn.q_proj
Pruning ...
realigning
initial loss 376.6242980957031
final loss 324.3662109375
quantized
not here
quantized in 21.33502769470215 seconds
36935 MiB free out of 48676 MiB total
finished: {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 27648
optimizing the following parameters: dict_keys(['self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 152051712
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0016541617796974606
epoch 1 loss: 0.001374148424474697
epoch 2 loss: 0.0012536066660686629
epoch 3 loss: 0.001174115112007712
epoch 4 loss: 0.0011160055942127656
finished adding batches
subset {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
16 self_attn.o_proj
Pruning ...
realigning
initial loss 14.1956787109375
final loss 14.09385871887207
quantized
not here
quantized in 22.24070644378662 seconds
37961 MiB free out of 48676 MiB total
finished: {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 36864
optimizing the following parameters: dict_keys(['mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135274496
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0012687703256233362
epoch 1 loss: 0.0010077258607452677
epoch 2 loss: 0.0008993160067802819
epoch 3 loss: 0.0008312441100315482
epoch 4 loss: 0.0007828560151210695
finished adding batches
subset {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
16 mlp.up_proj
Pruning ...
realigning
initial loss 240.14474487304688
final loss 236.9434051513672
quantized
not here
quantized in 55.64986848831177 seconds
36937 MiB free out of 48676 MiB total
16 mlp.gate_proj
Pruning ...
realigning
initial loss 313.15252685546875
final loss 286.06353759765625
quantized
not here
quantized in 54.51848316192627 seconds
36593 MiB free out of 48676 MiB total
finished: {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 69120
optimizing the following parameters: dict_keys(['mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 45096960
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.003304596513771685
epoch 1 loss: 0.003053821650610189
epoch 2 loss: 0.0029169213685236173
epoch 3 loss: 0.0028218876432219986
epoch 4 loss: 0.0027491844612086425
finished adding batches
subset {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
16 mlp.down_proj
Pruning ...
realigning
initial loss 8.57327938079834
final loss 8.553215980529785
quantized
not here
quantized in 56.92021298408508 seconds
35569 MiB free out of 48676 MiB total
finished: {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.002777658235572744
epoch 1 loss: 0.002747522394201951
epoch 2 loss: 0.0027298555396555457
epoch 3 loss: 0.0027186459919903427
epoch 4 loss: 0.0027113310225104215
10987 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
10987 MiB free out of 48676 MiB total
after cast to cpu
37961 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj'], ['self_attn.o_proj'], ['mlp.up_proj', 'mlp.gate_proj'], ['mlp.down_proj']]
finished adding batches
subset {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
17 self_attn.k_proj
Pruning ...
realigning
initial loss 444.4179382324219
final loss 362.08355712890625
quantized
not here
quantized in 22.747854471206665 seconds
37959 MiB free out of 48676 MiB total
17 self_attn.v_proj
Pruning ...
realigning
initial loss 133.13656616210938
final loss 132.49038696289062
quantized
not here
quantized in 19.822519063949585 seconds
37959 MiB free out of 48676 MiB total
17 self_attn.q_proj
Pruning ...
realigning
initial loss 392.1114807128906
final loss 333.1948547363281
quantized
not here
quantized in 21.090622663497925 seconds
37959 MiB free out of 48676 MiB total
finished: {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 27648
optimizing the following parameters: dict_keys(['self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 152051712
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0015275416071744985
epoch 1 loss: 0.0012431248251232319
epoch 2 loss: 0.0011209405138288275
epoch 3 loss: 0.0010438938584229618
epoch 4 loss: 0.0009860689015113167
finished adding batches
subset {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
17 self_attn.o_proj
Pruning ...
realigning
initial loss 10.550954818725586
final loss 10.285628318786621
quantized
not here
quantized in 23.72386884689331 seconds
37961 MiB free out of 48676 MiB total
finished: {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 36864
optimizing the following parameters: dict_keys(['mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135274496
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0008546493018002366
epoch 1 loss: 0.0006774524363208911
epoch 2 loss: 0.0006096729689488711
epoch 3 loss: 0.0005673029734225565
epoch 4 loss: 0.0005372520686250937
finished adding batches
subset {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
17 mlp.up_proj
Pruning ...
realigning
initial loss 264.2071838378906
final loss 261.68017578125
quantized
not here
quantized in 56.928377866744995 seconds
36937 MiB free out of 48676 MiB total
17 mlp.gate_proj
Pruning ...
realigning
initial loss 358.3164367675781
final loss 328.23406982421875
quantized
not here
quantized in 55.146806478500366 seconds
36593 MiB free out of 48676 MiB total
finished: {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 69120
optimizing the following parameters: dict_keys(['mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 45096960
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0036569892999978038
epoch 1 loss: 0.0034033494575851364
epoch 2 loss: 0.003259473745856667
epoch 3 loss: 0.0031576154397043865
epoch 4 loss: 0.0030789838147029513
finished adding batches
subset {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
17 mlp.down_proj
Pruning ...
realigning
initial loss 9.570024490356445
final loss 9.549007415771484
quantized
not here
quantized in 56.395626068115234 seconds
35569 MiB free out of 48676 MiB total
finished: {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0030838627862976864
epoch 1 loss: 0.003039478924620198
epoch 2 loss: 0.003015472539118491
epoch 3 loss: 0.0029998084482940612
epoch 4 loss: 0.0029893538576288847
10987 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
10987 MiB free out of 48676 MiB total
after cast to cpu
37961 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj'], ['self_attn.o_proj'], ['mlp.up_proj', 'mlp.gate_proj'], ['mlp.down_proj']]
finished adding batches
subset {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
18 self_attn.k_proj
Pruning ...
realigning
initial loss 463.1064758300781
final loss 374.8900146484375
quantized
not here
quantized in 22.64491319656372 seconds
37959 MiB free out of 48676 MiB total
18 self_attn.v_proj
Pruning ...
realigning
initial loss 161.95529174804688
final loss 161.40371704101562
quantized
not here
quantized in 20.4032940864563 seconds
37959 MiB free out of 48676 MiB total
18 self_attn.q_proj
Pruning ...
realigning
initial loss 415.4652099609375
final loss 343.18853759765625
quantized
not here
quantized in 21.705606937408447 seconds
37959 MiB free out of 48676 MiB total
finished: {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 27648
optimizing the following parameters: dict_keys(['self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 152051712
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0017221019797943882
epoch 1 loss: 0.0014094697708060266
epoch 2 loss: 0.001279240267649584
epoch 3 loss: 0.0011956345833823434
epoch 4 loss: 0.0011353830545886012
finished adding batches
subset {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
18 self_attn.o_proj
Pruning ...
realigning
initial loss 12.50228214263916
final loss 12.287908554077148
quantized
not here
quantized in 22.073423385620117 seconds
37961 MiB free out of 48676 MiB total
finished: {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 36864
optimizing the following parameters: dict_keys(['mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135274496
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0010708050972425553
epoch 1 loss: 0.0008554362948416383
epoch 2 loss: 0.0007711072926213092
epoch 3 loss: 0.0007186375842138659
epoch 4 loss: 0.0006816696072746709
finished adding batches
subset {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
18 mlp.up_proj
Pruning ...
realigning
initial loss 291.21826171875
final loss 289.1470642089844
quantized
not here
quantized in 56.939560413360596 seconds
36937 MiB free out of 48676 MiB total
18 mlp.gate_proj
Pruning ...
realigning
initial loss 398.16461181640625
final loss 368.20538330078125
quantized
not here
quantized in 54.72662043571472 seconds
36593 MiB free out of 48676 MiB total
finished: {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 69120
optimizing the following parameters: dict_keys(['mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 45096960
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.004371476763481041
epoch 1 loss: 0.004069608999998309
epoch 2 loss: 0.0038969583674770547
epoch 3 loss: 0.0037745337594969897
epoch 4 loss: 0.0036799804965994554
finished adding batches
subset {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
18 mlp.down_proj
Pruning ...
realigning
initial loss 11.456635475158691
final loss 11.38608455657959
quantized
not here
quantized in 59.1179940700531 seconds
35569 MiB free out of 48676 MiB total
finished: {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.003643049356469419
epoch 1 loss: 0.00356431945147051
epoch 2 loss: 0.003519156223774189
epoch 3 loss: 0.00348892369584064
epoch 4 loss: 0.0034680246189964237
10987 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
10987 MiB free out of 48676 MiB total
after cast to cpu
37961 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj'], ['self_attn.o_proj'], ['mlp.up_proj', 'mlp.gate_proj'], ['mlp.down_proj']]
finished adding batches
subset {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
19 self_attn.k_proj
Pruning ...
realigning
initial loss 439.12908935546875
final loss 359.85333251953125
quantized
not here
quantized in 22.37384867668152 seconds
37959 MiB free out of 48676 MiB total
19 self_attn.v_proj
Pruning ...
realigning
initial loss 163.34803771972656
final loss 162.82638549804688
quantized
not here
quantized in 19.924418926239014 seconds
37959 MiB free out of 48676 MiB total
19 self_attn.q_proj
Pruning ...
realigning
initial loss 388.6387634277344
final loss 332.9989013671875
quantized
not here
quantized in 21.1420681476593 seconds
37959 MiB free out of 48676 MiB total
finished: {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 27648
optimizing the following parameters: dict_keys(['self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 152051712
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0017292414950134116
epoch 1 loss: 0.0014081701647228329
epoch 2 loss: 0.0012682422493526246
epoch 3 loss: 0.0011775693128583953
epoch 4 loss: 0.0011115273882751353
finished adding batches
subset {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
19 self_attn.o_proj
Pruning ...
realigning
initial loss 10.704086303710938
final loss 10.56015396118164
quantized
not here
quantized in 23.64261484146118 seconds
37961 MiB free out of 48676 MiB total
finished: {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 36864
optimizing the following parameters: dict_keys(['mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135274496
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0010553562024142593
epoch 1 loss: 0.0008623248559160857
epoch 2 loss: 0.0007801499923516531
epoch 3 loss: 0.0007282729679900513
epoch 4 loss: 0.0006915314625075553
finished adding batches
subset {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
19 mlp.up_proj
Pruning ...
realigning
initial loss 313.7678527832031
final loss 311.00543212890625
quantized
not here
quantized in 55.11562466621399 seconds
36937 MiB free out of 48676 MiB total
19 mlp.gate_proj
Pruning ...
realigning
initial loss 422.10015869140625
final loss 394.65142822265625
quantized
not here
quantized in 54.6451313495636 seconds
36593 MiB free out of 48676 MiB total
finished: {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 69120
optimizing the following parameters: dict_keys(['mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 45096960
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0050512690540927
epoch 1 loss: 0.004718539010355016
epoch 2 loss: 0.004528196735918755
epoch 3 loss: 0.004391796228446765
epoch 4 loss: 0.004285585917386925
finished adding batches
subset {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
19 mlp.down_proj
Pruning ...
realigning
initial loss 11.796810150146484
final loss 11.769637107849121
quantized
not here
quantized in 58.4143545627594 seconds
35569 MiB free out of 48676 MiB total
finished: {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.003813499421084998
epoch 1 loss: 0.003772519325139001
epoch 2 loss: 0.0037494842945307028
epoch 3 loss: 0.003734623096534051
epoch 4 loss: 0.003724713718838757
10987 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
10987 MiB free out of 48676 MiB total
after cast to cpu
36937 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj'], ['self_attn.o_proj'], ['mlp.up_proj', 'mlp.gate_proj'], ['mlp.down_proj']]
finished adding batches
subset {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
20 self_attn.k_proj
Pruning ...
realigning
initial loss 443.82989501953125
final loss 370.9226989746094
quantized
not here
quantized in 22.836998224258423 seconds
37959 MiB free out of 48676 MiB total
20 self_attn.v_proj
Pruning ...
realigning
initial loss 168.20855712890625
final loss 167.84947204589844
quantized
not here
quantized in 20.401713848114014 seconds
37959 MiB free out of 48676 MiB total
20 self_attn.q_proj
Pruning ...
realigning
initial loss 423.1576232910156
final loss 353.97216796875
quantized
not here
quantized in 21.17186427116394 seconds
37959 MiB free out of 48676 MiB total
finished: {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 27648
optimizing the following parameters: dict_keys(['self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 152051712
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.002016948736127233
epoch 1 loss: 0.001633499495255819
epoch 2 loss: 0.0014721673423991888
epoch 3 loss: 0.0013686355596291833
epoch 4 loss: 0.0012938255349581596
finished adding batches
subset {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
20 self_attn.o_proj
Pruning ...
realigning
initial loss 23.794292449951172
final loss 23.104740142822266
quantized
not here
quantized in 23.15444040298462 seconds
37961 MiB free out of 48676 MiB total
finished: {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 36864
optimizing the following parameters: dict_keys(['mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135274496
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0017075658361136448
epoch 1 loss: 0.0012141747911300627
epoch 2 loss: 0.0010454352341184858
epoch 3 loss: 0.0009480648636781552
epoch 4 loss: 0.0008833653910187422
finished adding batches
subset {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
20 mlp.up_proj
Pruning ...
realigning
initial loss 339.876953125
final loss 336.5275573730469
quantized
not here
quantized in 57.02090549468994 seconds
36937 MiB free out of 48676 MiB total
20 mlp.gate_proj
Pruning ...
realigning
initial loss 460.1741943359375
final loss 430.68316650390625
quantized
not here
quantized in 55.330841064453125 seconds
36593 MiB free out of 48676 MiB total
finished: {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 69120
optimizing the following parameters: dict_keys(['mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 45096960
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.00623369132517837
epoch 1 loss: 0.005776043257355923
epoch 2 loss: 0.005505462206201628
epoch 3 loss: 0.005312364490237087
epoch 4 loss: 0.005163414276466938
finished adding batches
subset {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
20 mlp.down_proj
Pruning ...
realigning
initial loss 15.077155113220215
final loss 14.93879508972168
quantized
not here
quantized in 62.85812711715698 seconds
36593 MiB free out of 48676 MiB total
finished: {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.004805846092494903
epoch 1 loss: 0.004740398948342772
epoch 2 loss: 0.004705607792857336
epoch 3 loss: 0.004681328879087232
epoch 4 loss: 0.00466353179217549
11773 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
11773 MiB free out of 48676 MiB total
after cast to cpu
37961 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj'], ['self_attn.o_proj'], ['mlp.up_proj', 'mlp.gate_proj'], ['mlp.down_proj']]
finished adding batches
subset {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
21 self_attn.k_proj
Pruning ...
realigning
initial loss 450.7417297363281
final loss 386.83966064453125
quantized
not here
quantized in 22.52194571495056 seconds
37959 MiB free out of 48676 MiB total
21 self_attn.v_proj
Pruning ...
realigning
initial loss 202.38583374023438
final loss 201.96908569335938
quantized
not here
quantized in 20.861332416534424 seconds
37959 MiB free out of 48676 MiB total
21 self_attn.q_proj
Pruning ...
realigning
initial loss 428.03070068359375
final loss 371.8785705566406
quantized
not here
quantized in 21.52843189239502 seconds
37959 MiB free out of 48676 MiB total
finished: {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 27648
optimizing the following parameters: dict_keys(['self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 152051712
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.001875593153272348
epoch 1 loss: 0.001540161367302062
epoch 2 loss: 0.0013908885348428157
epoch 3 loss: 0.0012927041461807676
epoch 4 loss: 0.0012213938316563144
finished adding batches
subset {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
21 self_attn.o_proj
Pruning ...
realigning
initial loss 13.63022232055664
final loss 13.059173583984375
quantized
not here
quantized in 23.844271659851074 seconds
37961 MiB free out of 48676 MiB total
finished: {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 36864
optimizing the following parameters: dict_keys(['mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135274496
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0011178984796060831
epoch 1 loss: 0.0008756621291468036
epoch 2 loss: 0.0007901272897470335
epoch 3 loss: 0.0007385986159533786
epoch 4 loss: 0.0007030071305962338
finished adding batches
subset {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
21 mlp.up_proj
Pruning ...
realigning
initial loss 356.91656494140625
final loss 353.9314880371094
quantized
not here
quantized in 55.77201581001282 seconds
36937 MiB free out of 48676 MiB total
21 mlp.gate_proj
Pruning ...
realigning
initial loss 482.25640869140625
final loss 456.2606201171875
quantized
not here
quantized in 54.42532157897949 seconds
36593 MiB free out of 48676 MiB total
finished: {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 69120
optimizing the following parameters: dict_keys(['mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 45096960
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.006675929533230374
epoch 1 loss: 0.006244297324883519
epoch 2 loss: 0.005979401237709681
epoch 3 loss: 0.005786723089840962
epoch 4 loss: 0.005635888745018747
finished adding batches
subset {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
21 mlp.down_proj
Pruning ...
realigning
initial loss 16.183712005615234
final loss 16.097900390625
quantized
not here
quantized in 57.761651039123535 seconds
36593 MiB free out of 48676 MiB total
finished: {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.005157244882866507
epoch 1 loss: 0.005035387523093959
epoch 2 loss: 0.004975089443178149
epoch 3 loss: 0.004933155349135632
epoch 4 loss: 0.004902017793938285
10579 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
10579 MiB free out of 48676 MiB total
after cast to cpu
36937 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj'], ['self_attn.o_proj'], ['mlp.up_proj', 'mlp.gate_proj'], ['mlp.down_proj']]
finished adding batches
subset {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
22 self_attn.k_proj
Pruning ...
realigning
initial loss 488.4015808105469
final loss 411.4453125
quantized
not here
quantized in 22.956788301467896 seconds
36935 MiB free out of 48676 MiB total
22 self_attn.v_proj
Pruning ...
realigning
initial loss 209.29208374023438
final loss 208.7274169921875
quantized
not here
quantized in 20.778846502304077 seconds
36935 MiB free out of 48676 MiB total
22 self_attn.q_proj
Pruning ...
realigning
initial loss 453.19873046875
final loss 397.630615234375
quantized
not here
quantized in 21.42351984977722 seconds
36935 MiB free out of 48676 MiB total
finished: {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 27648
optimizing the following parameters: dict_keys(['self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 152051712
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.003198783762854873
epoch 1 loss: 0.0025084317585424287
epoch 2 loss: 0.0022418914013542235
epoch 3 loss: 0.0020708885558633483
epoch 4 loss: 0.001948214933690906
finished adding batches
subset {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
22 self_attn.o_proj
Pruning ...
realigning
initial loss 50.51085662841797
final loss 49.182830810546875
quantized
not here
quantized in 23.256816148757935 seconds
36937 MiB free out of 48676 MiB total
finished: {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 36864
optimizing the following parameters: dict_keys(['mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135274496
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.004032434893815662
epoch 1 loss: 0.002786404582366231
epoch 2 loss: 0.0023113154211387155
epoch 3 loss: 0.002024994493694976
epoch 4 loss: 0.0018340348651690874
finished adding batches
subset {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
22 mlp.up_proj
Pruning ...
realigning
initial loss 384.044189453125
final loss 380.4131164550781
quantized
not here
quantized in 56.33878135681152 seconds
36937 MiB free out of 48676 MiB total
22 mlp.gate_proj
Pruning ...
realigning
initial loss 518.6851196289062
final loss 492.4537658691406
quantized
not here
quantized in 55.54116463661194 seconds
36593 MiB free out of 48676 MiB total
finished: {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 69120
optimizing the following parameters: dict_keys(['mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 45096960
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.007940384151879698
epoch 1 loss: 0.007356969079410192
epoch 2 loss: 0.006998353717790451
epoch 3 loss: 0.006741526351106586
epoch 4 loss: 0.006547866567416349
finished adding batches
subset {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
22 mlp.down_proj
Pruning ...
realigning
initial loss 20.147869110107422
final loss 19.449630737304688
quantized
not here
quantized in 63.71525526046753 seconds
36593 MiB free out of 48676 MiB total
finished: {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.006193359899043571
epoch 1 loss: 0.0060387963567336556
epoch 2 loss: 0.0059534493448154535
epoch 3 loss: 0.005890062158869114
epoch 4 loss: 0.005840951551363105
10749 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
10749 MiB free out of 48676 MiB total
after cast to cpu
36937 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj'], ['self_attn.o_proj'], ['mlp.up_proj', 'mlp.gate_proj'], ['mlp.down_proj']]
finished adding batches
subset {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
23 self_attn.k_proj
Pruning ...
realigning
initial loss 501.7159118652344
final loss 433.26251220703125
quantized
not here
quantized in 22.58714199066162 seconds
36935 MiB free out of 48676 MiB total
23 self_attn.v_proj
Pruning ...
realigning
initial loss 255.75814819335938
final loss 255.2596435546875
quantized
not here
quantized in 20.55842900276184 seconds
36935 MiB free out of 48676 MiB total
23 self_attn.q_proj
Pruning ...
realigning
initial loss 459.60546875
final loss 416.1667175292969
quantized
not here
quantized in 21.172094106674194 seconds
36935 MiB free out of 48676 MiB total
finished: {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 27648
optimizing the following parameters: dict_keys(['self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 152051712
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0027116618748550536
epoch 1 loss: 0.0022271955631367746
epoch 2 loss: 0.0020089406907572993
epoch 3 loss: 0.0018664898816496134
epoch 4 loss: 0.00176399220526946
finished adding batches
subset {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
23 self_attn.o_proj
Pruning ...
realigning
initial loss 16.48623275756836
final loss 16.297950744628906
quantized
not here
quantized in 22.925625562667847 seconds
37961 MiB free out of 48676 MiB total
finished: {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 36864
optimizing the following parameters: dict_keys(['mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135274496
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0015870479501245427
epoch 1 loss: 0.0012748643503073254
epoch 2 loss: 0.0011545652737368073
epoch 3 loss: 0.0010821936361935514
epoch 4 loss: 0.0010328555617888924
finished adding batches
subset {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
23 mlp.up_proj
Pruning ...
realigning
initial loss 398.6807861328125
final loss 396.4687805175781
quantized
not here
quantized in 55.778173208236694 seconds
36937 MiB free out of 48676 MiB total
23 mlp.gate_proj
Pruning ...
realigning
initial loss 518.415771484375
final loss 499.6127014160156
quantized
not here
quantized in 54.92267441749573 seconds
36593 MiB free out of 48676 MiB total
finished: {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 69120
optimizing the following parameters: dict_keys(['mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 45096960
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.008591214500484057
epoch 1 loss: 0.008025900839129463
epoch 2 loss: 0.007671739665966015
epoch 3 loss: 0.007414053463435266
epoch 4 loss: 0.007213988585135667
finished adding batches
subset {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
23 mlp.down_proj
Pruning ...
realigning
initial loss 17.774986267089844
final loss 17.753238677978516
quantized
not here
quantized in 58.221786975860596 seconds
35569 MiB free out of 48676 MiB total
finished: {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.00578720759585849
epoch 1 loss: 0.005743552599597024
epoch 2 loss: 0.005716157145798206
epoch 3 loss: 0.005696579202776775
epoch 4 loss: 0.00568224106245907
10987 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
10987 MiB free out of 48676 MiB total
after cast to cpu
36937 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj'], ['self_attn.o_proj'], ['mlp.up_proj', 'mlp.gate_proj'], ['mlp.down_proj']]
finished adding batches
subset {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
24 self_attn.k_proj
Pruning ...
realigning
initial loss 465.8616943359375
final loss 410.983642578125
quantized
not here
quantized in 22.826313018798828 seconds
36935 MiB free out of 48676 MiB total
24 self_attn.v_proj
Pruning ...
realigning
initial loss 242.1951141357422
final loss 241.72885131835938
quantized
not here
quantized in 21.22513437271118 seconds
36935 MiB free out of 48676 MiB total
24 self_attn.q_proj
Pruning ...
realigning
initial loss 444.38714599609375
final loss 400.46044921875
quantized
not here
quantized in 21.835354328155518 seconds
36935 MiB free out of 48676 MiB total
finished: {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 27648
optimizing the following parameters: dict_keys(['self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 152051712
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0034661046465771506
epoch 1 loss: 0.002820768086166936
epoch 2 loss: 0.0025292660375271225
epoch 3 loss: 0.002334904084818845
epoch 4 loss: 0.002192310057580471
finished adding batches
subset {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
24 self_attn.o_proj
Pruning ...
realigning
initial loss 43.101959228515625
final loss 37.03864288330078
quantized
not here
quantized in 23.70410990715027 seconds
36937 MiB free out of 48676 MiB total
finished: {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 36864
optimizing the following parameters: dict_keys(['mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135274496
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0031119276463869028
epoch 1 loss: 0.002182993521273602
epoch 2 loss: 0.0018745577608569874
epoch 3 loss: 0.0016938875187406666
epoch 4 loss: 0.0015727582322142553
finished adding batches
subset {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
24 mlp.up_proj
Pruning ...
realigning
initial loss 422.5673828125
final loss 420.26263427734375
quantized
not here
quantized in 56.55357217788696 seconds
36937 MiB free out of 48676 MiB total
24 mlp.gate_proj
Pruning ...
realigning
initial loss 553.551513671875
final loss 530.30712890625
quantized
not here
quantized in 55.12529706954956 seconds
36593 MiB free out of 48676 MiB total
finished: {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 69120
optimizing the following parameters: dict_keys(['mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 45096960
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.009366099664475769
epoch 1 loss: 0.008750957444135565
epoch 2 loss: 0.008374285684112692
epoch 3 loss: 0.008097820547845913
epoch 4 loss: 0.007883620946813608
finished adding batches
subset {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
24 mlp.down_proj
Pruning ...
realigning
initial loss 21.674888610839844
final loss 21.357749938964844
quantized
not here
quantized in 57.33921718597412 seconds
36593 MiB free out of 48676 MiB total
finished: {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0066751881095115095
epoch 1 loss: 0.006401464095688425
epoch 2 loss: 0.006302267796854721
epoch 3 loss: 0.006235638062207727
epoch 4 loss: 0.00618579940055497
11773 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
11773 MiB free out of 48676 MiB total
after cast to cpu
36937 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj'], ['self_attn.o_proj'], ['mlp.up_proj', 'mlp.gate_proj'], ['mlp.down_proj']]
finished adding batches
subset {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
25 self_attn.k_proj
Pruning ...
realigning
initial loss 517.0880126953125
final loss 457.7782287597656
quantized
not here
quantized in 22.540083169937134 seconds
36935 MiB free out of 48676 MiB total
25 self_attn.v_proj
Pruning ...
realigning
initial loss 302.2027282714844
final loss 301.64697265625
quantized
not here
quantized in 20.695441484451294 seconds
37959 MiB free out of 48676 MiB total
25 self_attn.q_proj
Pruning ...
realigning
initial loss 487.6271667480469
final loss 449.2542419433594
quantized
not here
quantized in 21.39444327354431 seconds
37959 MiB free out of 48676 MiB total
finished: {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 27648
optimizing the following parameters: dict_keys(['self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 152051712
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.003316885196909425
epoch 1 loss: 0.002644515678184689
epoch 2 loss: 0.0023865807052061427
epoch 3 loss: 0.0022208488153410144
epoch 4 loss: 0.0021019172090745997
finished adding batches
subset {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
25 self_attn.o_proj
Pruning ...
realigning
initial loss 19.524307250976562
final loss 18.88616943359375
quantized
not here
quantized in 23.302889585494995 seconds
37961 MiB free out of 48676 MiB total
finished: {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 36864
optimizing the following parameters: dict_keys(['mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135274496
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0017136950255007832
epoch 1 loss: 0.0013141462577550556
epoch 2 loss: 0.0011746144468816055
epoch 3 loss: 0.001091065344553499
epoch 4 loss: 0.0010345865425733791
finished adding batches
subset {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
25 mlp.up_proj
Pruning ...
realigning
initial loss 448.2227783203125
final loss 445.8166809082031
quantized
not here
quantized in 56.58043646812439 seconds
36937 MiB free out of 48676 MiB total
25 mlp.gate_proj
Pruning ...
realigning
initial loss 597.008056640625
final loss 569.8843383789062
quantized
not here
quantized in 55.55751419067383 seconds
36593 MiB free out of 48676 MiB total
finished: {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 69120
optimizing the following parameters: dict_keys(['mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 45096960
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.010807997867232189
epoch 1 loss: 0.010090109339216724
epoch 2 loss: 0.009659962946898304
epoch 3 loss: 0.009343348603579216
epoch 4 loss: 0.009091485349927098
finished adding batches
subset {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
25 mlp.down_proj
Pruning ...
realigning
initial loss 18.73137092590332
final loss 18.714584350585938
quantized
not here
quantized in 58.16681957244873 seconds
35569 MiB free out of 48676 MiB total
finished: {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.006108527879405301
epoch 1 loss: 0.006075739413063275
epoch 2 loss: 0.00605582552816486
epoch 3 loss: 0.006042106484528631
epoch 4 loss: 0.006032485263858689
10987 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
10987 MiB free out of 48676 MiB total
after cast to cpu
36937 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj'], ['self_attn.o_proj'], ['mlp.up_proj', 'mlp.gate_proj'], ['mlp.down_proj']]
finished adding batches
subset {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
26 self_attn.k_proj
Pruning ...
realigning
initial loss 518.9476928710938
final loss 448.45245361328125
quantized
not here
quantized in 22.612420082092285 seconds
36935 MiB free out of 48676 MiB total
26 self_attn.v_proj
Pruning ...
realigning
initial loss 299.5340881347656
final loss 299.1121826171875
quantized
not here
quantized in 20.48169493675232 seconds
36935 MiB free out of 48676 MiB total
26 self_attn.q_proj
Pruning ...
realigning
initial loss 476.4132080078125
final loss 425.9782409667969
quantized
not here
quantized in 21.676939249038696 seconds
36935 MiB free out of 48676 MiB total
finished: {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 27648
optimizing the following parameters: dict_keys(['self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 152051712
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.004168392963038059
epoch 1 loss: 0.0034305467415833846
epoch 2 loss: 0.003089891970375902
epoch 3 loss: 0.0028652043038164265
epoch 4 loss: 0.0027021572623198153
finished adding batches
subset {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
26 self_attn.o_proj
Pruning ...
realigning
initial loss 39.056182861328125
final loss 27.1827335357666
quantized
not here
quantized in 24.075613975524902 seconds
36937 MiB free out of 48676 MiB total
finished: {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 36864
optimizing the following parameters: dict_keys(['mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135274496
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0025819214733928675
epoch 1 loss: 0.0020377808823468513
epoch 2 loss: 0.0018181588184233988
epoch 3 loss: 0.0016805948107503355
epoch 4 loss: 0.001584304479365528
finished adding batches
subset {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
26 mlp.up_proj
Pruning ...
realigning
initial loss 468.09967041015625
final loss 464.7267761230469
quantized
not here
quantized in 56.497432708740234 seconds
36937 MiB free out of 48676 MiB total
26 mlp.gate_proj
Pruning ...
realigning
initial loss 645.904296875
final loss 606.2734375
quantized
not here
quantized in 56.165992975234985 seconds
36593 MiB free out of 48676 MiB total
finished: {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 69120
optimizing the following parameters: dict_keys(['mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 45096960
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.010725616266427096
epoch 1 loss: 0.01010154665709706
epoch 2 loss: 0.009730828409374226
epoch 3 loss: 0.009457618252781685
epoch 4 loss: 0.009240241241059266
finished adding batches
subset {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
26 mlp.down_proj
Pruning ...
realigning
initial loss 21.0716552734375
final loss 20.988357543945312
quantized
not here
quantized in 59.0137152671814 seconds
36593 MiB free out of 48676 MiB total
finished: {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.006625526468269527
epoch 1 loss: 0.006512024116091197
epoch 2 loss: 0.006469093314080965
epoch 3 loss: 0.006441294630349148
epoch 4 loss: 0.006422006477805553
10749 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
10749 MiB free out of 48676 MiB total
after cast to cpu
36937 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj'], ['self_attn.o_proj'], ['mlp.up_proj', 'mlp.gate_proj'], ['mlp.down_proj']]
finished adding batches
subset {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
27 self_attn.k_proj
Pruning ...
realigning
initial loss 645.7902221679688
final loss 480.50164794921875
quantized
not here
quantized in 22.3138587474823 seconds
36935 MiB free out of 48676 MiB total
27 self_attn.v_proj
Pruning ...
realigning
initial loss 304.04638671875
final loss 302.6439208984375
quantized
not here
quantized in 21.165125370025635 seconds
36935 MiB free out of 48676 MiB total
27 self_attn.q_proj
Pruning ...
realigning
initial loss 529.8629760742188
final loss 443.6387634277344
quantized
not here
quantized in 21.604355573654175 seconds
36935 MiB free out of 48676 MiB total
finished: {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 27648
optimizing the following parameters: dict_keys(['self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 152051712
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0030890161124261795
epoch 1 loss: 0.0025517262911307625
epoch 2 loss: 0.0023186698845165665
epoch 3 loss: 0.0021675586413039127
epoch 4 loss: 0.0020591751472238684
finished adding batches
subset {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
27 self_attn.o_proj
Pruning ...
realigning
initial loss 26.697601318359375
final loss 26.013790130615234
quantized
not here
quantized in 22.88024139404297 seconds
36937 MiB free out of 48676 MiB total
finished: {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 36864
optimizing the following parameters: dict_keys(['mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135274496
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0020091908945687464
epoch 1 loss: 0.0014722535306646023
epoch 2 loss: 0.0013092044846416684
epoch 3 loss: 0.0012154979895058204
epoch 4 loss: 0.0011533962565408729
finished adding batches
subset {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
27 mlp.up_proj
Pruning ...
realigning
initial loss 498.94232177734375
final loss 494.0234375
quantized
not here
quantized in 57.38196587562561 seconds
36937 MiB free out of 48676 MiB total
27 mlp.gate_proj
Pruning ...
realigning
initial loss 708.8076171875
final loss 654.4136962890625
quantized
not here
quantized in 54.85603952407837 seconds
36593 MiB free out of 48676 MiB total
finished: {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 69120
optimizing the following parameters: dict_keys(['mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 45096960
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.012004322474240325
epoch 1 loss: 0.01130920965806581
epoch 2 loss: 0.010901572015427519
epoch 3 loss: 0.010601096051686909
epoch 4 loss: 0.010362376939156093
finished adding batches
subset {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
27 mlp.down_proj
Pruning ...
realigning
initial loss 23.03701400756836
final loss 22.989913940429688
quantized
not here
quantized in 57.98829698562622 seconds
35569 MiB free out of 48676 MiB total
finished: {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.007467143703252077
epoch 1 loss: 0.007380058104899945
epoch 2 loss: 0.007320994340261677
epoch 3 loss: 0.007275935226061847
epoch 4 loss: 0.007241259081638418
10987 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
10987 MiB free out of 48676 MiB total
after cast to cpu
36937 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj'], ['self_attn.o_proj'], ['mlp.up_proj', 'mlp.gate_proj'], ['mlp.down_proj']]
finished adding batches
subset {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
28 self_attn.k_proj
Pruning ...
realigning
initial loss 631.0696411132812
final loss 480.34490966796875
quantized
not here
quantized in 22.573925971984863 seconds
37959 MiB free out of 48676 MiB total
28 self_attn.v_proj
Pruning ...
realigning
initial loss 338.07373046875
final loss 337.23431396484375
quantized
not here
quantized in 20.760526418685913 seconds
37959 MiB free out of 48676 MiB total
28 self_attn.q_proj
Pruning ...
realigning
initial loss 532.474853515625
final loss 453.40753173828125
quantized
not here
quantized in 21.629124641418457 seconds
37959 MiB free out of 48676 MiB total
finished: {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 27648
optimizing the following parameters: dict_keys(['self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 152051712
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.005226511015280266
epoch 1 loss: 0.004208792013741913
epoch 2 loss: 0.0037940852034807904
epoch 3 loss: 0.0035232378795626573
epoch 4 loss: 0.00332594462815905
finished adding batches
subset {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
28 self_attn.o_proj
Pruning ...
realigning
initial loss 41.899658203125
final loss 39.94215393066406
quantized
not here
quantized in 23.230264902114868 seconds
37961 MiB free out of 48676 MiB total
finished: {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 36864
optimizing the following parameters: dict_keys(['mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135274496
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0032380122338508954
epoch 1 loss: 0.00249411735785543
epoch 2 loss: 0.0022210662900761236
epoch 3 loss: 0.002049099093710538
epoch 4 loss: 0.0019274259102530777
finished adding batches
subset {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
28 mlp.up_proj
Pruning ...
realigning
initial loss 564.9763793945312
final loss 553.3594360351562
quantized
not here
quantized in 56.79706811904907 seconds
36937 MiB free out of 48676 MiB total
28 mlp.gate_proj
Pruning ...
realigning
initial loss 753.2792358398438
final loss 694.9226684570312
quantized
not here
quantized in 55.51060247421265 seconds
36593 MiB free out of 48676 MiB total
finished: {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 69120
optimizing the following parameters: dict_keys(['mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 45096960
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.014474667543254327
epoch 1 loss: 0.013546611189667601
epoch 2 loss: 0.013029308945988305
epoch 3 loss: 0.012653232704906259
epoch 4 loss: 0.012356016181001905
finished adding batches
subset {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
28 mlp.down_proj
Pruning ...
realigning
initial loss 27.896343231201172
final loss 27.686376571655273
quantized
not here
quantized in 61.38700461387634 seconds
36593 MiB free out of 48676 MiB total
finished: {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.00892355107498588
epoch 1 loss: 0.00878990491037257
epoch 2 loss: 0.00870871288861963
epoch 3 loss: 0.008648115981486626
epoch 4 loss: 0.008601923276728485
11667 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
11667 MiB free out of 48676 MiB total
after cast to cpu
36937 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj'], ['self_attn.o_proj'], ['mlp.up_proj', 'mlp.gate_proj'], ['mlp.down_proj']]
finished adding batches
subset {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
29 self_attn.k_proj
Pruning ...
realigning
initial loss 581.1336669921875
final loss 434.73895263671875
quantized
not here
quantized in 22.422368049621582 seconds
36935 MiB free out of 48676 MiB total
29 self_attn.v_proj
Pruning ...
realigning
initial loss 315.8170166015625
final loss 315.0210266113281
quantized
not here
quantized in 20.48913550376892 seconds
36935 MiB free out of 48676 MiB total
29 self_attn.q_proj
Pruning ...
realigning
initial loss 503.704345703125
final loss 410.4604187011719
quantized
not here
quantized in 21.49277353286743 seconds
36935 MiB free out of 48676 MiB total
finished: {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 27648
optimizing the following parameters: dict_keys(['self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 152051712
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0050627984546736116
epoch 1 loss: 0.0039200278333737515
epoch 2 loss: 0.0034935717940243194
epoch 3 loss: 0.0032249552677967586
epoch 4 loss: 0.0030337941334437346
finished adding batches
subset {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
29 self_attn.o_proj
Pruning ...
realigning
initial loss 78.21246337890625
final loss 62.91063690185547
quantized
not here
quantized in 23.79368257522583 seconds
36937 MiB free out of 48676 MiB total
finished: {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 36864
optimizing the following parameters: dict_keys(['mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135274496
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.004090917855137377
epoch 1 loss: 0.002843598731487873
epoch 2 loss: 0.0024894487414712785
epoch 3 loss: 0.0022761043364880607
epoch 4 loss: 0.0021289195719873533
finished adding batches
subset {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
29 mlp.up_proj
Pruning ...
realigning
initial loss 676.763916015625
final loss 632.8826904296875
quantized
not here
quantized in 55.986894369125366 seconds
36937 MiB free out of 48676 MiB total
29 mlp.gate_proj
Pruning ...
realigning
initial loss 868.0123291015625
final loss 769.5999145507812
quantized
not here
quantized in 54.77952265739441 seconds
36593 MiB free out of 48676 MiB total
finished: {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 69120
optimizing the following parameters: dict_keys(['mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 45096960
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.019631669420050457
epoch 1 loss: 0.018329440390516538
epoch 2 loss: 0.017616537465073634
epoch 3 loss: 0.01709746899723541
epoch 4 loss: 0.01668516104837181
finished adding batches
subset {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
29 mlp.down_proj
Pruning ...
realigning
initial loss 32.8584098815918
final loss 32.15422821044922
quantized
not here
quantized in 62.452545404434204 seconds
36593 MiB free out of 48676 MiB total
finished: {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.010346532544645015
epoch 1 loss: 0.010170128341997042
epoch 2 loss: 0.010059334221296012
epoch 3 loss: 0.009972344923880883
epoch 4 loss: 0.009903462079819292
10749 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
10749 MiB free out of 48676 MiB total
after cast to cpu
36937 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj'], ['self_attn.o_proj'], ['mlp.up_proj', 'mlp.gate_proj'], ['mlp.down_proj']]
finished adding batches
subset {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
30 self_attn.k_proj
Pruning ...
realigning
initial loss 677.09375
final loss 469.13726806640625
quantized
not here
quantized in 21.990663051605225 seconds
36935 MiB free out of 48676 MiB total
30 self_attn.v_proj
Pruning ...
realigning
initial loss 353.2793884277344
final loss 350.83978271484375
quantized
not here
quantized in 20.932552576065063 seconds
36935 MiB free out of 48676 MiB total
30 self_attn.q_proj
Pruning ...
realigning
initial loss 550.3450317382812
final loss 429.6777648925781
quantized
not here
quantized in 20.91052770614624 seconds
36935 MiB free out of 48676 MiB total
finished: {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 27648
optimizing the following parameters: dict_keys(['self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 152051712
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0045537789665104356
epoch 1 loss: 0.003274783723099972
epoch 2 loss: 0.0029183633259890485
epoch 3 loss: 0.002705088632865227
epoch 4 loss: 0.002557194184191758
finished adding batches
subset {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
30 self_attn.o_proj
Pruning ...
realigning
initial loss 57.317108154296875
final loss 46.862918853759766
quantized
not here
quantized in 23.516315937042236 seconds
37961 MiB free out of 48676 MiB total
finished: {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 36864
optimizing the following parameters: dict_keys(['mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135274496
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0035055105181527324
epoch 1 loss: 0.002567242947407067
epoch 2 loss: 0.0022600046931984252
epoch 3 loss: 0.0020750741387018934
epoch 4 loss: 0.001951003217982361
finished adding batches
subset {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
30 mlp.up_proj
Pruning ...
realigning
initial loss 1243.5400390625
final loss 705.9345703125
quantized
not here
quantized in 56.51789832115173 seconds
36937 MiB free out of 48676 MiB total
30 mlp.gate_proj
Pruning ...
realigning
initial loss 1513.775390625
final loss 856.2279052734375
quantized
not here
quantized in 54.38965702056885 seconds
36593 MiB free out of 48676 MiB total
finished: {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 69120
optimizing the following parameters: dict_keys(['mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 45096960
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.04973596124909818
epoch 1 loss: 0.04502504240372218
epoch 2 loss: 0.041678303212393075
epoch 3 loss: 0.03880601155105978
epoch 4 loss: 0.03624465534812771
finished adding batches
subset {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
30 mlp.down_proj
Pruning ...
realigning
initial loss 47.89433670043945
final loss 46.75719451904297
quantized
not here
quantized in 60.216460943222046 seconds
36593 MiB free out of 48676 MiB total
finished: {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.015028613430331461
epoch 1 loss: 0.014785355509957299
epoch 2 loss: 0.01463092997437343
epoch 3 loss: 0.01450540035875747
epoch 4 loss: 0.01440245606499957
10749 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
10749 MiB free out of 48676 MiB total
after cast to cpu
36937 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj'], ['self_attn.o_proj'], ['mlp.up_proj', 'mlp.gate_proj'], ['mlp.down_proj']]
finished adding batches
subset {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
31 self_attn.k_proj
Pruning ...
realigning
initial loss 673.2779541015625
final loss 403.7685546875
quantized
not here
quantized in 22.10624599456787 seconds
36935 MiB free out of 48676 MiB total
31 self_attn.v_proj
Pruning ...
realigning
initial loss 201.6826171875
final loss 200.9287872314453
quantized
not here
quantized in 19.704457759857178 seconds
36935 MiB free out of 48676 MiB total
31 self_attn.q_proj
Pruning ...
realigning
initial loss 441.87384033203125
final loss 330.6483459472656
quantized
not here
quantized in 20.917311429977417 seconds
36935 MiB free out of 48676 MiB total
finished: {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 27648
optimizing the following parameters: dict_keys(['self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 152051712
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.05579733068589121
epoch 1 loss: 0.03960705605277326
epoch 2 loss: 0.03487890723044984
epoch 3 loss: 0.03228055285580922
epoch 4 loss: 0.030392092929105274
finished adding batches
subset {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
31 self_attn.o_proj
Pruning ...
realigning
initial loss 145.45556640625
final loss 82.04521179199219
quantized
not here
quantized in 23.1735577583313 seconds
36937 MiB free out of 48676 MiB total
finished: {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 36864
optimizing the following parameters: dict_keys(['mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135274496
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.008949041581217898
epoch 1 loss: 0.006795901332225185
epoch 2 loss: 0.006040199114067946
epoch 3 loss: 0.005587227871728828
epoch 4 loss: 0.005252692033536732
finished adding batches
subset {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
31 mlp.up_proj
Pruning ...
realigning
initial loss 1464.7041015625
final loss 662.513427734375
quantized
not here
quantized in 56.65085220336914 seconds
36937 MiB free out of 48676 MiB total
31 mlp.gate_proj
Pruning ...
realigning
initial loss 1114.3741455078125
final loss 749.8784790039062
quantized
not here
quantized in 53.804656744003296 seconds
36593 MiB free out of 48676 MiB total
finished: {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 69120
optimizing the following parameters: dict_keys(['mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 45096960
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.13437954743858427
epoch 1 loss: 0.10418767691589892
epoch 2 loss: 0.08607090421719477
epoch 3 loss: 0.07287067163269967
epoch 4 loss: 0.06321640854002908
finished adding batches
subset {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
31 mlp.down_proj
Pruning ...
realigning
initial loss 129.38560485839844
final loss 105.62286376953125
quantized
not here
quantized in 61.77938270568848 seconds
35569 MiB free out of 48676 MiB total
finished: {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.032659683463862166
epoch 1 loss: 0.03152953337121289
epoch 2 loss: 0.031053561353473924
epoch 3 loss: 0.030697924084961414
epoch 4 loss: 0.03040175932983402
10987 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
10987 MiB free out of 48676 MiB total
after cast to cpu
36937 MiB free out of 48676 MiB total
Total bits: 12995657728 Total params: 6476005376
average bits per value: 2.0067397992227978
total time taken: 28449.547039985657
Loading tokenizer for meta-llama/Llama-2-7b-hf
/home/lliu/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Dataset: wikitext2
Evaluating ...
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
Perplexity: 7.593173
