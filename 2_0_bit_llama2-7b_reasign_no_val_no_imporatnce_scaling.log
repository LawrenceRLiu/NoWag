/home/lliu/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.23s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.29it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.18it/s]
Model loaded. LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
4096
Loading tokenizer for meta-llama/Llama-2-7b-hf
Starting...
38748 MiB free out of 48676 MiB total
attention_mask torch.Size([1, 1, 4096, 4096]) cuda:5 torch.float16
position_ids torch.Size([1, 4096]) cuda:5 torch.int64
position_ids torch.Size([1, 4096])
attention_mask torch.Size([1, 1, 4096, 4096])
Ready.
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
0 self_attn.q_proj
Pruning ...
256
iter 0, train loss 1.2186399698257446, val loss None
iter 10, train loss 1.119516134262085, val loss None
iter 20, train loss 0.9884368181228638, val loss None
iter 30, train loss 1.046722173690796, val loss None
iter 40, train loss 0.984609842300415, val loss None
iter 50, train loss 0.9213398694992065, val loss None
iter 60, train loss 0.834230899810791, val loss None
iter 70, train loss 0.8075946569442749, val loss None
iter 80, train loss 0.7496522068977356, val loss None
iter 90, train loss 0.7204086184501648, val loss None
best loss 0.7134543061256409
not here
quantized in 34.16546583175659 seconds
36458 MiB free out of 48676 MiB total
0 self_attn.k_proj
Pruning ...
256
iter 0, train loss 0.9332952499389648, val loss None
iter 10, train loss 0.7587932348251343, val loss None
iter 20, train loss 0.7534413933753967, val loss None
iter 30, train loss 0.703830361366272, val loss None
iter 40, train loss 0.6517475843429565, val loss None
iter 50, train loss 0.630162239074707, val loss None
iter 60, train loss 0.6182987689971924, val loss None
iter 70, train loss 0.5846052169799805, val loss None
iter 80, train loss 0.575898289680481, val loss None
iter 90, train loss 0.5881614685058594, val loss None
best loss 0.561808168888092
not here
quantized in 32.61782360076904 seconds
36394 MiB free out of 48676 MiB total
0 self_attn.v_proj
Pruning ...
256
iter 0, train loss 0.09436560422182083, val loss None
iter 10, train loss 0.08059316128492355, val loss None
iter 20, train loss 0.0804050862789154, val loss None
iter 30, train loss 0.07679381221532822, val loss None
iter 40, train loss 0.07414661347866058, val loss None
iter 50, train loss 0.07325101643800735, val loss None
iter 60, train loss 0.07180514931678772, val loss None
iter 70, train loss 0.07146237045526505, val loss None
iter 80, train loss 0.0706852525472641, val loss None
iter 90, train loss 0.0700930804014206, val loss None
best loss 0.06996148079633713
not here
quantized in 32.99494695663452 seconds
36394 MiB free out of 48676 MiB total
0 self_attn.o_proj
Pruning ...
256
iter 0, train loss 0.011464841663837433, val loss None
iter 10, train loss 0.010397965088486671, val loss None
iter 20, train loss 0.010019666515290737, val loss None
iter 30, train loss 0.00937157217413187, val loss None
iter 40, train loss 0.009089747443795204, val loss None
iter 50, train loss 0.008979199454188347, val loss None
iter 60, train loss 0.00871557742357254, val loss None
iter 70, train loss 0.008652668446302414, val loss None
iter 80, train loss 0.008620553649961948, val loss None
iter 90, train loss 0.008694020099937916, val loss None
best loss 0.008584421128034592
not here
quantized in 31.989805698394775 seconds
36330 MiB free out of 48676 MiB total
0 mlp.gate_proj
Pruning ...
256
iter 0, train loss 3.914763927459717, val loss None
iter 10, train loss 4.074483394622803, val loss None
iter 20, train loss 3.913628101348877, val loss None
iter 30, train loss 3.8993492126464844, val loss None
iter 40, train loss 3.8955917358398438, val loss None
iter 50, train loss 3.8993489742279053, val loss None
iter 60, train loss 3.8939061164855957, val loss None
iter 70, train loss 3.8909707069396973, val loss None
iter 80, train loss 3.8890702724456787, val loss None
iter 90, train loss 3.8866477012634277, val loss None
best loss 3.7343382835388184
not here
quantized in 87.08159279823303 seconds
35942 MiB free out of 48676 MiB total
0 mlp.up_proj
Pruning ...
256
iter 0, train loss 3.5440115928649902, val loss None
iter 10, train loss 3.751767635345459, val loss None
iter 20, train loss 3.577709436416626, val loss None
iter 30, train loss 3.562427043914795, val loss None
iter 40, train loss 3.5612680912017822, val loss None
iter 50, train loss 3.555222749710083, val loss None
iter 60, train loss 3.5544967651367188, val loss None
iter 70, train loss 3.5540828704833984, val loss None
iter 80, train loss 3.5538244247436523, val loss None
iter 90, train loss 3.5520124435424805, val loss None
best loss 3.391655206680298
not here
quantized in 86.35130715370178 seconds
35662 MiB free out of 48676 MiB total
0 mlp.down_proj
Pruning ...
256
iter 0, train loss 0.010795675218105316, val loss None
iter 10, train loss 0.01196495071053505, val loss None
iter 20, train loss 0.011627107858657837, val loss None
iter 30, train loss 0.01134019996970892, val loss None
iter 40, train loss 0.011228447780013084, val loss None
iter 50, train loss 0.011204478330910206, val loss None
iter 60, train loss 0.011173442006111145, val loss None
iter 70, train loss 0.011148728430271149, val loss None
iter 80, train loss 0.011124704964458942, val loss None
iter 90, train loss 0.011111089028418064, val loss None
best loss 0.010174737311899662
not here
quantized in 93.79633212089539 seconds
35382 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35382 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31286 MiB free out of 48676 MiB total
after cast to cpu
38316 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
1 self_attn.q_proj
Pruning ...
256
iter 0, train loss 16.945751190185547, val loss None
iter 10, train loss 14.755410194396973, val loss None
iter 20, train loss 15.880239486694336, val loss None
iter 30, train loss 14.456892967224121, val loss None
iter 40, train loss 13.959577560424805, val loss None
iter 50, train loss 13.54292106628418, val loss None
iter 60, train loss 13.246273040771484, val loss None
iter 70, train loss 13.025635719299316, val loss None
iter 80, train loss 12.849157333374023, val loss None
iter 90, train loss 12.832318305969238, val loss None
best loss 12.754870414733887
not here
quantized in 36.24108624458313 seconds
36424 MiB free out of 48676 MiB total
1 self_attn.k_proj
Pruning ...
256
iter 0, train loss 17.59294891357422, val loss None
iter 10, train loss 15.048370361328125, val loss None
iter 20, train loss 14.319307327270508, val loss None
iter 30, train loss 13.812406539916992, val loss None
iter 40, train loss 13.346725463867188, val loss None
iter 50, train loss 12.890854835510254, val loss None
iter 60, train loss 12.745624542236328, val loss None
iter 70, train loss 12.640729904174805, val loss None
iter 80, train loss 12.643141746520996, val loss None
iter 90, train loss 12.590311050415039, val loss None
best loss 12.472143173217773
not here
quantized in 35.615628719329834 seconds
36382 MiB free out of 48676 MiB total
1 self_attn.v_proj
Pruning ...
256
iter 0, train loss 0.9122413396835327, val loss None
iter 10, train loss 0.9347096681594849, val loss None
iter 20, train loss 0.8760644197463989, val loss None
iter 30, train loss 0.8593235015869141, val loss None
iter 40, train loss 0.8463397026062012, val loss None
iter 50, train loss 0.8409578204154968, val loss None
iter 60, train loss 0.8379769325256348, val loss None
iter 70, train loss 0.8318275213241577, val loss None
iter 80, train loss 0.8313741683959961, val loss None
iter 90, train loss 0.8295465707778931, val loss None
best loss 0.82602858543396
not here
quantized in 32.38873887062073 seconds
36340 MiB free out of 48676 MiB total
1 self_attn.o_proj
Pruning ...
256
iter 0, train loss 0.08238847553730011, val loss None
iter 10, train loss 0.07831564545631409, val loss None
iter 20, train loss 0.07562695443630219, val loss None
iter 30, train loss 0.07412374764680862, val loss None
iter 40, train loss 0.07366389036178589, val loss None
iter 50, train loss 0.07376908510923386, val loss None
iter 60, train loss 0.07325610518455505, val loss None
iter 70, train loss 0.07317818701267242, val loss None
iter 80, train loss 0.07295165956020355, val loss None
iter 90, train loss 0.0725407674908638, val loss None
best loss 0.0722716748714447
not here
quantized in 31.338449716567993 seconds
36340 MiB free out of 48676 MiB total
1 mlp.gate_proj
Pruning ...
256
iter 0, train loss 16.40302276611328, val loss None
iter 10, train loss 17.54158592224121, val loss None
iter 20, train loss 17.291522979736328, val loss None
iter 30, train loss 17.122974395751953, val loss None
iter 40, train loss 17.000709533691406, val loss None
iter 50, train loss 16.908750534057617, val loss None
iter 60, train loss 16.831045150756836, val loss None
iter 70, train loss 16.774456024169922, val loss None
iter 80, train loss 16.760040283203125, val loss None
iter 90, train loss 16.74863052368164, val loss None
best loss 15.422050476074219
not here
quantized in 88.39071679115295 seconds
35952 MiB free out of 48676 MiB total
1 mlp.up_proj
Pruning ...
256
iter 0, train loss 13.27222728729248, val loss None
iter 10, train loss 13.751538276672363, val loss None
iter 20, train loss 13.461450576782227, val loss None
iter 30, train loss 13.472695350646973, val loss None
iter 40, train loss 13.463528633117676, val loss None
iter 50, train loss 13.460586547851562, val loss None
iter 60, train loss 13.460719108581543, val loss None
iter 70, train loss 13.454561233520508, val loss None
iter 80, train loss 13.45163345336914, val loss None
iter 90, train loss 13.446497917175293, val loss None
best loss 13.138684272766113
not here
quantized in 85.82444095611572 seconds
35672 MiB free out of 48676 MiB total
1 mlp.down_proj
Pruning ...
256
iter 0, train loss 0.08604798465967178, val loss None
iter 10, train loss 0.2327139526605606, val loss None
iter 20, train loss 0.15620800852775574, val loss None
iter 30, train loss 0.13550694286823273, val loss None
iter 40, train loss 0.11794553697109222, val loss None
iter 50, train loss 0.10315452516078949, val loss None
iter 60, train loss 0.09992377460002899, val loss None
iter 70, train loss 0.09836062788963318, val loss None
iter 80, train loss 0.09685447812080383, val loss None
iter 90, train loss 0.0968516543507576, val loss None
best loss 0.08604798465967178
not here
quantized in 91.78481435775757 seconds
35392 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35392 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31296 MiB free out of 48676 MiB total
after cast to cpu
38316 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
2 self_attn.q_proj
Pruning ...
256
iter 0, train loss 62.05599594116211, val loss None
iter 10, train loss 61.436309814453125, val loss None
iter 20, train loss 61.86345291137695, val loss None
iter 30, train loss 60.68553924560547, val loss None
iter 40, train loss 60.92036056518555, val loss None
iter 50, train loss 60.4442138671875, val loss None
iter 60, train loss 59.93548583984375, val loss None
iter 70, train loss 59.66608428955078, val loss None
iter 80, train loss 59.53892135620117, val loss None
iter 90, train loss 59.5058708190918, val loss None
best loss 56.895355224609375
not here
quantized in 34.73664450645447 seconds
36392 MiB free out of 48676 MiB total
2 self_attn.k_proj
Pruning ...
256
iter 0, train loss 73.08901977539062, val loss None
iter 10, train loss 70.54443359375, val loss None
iter 20, train loss 72.67113494873047, val loss None
iter 30, train loss 71.32539367675781, val loss None
iter 40, train loss 70.22872924804688, val loss None
iter 50, train loss 69.80856323242188, val loss None
iter 60, train loss 69.35455322265625, val loss None
iter 70, train loss 69.3481216430664, val loss None
iter 80, train loss 69.01400756835938, val loss None
iter 90, train loss 68.77104187011719, val loss None
best loss 65.68094635009766
not here
quantized in 33.92910313606262 seconds
36414 MiB free out of 48676 MiB total
2 self_attn.v_proj
Pruning ...
256
iter 0, train loss 13.670679092407227, val loss None
iter 10, train loss 14.228412628173828, val loss None
iter 20, train loss 13.825395584106445, val loss None
iter 30, train loss 13.803228378295898, val loss None
iter 40, train loss 13.767959594726562, val loss None
iter 50, train loss 13.747218132019043, val loss None
iter 60, train loss 13.743337631225586, val loss None
iter 70, train loss 13.735649108886719, val loss None
iter 80, train loss 13.727543830871582, val loss None
iter 90, train loss 13.734553337097168, val loss None
best loss 13.453323364257812
not here
quantized in 32.55418395996094 seconds
36404 MiB free out of 48676 MiB total
2 self_attn.o_proj
Pruning ...
256
iter 0, train loss 0.1196121796965599, val loss None
iter 10, train loss 0.11978708952665329, val loss None
iter 20, train loss 0.11763014644384384, val loss None
iter 30, train loss 0.1177930012345314, val loss None
iter 40, train loss 0.11698213219642639, val loss None
iter 50, train loss 0.11702995747327805, val loss None
iter 60, train loss 0.11630865931510925, val loss None
iter 70, train loss 0.11636316031217575, val loss None
iter 80, train loss 0.11624749004840851, val loss None
iter 90, train loss 0.11621277779340744, val loss None
best loss 0.11618171632289886
not here
quantized in 31.936964511871338 seconds
36340 MiB free out of 48676 MiB total
2 mlp.gate_proj
Pruning ...
256
iter 0, train loss 33.41579055786133, val loss None
iter 10, train loss 34.376895904541016, val loss None
iter 20, train loss 33.441497802734375, val loss None
iter 30, train loss 33.68362045288086, val loss None
iter 40, train loss 33.57344436645508, val loss None
iter 50, train loss 33.61870574951172, val loss None
iter 60, train loss 33.64488983154297, val loss None
iter 70, train loss 33.62866973876953, val loss None
iter 80, train loss 33.6023063659668, val loss None
iter 90, train loss 33.58666229248047, val loss None
best loss 32.91286087036133
not here
quantized in 86.10232424736023 seconds
36038 MiB free out of 48676 MiB total
2 mlp.up_proj
Pruning ...
256
iter 0, train loss 27.497846603393555, val loss None
iter 10, train loss 27.710920333862305, val loss None
iter 20, train loss 27.76258087158203, val loss None
iter 30, train loss 27.720447540283203, val loss None
iter 40, train loss 27.700786590576172, val loss None
iter 50, train loss 27.69084930419922, val loss None
iter 60, train loss 27.67658042907715, val loss None
iter 70, train loss 27.691843032836914, val loss None
iter 80, train loss 27.70444107055664, val loss None
iter 90, train loss 27.696828842163086, val loss None
best loss 27.4171199798584
not here
quantized in 84.24092817306519 seconds
35758 MiB free out of 48676 MiB total
2 mlp.down_proj
Pruning ...
256
iter 0, train loss 0.152591273188591, val loss None
iter 10, train loss 0.1529029905796051, val loss None
iter 20, train loss 0.15216948091983795, val loss None
iter 30, train loss 0.1520366668701172, val loss None
iter 40, train loss 0.15170712769031525, val loss None
iter 50, train loss 0.15152837336063385, val loss None
iter 60, train loss 0.15132007002830505, val loss None
iter 70, train loss 0.1510491967201233, val loss None
iter 80, train loss 0.1510685682296753, val loss None
iter 90, train loss 0.15100055932998657, val loss None
best loss 0.15091441571712494
not here
quantized in 89.70111513137817 seconds
35564 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35564 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31468 MiB free out of 48676 MiB total
after cast to cpu
38316 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
3 self_attn.q_proj
Pruning ...
256
iter 0, train loss 144.90451049804688, val loss None
iter 10, train loss 150.8054962158203, val loss None
iter 20, train loss 150.48858642578125, val loss None
iter 30, train loss 145.67665100097656, val loss None
iter 40, train loss 144.09097290039062, val loss None
iter 50, train loss 143.61474609375, val loss None
iter 60, train loss 143.1101837158203, val loss None
iter 70, train loss 143.1160430908203, val loss None
iter 80, train loss 143.1881103515625, val loss None
iter 90, train loss 143.1158905029297, val loss None
best loss 138.28799438476562
not here
quantized in 34.62879681587219 seconds
36392 MiB free out of 48676 MiB total
3 self_attn.k_proj
Pruning ...
256
iter 0, train loss 164.70419311523438, val loss None
iter 10, train loss 166.20843505859375, val loss None
iter 20, train loss 171.74874877929688, val loss None
iter 30, train loss 167.29031372070312, val loss None
iter 40, train loss 165.08299255371094, val loss None
iter 50, train loss 164.67080688476562, val loss None
iter 60, train loss 163.7854766845703, val loss None
iter 70, train loss 163.20602416992188, val loss None
iter 80, train loss 162.99378967285156, val loss None
iter 90, train loss 162.570556640625, val loss None
best loss 155.342529296875
not here
quantized in 33.63212180137634 seconds
36414 MiB free out of 48676 MiB total
3 self_attn.v_proj
Pruning ...
256
iter 0, train loss 35.3080940246582, val loss None
iter 10, train loss 35.770137786865234, val loss None
iter 20, train loss 35.43623352050781, val loss None
iter 30, train loss 35.35227966308594, val loss None
iter 40, train loss 35.181175231933594, val loss None
iter 50, train loss 35.159358978271484, val loss None
iter 60, train loss 35.126014709472656, val loss None
iter 70, train loss 35.10194778442383, val loss None
iter 80, train loss 35.078250885009766, val loss None
iter 90, train loss 35.0616569519043, val loss None
best loss 35.060302734375
not here
quantized in 32.15553092956543 seconds
36404 MiB free out of 48676 MiB total
3 self_attn.o_proj
Pruning ...
256
iter 0, train loss 0.3212473690509796, val loss None
iter 10, train loss 0.2852418124675751, val loss None
iter 20, train loss 0.26941871643066406, val loss None
iter 30, train loss 0.26170188188552856, val loss None
iter 40, train loss 0.2592753469944, val loss None
iter 50, train loss 0.2564605474472046, val loss None
iter 60, train loss 0.2535132169723511, val loss None
iter 70, train loss 0.2516123652458191, val loss None
iter 80, train loss 0.2513854503631592, val loss None
iter 90, train loss 0.2508618235588074, val loss None
best loss 0.24934183061122894
not here
quantized in 32.55520844459534 seconds
36372 MiB free out of 48676 MiB total
3 mlp.gate_proj
Pruning ...
256
iter 0, train loss 53.039215087890625, val loss None
iter 10, train loss 54.05447006225586, val loss None
iter 20, train loss 53.54362106323242, val loss None
iter 30, train loss 53.442054748535156, val loss None
iter 40, train loss 53.37009811401367, val loss None
iter 50, train loss 53.37699890136719, val loss None
iter 60, train loss 53.41632080078125, val loss None
iter 70, train loss 53.41552734375, val loss None
iter 80, train loss 53.39771270751953, val loss None
iter 90, train loss 53.3875732421875, val loss None
best loss 52.635284423828125
not here
quantized in 85.57314729690552 seconds
35984 MiB free out of 48676 MiB total
3 mlp.up_proj
Pruning ...
256
iter 0, train loss 43.882781982421875, val loss None
iter 10, train loss 43.986412048339844, val loss None
iter 20, train loss 43.95484161376953, val loss None
iter 30, train loss 43.98827362060547, val loss None
iter 40, train loss 43.934104919433594, val loss None
iter 50, train loss 43.914581298828125, val loss None
iter 60, train loss 43.945316314697266, val loss None
iter 70, train loss 43.98174285888672, val loss None
iter 80, train loss 43.97136688232422, val loss None
iter 90, train loss 43.958656311035156, val loss None
best loss 43.882781982421875
not here
quantized in 83.24674987792969 seconds
35704 MiB free out of 48676 MiB total
3 mlp.down_proj
Pruning ...
256
iter 0, train loss 0.29522085189819336, val loss None
iter 10, train loss 0.2965443432331085, val loss None
iter 20, train loss 0.2948909401893616, val loss None
iter 30, train loss 0.29556435346603394, val loss None
iter 40, train loss 0.2947760820388794, val loss None
iter 50, train loss 0.29400283098220825, val loss None
iter 60, train loss 0.29436466097831726, val loss None
iter 70, train loss 0.2947971820831299, val loss None
iter 80, train loss 0.2944430410861969, val loss None
iter 90, train loss 0.2943456768989563, val loss None
best loss 0.29400283098220825
not here
quantized in 89.29900074005127 seconds
35424 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35424 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31328 MiB free out of 48676 MiB total
after cast to cpu
38316 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
4 self_attn.q_proj
Pruning ...
256
iter 0, train loss 135.45152282714844, val loss None
iter 10, train loss 143.49830627441406, val loss None
iter 20, train loss 140.2900390625, val loss None
iter 30, train loss 137.57839965820312, val loss None
iter 40, train loss 135.9101104736328, val loss None
iter 50, train loss 135.24098205566406, val loss None
iter 60, train loss 135.0281982421875, val loss None
iter 70, train loss 134.88180541992188, val loss None
iter 80, train loss 134.970947265625, val loss None
iter 90, train loss 134.92645263671875, val loss None
best loss 130.7843017578125
not here
quantized in 34.23441958427429 seconds
36392 MiB free out of 48676 MiB total
4 self_attn.k_proj
Pruning ...
256
iter 0, train loss 149.69503784179688, val loss None
iter 10, train loss 157.72474670410156, val loss None
iter 20, train loss 157.87130737304688, val loss None
iter 30, train loss 152.55638122558594, val loss None
iter 40, train loss 150.77346801757812, val loss None
iter 50, train loss 150.41461181640625, val loss None
iter 60, train loss 150.09457397460938, val loss None
iter 70, train loss 149.70404052734375, val loss None
iter 80, train loss 149.6320343017578, val loss None
iter 90, train loss 149.5099639892578, val loss None
best loss 143.2315673828125
not here
quantized in 33.401461124420166 seconds
36350 MiB free out of 48676 MiB total
4 self_attn.v_proj
Pruning ...
256
iter 0, train loss 34.99394989013672, val loss None
iter 10, train loss 35.407371520996094, val loss None
iter 20, train loss 35.050254821777344, val loss None
iter 30, train loss 35.002166748046875, val loss None
iter 40, train loss 34.86277389526367, val loss None
iter 50, train loss 34.85710906982422, val loss None
iter 60, train loss 34.818607330322266, val loss None
iter 70, train loss 34.76155471801758, val loss None
iter 80, train loss 34.728057861328125, val loss None
iter 90, train loss 34.72806930541992, val loss None
best loss 34.72072219848633
not here
quantized in 32.05431365966797 seconds
36372 MiB free out of 48676 MiB total
4 self_attn.o_proj
Pruning ...
256
iter 0, train loss 0.6703832149505615, val loss None
iter 10, train loss 0.5747848153114319, val loss None
iter 20, train loss 0.5445079803466797, val loss None
iter 30, train loss 0.5263746976852417, val loss None
iter 40, train loss 0.5168761014938354, val loss None
iter 50, train loss 0.5065844058990479, val loss None
iter 60, train loss 0.5008282661437988, val loss None
iter 70, train loss 0.49957993626594543, val loss None
iter 80, train loss 0.49563708901405334, val loss None
iter 90, train loss 0.4945266544818878, val loss None
best loss 0.4914233684539795
not here
quantized in 32.32253313064575 seconds
36308 MiB free out of 48676 MiB total
4 mlp.gate_proj
Pruning ...
256
iter 0, train loss 77.77587890625, val loss None
iter 10, train loss 79.71919250488281, val loss None
iter 20, train loss 78.72579956054688, val loss None
iter 30, train loss 78.72364044189453, val loss None
iter 40, train loss 78.6020736694336, val loss None
iter 50, train loss 78.5497055053711, val loss None
iter 60, train loss 78.43387603759766, val loss None
iter 70, train loss 78.37782287597656, val loss None
iter 80, train loss 78.39281463623047, val loss None
iter 90, train loss 78.3902587890625, val loss None
best loss 77.16560363769531
not here
quantized in 86.03393244743347 seconds
36006 MiB free out of 48676 MiB total
4 mlp.up_proj
Pruning ...
256
iter 0, train loss 61.18034744262695, val loss None
iter 10, train loss 61.357933044433594, val loss None
iter 20, train loss 61.38775634765625, val loss None
iter 30, train loss 61.324954986572266, val loss None
iter 40, train loss 61.2733154296875, val loss None
iter 50, train loss 61.272029876708984, val loss None
iter 60, train loss 61.24834060668945, val loss None
iter 70, train loss 61.2513427734375, val loss None
iter 80, train loss 61.26003646850586, val loss None
iter 90, train loss 61.263710021972656, val loss None
best loss 61.18034744262695
not here
quantized in 83.70022511482239 seconds
35812 MiB free out of 48676 MiB total
4 mlp.down_proj
Pruning ...
256
iter 0, train loss 0.5791292190551758, val loss None
iter 10, train loss 0.5794622898101807, val loss None
iter 20, train loss 0.5757735371589661, val loss None
iter 30, train loss 0.5770032405853271, val loss None
iter 40, train loss 0.5751299858093262, val loss None
iter 50, train loss 0.5746051073074341, val loss None
iter 60, train loss 0.5735419988632202, val loss None
iter 70, train loss 0.5730450749397278, val loss None
iter 80, train loss 0.5726238489151001, val loss None
iter 90, train loss 0.5721076726913452, val loss None
best loss 0.5718129873275757
not here
quantized in 89.31544017791748 seconds
35532 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35532 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31436 MiB free out of 48676 MiB total
after cast to cpu
38316 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
5 self_attn.q_proj
Pruning ...
256
iter 0, train loss 152.32489013671875, val loss None
iter 10, train loss 162.36953735351562, val loss None
iter 20, train loss 158.0399169921875, val loss None
iter 30, train loss 153.69085693359375, val loss None
iter 40, train loss 153.53353881835938, val loss None
iter 50, train loss 152.56158447265625, val loss None
iter 60, train loss 152.37493896484375, val loss None
iter 70, train loss 152.12896728515625, val loss None
iter 80, train loss 151.96018981933594, val loss None
iter 90, train loss 151.683349609375, val loss None
best loss 147.4871063232422
not here
quantized in 34.44381093978882 seconds
36424 MiB free out of 48676 MiB total
5 self_attn.k_proj
Pruning ...
256
iter 0, train loss 179.4807891845703, val loss None
iter 10, train loss 182.75881958007812, val loss None
iter 20, train loss 185.4356231689453, val loss None
iter 30, train loss 180.41375732421875, val loss None
iter 40, train loss 177.95242309570312, val loss None
iter 50, train loss 177.51315307617188, val loss None
iter 60, train loss 177.65798950195312, val loss None
iter 70, train loss 177.14505004882812, val loss None
iter 80, train loss 177.05030822753906, val loss None
iter 90, train loss 176.8050994873047, val loss None
best loss 169.95211791992188
not here
quantized in 33.409629583358765 seconds
36414 MiB free out of 48676 MiB total
5 self_attn.v_proj
Pruning ...
256
iter 0, train loss 40.856651306152344, val loss None
iter 10, train loss 40.93919372558594, val loss None
iter 20, train loss 40.71692657470703, val loss None
iter 30, train loss 40.637962341308594, val loss None
iter 40, train loss 40.51630783081055, val loss None
iter 50, train loss 40.454044342041016, val loss None
iter 60, train loss 40.433631896972656, val loss None
iter 70, train loss 40.417232513427734, val loss None
iter 80, train loss 40.4112548828125, val loss None
iter 90, train loss 40.32603454589844, val loss None
best loss 40.32603454589844
not here
quantized in 31.583637237548828 seconds
36372 MiB free out of 48676 MiB total
5 self_attn.o_proj
Pruning ...
256
iter 0, train loss 0.8514696359634399, val loss None
iter 10, train loss 0.8465127944946289, val loss None
iter 20, train loss 0.8355289101600647, val loss None
iter 30, train loss 0.8335878252983093, val loss None
iter 40, train loss 0.8312695026397705, val loss None
iter 50, train loss 0.8266636729240417, val loss None
iter 60, train loss 0.8267617225646973, val loss None
iter 70, train loss 0.824463963508606, val loss None
iter 80, train loss 0.8221241235733032, val loss None
iter 90, train loss 0.8214583396911621, val loss None
best loss 0.8195038437843323
not here
quantized in 31.615469217300415 seconds
36308 MiB free out of 48676 MiB total
5 mlp.gate_proj
Pruning ...
256
iter 0, train loss 99.01100158691406, val loss None
iter 10, train loss 101.31204223632812, val loss None
iter 20, train loss 100.35995483398438, val loss None
iter 30, train loss 100.24646759033203, val loss None
iter 40, train loss 100.24256896972656, val loss None
iter 50, train loss 100.18551635742188, val loss None
iter 60, train loss 100.07144165039062, val loss None
iter 70, train loss 100.04634094238281, val loss None
iter 80, train loss 99.998779296875, val loss None
iter 90, train loss 100.00638580322266, val loss None
best loss 98.45313262939453
not here
quantized in 84.8215229511261 seconds
35920 MiB free out of 48676 MiB total
5 mlp.up_proj
Pruning ...
256
iter 0, train loss 77.06317138671875, val loss None
iter 10, train loss 77.24726867675781, val loss None
iter 20, train loss 77.33114624023438, val loss None
iter 30, train loss 77.29470825195312, val loss None
iter 40, train loss 77.28767395019531, val loss None
iter 50, train loss 77.27711486816406, val loss None
iter 60, train loss 77.27977752685547, val loss None
iter 70, train loss 77.23416137695312, val loss None
iter 80, train loss 77.22860717773438, val loss None
iter 90, train loss 77.23123931884766, val loss None
best loss 77.06317138671875
not here
quantized in 84.12226462364197 seconds
35640 MiB free out of 48676 MiB total
5 mlp.down_proj
Pruning ...
256
iter 0, train loss 0.8587191700935364, val loss None
iter 10, train loss 0.8610712289810181, val loss None
iter 20, train loss 0.8578512668609619, val loss None
iter 30, train loss 0.8566398620605469, val loss None
iter 40, train loss 0.8554424047470093, val loss None
iter 50, train loss 0.8542071580886841, val loss None
iter 60, train loss 0.8546764850616455, val loss None
iter 70, train loss 0.8547787070274353, val loss None
iter 80, train loss 0.8537999391555786, val loss None
iter 90, train loss 0.853652834892273, val loss None
best loss 0.8533834218978882
not here
quantized in 88.0436499118805 seconds
35360 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35360 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31264 MiB free out of 48676 MiB total
after cast to cpu
38316 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
6 self_attn.q_proj
Pruning ...
256
iter 0, train loss 227.3242950439453, val loss None
iter 10, train loss 240.26199340820312, val loss None
iter 20, train loss 239.65274047851562, val loss None
iter 30, train loss 234.73724365234375, val loss None
iter 40, train loss 231.55557250976562, val loss None
iter 50, train loss 230.66464233398438, val loss None
iter 60, train loss 230.24403381347656, val loss None
iter 70, train loss 229.38912963867188, val loss None
iter 80, train loss 228.65960693359375, val loss None
iter 90, train loss 228.69766235351562, val loss None
best loss 212.03768920898438
not here
quantized in 34.19038724899292 seconds
36392 MiB free out of 48676 MiB total
6 self_attn.k_proj
Pruning ...
256
iter 0, train loss 251.39476013183594, val loss None
iter 10, train loss 254.25863647460938, val loss None
iter 20, train loss 256.4317626953125, val loss None
iter 30, train loss 250.3749542236328, val loss None
iter 40, train loss 249.5502471923828, val loss None
iter 50, train loss 247.64962768554688, val loss None
iter 60, train loss 246.32591247558594, val loss None
iter 70, train loss 245.7530059814453, val loss None
iter 80, train loss 245.5465545654297, val loss None
iter 90, train loss 245.35630798339844, val loss None
best loss 227.23336791992188
not here
quantized in 33.42474341392517 seconds
36350 MiB free out of 48676 MiB total
6 self_attn.v_proj
Pruning ...
256
iter 0, train loss 58.309566497802734, val loss None
iter 10, train loss 58.65383529663086, val loss None
iter 20, train loss 58.25863265991211, val loss None
iter 30, train loss 58.07404327392578, val loss None
iter 40, train loss 58.0368537902832, val loss None
iter 50, train loss 57.91162872314453, val loss None
iter 60, train loss 57.94987487792969, val loss None
iter 70, train loss 57.887699127197266, val loss None
iter 80, train loss 57.76502227783203, val loss None
iter 90, train loss 57.71744918823242, val loss None
best loss 57.702911376953125
not here
quantized in 31.633233308792114 seconds
36372 MiB free out of 48676 MiB total
6 self_attn.o_proj
Pruning ...
256
iter 0, train loss 1.7723191976547241, val loss None
iter 10, train loss 1.6278396844863892, val loss None
iter 20, train loss 1.595978856086731, val loss None
iter 30, train loss 1.5615906715393066, val loss None
iter 40, train loss 1.5508670806884766, val loss None
iter 50, train loss 1.5379602909088135, val loss None
iter 60, train loss 1.5296616554260254, val loss None
iter 70, train loss 1.5210630893707275, val loss None
iter 80, train loss 1.5183560848236084, val loss None
iter 90, train loss 1.5125551223754883, val loss None
best loss 1.510138750076294
not here
quantized in 31.457706212997437 seconds
36372 MiB free out of 48676 MiB total
6 mlp.gate_proj
Pruning ...
256
iter 0, train loss 124.15156555175781, val loss None
iter 10, train loss 129.25595092773438, val loss None
iter 20, train loss 126.81168365478516, val loss None
iter 30, train loss 126.58024597167969, val loss None
iter 40, train loss 126.15327453613281, val loss None
iter 50, train loss 126.08561706542969, val loss None
iter 60, train loss 126.15098571777344, val loss None
iter 70, train loss 126.16761779785156, val loss None
iter 80, train loss 126.15055847167969, val loss None
iter 90, train loss 126.13067626953125, val loss None
best loss 122.96552276611328
not here
quantized in 85.10853695869446 seconds
35984 MiB free out of 48676 MiB total
6 mlp.up_proj
Pruning ...
256
iter 0, train loss 91.9324951171875, val loss None
iter 10, train loss 92.08028411865234, val loss None
iter 20, train loss 92.23472595214844, val loss None
iter 30, train loss 92.1444091796875, val loss None
iter 40, train loss 92.02401733398438, val loss None
iter 50, train loss 92.07868957519531, val loss None
iter 60, train loss 92.05224609375, val loss None
iter 70, train loss 92.04895782470703, val loss None
iter 80, train loss 92.05375671386719, val loss None
iter 90, train loss 92.04725646972656, val loss None
best loss 91.9324951171875
not here
quantized in 83.31117987632751 seconds
35704 MiB free out of 48676 MiB total
6 mlp.down_proj
Pruning ...
256
iter 0, train loss 1.307936191558838, val loss None
iter 10, train loss 1.2997806072235107, val loss None
iter 20, train loss 1.29118812084198, val loss None
iter 30, train loss 1.2843101024627686, val loss None
iter 40, train loss 1.2777069807052612, val loss None
iter 50, train loss 1.274322509765625, val loss None
iter 60, train loss 1.2751998901367188, val loss None
iter 70, train loss 1.2724158763885498, val loss None
iter 80, train loss 1.2712733745574951, val loss None
iter 90, train loss 1.2718027830123901, val loss None
best loss 1.2697021961212158
not here
quantized in 88.63082218170166 seconds
35596 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35596 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31500 MiB free out of 48676 MiB total
after cast to cpu
38316 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
7 self_attn.q_proj
Pruning ...
256
iter 0, train loss 247.66612243652344, val loss None
iter 10, train loss 263.5560302734375, val loss None
iter 20, train loss 260.90057373046875, val loss None
iter 30, train loss 254.7598114013672, val loss None
iter 40, train loss 252.70904541015625, val loss None
iter 50, train loss 252.20980834960938, val loss None
iter 60, train loss 251.70953369140625, val loss None
iter 70, train loss 251.237060546875, val loss None
iter 80, train loss 250.89645385742188, val loss None
iter 90, train loss 250.6876983642578, val loss None
best loss 229.5035400390625
not here
quantized in 34.40996837615967 seconds
36392 MiB free out of 48676 MiB total
7 self_attn.k_proj
Pruning ...
256
iter 0, train loss 259.3809814453125, val loss None
iter 10, train loss 268.70947265625, val loss None
iter 20, train loss 271.2238464355469, val loss None
iter 30, train loss 265.1650390625, val loss None
iter 40, train loss 261.45947265625, val loss None
iter 50, train loss 259.8099670410156, val loss None
iter 60, train loss 259.2908630371094, val loss None
iter 70, train loss 258.261962890625, val loss None
iter 80, train loss 257.5028381347656, val loss None
iter 90, train loss 257.2508544921875, val loss None
best loss 237.4252166748047
not here
quantized in 33.04654598236084 seconds
36414 MiB free out of 48676 MiB total
7 self_attn.v_proj
Pruning ...
256
iter 0, train loss 65.17512512207031, val loss None
iter 10, train loss 65.57012176513672, val loss None
iter 20, train loss 65.08628845214844, val loss None
iter 30, train loss 64.93022918701172, val loss None
iter 40, train loss 64.75078582763672, val loss None
iter 50, train loss 64.55435180664062, val loss None
iter 60, train loss 64.59172821044922, val loss None
iter 70, train loss 64.3909683227539, val loss None
iter 80, train loss 64.43804931640625, val loss None
iter 90, train loss 64.44808197021484, val loss None
best loss 64.3447265625
not here
quantized in 31.74982786178589 seconds
36372 MiB free out of 48676 MiB total
7 self_attn.o_proj
Pruning ...
256
iter 0, train loss 2.4993715286254883, val loss None
iter 10, train loss 2.389350652694702, val loss None
iter 20, train loss 2.365412473678589, val loss None
iter 30, train loss 2.3542912006378174, val loss None
iter 40, train loss 2.3428127765655518, val loss None
iter 50, train loss 2.3344497680664062, val loss None
iter 60, train loss 2.329540491104126, val loss None
iter 70, train loss 2.331770420074463, val loss None
iter 80, train loss 2.317260503768921, val loss None
iter 90, train loss 2.30859112739563, val loss None
best loss 2.306985378265381
not here
quantized in 31.454893112182617 seconds
36308 MiB free out of 48676 MiB total
7 mlp.gate_proj
Pruning ...
256
iter 0, train loss 142.29278564453125, val loss None
iter 10, train loss 148.93212890625, val loss None
iter 20, train loss 146.20077514648438, val loss None
iter 30, train loss 146.3705291748047, val loss None
iter 40, train loss 145.66900634765625, val loss None
iter 50, train loss 145.31460571289062, val loss None
iter 60, train loss 145.14808654785156, val loss None
iter 70, train loss 145.05648803710938, val loss None
iter 80, train loss 144.97657775878906, val loss None
iter 90, train loss 144.8304901123047, val loss None
best loss 140.73760986328125
not here
quantized in 85.09336376190186 seconds
35920 MiB free out of 48676 MiB total
7 mlp.up_proj
Pruning ...
256
iter 0, train loss 106.34905242919922, val loss None
iter 10, train loss 106.64762878417969, val loss None
iter 20, train loss 106.6978759765625, val loss None
iter 30, train loss 106.52462768554688, val loss None
iter 40, train loss 106.513671875, val loss None
iter 50, train loss 106.45049285888672, val loss None
iter 60, train loss 106.34684753417969, val loss None
iter 70, train loss 106.35591888427734, val loss None
iter 80, train loss 106.34147644042969, val loss None
iter 90, train loss 106.36906433105469, val loss None
best loss 106.34147644042969
not here
quantized in 83.34414196014404 seconds
35640 MiB free out of 48676 MiB total
7 mlp.down_proj
Pruning ...
256
iter 0, train loss 1.7702804803848267, val loss None
iter 10, train loss 1.7705402374267578, val loss None
iter 20, train loss 1.7628912925720215, val loss None
iter 30, train loss 1.7611891031265259, val loss None
iter 40, train loss 1.7637746334075928, val loss None
iter 50, train loss 1.7616251707077026, val loss None
iter 60, train loss 1.7597935199737549, val loss None
iter 70, train loss 1.759207010269165, val loss None
iter 80, train loss 1.7575874328613281, val loss None
iter 90, train loss 1.7560992240905762, val loss None
best loss 1.7548623085021973
not here
quantized in 88.11039996147156 seconds
35360 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35360 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31264 MiB free out of 48676 MiB total
after cast to cpu
38316 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
8 self_attn.q_proj
Pruning ...
256
iter 0, train loss 239.58583068847656, val loss None
iter 10, train loss 250.6124267578125, val loss None
iter 20, train loss 246.43482971191406, val loss None
iter 30, train loss 243.22122192382812, val loss None
iter 40, train loss 241.2755126953125, val loss None
iter 50, train loss 240.87635803222656, val loss None
iter 60, train loss 240.63121032714844, val loss None
iter 70, train loss 240.40155029296875, val loss None
iter 80, train loss 240.28787231445312, val loss None
iter 90, train loss 240.05111694335938, val loss None
best loss 225.84344482421875
not here
quantized in 33.9104163646698 seconds
36392 MiB free out of 48676 MiB total
8 self_attn.k_proj
Pruning ...
256
iter 0, train loss 249.51785278320312, val loss None
iter 10, train loss 256.1458435058594, val loss None
iter 20, train loss 260.0130615234375, val loss None
iter 30, train loss 254.87803649902344, val loss None
iter 40, train loss 252.72811889648438, val loss None
iter 50, train loss 250.7745819091797, val loss None
iter 60, train loss 249.94461059570312, val loss None
iter 70, train loss 249.6554412841797, val loss None
iter 80, train loss 249.40158081054688, val loss None
iter 90, train loss 249.22076416015625, val loss None
best loss 237.55349731445312
not here
quantized in 33.14540982246399 seconds
36350 MiB free out of 48676 MiB total
8 self_attn.v_proj
Pruning ...
256
iter 0, train loss 67.2261962890625, val loss None
iter 10, train loss 68.10111999511719, val loss None
iter 20, train loss 67.27584838867188, val loss None
iter 30, train loss 67.09367370605469, val loss None
iter 40, train loss 66.86921691894531, val loss None
iter 50, train loss 66.77983856201172, val loss None
iter 60, train loss 66.6821517944336, val loss None
iter 70, train loss 66.46305084228516, val loss None
iter 80, train loss 66.50839233398438, val loss None
iter 90, train loss 66.49362182617188, val loss None
best loss 66.45883178710938
not here
quantized in 31.85789728164673 seconds
36372 MiB free out of 48676 MiB total
8 self_attn.o_proj
Pruning ...
256
iter 0, train loss 4.3166375160217285, val loss None
iter 10, train loss 4.03630256652832, val loss None
iter 20, train loss 3.9065842628479004, val loss None
iter 30, train loss 3.808816909790039, val loss None
iter 40, train loss 3.7379794120788574, val loss None
iter 50, train loss 3.653243064880371, val loss None
iter 60, train loss 3.632129669189453, val loss None
iter 70, train loss 3.611987590789795, val loss None
iter 80, train loss 3.6016693115234375, val loss None
iter 90, train loss 3.586024284362793, val loss None
best loss 3.5728707313537598
not here
quantized in 31.187469244003296 seconds
36308 MiB free out of 48676 MiB total
8 mlp.gate_proj
Pruning ...
256
iter 0, train loss 148.33493041992188, val loss None
iter 10, train loss 154.78163146972656, val loss None
iter 20, train loss 151.63819885253906, val loss None
iter 30, train loss 151.45863342285156, val loss None
iter 40, train loss 150.90208435058594, val loss None
iter 50, train loss 150.34716796875, val loss None
iter 60, train loss 150.2481689453125, val loss None
iter 70, train loss 150.23321533203125, val loss None
iter 80, train loss 150.138671875, val loss None
iter 90, train loss 150.18978881835938, val loss None
best loss 147.13876342773438
not here
quantized in 84.4622540473938 seconds
36006 MiB free out of 48676 MiB total
8 mlp.up_proj
Pruning ...
256
iter 0, train loss 118.00161743164062, val loss None
iter 10, train loss 118.69252014160156, val loss None
iter 20, train loss 118.59840393066406, val loss None
iter 30, train loss 118.28828430175781, val loss None
iter 40, train loss 118.30782318115234, val loss None
iter 50, train loss 118.23332977294922, val loss None
iter 60, train loss 118.22161102294922, val loss None
iter 70, train loss 118.1141586303711, val loss None
iter 80, train loss 118.09325408935547, val loss None
iter 90, train loss 118.0878677368164, val loss None
best loss 118.00121307373047
not here
quantized in 83.74619841575623 seconds
35812 MiB free out of 48676 MiB total
8 mlp.down_proj
Pruning ...
256
iter 0, train loss 2.2022945880889893, val loss None
iter 10, train loss 2.2043137550354004, val loss None
iter 20, train loss 2.1961426734924316, val loss None
iter 30, train loss 2.196342706680298, val loss None
iter 40, train loss 2.1903393268585205, val loss None
iter 50, train loss 2.186901569366455, val loss None
iter 60, train loss 2.181702136993408, val loss None
iter 70, train loss 2.1796507835388184, val loss None
iter 80, train loss 2.1755802631378174, val loss None
iter 90, train loss 2.1750919818878174, val loss None
best loss 2.1724562644958496
not here
quantized in 88.25452327728271 seconds
35532 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35532 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31436 MiB free out of 48676 MiB total
after cast to cpu
38316 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
9 self_attn.q_proj
Pruning ...
256
iter 0, train loss 247.7323455810547, val loss None
iter 10, train loss 262.54718017578125, val loss None
iter 20, train loss 253.82989501953125, val loss None
iter 30, train loss 250.35183715820312, val loss None
iter 40, train loss 249.23135375976562, val loss None
iter 50, train loss 249.0919952392578, val loss None
iter 60, train loss 248.36978149414062, val loss None
iter 70, train loss 247.94834899902344, val loss None
iter 80, train loss 247.5814208984375, val loss None
iter 90, train loss 247.472900390625, val loss None
best loss 235.99612426757812
not here
quantized in 33.81963014602661 seconds
36392 MiB free out of 48676 MiB total
9 self_attn.k_proj
Pruning ...
256
iter 0, train loss 271.36724853515625, val loss None
iter 10, train loss 280.55010986328125, val loss None
iter 20, train loss 284.0389404296875, val loss None
iter 30, train loss 274.8285827636719, val loss None
iter 40, train loss 272.3030700683594, val loss None
iter 50, train loss 270.61224365234375, val loss None
iter 60, train loss 270.2779846191406, val loss None
iter 70, train loss 269.8765869140625, val loss None
iter 80, train loss 269.72052001953125, val loss None
iter 90, train loss 269.543212890625, val loss None
best loss 258.2686767578125
not here
quantized in 33.17463135719299 seconds
36414 MiB free out of 48676 MiB total
9 self_attn.v_proj
Pruning ...
256
iter 0, train loss 72.84590148925781, val loss None
iter 10, train loss 73.48674774169922, val loss None
iter 20, train loss 72.73628997802734, val loss None
iter 30, train loss 72.56935119628906, val loss None
iter 40, train loss 72.47735595703125, val loss None
iter 50, train loss 72.3699722290039, val loss None
iter 60, train loss 72.33428192138672, val loss None
iter 70, train loss 72.2987289428711, val loss None
iter 80, train loss 72.15863037109375, val loss None
iter 90, train loss 72.13331604003906, val loss None
best loss 72.05130767822266
not here
quantized in 31.585877895355225 seconds
36404 MiB free out of 48676 MiB total
9 self_attn.o_proj
Pruning ...
256
iter 0, train loss 5.052814483642578, val loss None
iter 10, train loss 4.833791732788086, val loss None
iter 20, train loss 4.786388874053955, val loss None
iter 30, train loss 4.7409257888793945, val loss None
iter 40, train loss 4.734066486358643, val loss None
iter 50, train loss 4.714804649353027, val loss None
iter 60, train loss 4.690428733825684, val loss None
iter 70, train loss 4.668654918670654, val loss None
iter 80, train loss 4.659396171569824, val loss None
iter 90, train loss 4.650572776794434, val loss None
best loss 4.639586925506592
not here
quantized in 31.196747541427612 seconds
36340 MiB free out of 48676 MiB total
9 mlp.gate_proj
Pruning ...
256
iter 0, train loss 154.23406982421875, val loss None
iter 10, train loss 161.29881286621094, val loss None
iter 20, train loss 156.69287109375, val loss None
iter 30, train loss 156.84722900390625, val loss None
iter 40, train loss 156.18624877929688, val loss None
iter 50, train loss 155.94947814941406, val loss None
iter 60, train loss 155.70843505859375, val loss None
iter 70, train loss 155.65118408203125, val loss None
iter 80, train loss 155.6173095703125, val loss None
iter 90, train loss 155.61148071289062, val loss None
best loss 152.3134002685547
not here
quantized in 84.35796356201172 seconds
35952 MiB free out of 48676 MiB total
9 mlp.up_proj
Pruning ...
256
iter 0, train loss 127.09430694580078, val loss None
iter 10, train loss 127.89218139648438, val loss None
iter 20, train loss 127.90873718261719, val loss None
iter 30, train loss 127.7232894897461, val loss None
iter 40, train loss 127.5870361328125, val loss None
iter 50, train loss 127.67656707763672, val loss None
iter 60, train loss 127.71546936035156, val loss None
iter 70, train loss 127.68843841552734, val loss None
iter 80, train loss 127.69828033447266, val loss None
iter 90, train loss 127.67562866210938, val loss None
best loss 126.94438934326172
not here
quantized in 84.41980957984924 seconds
35672 MiB free out of 48676 MiB total
9 mlp.down_proj
Pruning ...
256
iter 0, train loss 2.651984214782715, val loss None
iter 10, train loss 2.6465976238250732, val loss None
iter 20, train loss 2.641106367111206, val loss None
iter 30, train loss 2.6391403675079346, val loss None
iter 40, train loss 2.6325602531433105, val loss None
iter 50, train loss 2.6271560192108154, val loss None
iter 60, train loss 2.620365619659424, val loss None
iter 70, train loss 2.619328260421753, val loss None
iter 80, train loss 2.6185150146484375, val loss None
iter 90, train loss 2.6189651489257812, val loss None
best loss 2.6179616451263428
not here
quantized in 88.13093900680542 seconds
35392 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35392 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31296 MiB free out of 48676 MiB total
after cast to cpu
38316 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
10 self_attn.q_proj
Pruning ...
256
iter 0, train loss 252.91595458984375, val loss None
iter 10, train loss 265.38592529296875, val loss None
iter 20, train loss 258.23956298828125, val loss None
iter 30, train loss 254.33251953125, val loss None
iter 40, train loss 252.57322692871094, val loss None
iter 50, train loss 251.85794067382812, val loss None
iter 60, train loss 251.67127990722656, val loss None
iter 70, train loss 251.5135498046875, val loss None
iter 80, train loss 251.676025390625, val loss None
iter 90, train loss 251.52630615234375, val loss None
best loss 241.2703094482422
not here
quantized in 34.280455589294434 seconds
36392 MiB free out of 48676 MiB total
10 self_attn.k_proj
Pruning ...
256
iter 0, train loss 279.0687561035156, val loss None
iter 10, train loss 294.9280700683594, val loss None
iter 20, train loss 293.95843505859375, val loss None
iter 30, train loss 286.96734619140625, val loss None
iter 40, train loss 282.81231689453125, val loss None
iter 50, train loss 282.1671142578125, val loss None
iter 60, train loss 280.9770812988281, val loss None
iter 70, train loss 281.2882080078125, val loss None
iter 80, train loss 281.54144287109375, val loss None
iter 90, train loss 281.37738037109375, val loss None
best loss 267.88818359375
not here
quantized in 32.92441654205322 seconds
36350 MiB free out of 48676 MiB total
10 self_attn.v_proj
Pruning ...
256
iter 0, train loss 73.11982727050781, val loss None
iter 10, train loss 73.56654357910156, val loss None
iter 20, train loss 73.13006591796875, val loss None
iter 30, train loss 72.79732513427734, val loss None
iter 40, train loss 72.77298736572266, val loss None
iter 50, train loss 72.76182556152344, val loss None
iter 60, train loss 72.65483856201172, val loss None
iter 70, train loss 72.58232879638672, val loss None
iter 80, train loss 72.63148498535156, val loss None
iter 90, train loss 72.6106185913086, val loss None
best loss 72.5479507446289
not here
quantized in 31.696973085403442 seconds
36372 MiB free out of 48676 MiB total
10 self_attn.o_proj
Pruning ...
256
iter 0, train loss 7.838111877441406, val loss None
iter 10, train loss 7.329972743988037, val loss None
iter 20, train loss 7.059642791748047, val loss None
iter 30, train loss 6.803374290466309, val loss None
iter 40, train loss 6.624670505523682, val loss None
iter 50, train loss 6.575424671173096, val loss None
iter 60, train loss 6.539286136627197, val loss None
iter 70, train loss 6.504560470581055, val loss None
iter 80, train loss 6.4902729988098145, val loss None
iter 90, train loss 6.438442230224609, val loss None
best loss 6.433738708496094
not here
quantized in 31.17755627632141 seconds
36308 MiB free out of 48676 MiB total
10 mlp.gate_proj
Pruning ...
256
iter 0, train loss 160.7566680908203, val loss None
iter 10, train loss 168.21261596679688, val loss None
iter 20, train loss 165.3714599609375, val loss None
iter 30, train loss 165.088134765625, val loss None
iter 40, train loss 164.24850463867188, val loss None
iter 50, train loss 163.8834991455078, val loss None
iter 60, train loss 163.58180236816406, val loss None
iter 70, train loss 163.44906616210938, val loss None
iter 80, train loss 163.44509887695312, val loss None
iter 90, train loss 163.41506958007812, val loss None
best loss 159.22073364257812
not here
quantized in 85.13209700584412 seconds
36006 MiB free out of 48676 MiB total
10 mlp.up_proj
Pruning ...
256
iter 0, train loss 135.8369598388672, val loss None
iter 10, train loss 136.8093719482422, val loss None
iter 20, train loss 136.8173828125, val loss None
iter 30, train loss 136.44818115234375, val loss None
iter 40, train loss 136.255615234375, val loss None
iter 50, train loss 136.14898681640625, val loss None
iter 60, train loss 136.10887145996094, val loss None
iter 70, train loss 136.05438232421875, val loss None
iter 80, train loss 136.069091796875, val loss None
iter 90, train loss 135.99676513671875, val loss None
best loss 135.45677185058594
not here
quantized in 83.97434306144714 seconds
35812 MiB free out of 48676 MiB total
10 mlp.down_proj
Pruning ...
256
iter 0, train loss 3.1736321449279785, val loss None
iter 10, train loss 3.1655328273773193, val loss None
iter 20, train loss 3.1309022903442383, val loss None
iter 30, train loss 3.1112935543060303, val loss None
iter 40, train loss 3.094322919845581, val loss None
iter 50, train loss 3.0960793495178223, val loss None
iter 60, train loss 3.090912103652954, val loss None
iter 70, train loss 3.0880374908447266, val loss None
iter 80, train loss 3.0779342651367188, val loss None
iter 90, train loss 3.072171211242676, val loss None
best loss 3.068561553955078
not here
quantized in 89.52839493751526 seconds
35532 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35532 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31436 MiB free out of 48676 MiB total
after cast to cpu
38316 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
11 self_attn.q_proj
Pruning ...
256
iter 0, train loss 289.64129638671875, val loss None
iter 10, train loss 307.3116455078125, val loss None
iter 20, train loss 302.74505615234375, val loss None
iter 30, train loss 298.855712890625, val loss None
iter 40, train loss 295.5408630371094, val loss None
iter 50, train loss 294.4111022949219, val loss None
iter 60, train loss 293.05145263671875, val loss None
iter 70, train loss 292.4547119140625, val loss None
iter 80, train loss 291.7750549316406, val loss None
iter 90, train loss 291.45556640625, val loss None
best loss 273.18035888671875
not here
quantized in 34.29299879074097 seconds
36392 MiB free out of 48676 MiB total
11 self_attn.k_proj
Pruning ...
256
iter 0, train loss 294.18328857421875, val loss None
iter 10, train loss 308.6998291015625, val loss None
iter 20, train loss 309.94110107421875, val loss None
iter 30, train loss 303.2602844238281, val loss None
iter 40, train loss 300.9148254394531, val loss None
iter 50, train loss 299.5574951171875, val loss None
iter 60, train loss 298.6275634765625, val loss None
iter 70, train loss 298.42498779296875, val loss None
iter 80, train loss 298.018310546875, val loss None
iter 90, train loss 297.500732421875, val loss None
best loss 278.237548828125
not here
quantized in 33.328192710876465 seconds
36414 MiB free out of 48676 MiB total
11 self_attn.v_proj
Pruning ...
256
iter 0, train loss 99.40479278564453, val loss None
iter 10, train loss 99.74076843261719, val loss None
iter 20, train loss 99.14663696289062, val loss None
iter 30, train loss 98.6866226196289, val loss None
iter 40, train loss 98.41612243652344, val loss None
iter 50, train loss 98.33428192138672, val loss None
iter 60, train loss 98.240478515625, val loss None
iter 70, train loss 98.1846923828125, val loss None
iter 80, train loss 98.16787719726562, val loss None
iter 90, train loss 98.15091705322266, val loss None
best loss 98.09227752685547
not here
quantized in 31.71517300605774 seconds
36436 MiB free out of 48676 MiB total
11 self_attn.o_proj
Pruning ...
256
iter 0, train loss 8.62965202331543, val loss None
iter 10, train loss 8.583014488220215, val loss None
iter 20, train loss 8.530324935913086, val loss None
iter 30, train loss 8.477981567382812, val loss None
iter 40, train loss 8.456428527832031, val loss None
iter 50, train loss 8.425895690917969, val loss None
iter 60, train loss 8.485846519470215, val loss None
iter 70, train loss 8.429360389709473, val loss None
iter 80, train loss 8.392529487609863, val loss None
iter 90, train loss 8.393579483032227, val loss None
best loss 8.36863899230957
not here
quantized in 30.790587186813354 seconds
36372 MiB free out of 48676 MiB total
11 mlp.gate_proj
Pruning ...
256
iter 0, train loss 174.1470489501953, val loss None
iter 10, train loss 183.05117797851562, val loss None
iter 20, train loss 180.1298370361328, val loss None
iter 30, train loss 179.3858184814453, val loss None
iter 40, train loss 178.11294555664062, val loss None
iter 50, train loss 177.8515625, val loss None
iter 60, train loss 177.53952026367188, val loss None
iter 70, train loss 177.2490234375, val loss None
iter 80, train loss 177.0365753173828, val loss None
iter 90, train loss 176.9203643798828, val loss None
best loss 172.1595916748047
not here
quantized in 84.92021155357361 seconds
35984 MiB free out of 48676 MiB total
11 mlp.up_proj
Pruning ...
256
iter 0, train loss 150.85311889648438, val loss None
iter 10, train loss 151.99417114257812, val loss None
iter 20, train loss 152.26742553710938, val loss None
iter 30, train loss 151.89697265625, val loss None
iter 40, train loss 152.04153442382812, val loss None
iter 50, train loss 151.93817138671875, val loss None
iter 60, train loss 151.808349609375, val loss None
iter 70, train loss 151.7252197265625, val loss None
iter 80, train loss 151.70167541503906, val loss None
iter 90, train loss 151.7248992919922, val loss None
best loss 150.80026245117188
not here
quantized in 83.40597152709961 seconds
35704 MiB free out of 48676 MiB total
11 mlp.down_proj
Pruning ...
256
iter 0, train loss 3.517988681793213, val loss None
iter 10, train loss 3.509150266647339, val loss None
iter 20, train loss 3.4928221702575684, val loss None
iter 30, train loss 3.4737255573272705, val loss None
iter 40, train loss 3.471635341644287, val loss None
iter 50, train loss 3.4658243656158447, val loss None
iter 60, train loss 3.4693808555603027, val loss None
iter 70, train loss 3.4624075889587402, val loss None
iter 80, train loss 3.456815719604492, val loss None
iter 90, train loss 3.4536702632904053, val loss None
best loss 3.447617530822754
not here
quantized in 88.58946013450623 seconds
35596 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35596 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31500 MiB free out of 48676 MiB total
after cast to cpu
38316 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
12 self_attn.q_proj
Pruning ...
256
iter 0, train loss 298.320556640625, val loss None
iter 10, train loss 314.9528503417969, val loss None
iter 20, train loss 306.11737060546875, val loss None
iter 30, train loss 303.2518310546875, val loss None
iter 40, train loss 301.1985168457031, val loss None
iter 50, train loss 300.1950378417969, val loss None
iter 60, train loss 299.4460754394531, val loss None
iter 70, train loss 299.27484130859375, val loss None
iter 80, train loss 298.8341369628906, val loss None
iter 90, train loss 298.4543762207031, val loss None
best loss 282.9736633300781
not here
quantized in 34.332841873168945 seconds
36392 MiB free out of 48676 MiB total
12 self_attn.k_proj
Pruning ...
256
iter 0, train loss 331.8760681152344, val loss None
iter 10, train loss 348.0621032714844, val loss None
iter 20, train loss 347.98284912109375, val loss None
iter 30, train loss 343.0603942871094, val loss None
iter 40, train loss 337.4061279296875, val loss None
iter 50, train loss 335.0487365722656, val loss None
iter 60, train loss 334.25372314453125, val loss None
iter 70, train loss 333.296875, val loss None
iter 80, train loss 332.596435546875, val loss None
iter 90, train loss 332.6889343261719, val loss None
best loss 311.4654235839844
not here
quantized in 32.93180584907532 seconds
36350 MiB free out of 48676 MiB total
12 self_attn.v_proj
Pruning ...
256
iter 0, train loss 97.25718688964844, val loss None
iter 10, train loss 97.72517395019531, val loss None
iter 20, train loss 97.21895599365234, val loss None
iter 30, train loss 96.98194885253906, val loss None
iter 40, train loss 97.00479125976562, val loss None
iter 50, train loss 96.91983032226562, val loss None
iter 60, train loss 96.86640930175781, val loss None
iter 70, train loss 96.81490325927734, val loss None
iter 80, train loss 96.76423645019531, val loss None
iter 90, train loss 96.76204681396484, val loss None
best loss 96.72212982177734
not here
quantized in 31.691542148590088 seconds
36372 MiB free out of 48676 MiB total
12 self_attn.o_proj
Pruning ...
256
iter 0, train loss 9.462912559509277, val loss None
iter 10, train loss 9.316757202148438, val loss None
iter 20, train loss 9.257519721984863, val loss None
iter 30, train loss 9.206720352172852, val loss None
iter 40, train loss 9.137978553771973, val loss None
iter 50, train loss 9.089563369750977, val loss None
iter 60, train loss 9.028213500976562, val loss None
iter 70, train loss 9.043773651123047, val loss None
iter 80, train loss 8.989449501037598, val loss None
iter 90, train loss 8.977903366088867, val loss None
best loss 8.944931030273438
not here
quantized in 31.017756462097168 seconds
36308 MiB free out of 48676 MiB total
12 mlp.gate_proj
Pruning ...
256
iter 0, train loss 184.54714965820312, val loss None
iter 10, train loss 193.57936096191406, val loss None
iter 20, train loss 189.49224853515625, val loss None
iter 30, train loss 188.92770385742188, val loss None
iter 40, train loss 187.99708557128906, val loss None
iter 50, train loss 187.76792907714844, val loss None
iter 60, train loss 187.57411193847656, val loss None
iter 70, train loss 187.46829223632812, val loss None
iter 80, train loss 187.38650512695312, val loss None
iter 90, train loss 187.2140350341797, val loss None
best loss 182.66632080078125
not here
quantized in 84.64547085762024 seconds
35920 MiB free out of 48676 MiB total
12 mlp.up_proj
Pruning ...
256
iter 0, train loss 164.5343017578125, val loss None
iter 10, train loss 165.46902465820312, val loss None
iter 20, train loss 165.6342010498047, val loss None
iter 30, train loss 165.28211975097656, val loss None
iter 40, train loss 165.25634765625, val loss None
iter 50, train loss 165.1560516357422, val loss None
iter 60, train loss 165.1304931640625, val loss None
iter 70, train loss 165.08831787109375, val loss None
iter 80, train loss 165.0323486328125, val loss None
iter 90, train loss 165.08468627929688, val loss None
best loss 164.4619140625
not here
quantized in 83.50458359718323 seconds
35640 MiB free out of 48676 MiB total
12 mlp.down_proj
Pruning ...
256
iter 0, train loss 3.8945326805114746, val loss None
iter 10, train loss 3.914228677749634, val loss None
iter 20, train loss 3.902627468109131, val loss None
iter 30, train loss 3.8988308906555176, val loss None
iter 40, train loss 3.8978817462921143, val loss None
iter 50, train loss 3.898271322250366, val loss None
iter 60, train loss 3.8955562114715576, val loss None
iter 70, train loss 3.895846128463745, val loss None
iter 80, train loss 3.8960964679718018, val loss None
iter 90, train loss 3.8962361812591553, val loss None
best loss 3.894486427307129
not here
quantized in 87.8329107761383 seconds
35360 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35360 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31264 MiB free out of 48676 MiB total
after cast to cpu
38316 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
13 self_attn.q_proj
Pruning ...
256
iter 0, train loss 309.0381164550781, val loss None
iter 10, train loss 323.73931884765625, val loss None
iter 20, train loss 315.1010437011719, val loss None
iter 30, train loss 311.0987243652344, val loss None
iter 40, train loss 309.5902099609375, val loss None
iter 50, train loss 307.907958984375, val loss None
iter 60, train loss 307.14190673828125, val loss None
iter 70, train loss 306.47088623046875, val loss None
iter 80, train loss 306.38800048828125, val loss None
iter 90, train loss 306.2843322753906, val loss None
best loss 290.0023193359375
not here
quantized in 33.92275786399841 seconds
36392 MiB free out of 48676 MiB total
13 self_attn.k_proj
Pruning ...
256
iter 0, train loss 331.3533630371094, val loss None
iter 10, train loss 345.4925537109375, val loss None
iter 20, train loss 347.16412353515625, val loss None
iter 30, train loss 336.79571533203125, val loss None
iter 40, train loss 333.380126953125, val loss None
iter 50, train loss 331.9764404296875, val loss None
iter 60, train loss 330.2327575683594, val loss None
iter 70, train loss 329.0539245605469, val loss None
iter 80, train loss 328.5310363769531, val loss None
iter 90, train loss 328.5162353515625, val loss None
best loss 310.04522705078125
not here
quantized in 33.04943585395813 seconds
36414 MiB free out of 48676 MiB total
13 self_attn.v_proj
Pruning ...
256
iter 0, train loss 107.74919128417969, val loss None
iter 10, train loss 107.80769348144531, val loss None
iter 20, train loss 107.55533599853516, val loss None
iter 30, train loss 107.17097473144531, val loss None
iter 40, train loss 107.1738510131836, val loss None
iter 50, train loss 107.04460144042969, val loss None
iter 60, train loss 106.74339294433594, val loss None
iter 70, train loss 106.81893920898438, val loss None
iter 80, train loss 106.71141052246094, val loss None
iter 90, train loss 106.79371643066406, val loss None
best loss 106.68695068359375
not here
quantized in 31.450892448425293 seconds
36404 MiB free out of 48676 MiB total
13 self_attn.o_proj
Pruning ...
256
iter 0, train loss 8.95034408569336, val loss None
iter 10, train loss 8.700993537902832, val loss None
iter 20, train loss 8.641181945800781, val loss None
iter 30, train loss 8.539131164550781, val loss None
iter 40, train loss 8.473716735839844, val loss None
iter 50, train loss 8.420818328857422, val loss None
iter 60, train loss 8.41820240020752, val loss None
iter 70, train loss 8.371993064880371, val loss None
iter 80, train loss 8.341485023498535, val loss None
iter 90, train loss 8.340387344360352, val loss None
best loss 8.32891845703125
not here
quantized in 31.03785991668701 seconds
36372 MiB free out of 48676 MiB total
13 mlp.gate_proj
Pruning ...
256
iter 0, train loss 196.26002502441406, val loss None
iter 10, train loss 204.78192138671875, val loss None
iter 20, train loss 201.13424682617188, val loss None
iter 30, train loss 200.37600708007812, val loss None
iter 40, train loss 199.8160400390625, val loss None
iter 50, train loss 199.66563415527344, val loss None
iter 60, train loss 199.3792266845703, val loss None
iter 70, train loss 199.2259063720703, val loss None
iter 80, train loss 199.1602783203125, val loss None
iter 90, train loss 199.13064575195312, val loss None
best loss 193.94300842285156
not here
quantized in 85.13425421714783 seconds
35984 MiB free out of 48676 MiB total
13 mlp.up_proj
Pruning ...
256
iter 0, train loss 179.71578979492188, val loss None
iter 10, train loss 180.57827758789062, val loss None
iter 20, train loss 181.0262908935547, val loss None
iter 30, train loss 180.68807983398438, val loss None
iter 40, train loss 180.68588256835938, val loss None
iter 50, train loss 180.48782348632812, val loss None
iter 60, train loss 180.36074829101562, val loss None
iter 70, train loss 180.37863159179688, val loss None
iter 80, train loss 180.32040405273438, val loss None
iter 90, train loss 180.3668975830078, val loss None
best loss 179.52828979492188
not here
quantized in 83.77577877044678 seconds
35704 MiB free out of 48676 MiB total
13 mlp.down_proj
Pruning ...
256
iter 0, train loss 4.805419445037842, val loss None
iter 10, train loss 4.79272985458374, val loss None
iter 20, train loss 4.74436092376709, val loss None
iter 30, train loss 4.71890926361084, val loss None
iter 40, train loss 4.690912246704102, val loss None
iter 50, train loss 4.668284893035889, val loss None
iter 60, train loss 4.655484199523926, val loss None
iter 70, train loss 4.644978046417236, val loss None
iter 80, train loss 4.646762847900391, val loss None
iter 90, train loss 4.637228965759277, val loss None
best loss 4.628342628479004
not here
quantized in 88.63771986961365 seconds
35424 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35424 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31328 MiB free out of 48676 MiB total
after cast to cpu
38316 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
14 self_attn.q_proj
Pruning ...
256
iter 0, train loss 323.0922546386719, val loss None
iter 10, train loss 343.6871337890625, val loss None
iter 20, train loss 332.7177734375, val loss None
iter 30, train loss 326.7496337890625, val loss None
iter 40, train loss 324.177978515625, val loss None
iter 50, train loss 322.7576904296875, val loss None
iter 60, train loss 321.461181640625, val loss None
iter 70, train loss 321.15142822265625, val loss None
iter 80, train loss 320.81549072265625, val loss None
iter 90, train loss 320.55950927734375, val loss None
best loss 300.71368408203125
not here
quantized in 34.14171075820923 seconds
36392 MiB free out of 48676 MiB total
14 self_attn.k_proj
Pruning ...
256
iter 0, train loss 353.3996276855469, val loss None
iter 10, train loss 368.5674133300781, val loss None
iter 20, train loss 367.3884582519531, val loss None
iter 30, train loss 358.5946044921875, val loss None
iter 40, train loss 354.77813720703125, val loss None
iter 50, train loss 353.0548095703125, val loss None
iter 60, train loss 352.4801025390625, val loss None
iter 70, train loss 351.9058837890625, val loss None
iter 80, train loss 351.42755126953125, val loss None
iter 90, train loss 350.9671630859375, val loss None
best loss 325.01934814453125
not here
quantized in 33.37521576881409 seconds
36350 MiB free out of 48676 MiB total
14 self_attn.v_proj
Pruning ...
256
iter 0, train loss 108.47671508789062, val loss None
iter 10, train loss 108.10626220703125, val loss None
iter 20, train loss 107.85432434082031, val loss None
iter 30, train loss 107.3967056274414, val loss None
iter 40, train loss 107.34162902832031, val loss None
iter 50, train loss 107.15922546386719, val loss None
iter 60, train loss 107.11741638183594, val loss None
iter 70, train loss 106.90560913085938, val loss None
iter 80, train loss 107.005859375, val loss None
iter 90, train loss 106.92829895019531, val loss None
best loss 106.77813720703125
not here
quantized in 31.65347146987915 seconds
36372 MiB free out of 48676 MiB total
14 self_attn.o_proj
Pruning ...
256
iter 0, train loss 12.200017929077148, val loss None
iter 10, train loss 11.80148696899414, val loss None
iter 20, train loss 11.652324676513672, val loss None
iter 30, train loss 11.543241500854492, val loss None
iter 40, train loss 11.370251655578613, val loss None
iter 50, train loss 11.269512176513672, val loss None
iter 60, train loss 11.167915344238281, val loss None
iter 70, train loss 11.09846305847168, val loss None
iter 80, train loss 11.034024238586426, val loss None
iter 90, train loss 11.002349853515625, val loss None
best loss 10.970887184143066
not here
quantized in 31.071089029312134 seconds
36308 MiB free out of 48676 MiB total
14 mlp.gate_proj
Pruning ...
256
iter 0, train loss 211.7499542236328, val loss None
iter 10, train loss 222.2384490966797, val loss None
iter 20, train loss 216.49867248535156, val loss None
iter 30, train loss 216.17520141601562, val loss None
iter 40, train loss 215.7508544921875, val loss None
iter 50, train loss 215.3061981201172, val loss None
iter 60, train loss 214.96792602539062, val loss None
iter 70, train loss 215.0693359375, val loss None
iter 80, train loss 215.0943603515625, val loss None
iter 90, train loss 214.97833251953125, val loss None
best loss 208.7899932861328
not here
quantized in 84.99021649360657 seconds
36006 MiB free out of 48676 MiB total
14 mlp.up_proj
Pruning ...
256
iter 0, train loss 193.93252563476562, val loss None
iter 10, train loss 194.8438720703125, val loss None
iter 20, train loss 195.30825805664062, val loss None
iter 30, train loss 195.01666259765625, val loss None
iter 40, train loss 195.04754638671875, val loss None
iter 50, train loss 195.03564453125, val loss None
iter 60, train loss 195.02871704101562, val loss None
iter 70, train loss 194.99610900878906, val loss None
iter 80, train loss 194.923583984375, val loss None
iter 90, train loss 194.8468017578125, val loss None
best loss 193.74502563476562
not here
quantized in 84.05581426620483 seconds
35726 MiB free out of 48676 MiB total
14 mlp.down_proj
Pruning ...
256
iter 0, train loss 5.197177410125732, val loss None
iter 10, train loss 5.21204948425293, val loss None
iter 20, train loss 5.1884942054748535, val loss None
iter 30, train loss 5.182791233062744, val loss None
iter 40, train loss 5.178040504455566, val loss None
iter 50, train loss 5.1726393699646, val loss None
iter 60, train loss 5.172036647796631, val loss None
iter 70, train loss 5.168617248535156, val loss None
iter 80, train loss 5.164822578430176, val loss None
iter 90, train loss 5.160460472106934, val loss None
best loss 5.158695220947266
not here
quantized in 87.88529562950134 seconds
35532 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35532 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31436 MiB free out of 48676 MiB total
after cast to cpu
38316 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
15 self_attn.q_proj
Pruning ...
256
iter 0, train loss 304.4560546875, val loss None
iter 10, train loss 318.06597900390625, val loss None
iter 20, train loss 306.6265563964844, val loss None
iter 30, train loss 302.99102783203125, val loss None
iter 40, train loss 302.7456970214844, val loss None
iter 50, train loss 301.43084716796875, val loss None
iter 60, train loss 300.37078857421875, val loss None
iter 70, train loss 299.5350036621094, val loss None
iter 80, train loss 299.05889892578125, val loss None
iter 90, train loss 298.7764892578125, val loss None
best loss 282.39044189453125
not here
quantized in 33.97985124588013 seconds
36392 MiB free out of 48676 MiB total
15 self_attn.k_proj
Pruning ...
256
iter 0, train loss 338.29486083984375, val loss None
iter 10, train loss 353.3585205078125, val loss None
iter 20, train loss 357.366455078125, val loss None
iter 30, train loss 350.5108337402344, val loss None
iter 40, train loss 345.6073303222656, val loss None
iter 50, train loss 343.3242492675781, val loss None
iter 60, train loss 342.2974548339844, val loss None
iter 70, train loss 342.2779235839844, val loss None
iter 80, train loss 342.1890869140625, val loss None
iter 90, train loss 341.5166015625, val loss None
best loss 310.59722900390625
not here
quantized in 33.050315380096436 seconds
36414 MiB free out of 48676 MiB total
15 self_attn.v_proj
Pruning ...
256
iter 0, train loss 112.5970230102539, val loss None
iter 10, train loss 112.5164566040039, val loss None
iter 20, train loss 112.19992065429688, val loss None
iter 30, train loss 111.60026550292969, val loss None
iter 40, train loss 111.79826354980469, val loss None
iter 50, train loss 111.36865234375, val loss None
iter 60, train loss 111.36820983886719, val loss None
iter 70, train loss 111.1859130859375, val loss None
iter 80, train loss 111.03546905517578, val loss None
iter 90, train loss 111.06690216064453, val loss None
best loss 111.0047607421875
not here
quantized in 31.972135543823242 seconds
36404 MiB free out of 48676 MiB total
15 self_attn.o_proj
Pruning ...
256
iter 0, train loss 11.442038536071777, val loss None
iter 10, train loss 11.360502243041992, val loss None
iter 20, train loss 11.259401321411133, val loss None
iter 30, train loss 11.18591022491455, val loss None
iter 40, train loss 11.140284538269043, val loss None
iter 50, train loss 11.078615188598633, val loss None
iter 60, train loss 11.057371139526367, val loss None
iter 70, train loss 11.012040138244629, val loss None
iter 80, train loss 11.012212753295898, val loss None
iter 90, train loss 10.993474006652832, val loss None
best loss 10.97425365447998
not here
quantized in 31.305331468582153 seconds
36340 MiB free out of 48676 MiB total
15 mlp.gate_proj
Pruning ...
256
iter 0, train loss 231.181640625, val loss None
iter 10, train loss 242.92059326171875, val loss None
iter 20, train loss 237.65931701660156, val loss None
iter 30, train loss 238.31320190429688, val loss None
iter 40, train loss 237.90724182128906, val loss None
iter 50, train loss 237.78768920898438, val loss None
iter 60, train loss 237.4967498779297, val loss None
iter 70, train loss 237.3904571533203, val loss None
iter 80, train loss 237.2262420654297, val loss None
iter 90, train loss 237.20584106445312, val loss None
best loss 228.06309509277344
not here
quantized in 85.18393230438232 seconds
35952 MiB free out of 48676 MiB total
15 mlp.up_proj
Pruning ...
256
iter 0, train loss 212.13858032226562, val loss None
iter 10, train loss 214.03817749023438, val loss None
iter 20, train loss 214.03948974609375, val loss None
iter 30, train loss 213.5397186279297, val loss None
iter 40, train loss 213.36732482910156, val loss None
iter 50, train loss 212.99319458007812, val loss None
iter 60, train loss 212.99188232421875, val loss None
iter 70, train loss 213.05418395996094, val loss None
iter 80, train loss 213.02578735351562, val loss None
iter 90, train loss 213.0537109375, val loss None
best loss 212.13858032226562
not here
quantized in 83.28784251213074 seconds
35672 MiB free out of 48676 MiB total
15 mlp.down_proj
Pruning ...
256
iter 0, train loss 6.485691070556641, val loss None
iter 10, train loss 6.486364364624023, val loss None
iter 20, train loss 6.465095520019531, val loss None
iter 30, train loss 6.446889400482178, val loss None
iter 40, train loss 6.428837299346924, val loss None
iter 50, train loss 6.418290138244629, val loss None
iter 60, train loss 6.407940864562988, val loss None
iter 70, train loss 6.404728889465332, val loss None
iter 80, train loss 6.393086910247803, val loss None
iter 90, train loss 6.401798248291016, val loss None
best loss 6.393086910247803
not here
quantized in 88.74122595787048 seconds
35392 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35392 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31296 MiB free out of 48676 MiB total
after cast to cpu
38316 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
16 self_attn.q_proj
Pruning ...
256
iter 0, train loss 318.49017333984375, val loss None
iter 10, train loss 331.16522216796875, val loss None
iter 20, train loss 325.1634826660156, val loss None
iter 30, train loss 319.0744323730469, val loss None
iter 40, train loss 316.20782470703125, val loss None
iter 50, train loss 316.3716735839844, val loss None
iter 60, train loss 316.46868896484375, val loss None
iter 70, train loss 316.24139404296875, val loss None
iter 80, train loss 315.8340148925781, val loss None
iter 90, train loss 315.4849853515625, val loss None
best loss 295.1405334472656
not here
quantized in 34.43979859352112 seconds
36392 MiB free out of 48676 MiB total
16 self_attn.k_proj
Pruning ...
256
iter 0, train loss 356.3509521484375, val loss None
iter 10, train loss 368.5484313964844, val loss None
iter 20, train loss 370.41326904296875, val loss None
iter 30, train loss 364.60345458984375, val loss None
iter 40, train loss 360.66644287109375, val loss None
iter 50, train loss 357.37738037109375, val loss None
iter 60, train loss 356.4488220214844, val loss None
iter 70, train loss 355.8761901855469, val loss None
iter 80, train loss 355.6799011230469, val loss None
iter 90, train loss 355.46282958984375, val loss None
best loss 323.0590515136719
not here
quantized in 33.48607087135315 seconds
36350 MiB free out of 48676 MiB total
16 self_attn.v_proj
Pruning ...
256
iter 0, train loss 128.28749084472656, val loss None
iter 10, train loss 127.3418197631836, val loss None
iter 20, train loss 126.99988555908203, val loss None
iter 30, train loss 126.507568359375, val loss None
iter 40, train loss 126.0587387084961, val loss None
iter 50, train loss 126.13076782226562, val loss None
iter 60, train loss 125.82575225830078, val loss None
iter 70, train loss 125.85005950927734, val loss None
iter 80, train loss 125.7894515991211, val loss None
iter 90, train loss 125.75453186035156, val loss None
best loss 125.7181396484375
not here
quantized in 31.73848056793213 seconds
36372 MiB free out of 48676 MiB total
16 self_attn.o_proj
Pruning ...
256
iter 0, train loss 13.670650482177734, val loss None
iter 10, train loss 13.639856338500977, val loss None
iter 20, train loss 13.520771980285645, val loss None
iter 30, train loss 13.459577560424805, val loss None
iter 40, train loss 13.39615249633789, val loss None
iter 50, train loss 13.381368637084961, val loss None
iter 60, train loss 13.3186674118042, val loss None
iter 70, train loss 13.318551063537598, val loss None
iter 80, train loss 13.251615524291992, val loss None
iter 90, train loss 13.273147583007812, val loss None
best loss 13.249709129333496
not here
quantized in 31.420305490493774 seconds
36308 MiB free out of 48676 MiB total
16 mlp.gate_proj
Pruning ...
256
iter 0, train loss 268.408447265625, val loss None
iter 10, train loss 283.35894775390625, val loss None
iter 20, train loss 275.82373046875, val loss None
iter 30, train loss 276.51593017578125, val loss None
iter 40, train loss 276.171142578125, val loss None
iter 50, train loss 275.77178955078125, val loss None
iter 60, train loss 275.4917297363281, val loss None
iter 70, train loss 275.3160705566406, val loss None
iter 80, train loss 275.1686706542969, val loss None
iter 90, train loss 275.2090148925781, val loss None
best loss 263.122314453125
not here
quantized in 86.87009787559509 seconds
35920 MiB free out of 48676 MiB total
16 mlp.up_proj
Pruning ...
256
iter 0, train loss 241.1488037109375, val loss None
iter 10, train loss 242.64837646484375, val loss None
iter 20, train loss 242.52880859375, val loss None
iter 30, train loss 242.19252014160156, val loss None
iter 40, train loss 242.3515625, val loss None
iter 50, train loss 242.52308654785156, val loss None
iter 60, train loss 242.39161682128906, val loss None
iter 70, train loss 242.23092651367188, val loss None
iter 80, train loss 242.12652587890625, val loss None
iter 90, train loss 242.16909790039062, val loss None
best loss 240.81793212890625
not here
quantized in 84.73640942573547 seconds
35640 MiB free out of 48676 MiB total
16 mlp.down_proj
Pruning ...
256
iter 0, train loss 8.51923942565918, val loss None
iter 10, train loss 8.521364212036133, val loss None
iter 20, train loss 8.496885299682617, val loss None
iter 30, train loss 8.490232467651367, val loss None
iter 40, train loss 8.488561630249023, val loss None
iter 50, train loss 8.472533226013184, val loss None
iter 60, train loss 8.475311279296875, val loss None
iter 70, train loss 8.46833324432373, val loss None
iter 80, train loss 8.46617317199707, val loss None
iter 90, train loss 8.464468955993652, val loss None
best loss 8.457839965820312
not here
quantized in 88.02791595458984 seconds
35360 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35360 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31264 MiB free out of 48676 MiB total
after cast to cpu
38316 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
17 self_attn.q_proj
Pruning ...
256
iter 0, train loss 331.99298095703125, val loss None
iter 10, train loss 352.0954895019531, val loss None
iter 20, train loss 342.62786865234375, val loss None
iter 30, train loss 336.6929626464844, val loss None
iter 40, train loss 334.6585693359375, val loss None
iter 50, train loss 331.6736755371094, val loss None
iter 60, train loss 331.31585693359375, val loss None
iter 70, train loss 331.49285888671875, val loss None
iter 80, train loss 331.5736083984375, val loss None
iter 90, train loss 331.1021728515625, val loss None
best loss 306.7223205566406
not here
quantized in 34.51409077644348 seconds
36392 MiB free out of 48676 MiB total
17 self_attn.k_proj
Pruning ...
256
iter 0, train loss 365.8414306640625, val loss None
iter 10, train loss 381.7161560058594, val loss None
iter 20, train loss 384.1397705078125, val loss None
iter 30, train loss 376.6250305175781, val loss None
iter 40, train loss 371.8164367675781, val loss None
iter 50, train loss 368.92083740234375, val loss None
iter 60, train loss 367.3677978515625, val loss None
iter 70, train loss 366.6269226074219, val loss None
iter 80, train loss 365.9910888671875, val loss None
iter 90, train loss 365.6962890625, val loss None
best loss 331.72344970703125
not here
quantized in 33.50174856185913 seconds
36414 MiB free out of 48676 MiB total
17 self_attn.v_proj
Pruning ...
256
iter 0, train loss 135.86695861816406, val loss None
iter 10, train loss 136.03744506835938, val loss None
iter 20, train loss 135.571533203125, val loss None
iter 30, train loss 135.0964813232422, val loss None
iter 40, train loss 134.94583129882812, val loss None
iter 50, train loss 135.02597045898438, val loss None
iter 60, train loss 134.9051055908203, val loss None
iter 70, train loss 134.79769897460938, val loss None
iter 80, train loss 134.73342895507812, val loss None
iter 90, train loss 134.59317016601562, val loss None
best loss 134.49530029296875
not here
quantized in 31.534329175949097 seconds
36436 MiB free out of 48676 MiB total
17 self_attn.o_proj
Pruning ...
256
iter 0, train loss 10.084373474121094, val loss None
iter 10, train loss 9.663938522338867, val loss None
iter 20, train loss 9.547067642211914, val loss None
iter 30, train loss 9.452500343322754, val loss None
iter 40, train loss 9.365744590759277, val loss None
iter 50, train loss 9.429238319396973, val loss None
iter 60, train loss 9.36164665222168, val loss None
iter 70, train loss 9.385871887207031, val loss None
iter 80, train loss 9.365314483642578, val loss None
iter 90, train loss 9.321659088134766, val loss None
best loss 9.300018310546875
not here
quantized in 31.620657920837402 seconds
36404 MiB free out of 48676 MiB total
17 mlp.gate_proj
Pruning ...
256
iter 0, train loss 303.43438720703125, val loss None
iter 10, train loss 318.54937744140625, val loss None
iter 20, train loss 312.54986572265625, val loss None
iter 30, train loss 313.94561767578125, val loss None
iter 40, train loss 312.93359375, val loss None
iter 50, train loss 312.62078857421875, val loss None
iter 60, train loss 312.66534423828125, val loss None
iter 70, train loss 312.6259765625, val loss None
iter 80, train loss 312.4334716796875, val loss None
iter 90, train loss 312.34881591796875, val loss None
best loss 298.13775634765625
not here
quantized in 85.46316027641296 seconds
36102 MiB free out of 48676 MiB total
17 mlp.up_proj
Pruning ...
256
iter 0, train loss 265.3111877441406, val loss None
iter 10, train loss 266.1746520996094, val loss None
iter 20, train loss 266.7867431640625, val loss None
iter 30, train loss 266.1727600097656, val loss None
iter 40, train loss 266.3645324707031, val loss None
iter 50, train loss 266.25482177734375, val loss None
iter 60, train loss 266.32958984375, val loss None
iter 70, train loss 266.40771484375, val loss None
iter 80, train loss 266.40740966796875, val loss None
iter 90, train loss 266.33013916015625, val loss None
best loss 265.160888671875
not here
quantized in 84.11428427696228 seconds
35822 MiB free out of 48676 MiB total
17 mlp.down_proj
Pruning ...
256
iter 0, train loss 9.283924102783203, val loss None
iter 10, train loss 9.254085540771484, val loss None
iter 20, train loss 9.230716705322266, val loss None
iter 30, train loss 9.19705867767334, val loss None
iter 40, train loss 9.162585258483887, val loss None
iter 50, train loss 9.127567291259766, val loss None
iter 60, train loss 9.124045372009277, val loss None
iter 70, train loss 9.09760570526123, val loss None
iter 80, train loss 9.09583854675293, val loss None
iter 90, train loss 9.08787727355957, val loss None
best loss 9.080916404724121
not here
quantized in 87.78251719474792 seconds
35628 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35628 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31532 MiB free out of 48676 MiB total
after cast to cpu
38316 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
18 self_attn.q_proj
Pruning ...
256
iter 0, train loss 357.6390686035156, val loss None
iter 10, train loss 378.61688232421875, val loss None
iter 20, train loss 365.9425048828125, val loss None
iter 30, train loss 356.396484375, val loss None
iter 40, train loss 354.49896240234375, val loss None
iter 50, train loss 353.0452880859375, val loss None
iter 60, train loss 350.7314758300781, val loss None
iter 70, train loss 350.0737609863281, val loss None
iter 80, train loss 349.977783203125, val loss None
iter 90, train loss 350.1548156738281, val loss None
best loss 327.84075927734375
not here
quantized in 34.49956822395325 seconds
36392 MiB free out of 48676 MiB total
18 self_attn.k_proj
Pruning ...
256
iter 0, train loss 389.1035461425781, val loss None
iter 10, train loss 405.864501953125, val loss None
iter 20, train loss 397.3983459472656, val loss None
iter 30, train loss 391.6263122558594, val loss None
iter 40, train loss 384.45172119140625, val loss None
iter 50, train loss 381.1766662597656, val loss None
iter 60, train loss 380.12017822265625, val loss None
iter 70, train loss 379.08251953125, val loss None
iter 80, train loss 378.5828552246094, val loss None
iter 90, train loss 378.19073486328125, val loss None
best loss 351.33258056640625
not here
quantized in 33.44565153121948 seconds
36350 MiB free out of 48676 MiB total
18 self_attn.v_proj
Pruning ...
256
iter 0, train loss 163.9076690673828, val loss None
iter 10, train loss 163.60475158691406, val loss None
iter 20, train loss 163.1196746826172, val loss None
iter 30, train loss 162.9139862060547, val loss None
iter 40, train loss 162.78973388671875, val loss None
iter 50, train loss 162.5964813232422, val loss None
iter 60, train loss 162.4788818359375, val loss None
iter 70, train loss 162.1907196044922, val loss None
iter 80, train loss 162.353515625, val loss None
iter 90, train loss 162.4132080078125, val loss None
best loss 162.1907196044922
not here
quantized in 31.4475314617157 seconds
36372 MiB free out of 48676 MiB total
18 self_attn.o_proj
Pruning ...
256
iter 0, train loss 8.645320892333984, val loss None
iter 10, train loss 8.449678421020508, val loss None
iter 20, train loss 8.367566108703613, val loss None
iter 30, train loss 8.306517601013184, val loss None
iter 40, train loss 8.289631843566895, val loss None
iter 50, train loss 8.24679183959961, val loss None
iter 60, train loss 8.207167625427246, val loss None
iter 70, train loss 8.17900276184082, val loss None
iter 80, train loss 8.174283027648926, val loss None
iter 90, train loss 8.165153503417969, val loss None
best loss 8.147720336914062
not here
quantized in 31.790220260620117 seconds
36372 MiB free out of 48676 MiB total
18 mlp.gate_proj
Pruning ...
256
iter 0, train loss 343.2855224609375, val loss None
iter 10, train loss 359.38592529296875, val loss None
iter 20, train loss 353.3619384765625, val loss None
iter 30, train loss 354.5042724609375, val loss None
iter 40, train loss 354.5662841796875, val loss None
iter 50, train loss 353.52154541015625, val loss None
iter 60, train loss 353.7265625, val loss None
iter 70, train loss 353.80322265625, val loss None
iter 80, train loss 353.69561767578125, val loss None
iter 90, train loss 353.550048828125, val loss None
best loss 338.137939453125
not here
quantized in 85.5603129863739 seconds
36070 MiB free out of 48676 MiB total
18 mlp.up_proj
Pruning ...
256
iter 0, train loss 291.70501708984375, val loss None
iter 10, train loss 292.49114990234375, val loss None
iter 20, train loss 292.8514709472656, val loss None
iter 30, train loss 292.54840087890625, val loss None
iter 40, train loss 292.42169189453125, val loss None
iter 50, train loss 292.42010498046875, val loss None
iter 60, train loss 292.2789611816406, val loss None
iter 70, train loss 292.22723388671875, val loss None
iter 80, train loss 292.1284484863281, val loss None
iter 90, train loss 292.0296630859375, val loss None
best loss 291.70501708984375
not here
quantized in 83.23271346092224 seconds
35790 MiB free out of 48676 MiB total
18 mlp.down_proj
Pruning ...
256
iter 0, train loss 10.746509552001953, val loss None
iter 10, train loss 10.743122100830078, val loss None
iter 20, train loss 10.682804107666016, val loss None
iter 30, train loss 10.683298110961914, val loss None
iter 40, train loss 10.632644653320312, val loss None
iter 50, train loss 10.605295181274414, val loss None
iter 60, train loss 10.574041366577148, val loss None
iter 70, train loss 10.555643081665039, val loss None
iter 80, train loss 10.550577163696289, val loss None
iter 90, train loss 10.531816482543945, val loss None
best loss 10.526115417480469
not here
quantized in 87.79943680763245 seconds
35510 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35510 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31414 MiB free out of 48676 MiB total
after cast to cpu
38316 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
19 self_attn.q_proj
Pruning ...
256
iter 0, train loss 341.6650695800781, val loss None
iter 10, train loss 359.8393859863281, val loss None
iter 20, train loss 349.0722961425781, val loss None
iter 30, train loss 342.2051696777344, val loss None
iter 40, train loss 340.8063659667969, val loss None
iter 50, train loss 338.94921875, val loss None
iter 60, train loss 338.59014892578125, val loss None
iter 70, train loss 337.801025390625, val loss None
iter 80, train loss 337.6044006347656, val loss None
iter 90, train loss 337.17840576171875, val loss None
best loss 316.0809020996094
not here
quantized in 34.52071762084961 seconds
36392 MiB free out of 48676 MiB total
19 self_attn.k_proj
Pruning ...
256
iter 0, train loss 368.68646240234375, val loss None
iter 10, train loss 384.92626953125, val loss None
iter 20, train loss 380.25225830078125, val loss None
iter 30, train loss 374.0355224609375, val loss None
iter 40, train loss 368.8060302734375, val loss None
iter 50, train loss 364.7705078125, val loss None
iter 60, train loss 363.32452392578125, val loss None
iter 70, train loss 362.6015930175781, val loss None
iter 80, train loss 362.6506042480469, val loss None
iter 90, train loss 362.6341552734375, val loss None
best loss 336.52398681640625
not here
quantized in 32.84458136558533 seconds
36414 MiB free out of 48676 MiB total
19 self_attn.v_proj
Pruning ...
256
iter 0, train loss 165.5906219482422, val loss None
iter 10, train loss 165.24673461914062, val loss None
iter 20, train loss 164.859130859375, val loss None
iter 30, train loss 164.50753784179688, val loss None
iter 40, train loss 164.35203552246094, val loss None
iter 50, train loss 163.998779296875, val loss None
iter 60, train loss 163.87136840820312, val loss None
iter 70, train loss 163.75662231445312, val loss None
iter 80, train loss 163.71986389160156, val loss None
iter 90, train loss 163.63816833496094, val loss None
best loss 163.5115966796875
not here
quantized in 31.496237754821777 seconds
36404 MiB free out of 48676 MiB total
19 self_attn.o_proj
Pruning ...
256
iter 0, train loss 9.132552146911621, val loss None
iter 10, train loss 8.727035522460938, val loss None
iter 20, train loss 8.725069999694824, val loss None
iter 30, train loss 8.649860382080078, val loss None
iter 40, train loss 8.555965423583984, val loss None
iter 50, train loss 8.511672019958496, val loss None
iter 60, train loss 8.470930099487305, val loss None
iter 70, train loss 8.506802558898926, val loss None
iter 80, train loss 8.519989013671875, val loss None
iter 90, train loss 8.517220497131348, val loss None
best loss 8.445066452026367
not here
quantized in 31.678298473358154 seconds
36340 MiB free out of 48676 MiB total
19 mlp.gate_proj
Pruning ...
256
iter 0, train loss 363.724365234375, val loss None
iter 10, train loss 379.1881103515625, val loss None
iter 20, train loss 375.6314697265625, val loss None
iter 30, train loss 377.02374267578125, val loss None
iter 40, train loss 375.97906494140625, val loss None
iter 50, train loss 375.686279296875, val loss None
iter 60, train loss 375.488037109375, val loss None
iter 70, train loss 375.1124267578125, val loss None
iter 80, train loss 374.821533203125, val loss None
iter 90, train loss 374.67413330078125, val loss None
best loss 360.0799560546875
not here
quantized in 84.97957015037537 seconds
36038 MiB free out of 48676 MiB total
19 mlp.up_proj
Pruning ...
256
iter 0, train loss 311.36798095703125, val loss None
iter 10, train loss 311.70562744140625, val loss None
iter 20, train loss 312.1540222167969, val loss None
iter 30, train loss 311.98681640625, val loss None
iter 40, train loss 312.00421142578125, val loss None
iter 50, train loss 312.07501220703125, val loss None
iter 60, train loss 312.013671875, val loss None
iter 70, train loss 311.96026611328125, val loss None
iter 80, train loss 311.93316650390625, val loss None
iter 90, train loss 311.9425048828125, val loss None
best loss 311.2412414550781
not here
quantized in 83.25856351852417 seconds
35844 MiB free out of 48676 MiB total
19 mlp.down_proj
Pruning ...
256
iter 0, train loss 11.37321662902832, val loss None
iter 10, train loss 11.377280235290527, val loss None
iter 20, train loss 11.318408966064453, val loss None
iter 30, train loss 11.296916961669922, val loss None
iter 40, train loss 11.275850296020508, val loss None
iter 50, train loss 11.275169372558594, val loss None
iter 60, train loss 11.24717903137207, val loss None
iter 70, train loss 11.232765197753906, val loss None
iter 80, train loss 11.226300239562988, val loss None
iter 90, train loss 11.211299896240234, val loss None
best loss 11.20181655883789
not here
quantized in 87.6642906665802 seconds
35564 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35564 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31468 MiB free out of 48676 MiB total
after cast to cpu
38316 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
20 self_attn.q_proj
Pruning ...
256
iter 0, train loss 353.7380065917969, val loss None
iter 10, train loss 374.21405029296875, val loss None
iter 20, train loss 367.2873840332031, val loss None
iter 30, train loss 361.1188659667969, val loss None
iter 40, train loss 357.5062255859375, val loss None
iter 50, train loss 355.5589904785156, val loss None
iter 60, train loss 353.2715759277344, val loss None
iter 70, train loss 352.2312927246094, val loss None
iter 80, train loss 352.0940856933594, val loss None
iter 90, train loss 351.8382263183594, val loss None
best loss 326.63970947265625
not here
quantized in 34.1814169883728 seconds
36392 MiB free out of 48676 MiB total
20 self_attn.k_proj
Pruning ...
256
iter 0, train loss 381.61578369140625, val loss None
iter 10, train loss 399.03485107421875, val loss None
iter 20, train loss 395.42181396484375, val loss None
iter 30, train loss 385.82061767578125, val loss None
iter 40, train loss 381.1728820800781, val loss None
iter 50, train loss 378.41656494140625, val loss None
iter 60, train loss 376.02178955078125, val loss None
iter 70, train loss 374.51995849609375, val loss None
iter 80, train loss 373.425537109375, val loss None
iter 90, train loss 372.919189453125, val loss None
best loss 348.8526916503906
not here
quantized in 33.075265407562256 seconds
36350 MiB free out of 48676 MiB total
20 self_attn.v_proj
Pruning ...
256
iter 0, train loss 171.53558349609375, val loss None
iter 10, train loss 171.1063995361328, val loss None
iter 20, train loss 170.93182373046875, val loss None
iter 30, train loss 170.23072814941406, val loss None
iter 40, train loss 170.41250610351562, val loss None
iter 50, train loss 170.1013946533203, val loss None
iter 60, train loss 170.17156982421875, val loss None
iter 70, train loss 169.9951934814453, val loss None
iter 80, train loss 169.80967712402344, val loss None
iter 90, train loss 170.0469512939453, val loss None
best loss 169.80967712402344
not here
quantized in 31.50163459777832 seconds
36372 MiB free out of 48676 MiB total
20 self_attn.o_proj
Pruning ...
256
iter 0, train loss 13.334442138671875, val loss None
iter 10, train loss 11.844655990600586, val loss None
iter 20, train loss 11.04076099395752, val loss None
iter 30, train loss 10.615748405456543, val loss None
iter 40, train loss 10.393320083618164, val loss None
iter 50, train loss 10.262636184692383, val loss None
iter 60, train loss 10.202799797058105, val loss None
iter 70, train loss 10.150543212890625, val loss None
iter 80, train loss 10.103645324707031, val loss None
iter 90, train loss 10.061134338378906, val loss None
best loss 10.044060707092285
not here
quantized in 32.27938389778137 seconds
36308 MiB free out of 48676 MiB total
20 mlp.gate_proj
Pruning ...
256
iter 0, train loss 394.58843994140625, val loss None
iter 10, train loss 411.54132080078125, val loss None
iter 20, train loss 408.08123779296875, val loss None
iter 30, train loss 410.4739074707031, val loss None
iter 40, train loss 410.2359924316406, val loss None
iter 50, train loss 409.9612121582031, val loss None
iter 60, train loss 409.2193603515625, val loss None
iter 70, train loss 409.2057189941406, val loss None
iter 80, train loss 408.85205078125, val loss None
iter 90, train loss 408.6368713378906, val loss None
best loss 390.0487060546875
not here
quantized in 85.12166094779968 seconds
36006 MiB free out of 48676 MiB total
20 mlp.up_proj
Pruning ...
256
iter 0, train loss 335.3162536621094, val loss None
iter 10, train loss 335.4897766113281, val loss None
iter 20, train loss 336.52838134765625, val loss None
iter 30, train loss 336.447509765625, val loss None
iter 40, train loss 336.6338806152344, val loss None
iter 50, train loss 336.49853515625, val loss None
iter 60, train loss 336.57403564453125, val loss None
iter 70, train loss 336.53839111328125, val loss None
iter 80, train loss 336.40252685546875, val loss None
iter 90, train loss 336.388671875, val loss None
best loss 334.9835205078125
not here
quantized in 83.67773389816284 seconds
35726 MiB free out of 48676 MiB total
20 mlp.down_proj
Pruning ...
256
iter 0, train loss 13.90373420715332, val loss None
iter 10, train loss 13.901968002319336, val loss None
iter 20, train loss 13.848993301391602, val loss None
iter 30, train loss 13.824237823486328, val loss None
iter 40, train loss 13.804667472839355, val loss None
iter 50, train loss 13.80633544921875, val loss None
iter 60, train loss 13.797340393066406, val loss None
iter 70, train loss 13.792959213256836, val loss None
iter 80, train loss 13.785684585571289, val loss None
iter 90, train loss 13.77967357635498, val loss None
best loss 13.776100158691406
not here
quantized in 88.2119870185852 seconds
35446 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35446 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31350 MiB free out of 48676 MiB total
after cast to cpu
38316 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
21 self_attn.q_proj
Pruning ...
256
iter 0, train loss 364.87982177734375, val loss None
iter 10, train loss 386.3757019042969, val loss None
iter 20, train loss 376.4089050292969, val loss None
iter 30, train loss 369.8800354003906, val loss None
iter 40, train loss 366.7666931152344, val loss None
iter 50, train loss 364.0087890625, val loss None
iter 60, train loss 362.60858154296875, val loss None
iter 70, train loss 361.8724060058594, val loss None
iter 80, train loss 361.707763671875, val loss None
iter 90, train loss 361.400390625, val loss None
best loss 344.791259765625
not here
quantized in 33.99554085731506 seconds
36392 MiB free out of 48676 MiB total
21 self_attn.k_proj
Pruning ...
256
iter 0, train loss 386.4337158203125, val loss None
iter 10, train loss 403.6182861328125, val loss None
iter 20, train loss 391.14544677734375, val loss None
iter 30, train loss 381.3998107910156, val loss None
iter 40, train loss 377.4256591796875, val loss None
iter 50, train loss 375.00299072265625, val loss None
iter 60, train loss 374.966064453125, val loss None
iter 70, train loss 374.60675048828125, val loss None
iter 80, train loss 374.1069030761719, val loss None
iter 90, train loss 373.6497802734375, val loss None
best loss 363.7640075683594
not here
quantized in 32.46934723854065 seconds
36414 MiB free out of 48676 MiB total
21 self_attn.v_proj
Pruning ...
256
iter 0, train loss 202.49867248535156, val loss None
iter 10, train loss 201.61126708984375, val loss None
iter 20, train loss 201.24855041503906, val loss None
iter 30, train loss 200.96141052246094, val loss None
iter 40, train loss 201.1331024169922, val loss None
iter 50, train loss 200.82882690429688, val loss None
iter 60, train loss 200.7347412109375, val loss None
iter 70, train loss 200.81629943847656, val loss None
iter 80, train loss 200.75778198242188, val loss None
iter 90, train loss 200.71559143066406, val loss None
best loss 200.5244598388672
not here
quantized in 31.4458749294281 seconds
36404 MiB free out of 48676 MiB total
21 self_attn.o_proj
Pruning ...
256
iter 0, train loss 10.145528793334961, val loss None
iter 10, train loss 9.57136344909668, val loss None
iter 20, train loss 9.206791877746582, val loss None
iter 30, train loss 9.013991355895996, val loss None
iter 40, train loss 8.923151016235352, val loss None
iter 50, train loss 8.887923240661621, val loss None
iter 60, train loss 8.812010765075684, val loss None
iter 70, train loss 8.766525268554688, val loss None
iter 80, train loss 8.753530502319336, val loss None
iter 90, train loss 8.746587753295898, val loss None
best loss 8.740346908569336
not here
quantized in 31.990293979644775 seconds
36372 MiB free out of 48676 MiB total
21 mlp.gate_proj
Pruning ...
256
iter 0, train loss 419.307373046875, val loss None
iter 10, train loss 436.3088684082031, val loss None
iter 20, train loss 433.10589599609375, val loss None
iter 30, train loss 433.5347900390625, val loss None
iter 40, train loss 432.2864685058594, val loss None
iter 50, train loss 432.1805419921875, val loss None
iter 60, train loss 431.9425964355469, val loss None
iter 70, train loss 431.4709167480469, val loss None
iter 80, train loss 431.2569885253906, val loss None
iter 90, train loss 431.2777404785156, val loss None
best loss 415.2890930175781
not here
quantized in 84.37846684455872 seconds
36070 MiB free out of 48676 MiB total
21 mlp.up_proj
Pruning ...
256
iter 0, train loss 351.4643249511719, val loss None
iter 10, train loss 352.19873046875, val loss None
iter 20, train loss 352.39459228515625, val loss None
iter 30, train loss 352.2947082519531, val loss None
iter 40, train loss 352.4325866699219, val loss None
iter 50, train loss 352.47930908203125, val loss None
iter 60, train loss 352.40911865234375, val loss None
iter 70, train loss 352.2690124511719, val loss None
iter 80, train loss 352.12786865234375, val loss None
iter 90, train loss 352.11541748046875, val loss None
best loss 351.4643249511719
not here
quantized in 83.23452281951904 seconds
35790 MiB free out of 48676 MiB total
21 mlp.down_proj
Pruning ...
256
iter 0, train loss 14.200675010681152, val loss None
iter 10, train loss 14.232521057128906, val loss None
iter 20, train loss 14.160314559936523, val loss None
iter 30, train loss 14.138175964355469, val loss None
iter 40, train loss 14.108331680297852, val loss None
iter 50, train loss 14.086273193359375, val loss None
iter 60, train loss 14.077423095703125, val loss None
iter 70, train loss 14.05596923828125, val loss None
iter 80, train loss 14.042109489440918, val loss None
iter 90, train loss 14.040763854980469, val loss None
best loss 14.035426139831543
not here
quantized in 88.13660597801208 seconds
35510 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35510 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31414 MiB free out of 48676 MiB total
after cast to cpu
38316 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
22 self_attn.q_proj
Pruning ...
256
iter 0, train loss 384.1779479980469, val loss None
iter 10, train loss 409.8580017089844, val loss None
iter 20, train loss 397.1822814941406, val loss None
iter 30, train loss 391.37713623046875, val loss None
iter 40, train loss 389.0728454589844, val loss None
iter 50, train loss 385.10552978515625, val loss None
iter 60, train loss 383.9710998535156, val loss None
iter 70, train loss 382.7042236328125, val loss None
iter 80, train loss 382.4801940917969, val loss None
iter 90, train loss 381.8426818847656, val loss None
best loss 366.0929260253906
not here
quantized in 33.97968292236328 seconds
36392 MiB free out of 48676 MiB total
22 self_attn.k_proj
Pruning ...
256
iter 0, train loss 411.23309326171875, val loss None
iter 10, train loss 429.2550354003906, val loss None
iter 20, train loss 419.29827880859375, val loss None
iter 30, train loss 413.65008544921875, val loss None
iter 40, train loss 410.1926574707031, val loss None
iter 50, train loss 408.9377136230469, val loss None
iter 60, train loss 407.7817687988281, val loss None
iter 70, train loss 407.18902587890625, val loss None
iter 80, train loss 407.11029052734375, val loss None
iter 90, train loss 406.70989990234375, val loss None
best loss 387.4787292480469
not here
quantized in 32.86323547363281 seconds
36350 MiB free out of 48676 MiB total
22 self_attn.v_proj
Pruning ...
256
iter 0, train loss 208.94522094726562, val loss None
iter 10, train loss 208.6470489501953, val loss None
iter 20, train loss 208.23928833007812, val loss None
iter 30, train loss 207.96157836914062, val loss None
iter 40, train loss 207.93780517578125, val loss None
iter 50, train loss 207.5714569091797, val loss None
iter 60, train loss 207.54714965820312, val loss None
iter 70, train loss 207.03851318359375, val loss None
iter 80, train loss 207.0897216796875, val loss None
iter 90, train loss 207.1634521484375, val loss None
best loss 207.03851318359375
not here
quantized in 31.487818717956543 seconds
36372 MiB free out of 48676 MiB total
22 self_attn.o_proj
Pruning ...
256
iter 0, train loss 54.68791961669922, val loss None
iter 10, train loss 40.41455078125, val loss None
iter 20, train loss 30.187572479248047, val loss None
iter 30, train loss 22.728931427001953, val loss None
iter 40, train loss 18.350936889648438, val loss None
iter 50, train loss 15.939899444580078, val loss None
iter 60, train loss 15.464401245117188, val loss None
iter 70, train loss 15.233450889587402, val loss None
iter 80, train loss 15.11351490020752, val loss None
iter 90, train loss 14.951330184936523, val loss None
best loss 14.693743705749512
not here
quantized in 32.906769037246704 seconds
36372 MiB free out of 48676 MiB total
22 mlp.gate_proj
Pruning ...
256
iter 0, train loss 429.72393798828125, val loss None
iter 10, train loss 445.33624267578125, val loss None
iter 20, train loss 442.1246337890625, val loss None
iter 30, train loss 442.1202392578125, val loss None
iter 40, train loss 442.5634460449219, val loss None
iter 50, train loss 442.3876953125, val loss None
iter 60, train loss 441.90301513671875, val loss None
iter 70, train loss 441.5531005859375, val loss None
iter 80, train loss 441.4563293457031, val loss None
iter 90, train loss 441.1620788574219, val loss None
best loss 425.92364501953125
not here
quantized in 84.74825739860535 seconds
36070 MiB free out of 48676 MiB total
22 mlp.up_proj
Pruning ...
256
iter 0, train loss 356.5693054199219, val loss None
iter 10, train loss 357.1678161621094, val loss None
iter 20, train loss 357.9212646484375, val loss None
iter 30, train loss 357.6070251464844, val loss None
iter 40, train loss 358.0306701660156, val loss None
iter 50, train loss 357.9957275390625, val loss None
iter 60, train loss 357.90936279296875, val loss None
iter 70, train loss 357.964111328125, val loss None
iter 80, train loss 358.0037841796875, val loss None
iter 90, train loss 357.9151916503906, val loss None
best loss 356.5693054199219
not here
quantized in 83.43241834640503 seconds
35790 MiB free out of 48676 MiB total
22 mlp.down_proj
Pruning ...
256
iter 0, train loss 15.036933898925781, val loss None
iter 10, train loss 15.072683334350586, val loss None
iter 20, train loss 15.019658088684082, val loss None
iter 30, train loss 15.007304191589355, val loss None
iter 40, train loss 15.006158828735352, val loss None
iter 50, train loss 14.98572063446045, val loss None
iter 60, train loss 14.984573364257812, val loss None
iter 70, train loss 14.97738265991211, val loss None
iter 80, train loss 14.970552444458008, val loss None
iter 90, train loss 14.965015411376953, val loss None
best loss 14.961614608764648
not here
quantized in 87.69688200950623 seconds
35596 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35596 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31500 MiB free out of 48676 MiB total
after cast to cpu
38316 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
23 self_attn.q_proj
Pruning ...
256
iter 0, train loss 412.12811279296875, val loss None
iter 10, train loss 423.55816650390625, val loss None
iter 20, train loss 409.94805908203125, val loss None
iter 30, train loss 406.3114929199219, val loss None
iter 40, train loss 404.735595703125, val loss None
iter 50, train loss 403.0005798339844, val loss None
iter 60, train loss 402.1419677734375, val loss None
iter 70, train loss 401.32342529296875, val loss None
iter 80, train loss 401.09283447265625, val loss None
iter 90, train loss 400.9850769042969, val loss None
best loss 396.08538818359375
not here
quantized in 33.72068786621094 seconds
36392 MiB free out of 48676 MiB total
23 self_attn.k_proj
Pruning ...
256
iter 0, train loss 429.60565185546875, val loss None
iter 10, train loss 451.21124267578125, val loss None
iter 20, train loss 437.0885009765625, val loss None
iter 30, train loss 428.6124267578125, val loss None
iter 40, train loss 426.6437072753906, val loss None
iter 50, train loss 424.41473388671875, val loss None
iter 60, train loss 422.471923828125, val loss None
iter 70, train loss 421.61376953125, val loss None
iter 80, train loss 421.47723388671875, val loss None
iter 90, train loss 421.6395263671875, val loss None
best loss 411.6067199707031
not here
quantized in 32.511242389678955 seconds
36414 MiB free out of 48676 MiB total
23 self_attn.v_proj
Pruning ...
256
iter 0, train loss 251.97000122070312, val loss None
iter 10, train loss 251.34249877929688, val loss None
iter 20, train loss 251.08383178710938, val loss None
iter 30, train loss 250.8280792236328, val loss None
iter 40, train loss 250.41915893554688, val loss None
iter 50, train loss 250.50146484375, val loss None
iter 60, train loss 249.9159393310547, val loss None
iter 70, train loss 249.94613647460938, val loss None
iter 80, train loss 249.79795837402344, val loss None
iter 90, train loss 249.85861206054688, val loss None
best loss 249.73648071289062
not here
quantized in 31.216675758361816 seconds
36372 MiB free out of 48676 MiB total
23 self_attn.o_proj
Pruning ...
256
iter 0, train loss 11.941308975219727, val loss None
iter 10, train loss 11.697538375854492, val loss None
iter 20, train loss 11.666025161743164, val loss None
iter 30, train loss 11.585105895996094, val loss None
iter 40, train loss 11.4996919631958, val loss None
iter 50, train loss 11.491813659667969, val loss None
iter 60, train loss 11.404577255249023, val loss None
iter 70, train loss 11.370355606079102, val loss None
iter 80, train loss 11.360757827758789, val loss None
iter 90, train loss 11.326108932495117, val loss None
best loss 11.304994583129883
not here
quantized in 31.91385555267334 seconds
36308 MiB free out of 48676 MiB total
23 mlp.gate_proj
Pruning ...
256
iter 0, train loss 464.36627197265625, val loss None
iter 10, train loss 477.2361145019531, val loss None
iter 20, train loss 476.9930725097656, val loss None
iter 30, train loss 476.638916015625, val loss None
iter 40, train loss 475.30401611328125, val loss None
iter 50, train loss 475.021728515625, val loss None
iter 60, train loss 474.8582458496094, val loss None
iter 70, train loss 474.4137268066406, val loss None
iter 80, train loss 474.1612548828125, val loss None
iter 90, train loss 474.05877685546875, val loss None
best loss 461.0738525390625
not here
quantized in 85.09589695930481 seconds
35920 MiB free out of 48676 MiB total
23 mlp.up_proj
Pruning ...
256
iter 0, train loss 388.0773620605469, val loss None
iter 10, train loss 387.94683837890625, val loss None
iter 20, train loss 388.2779541015625, val loss None
iter 30, train loss 388.3890075683594, val loss None
iter 40, train loss 388.12255859375, val loss None
iter 50, train loss 387.7267761230469, val loss None
iter 60, train loss 387.8476867675781, val loss None
iter 70, train loss 388.0122985839844, val loss None
iter 80, train loss 387.841552734375, val loss None
iter 90, train loss 387.9052734375, val loss None
best loss 387.6070251464844
not here
quantized in 83.8421242237091 seconds
35726 MiB free out of 48676 MiB total
23 mlp.down_proj
Pruning ...
256
iter 0, train loss 16.740604400634766, val loss None
iter 10, train loss 16.752105712890625, val loss None
iter 20, train loss 16.700178146362305, val loss None
iter 30, train loss 16.673309326171875, val loss None
iter 40, train loss 16.66974639892578, val loss None
iter 50, train loss 16.658302307128906, val loss None
iter 60, train loss 16.64797592163086, val loss None
iter 70, train loss 16.647315979003906, val loss None
iter 80, train loss 16.634164810180664, val loss None
iter 90, train loss 16.632129669189453, val loss None
best loss 16.631290435791016
not here
quantized in 88.11270093917847 seconds
35446 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35446 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31350 MiB free out of 48676 MiB total
after cast to cpu
38316 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
24 self_attn.q_proj
Pruning ...
256
iter 0, train loss 377.4541931152344, val loss None
iter 10, train loss 393.2788391113281, val loss None
iter 20, train loss 376.294189453125, val loss None
iter 30, train loss 375.6317138671875, val loss None
iter 40, train loss 374.7855529785156, val loss None
iter 50, train loss 373.6700439453125, val loss None
iter 60, train loss 373.2139587402344, val loss None
iter 70, train loss 373.174560546875, val loss None
iter 80, train loss 373.309814453125, val loss None
iter 90, train loss 373.3333740234375, val loss None
best loss 360.6430358886719
not here
quantized in 34.00497078895569 seconds
36392 MiB free out of 48676 MiB total
24 self_attn.k_proj
Pruning ...
256
iter 0, train loss 401.4769592285156, val loss None
iter 10, train loss 408.55816650390625, val loss None
iter 20, train loss 400.25274658203125, val loss None
iter 30, train loss 395.95770263671875, val loss None
iter 40, train loss 394.6086120605469, val loss None
iter 50, train loss 393.39678955078125, val loss None
iter 60, train loss 391.7740478515625, val loss None
iter 70, train loss 390.89019775390625, val loss None
iter 80, train loss 390.5501708984375, val loss None
iter 90, train loss 390.40521240234375, val loss None
best loss 380.00616455078125
not here
quantized in 32.995126485824585 seconds
36350 MiB free out of 48676 MiB total
24 self_attn.v_proj
Pruning ...
256
iter 0, train loss 239.70657348632812, val loss None
iter 10, train loss 238.90866088867188, val loss None
iter 20, train loss 237.84524536132812, val loss None
iter 30, train loss 237.91709899902344, val loss None
iter 40, train loss 237.2764434814453, val loss None
iter 50, train loss 237.41650390625, val loss None
iter 60, train loss 237.50100708007812, val loss None
iter 70, train loss 237.3361053466797, val loss None
iter 80, train loss 236.92672729492188, val loss None
iter 90, train loss 236.78533935546875, val loss None
best loss 236.7020263671875
not here
quantized in 31.75176239013672 seconds
36372 MiB free out of 48676 MiB total
24 self_attn.o_proj
Pruning ...
256
iter 0, train loss 29.076217651367188, val loss None
iter 10, train loss 22.529762268066406, val loss None
iter 20, train loss 19.5044002532959, val loss None
iter 30, train loss 17.534387588500977, val loss None
iter 40, train loss 17.005029678344727, val loss None
iter 50, train loss 16.763431549072266, val loss None
iter 60, train loss 16.688785552978516, val loss None
iter 70, train loss 16.561534881591797, val loss None
iter 80, train loss 16.51058006286621, val loss None
iter 90, train loss 16.434144973754883, val loss None
best loss 16.351364135742188
not here
quantized in 32.48677110671997 seconds
36372 MiB free out of 48676 MiB total
24 mlp.gate_proj
Pruning ...
256
iter 0, train loss 480.7921447753906, val loss None
iter 10, train loss 494.95379638671875, val loss None
iter 20, train loss 493.3591613769531, val loss None
iter 30, train loss 492.75146484375, val loss None
iter 40, train loss 492.01397705078125, val loss None
iter 50, train loss 491.6566162109375, val loss None
iter 60, train loss 491.22137451171875, val loss None
iter 70, train loss 490.9096984863281, val loss None
iter 80, train loss 490.63543701171875, val loss None
iter 90, train loss 490.7454833984375, val loss None
best loss 477.4815673828125
not here
quantized in 85.39553165435791 seconds
35984 MiB free out of 48676 MiB total
24 mlp.up_proj
Pruning ...
256
iter 0, train loss 403.9950866699219, val loss None
iter 10, train loss 404.5594482421875, val loss None
iter 20, train loss 404.92266845703125, val loss None
iter 30, train loss 404.7560729980469, val loss None
iter 40, train loss 404.94403076171875, val loss None
iter 50, train loss 405.1221923828125, val loss None
iter 60, train loss 405.1075134277344, val loss None
iter 70, train loss 404.9244384765625, val loss None
iter 80, train loss 404.8341979980469, val loss None
iter 90, train loss 404.8004150390625, val loss None
best loss 403.9950866699219
not here
quantized in 83.45385694503784 seconds
35704 MiB free out of 48676 MiB total
24 mlp.down_proj
Pruning ...
256
iter 0, train loss 17.45137596130371, val loss None
iter 10, train loss 17.459505081176758, val loss None
iter 20, train loss 17.40424346923828, val loss None
iter 30, train loss 17.38170623779297, val loss None
iter 40, train loss 17.371055603027344, val loss None
iter 50, train loss 17.371530532836914, val loss None
iter 60, train loss 17.36237144470215, val loss None
iter 70, train loss 17.358562469482422, val loss None
iter 80, train loss 17.354333877563477, val loss None
iter 90, train loss 17.346647262573242, val loss None
best loss 17.34592056274414
not here
quantized in 87.80800318717957 seconds
35596 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35596 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31500 MiB free out of 48676 MiB total
after cast to cpu
38316 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
25 self_attn.q_proj
Pruning ...
256
iter 0, train loss 427.89849853515625, val loss None
iter 10, train loss 442.23968505859375, val loss None
iter 20, train loss 425.9282531738281, val loss None
iter 30, train loss 425.83062744140625, val loss None
iter 40, train loss 425.85791015625, val loss None
iter 50, train loss 425.8461608886719, val loss None
iter 60, train loss 425.5880126953125, val loss None
iter 70, train loss 424.8458557128906, val loss None
iter 80, train loss 424.3514099121094, val loss None
iter 90, train loss 424.1676330566406, val loss None
best loss 414.9928283691406
not here
quantized in 33.691076040267944 seconds
36392 MiB free out of 48676 MiB total
25 self_attn.k_proj
Pruning ...
256
iter 0, train loss 445.9227600097656, val loss None
iter 10, train loss 466.72686767578125, val loss None
iter 20, train loss 450.1007080078125, val loss None
iter 30, train loss 436.882568359375, val loss None
iter 40, train loss 436.48944091796875, val loss None
iter 50, train loss 436.16412353515625, val loss None
iter 60, train loss 436.1142272949219, val loss None
iter 70, train loss 435.7294006347656, val loss None
iter 80, train loss 435.663330078125, val loss None
iter 90, train loss 435.6822509765625, val loss None
best loss 428.24554443359375
not here
quantized in 32.25867319107056 seconds
36414 MiB free out of 48676 MiB total
25 self_attn.v_proj
Pruning ...
256
iter 0, train loss 291.8741760253906, val loss None
iter 10, train loss 291.5895080566406, val loss None
iter 20, train loss 290.7064208984375, val loss None
iter 30, train loss 289.9393310546875, val loss None
iter 40, train loss 289.9235534667969, val loss None
iter 50, train loss 290.15350341796875, val loss None
iter 60, train loss 290.00982666015625, val loss None
iter 70, train loss 289.92193603515625, val loss None
iter 80, train loss 289.5419921875, val loss None
iter 90, train loss 289.6734619140625, val loss None
best loss 289.47589111328125
not here
quantized in 31.481990814208984 seconds
36404 MiB free out of 48676 MiB total
25 self_attn.o_proj
Pruning ...
256
iter 0, train loss 15.721708297729492, val loss None
iter 10, train loss 16.118793487548828, val loss None
iter 20, train loss 16.242366790771484, val loss None
iter 30, train loss 16.000770568847656, val loss None
iter 40, train loss 15.85536003112793, val loss None
iter 50, train loss 15.762064933776855, val loss None
iter 60, train loss 15.718659400939941, val loss None
iter 70, train loss 15.677719116210938, val loss None
iter 80, train loss 15.744455337524414, val loss None
iter 90, train loss 15.74055290222168, val loss None
best loss 15.460187911987305
not here
quantized in 31.780539751052856 seconds
36372 MiB free out of 48676 MiB total
25 mlp.gate_proj
Pruning ...
256
iter 0, train loss 518.520263671875, val loss None
iter 10, train loss 533.427734375, val loss None
iter 20, train loss 533.8729248046875, val loss None
iter 30, train loss 532.4268798828125, val loss None
iter 40, train loss 530.9986572265625, val loss None
iter 50, train loss 530.3331909179688, val loss None
iter 60, train loss 529.619140625, val loss None
iter 70, train loss 529.2535400390625, val loss None
iter 80, train loss 529.3648681640625, val loss None
iter 90, train loss 529.0383911132812, val loss None
best loss 513.6431274414062
not here
quantized in 85.94918608665466 seconds
36070 MiB free out of 48676 MiB total
25 mlp.up_proj
Pruning ...
256
iter 0, train loss 431.2355651855469, val loss None
iter 10, train loss 430.9641418457031, val loss None
iter 20, train loss 431.3208312988281, val loss None
iter 30, train loss 431.61669921875, val loss None
iter 40, train loss 431.64971923828125, val loss None
iter 50, train loss 431.594482421875, val loss None
iter 60, train loss 431.6203918457031, val loss None
iter 70, train loss 431.70574951171875, val loss None
iter 80, train loss 431.62451171875, val loss None
iter 90, train loss 431.5966491699219, val loss None
best loss 430.57171630859375
not here
quantized in 83.4292323589325 seconds
35790 MiB free out of 48676 MiB total
25 mlp.down_proj
Pruning ...
256
iter 0, train loss 18.1530818939209, val loss None
iter 10, train loss 18.145767211914062, val loss None
iter 20, train loss 18.09886360168457, val loss None
iter 30, train loss 18.09183692932129, val loss None
iter 40, train loss 18.066638946533203, val loss None
iter 50, train loss 18.056177139282227, val loss None
iter 60, train loss 18.04513931274414, val loss None
iter 70, train loss 18.042144775390625, val loss None
iter 80, train loss 18.03260040283203, val loss None
iter 90, train loss 18.01921844482422, val loss None
best loss 18.017580032348633
not here
quantized in 87.4945342540741 seconds
35682 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35682 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31586 MiB free out of 48676 MiB total
after cast to cpu
38316 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
26 self_attn.q_proj
Pruning ...
256
iter 0, train loss 409.3499450683594, val loss None
iter 10, train loss 432.3671569824219, val loss None
iter 20, train loss 420.1683044433594, val loss None
iter 30, train loss 411.041015625, val loss None
iter 40, train loss 407.14691162109375, val loss None
iter 50, train loss 405.8597412109375, val loss None
iter 60, train loss 404.9402160644531, val loss None
iter 70, train loss 404.42333984375, val loss None
iter 80, train loss 404.15887451171875, val loss None
iter 90, train loss 403.7956237792969, val loss None
best loss 390.41632080078125
not here
quantized in 33.834500312805176 seconds
36392 MiB free out of 48676 MiB total
26 self_attn.k_proj
Pruning ...
256
iter 0, train loss 439.16253662109375, val loss None
iter 10, train loss 457.1228332519531, val loss None
iter 20, train loss 448.6300964355469, val loss None
iter 30, train loss 441.6529235839844, val loss None
iter 40, train loss 438.00250244140625, val loss None
iter 50, train loss 434.11492919921875, val loss None
iter 60, train loss 432.90594482421875, val loss None
iter 70, train loss 432.2882995605469, val loss None
iter 80, train loss 431.6800537109375, val loss None
iter 90, train loss 431.82916259765625, val loss None
best loss 409.66754150390625
not here
quantized in 32.59089112281799 seconds
36350 MiB free out of 48676 MiB total
26 self_attn.v_proj
Pruning ...
256
iter 0, train loss 293.0843505859375, val loss None
iter 10, train loss 292.3013916015625, val loss None
iter 20, train loss 291.5218811035156, val loss None
iter 30, train loss 290.62646484375, val loss None
iter 40, train loss 290.3366394042969, val loss None
iter 50, train loss 289.95263671875, val loss None
iter 60, train loss 289.99432373046875, val loss None
iter 70, train loss 289.9852294921875, val loss None
iter 80, train loss 289.6123962402344, val loss None
iter 90, train loss 289.20989990234375, val loss None
best loss 288.986328125
not here
quantized in 31.234387636184692 seconds
36372 MiB free out of 48676 MiB total
26 self_attn.o_proj
Pruning ...
256
iter 0, train loss 46.367919921875, val loss None
iter 10, train loss 32.536582946777344, val loss None
iter 20, train loss 26.31968879699707, val loss None
iter 30, train loss 21.930923461914062, val loss None
iter 40, train loss 20.686723709106445, val loss None
iter 50, train loss 20.304494857788086, val loss None
iter 60, train loss 20.04735565185547, val loss None
iter 70, train loss 19.86821746826172, val loss None
iter 80, train loss 19.823917388916016, val loss None
iter 90, train loss 19.758193969726562, val loss None
best loss 19.72480010986328
not here
quantized in 32.160160779953 seconds
36372 MiB free out of 48676 MiB total
26 mlp.gate_proj
Pruning ...
256
iter 0, train loss 541.4804077148438, val loss None
iter 10, train loss 563.4669189453125, val loss None
iter 20, train loss 562.3662109375, val loss None
iter 30, train loss 561.6070556640625, val loss None
iter 40, train loss 562.181884765625, val loss None
iter 50, train loss 562.8165893554688, val loss None
iter 60, train loss 562.2905883789062, val loss None
iter 70, train loss 562.215576171875, val loss None
iter 80, train loss 561.9632568359375, val loss None
iter 90, train loss 561.787353515625, val loss None
best loss 535.9256591796875
not here
quantized in 84.90375924110413 seconds
35984 MiB free out of 48676 MiB total
26 mlp.up_proj
Pruning ...
256
iter 0, train loss 457.11798095703125, val loss None
iter 10, train loss 457.5055847167969, val loss None
iter 20, train loss 458.84161376953125, val loss None
iter 30, train loss 458.2834777832031, val loss None
iter 40, train loss 458.41668701171875, val loss None
iter 50, train loss 458.64208984375, val loss None
iter 60, train loss 458.8222961425781, val loss None
iter 70, train loss 458.6885070800781, val loss None
iter 80, train loss 458.6434326171875, val loss None
iter 90, train loss 458.50482177734375, val loss None
best loss 456.82769775390625
not here
quantized in 83.49833011627197 seconds
35704 MiB free out of 48676 MiB total
26 mlp.down_proj
Pruning ...
256
iter 0, train loss 19.811555862426758, val loss None
iter 10, train loss 19.829713821411133, val loss None
iter 20, train loss 19.811561584472656, val loss None
iter 30, train loss 19.79006576538086, val loss None
iter 40, train loss 19.775373458862305, val loss None
iter 50, train loss 19.754207611083984, val loss None
iter 60, train loss 19.74517059326172, val loss None
iter 70, train loss 19.729515075683594, val loss None
iter 80, train loss 19.730073928833008, val loss None
iter 90, train loss 19.72219467163086, val loss None
best loss 19.718978881835938
not here
quantized in 88.16690587997437 seconds
35424 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35424 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31328 MiB free out of 48676 MiB total
after cast to cpu
38316 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
27 self_attn.q_proj
Pruning ...
256
iter 0, train loss 456.8406982421875, val loss None
iter 10, train loss 482.0545349121094, val loss None
iter 20, train loss 459.5021667480469, val loss None
iter 30, train loss 451.884521484375, val loss None
iter 40, train loss 451.85772705078125, val loss None
iter 50, train loss 449.0738830566406, val loss None
iter 60, train loss 447.78985595703125, val loss None
iter 70, train loss 446.87811279296875, val loss None
iter 80, train loss 446.6869201660156, val loss None
iter 90, train loss 446.0899658203125, val loss None
best loss 427.73968505859375
not here
quantized in 33.94531607627869 seconds
36392 MiB free out of 48676 MiB total
27 self_attn.k_proj
Pruning ...
256
iter 0, train loss 493.336181640625, val loss None
iter 10, train loss 509.537353515625, val loss None
iter 20, train loss 495.33697509765625, val loss None
iter 30, train loss 485.84295654296875, val loss None
iter 40, train loss 479.53790283203125, val loss None
iter 50, train loss 477.810546875, val loss None
iter 60, train loss 475.5102233886719, val loss None
iter 70, train loss 473.8599853515625, val loss None
iter 80, train loss 472.43646240234375, val loss None
iter 90, train loss 472.1991882324219, val loss None
best loss 445.818603515625
not here
quantized in 32.97257041931152 seconds
36414 MiB free out of 48676 MiB total
27 self_attn.v_proj
Pruning ...
256
iter 0, train loss 301.22698974609375, val loss None
iter 10, train loss 300.377197265625, val loss None
iter 20, train loss 300.3607482910156, val loss None
iter 30, train loss 299.9759216308594, val loss None
iter 40, train loss 299.6086730957031, val loss None
iter 50, train loss 299.2523193359375, val loss None
iter 60, train loss 299.14324951171875, val loss None
iter 70, train loss 299.33648681640625, val loss None
iter 80, train loss 299.4876403808594, val loss None
iter 90, train loss 299.26055908203125, val loss None
best loss 298.9015808105469
not here
quantized in 31.667556285858154 seconds
36404 MiB free out of 48676 MiB total
27 self_attn.o_proj
Pruning ...
256
iter 0, train loss 18.660268783569336, val loss None
iter 10, train loss 16.91873550415039, val loss None
iter 20, train loss 16.237932205200195, val loss None
iter 30, train loss 15.517407417297363, val loss None
iter 40, train loss 15.030476570129395, val loss None
iter 50, train loss 14.837064743041992, val loss None
iter 60, train loss 14.796525001525879, val loss None
iter 70, train loss 14.699150085449219, val loss None
iter 80, train loss 14.670144081115723, val loss None
iter 90, train loss 14.60651969909668, val loss None
best loss 14.556388854980469
not here
quantized in 32.19331431388855 seconds
36404 MiB free out of 48676 MiB total
27 mlp.gate_proj
Pruning ...
256
iter 0, train loss 583.18310546875, val loss None
iter 10, train loss 605.28466796875, val loss None
iter 20, train loss 601.8572998046875, val loss None
iter 30, train loss 605.7593994140625, val loss None
iter 40, train loss 605.5150756835938, val loss None
iter 50, train loss 604.8389892578125, val loss None
iter 60, train loss 604.3512573242188, val loss None
iter 70, train loss 604.44677734375, val loss None
iter 80, train loss 604.4964599609375, val loss None
iter 90, train loss 604.5045166015625, val loss None
best loss 575.830078125
not here
quantized in 86.1449122428894 seconds
36102 MiB free out of 48676 MiB total
27 mlp.up_proj
Pruning ...
256
iter 0, train loss 491.501708984375, val loss None
iter 10, train loss 493.10504150390625, val loss None
iter 20, train loss 492.5947570800781, val loss None
iter 30, train loss 491.9510192871094, val loss None
iter 40, train loss 492.2724914550781, val loss None
iter 50, train loss 492.0389099121094, val loss None
iter 60, train loss 492.28839111328125, val loss None
iter 70, train loss 492.22882080078125, val loss None
iter 80, train loss 492.1058349609375, val loss None
iter 90, train loss 492.0946960449219, val loss None
best loss 490.1851806640625
not here
quantized in 85.05720138549805 seconds
35822 MiB free out of 48676 MiB total
27 mlp.down_proj
Pruning ...
256
iter 0, train loss 22.749608993530273, val loss None
iter 10, train loss 22.76540756225586, val loss None
iter 20, train loss 22.740514755249023, val loss None
iter 30, train loss 22.745922088623047, val loss None
iter 40, train loss 22.67803382873535, val loss None
iter 50, train loss 22.691362380981445, val loss None
iter 60, train loss 22.690465927124023, val loss None
iter 70, train loss 22.67547607421875, val loss None
iter 80, train loss 22.66881561279297, val loss None
iter 90, train loss 22.661643981933594, val loss None
best loss 22.64447593688965
not here
quantized in 87.59001970291138 seconds
35628 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35628 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31532 MiB free out of 48676 MiB total
after cast to cpu
38316 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
28 self_attn.q_proj
Pruning ...
256
iter 0, train loss 456.3299560546875, val loss None
iter 10, train loss 479.1765441894531, val loss None
iter 20, train loss 463.1767578125, val loss None
iter 30, train loss 452.83538818359375, val loss None
iter 40, train loss 448.3700866699219, val loss None
iter 50, train loss 447.5229797363281, val loss None
iter 60, train loss 446.4179382324219, val loss None
iter 70, train loss 446.1275634765625, val loss None
iter 80, train loss 445.67620849609375, val loss None
iter 90, train loss 445.0599060058594, val loss None
best loss 423.0791015625
not here
quantized in 33.950620889663696 seconds
36392 MiB free out of 48676 MiB total
28 self_attn.k_proj
Pruning ...
256
iter 0, train loss 487.814697265625, val loss None
iter 10, train loss 511.68756103515625, val loss None
iter 20, train loss 495.1883544921875, val loss None
iter 30, train loss 484.8852844238281, val loss None
iter 40, train loss 481.1788635253906, val loss None
iter 50, train loss 477.6193542480469, val loss None
iter 60, train loss 474.904296875, val loss None
iter 70, train loss 474.2668762207031, val loss None
iter 80, train loss 474.7713928222656, val loss None
iter 90, train loss 474.6983642578125, val loss None
best loss 440.42608642578125
not here
quantized in 33.24050235748291 seconds
36350 MiB free out of 48676 MiB total
28 self_attn.v_proj
Pruning ...
256
iter 0, train loss 333.37664794921875, val loss None
iter 10, train loss 333.1026611328125, val loss None
iter 20, train loss 332.7637939453125, val loss None
iter 30, train loss 332.8555908203125, val loss None
iter 40, train loss 332.2730712890625, val loss None
iter 50, train loss 332.2381896972656, val loss None
iter 60, train loss 331.9769287109375, val loss None
iter 70, train loss 331.7525939941406, val loss None
iter 80, train loss 331.70867919921875, val loss None
iter 90, train loss 331.479248046875, val loss None
best loss 331.36572265625
not here
quantized in 31.568547010421753 seconds
36372 MiB free out of 48676 MiB total
28 self_attn.o_proj
Pruning ...
256
iter 0, train loss 28.861730575561523, val loss None
iter 10, train loss 28.151491165161133, val loss None
iter 20, train loss 27.763002395629883, val loss None
iter 30, train loss 27.342214584350586, val loss None
iter 40, train loss 27.143470764160156, val loss None
iter 50, train loss 27.016021728515625, val loss None
iter 60, train loss 26.946823120117188, val loss None
iter 70, train loss 26.906478881835938, val loss None
iter 80, train loss 26.820858001708984, val loss None
iter 90, train loss 26.78009033203125, val loss None
best loss 26.709945678710938
not here
quantized in 31.74061894416809 seconds
36308 MiB free out of 48676 MiB total
28 mlp.gate_proj
Pruning ...
256
iter 0, train loss 615.058349609375, val loss None
iter 10, train loss 640.3731689453125, val loss None
iter 20, train loss 633.1619873046875, val loss None
iter 30, train loss 635.1587524414062, val loss None
iter 40, train loss 634.8621826171875, val loss None
iter 50, train loss 634.4866333007812, val loss None
iter 60, train loss 634.2029418945312, val loss None
iter 70, train loss 634.1376953125, val loss None
iter 80, train loss 634.5443725585938, val loss None
iter 90, train loss 634.5758056640625, val loss None
best loss 606.2225341796875
not here
quantized in 85.00419116020203 seconds
35920 MiB free out of 48676 MiB total
28 mlp.up_proj
Pruning ...
256
iter 0, train loss 543.5577392578125, val loss None
iter 10, train loss 552.129150390625, val loss None
iter 20, train loss 542.989013671875, val loss None
iter 30, train loss 542.6102905273438, val loss None
iter 40, train loss 541.90576171875, val loss None
iter 50, train loss 542.2696533203125, val loss None
iter 60, train loss 542.2274780273438, val loss None
iter 70, train loss 542.0693359375, val loss None
iter 80, train loss 541.833984375, val loss None
iter 90, train loss 541.7147827148438, val loss None
best loss 538.26806640625
not here
quantized in 86.00117826461792 seconds
35640 MiB free out of 48676 MiB total
28 mlp.down_proj
Pruning ...
256
iter 0, train loss 27.682546615600586, val loss None
iter 10, train loss 27.758655548095703, val loss None
iter 20, train loss 27.756202697753906, val loss None
iter 30, train loss 27.683612823486328, val loss None
iter 40, train loss 27.638553619384766, val loss None
iter 50, train loss 27.574350357055664, val loss None
iter 60, train loss 27.525585174560547, val loss None
iter 70, train loss 27.508602142333984, val loss None
iter 80, train loss 27.521696090698242, val loss None
iter 90, train loss 27.490182876586914, val loss None
best loss 27.486560821533203
not here
quantized in 87.8200306892395 seconds
35360 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35360 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31264 MiB free out of 48676 MiB total
after cast to cpu
38316 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
29 self_attn.q_proj
Pruning ...
256
iter 0, train loss 413.879150390625, val loss None
iter 10, train loss 435.4609375, val loss None
iter 20, train loss 419.44024658203125, val loss None
iter 30, train loss 413.886474609375, val loss None
iter 40, train loss 408.0811767578125, val loss None
iter 50, train loss 405.55743408203125, val loss None
iter 60, train loss 404.7166442871094, val loss None
iter 70, train loss 403.3882751464844, val loss None
iter 80, train loss 403.3177185058594, val loss None
iter 90, train loss 403.26318359375, val loss None
best loss 376.1884765625
not here
quantized in 34.32897996902466 seconds
36392 MiB free out of 48676 MiB total
29 self_attn.k_proj
Pruning ...
256
iter 0, train loss 445.6942138671875, val loss None
iter 10, train loss 465.5337219238281, val loss None
iter 20, train loss 454.45587158203125, val loss None
iter 30, train loss 445.40191650390625, val loss None
iter 40, train loss 438.9744873046875, val loss None
iter 50, train loss 434.700927734375, val loss None
iter 60, train loss 432.1552734375, val loss None
iter 70, train loss 430.69891357421875, val loss None
iter 80, train loss 428.66424560546875, val loss None
iter 90, train loss 427.54644775390625, val loss None
best loss 393.2675476074219
not here
quantized in 33.789164543151855 seconds
36414 MiB free out of 48676 MiB total
29 self_attn.v_proj
Pruning ...
256
iter 0, train loss 313.5054626464844, val loss None
iter 10, train loss 313.05303955078125, val loss None
iter 20, train loss 312.23760986328125, val loss None
iter 30, train loss 312.289794921875, val loss None
iter 40, train loss 312.2462158203125, val loss None
iter 50, train loss 311.6624755859375, val loss None
iter 60, train loss 311.0281066894531, val loss None
iter 70, train loss 310.725830078125, val loss None
iter 80, train loss 310.7232666015625, val loss None
iter 90, train loss 310.6031494140625, val loss None
best loss 310.5896301269531
not here
quantized in 31.57156467437744 seconds
36372 MiB free out of 48676 MiB total
29 self_attn.o_proj
Pruning ...
256
iter 0, train loss 36.1864128112793, val loss None
iter 10, train loss 29.039867401123047, val loss None
iter 20, train loss 28.03841781616211, val loss None
iter 30, train loss 26.968414306640625, val loss None
iter 40, train loss 26.49073600769043, val loss None
iter 50, train loss 26.191633224487305, val loss None
iter 60, train loss 25.946918487548828, val loss None
iter 70, train loss 25.88298797607422, val loss None
iter 80, train loss 25.659957885742188, val loss None
iter 90, train loss 25.62461280822754, val loss None
best loss 25.492324829101562
not here
quantized in 32.04264807701111 seconds
36308 MiB free out of 48676 MiB total
29 mlp.gate_proj
Pruning ...
256
iter 0, train loss 638.1240234375, val loss None
iter 10, train loss 671.6539306640625, val loss None
iter 20, train loss 656.9767456054688, val loss None
iter 30, train loss 658.3939208984375, val loss None
iter 40, train loss 658.8455810546875, val loss None
iter 50, train loss 656.8945922851562, val loss None
iter 60, train loss 654.6060791015625, val loss None
iter 70, train loss 652.9476318359375, val loss None
iter 80, train loss 652.6520385742188, val loss None
iter 90, train loss 652.3546752929688, val loss None
best loss 622.82666015625
not here
quantized in 85.60875034332275 seconds
35920 MiB free out of 48676 MiB total
29 mlp.up_proj
Pruning ...
256
iter 0, train loss 571.7041015625, val loss None
iter 10, train loss 594.18701171875, val loss None
iter 20, train loss 570.5045776367188, val loss None
iter 30, train loss 568.35888671875, val loss None
iter 40, train loss 567.2716064453125, val loss None
iter 50, train loss 564.3709716796875, val loss None
iter 60, train loss 563.685791015625, val loss None
iter 70, train loss 563.6649169921875, val loss None
iter 80, train loss 563.2377319335938, val loss None
iter 90, train loss 563.1760864257812, val loss None
best loss 560.931396484375
not here
quantized in 85.43673992156982 seconds
35640 MiB free out of 48676 MiB total
29 mlp.down_proj
Pruning ...
256
iter 0, train loss 33.147438049316406, val loss None
iter 10, train loss 33.200279235839844, val loss None
iter 20, train loss 33.13063430786133, val loss None
iter 30, train loss 33.1821174621582, val loss None
iter 40, train loss 33.05640411376953, val loss None
iter 50, train loss 32.96519470214844, val loss None
iter 60, train loss 33.00297927856445, val loss None
iter 70, train loss 32.93070983886719, val loss None
iter 80, train loss 32.9238395690918, val loss None
iter 90, train loss 32.887725830078125, val loss None
best loss 32.866920471191406
not here
quantized in 87.7243287563324 seconds
35360 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35360 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31264 MiB free out of 48676 MiB total
after cast to cpu
38316 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
30 self_attn.q_proj
Pruning ...
256
iter 0, train loss 441.8204650878906, val loss None
iter 10, train loss 469.2861022949219, val loss None
iter 20, train loss 456.84735107421875, val loss None
iter 30, train loss 445.2853698730469, val loss None
iter 40, train loss 440.0373229980469, val loss None
iter 50, train loss 436.26483154296875, val loss None
iter 60, train loss 433.4871826171875, val loss None
iter 70, train loss 432.6597900390625, val loss None
iter 80, train loss 432.728759765625, val loss None
iter 90, train loss 432.38238525390625, val loss None
best loss 400.77813720703125
not here
quantized in 34.60465741157532 seconds
36392 MiB free out of 48676 MiB total
30 self_attn.k_proj
Pruning ...
256
iter 0, train loss 474.80255126953125, val loss None
iter 10, train loss 500.065185546875, val loss None
iter 20, train loss 482.5455322265625, val loss None
iter 30, train loss 465.6278076171875, val loss None
iter 40, train loss 457.65887451171875, val loss None
iter 50, train loss 455.3386535644531, val loss None
iter 60, train loss 454.22149658203125, val loss None
iter 70, train loss 452.9173278808594, val loss None
iter 80, train loss 452.4718933105469, val loss None
iter 90, train loss 452.0235595703125, val loss None
best loss 415.9682922363281
not here
quantized in 33.50223350524902 seconds
36350 MiB free out of 48676 MiB total
30 self_attn.v_proj
Pruning ...
256
iter 0, train loss 349.52838134765625, val loss None
iter 10, train loss 350.31646728515625, val loss None
iter 20, train loss 348.7396240234375, val loss None
iter 30, train loss 349.15374755859375, val loss None
iter 40, train loss 348.5959777832031, val loss None
iter 50, train loss 347.9755554199219, val loss None
iter 60, train loss 347.72137451171875, val loss None
iter 70, train loss 347.50213623046875, val loss None
iter 80, train loss 347.42669677734375, val loss None
iter 90, train loss 347.50262451171875, val loss None
best loss 347.1710510253906
not here
quantized in 31.676629304885864 seconds
36372 MiB free out of 48676 MiB total
30 self_attn.o_proj
Pruning ...
256
iter 0, train loss 37.34272003173828, val loss None
iter 10, train loss 31.863422393798828, val loss None
iter 20, train loss 30.10647201538086, val loss None
iter 30, train loss 28.78764533996582, val loss None
iter 40, train loss 27.951446533203125, val loss None
iter 50, train loss 27.742469787597656, val loss None
iter 60, train loss 27.534622192382812, val loss None
iter 70, train loss 27.210514068603516, val loss None
iter 80, train loss 27.211090087890625, val loss None
iter 90, train loss 27.033546447753906, val loss None
best loss 26.87646484375
not here
quantized in 32.09437394142151 seconds
36308 MiB free out of 48676 MiB total
30 mlp.gate_proj
Pruning ...
256
iter 0, train loss 727.4953002929688, val loss None
iter 10, train loss 759.9996337890625, val loss None
iter 20, train loss 721.849609375, val loss None
iter 30, train loss 708.6885986328125, val loss None
iter 40, train loss 703.0330200195312, val loss None
iter 50, train loss 700.417236328125, val loss None
iter 60, train loss 700.1360473632812, val loss None
iter 70, train loss 700.831787109375, val loss None
iter 80, train loss 701.2022705078125, val loss None
iter 90, train loss 701.4548950195312, val loss None
best loss 669.8668212890625
not here
quantized in 87.35239887237549 seconds
35920 MiB free out of 48676 MiB total
30 mlp.up_proj
Pruning ...
256
iter 0, train loss 630.1407470703125, val loss None
iter 10, train loss 663.9262084960938, val loss None
iter 20, train loss 630.8645629882812, val loss None
iter 30, train loss 616.2108154296875, val loss None
iter 40, train loss 612.36083984375, val loss None
iter 50, train loss 608.9769897460938, val loss None
iter 60, train loss 607.9696044921875, val loss None
iter 70, train loss 606.6328735351562, val loss None
iter 80, train loss 605.9234619140625, val loss None
iter 90, train loss 605.1930541992188, val loss None
best loss 589.2362670898438
not here
quantized in 86.96645665168762 seconds
35640 MiB free out of 48676 MiB total
30 mlp.down_proj
Pruning ...
256
iter 0, train loss 47.617088317871094, val loss None
iter 10, train loss 48.393959045410156, val loss None
iter 20, train loss 47.683143615722656, val loss None
iter 30, train loss 47.49302291870117, val loss None
iter 40, train loss 47.25790786743164, val loss None
iter 50, train loss 47.16632843017578, val loss None
iter 60, train loss 47.0950927734375, val loss None
iter 70, train loss 46.85137176513672, val loss None
iter 80, train loss 46.782623291015625, val loss None
iter 90, train loss 46.61746597290039, val loss None
best loss 46.599876403808594
not here
quantized in 89.52446746826172 seconds
35360 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35360 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31264 MiB free out of 48676 MiB total
after cast to cpu
38316 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
31 self_attn.q_proj
Pruning ...
256
iter 0, train loss 338.1169128417969, val loss None
iter 10, train loss 350.7038269042969, val loss None
iter 20, train loss 344.4659118652344, val loss None
iter 30, train loss 335.3258056640625, val loss None
iter 40, train loss 330.6491394042969, val loss None
iter 50, train loss 327.37701416015625, val loss None
iter 60, train loss 325.8333740234375, val loss None
iter 70, train loss 325.4200134277344, val loss None
iter 80, train loss 324.7505187988281, val loss None
iter 90, train loss 324.2857666015625, val loss None
best loss 288.80194091796875
not here
quantized in 35.21145272254944 seconds
36392 MiB free out of 48676 MiB total
31 self_attn.k_proj
Pruning ...
256
iter 0, train loss 399.2658996582031, val loss None
iter 10, train loss 400.8996276855469, val loss None
iter 20, train loss 392.7237854003906, val loss None
iter 30, train loss 380.529296875, val loss None
iter 40, train loss 374.5780029296875, val loss None
iter 50, train loss 370.3291015625, val loss None
iter 60, train loss 367.4553527832031, val loss None
iter 70, train loss 366.59820556640625, val loss None
iter 80, train loss 366.15802001953125, val loss None
iter 90, train loss 365.9901123046875, val loss None
best loss 318.2715759277344
not here
quantized in 34.66980051994324 seconds
36414 MiB free out of 48676 MiB total
31 self_attn.v_proj
Pruning ...
256
iter 0, train loss 203.207763671875, val loss None
iter 10, train loss 203.62437438964844, val loss None
iter 20, train loss 203.73695373535156, val loss None
iter 30, train loss 202.9208221435547, val loss None
iter 40, train loss 203.76553344726562, val loss None
iter 50, train loss 203.32809448242188, val loss None
iter 60, train loss 202.70822143554688, val loss None
iter 70, train loss 202.88047790527344, val loss None
iter 80, train loss 203.18600463867188, val loss None
iter 90, train loss 203.0242919921875, val loss None
best loss 202.70822143554688
not here
quantized in 31.76513361930847 seconds
36404 MiB free out of 48676 MiB total
31 self_attn.o_proj
Pruning ...
256
iter 0, train loss 236.12002563476562, val loss None
iter 10, train loss 173.6438751220703, val loss None
iter 20, train loss 146.63192749023438, val loss None
iter 30, train loss 124.68778991699219, val loss None
iter 40, train loss 105.64778900146484, val loss None
iter 50, train loss 95.19120025634766, val loss None
iter 60, train loss 88.145751953125, val loss None
iter 70, train loss 85.31766510009766, val loss None
iter 80, train loss 84.98880004882812, val loss None
iter 90, train loss 84.08381652832031, val loss None
best loss 83.87060546875
not here
quantized in 33.38854765892029 seconds
36340 MiB free out of 48676 MiB total
31 mlp.gate_proj
Pruning ...
256
iter 0, train loss 681.6455688476562, val loss None
iter 10, train loss 683.573974609375, val loss None
iter 20, train loss 660.864013671875, val loss None
iter 30, train loss 642.3696899414062, val loss None
iter 40, train loss 638.155029296875, val loss None
iter 50, train loss 638.4215698242188, val loss None
iter 60, train loss 637.2999267578125, val loss None
iter 70, train loss 635.7183837890625, val loss None
iter 80, train loss 635.217041015625, val loss None
iter 90, train loss 635.134033203125, val loss None
best loss 581.6270751953125
not here
quantized in 88.93768835067749 seconds
36038 MiB free out of 48676 MiB total
31 mlp.up_proj
Pruning ...
256
iter 0, train loss 662.4013671875, val loss None
iter 10, train loss 653.7120361328125, val loss None
iter 20, train loss 642.7015380859375, val loss None
iter 30, train loss 619.6610717773438, val loss None
iter 40, train loss 605.7496337890625, val loss None
iter 50, train loss 596.5802001953125, val loss None
iter 60, train loss 592.3817138671875, val loss None
iter 70, train loss 591.60595703125, val loss None
iter 80, train loss 590.23486328125, val loss None
iter 90, train loss 589.9345092773438, val loss None
best loss 536.729248046875
not here
quantized in 92.16147685050964 seconds
35844 MiB free out of 48676 MiB total
31 mlp.down_proj
Pruning ...
256
iter 0, train loss 103.9017105102539, val loss None
iter 10, train loss 109.86329650878906, val loss None
iter 20, train loss 103.74380493164062, val loss None
iter 30, train loss 99.8370361328125, val loss None
iter 40, train loss 96.86021423339844, val loss None
iter 50, train loss 94.64994812011719, val loss None
iter 60, train loss 93.02510070800781, val loss None
iter 70, train loss 91.76803588867188, val loss None
iter 80, train loss 91.27192687988281, val loss None
iter 90, train loss 91.19799041748047, val loss None
best loss 90.1849365234375
not here
quantized in 91.3823971748352 seconds
35650 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35650 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31554 MiB free out of 48676 MiB total
after cast to cpu
38316 MiB free out of 48676 MiB total
Total bits: 13017415680.0 Total params: 6476005376
average bits per value: 2.0100995790155443
total time taken: 13203.39620065689
Loading tokenizer for meta-llama/Llama-2-7b-hf
/home/lliu/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Dataset: wikitext2
Evaluating ...
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
Perplexity: 7.434139
