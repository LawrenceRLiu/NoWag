/home/lliu/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.15s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.18it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.12it/s]
Model loaded. LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
4096
Loading tokenizer for meta-llama/Llama-2-7b-hf
Starting...
40099 MiB free out of 48676 MiB total
attention_mask torch.Size([1, 1, 4096, 4096]) cuda:7 torch.float16
position_ids torch.Size([1, 4096]) cuda:7 torch.int64
position_ids torch.Size([1, 4096])
attention_mask torch.Size([1, 1, 4096, 4096])
Ready.
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
layer0: self_attn.q_proj
norm_0 tensor([0.8848, 0.6528, 0.2561,  ..., 0.5371, 0.5562, 0.4512], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.7427, 1.1660, 1.0791,  ..., 1.8926, 1.8721, 1.3779], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 1.2158797979354858, val loss None, lr 0.001
iter 10, train loss 1.0846924781799316, val loss None, lr 0.001
iter 20, train loss 1.0104711055755615, val loss None, lr 0.001
iter 30, train loss 1.0925076007843018, val loss None, lr 0.001
iter 40, train loss 1.0313023328781128, val loss None, lr 0.001
iter 50, train loss 0.9250369668006897, val loss None, lr 0.001
iter 60, train loss 0.8796370625495911, val loss None, lr 0.001
iter 70, train loss 0.8871908783912659, val loss None, lr 0.001
iter 80, train loss 0.8730461597442627, val loss None, lr 0.001
iter 90, train loss 0.8842543363571167, val loss None, lr 0.001
best loss 0.8519408106803894
layer0: self_attn.k_proj
norm_0 tensor([1.1758, 0.8267, 0.3096,  ..., 0.5889, 0.6553, 0.5815], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.8984, 1.2832, 1.1631,  ..., 0.9399, 1.2178, 0.8291], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 0.9143508672714233, val loss None, lr 0.001
iter 10, train loss 0.7886734008789062, val loss None, lr 0.001
iter 20, train loss 0.7405134439468384, val loss None, lr 0.001
iter 30, train loss 0.7310048341751099, val loss None, lr 0.001
iter 40, train loss 0.7108989953994751, val loss None, lr 0.001
iter 50, train loss 0.6999050378799438, val loss None, lr 0.001
iter 60, train loss 0.6941316723823547, val loss None, lr 0.001
iter 70, train loss 0.6568111181259155, val loss None, lr 0.001
iter 80, train loss 0.6660761833190918, val loss None, lr 0.001
iter 90, train loss 0.6518781781196594, val loss None, lr 0.001
best loss 0.6358321309089661
layer0: self_attn.v_proj
norm_0 tensor([0.7236, 0.7617, 0.5034,  ..., 0.7656, 0.7676, 0.6851], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.6997, 0.7095, 0.6719,  ..., 0.6890, 0.7085, 0.7065], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 0.09465141594409943, val loss None, lr 0.001
iter 10, train loss 0.08290598541498184, val loss None, lr 0.001
iter 20, train loss 0.079757459461689, val loss None, lr 0.001
iter 30, train loss 0.07398471981287003, val loss None, lr 0.001
iter 40, train loss 0.07243078202009201, val loss None, lr 0.001
iter 50, train loss 0.0710257962346077, val loss None, lr 0.001
iter 60, train loss 0.06978785991668701, val loss None, lr 0.001
iter 70, train loss 0.07047859579324722, val loss None, lr 0.001
iter 80, train loss 0.06830619275569916, val loss None, lr 0.001
iter 90, train loss 0.0676121637225151, val loss None, lr 0.001
best loss 0.06716331839561462
layer0: self_attn.o_proj
norm_0 tensor([0.2800, 0.2820, 0.2651,  ..., 0.2622, 0.2673, 0.2649], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.8887, 0.9336, 0.9106,  ..., 0.9902, 0.9199, 0.9688], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 0.010639030486345291, val loss None, lr 0.001
iter 10, train loss 0.00955622736364603, val loss None, lr 0.001
iter 20, train loss 0.009270073845982552, val loss None, lr 0.001
iter 30, train loss 0.008739786222577095, val loss None, lr 0.001
iter 40, train loss 0.008702765218913555, val loss None, lr 0.001
iter 50, train loss 0.008588330820202827, val loss None, lr 0.001
iter 60, train loss 0.008505516685545444, val loss None, lr 0.001
iter 70, train loss 0.008244041353464127, val loss None, lr 0.001
iter 80, train loss 0.008258914574980736, val loss None, lr 0.001
iter 90, train loss 0.00819958932697773, val loss None, lr 0.001
best loss 0.007978186011314392
layer0: mlp.gate_proj
norm_0 tensor([1.6787, 1.7383, 1.7090,  ..., 1.7266, 1.7061, 1.7061], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.6108, 0.6123, 0.6055,  ..., 0.6016, 0.6216, 0.6255], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 3.913414478302002, val loss None, lr 0.001
iter 10, train loss 4.117529392242432, val loss None, lr 0.001
iter 20, train loss 3.9085304737091064, val loss None, lr 0.001
iter 30, train loss 3.911072254180908, val loss None, lr 0.001
iter 40, train loss 3.8913440704345703, val loss None, lr 0.001
iter 50, train loss 3.8604817390441895, val loss None, lr 0.001
iter 60, train loss 3.86551833152771, val loss None, lr 0.001
iter 70, train loss 3.8671436309814453, val loss None, lr 0.001
iter 80, train loss 3.864445686340332, val loss None, lr 0.001
iter 90, train loss 3.8640944957733154, val loss None, lr 0.001
best loss 3.7355499267578125
layer0: mlp.up_proj
norm_0 tensor([1.6846, 1.6572, 1.7002,  ..., 1.7119, 1.7070, 1.6826], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.6309, 0.6152, 0.6260,  ..., 0.5967, 0.5894, 0.6016], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 3.55928373336792, val loss None, lr 0.001
iter 10, train loss 3.791386604309082, val loss None, lr 0.001
iter 20, train loss 3.568047046661377, val loss None, lr 0.001
iter 30, train loss 3.5695552825927734, val loss None, lr 0.001
iter 40, train loss 3.557133197784424, val loss None, lr 0.001
iter 50, train loss 3.5514302253723145, val loss None, lr 0.001
iter 60, train loss 3.5470938682556152, val loss None, lr 0.001
iter 70, train loss 3.5625596046447754, val loss None, lr 0.001
iter 80, train loss 3.5605275630950928, val loss None, lr 0.001
iter 90, train loss 3.5629420280456543, val loss None, lr 0.001
best loss 3.4052915573120117
layer0: mlp.down_proj
norm_0 tensor([1.0811, 1.0771, 1.0850,  ..., 1.0762, 1.0713, 1.0908], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.6289, 1.6006, 1.6270,  ..., 1.6133, 1.6123, 1.5957], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 0.010795352049171925, val loss None, lr 0.001
iter 10, train loss 0.012119828723371029, val loss None, lr 0.001
iter 20, train loss 0.011741011403501034, val loss None, lr 0.001
iter 30, train loss 0.01144045777618885, val loss None, lr 0.001
iter 40, train loss 0.011209355667233467, val loss None, lr 0.001
iter 50, train loss 0.010985336266458035, val loss None, lr 0.001
iter 60, train loss 0.010839631780982018, val loss None, lr 0.001
iter 70, train loss 0.010790310800075531, val loss None, lr 0.001
iter 80, train loss 0.010729474946856499, val loss None, lr 0.001
iter 90, train loss 0.010681116953492165, val loss None, lr 0.001
best loss 0.010184906423091888
40099 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
32049 MiB free out of 48676 MiB total
after cast to cpu
37397 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
layer1: self_attn.q_proj
norm_0 tensor([1.9258, 1.9043, 1.8604,  ..., 1.6787, 1.9346, 1.6709], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.1504, 0.9507, 1.1025,  ..., 0.2090, 0.2136, 0.2512], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 17.008289337158203, val loss None, lr 0.001
iter 10, train loss 14.121826171875, val loss None, lr 0.001
iter 20, train loss 16.76382827758789, val loss None, lr 0.001
iter 30, train loss 15.052654266357422, val loss None, lr 0.001
iter 40, train loss 14.08888053894043, val loss None, lr 0.001
iter 50, train loss 14.931838989257812, val loss None, lr 0.001
iter 60, train loss 15.380973815917969, val loss None, lr 0.001
iter 70, train loss 14.29894733428955, val loss None, lr 0.001
iter 80, train loss 14.26708984375, val loss None, lr 0.001
iter 90, train loss 13.94528579711914, val loss None, lr 0.001
best loss 13.636051177978516
layer1: self_attn.k_proj
norm_0 tensor([2.0254, 1.9668, 2.0371,  ..., 1.6631, 1.8975, 1.8086], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.9346, 1.0586, 1.0898,  ..., 0.3875, 0.3777, 0.3123], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 17.603097915649414, val loss None, lr 0.001
iter 10, train loss 14.870248794555664, val loss None, lr 0.001
iter 20, train loss 13.979528427124023, val loss None, lr 0.001
iter 30, train loss 14.192575454711914, val loss None, lr 0.001
iter 40, train loss 14.574735641479492, val loss None, lr 0.001
iter 50, train loss 13.689945220947266, val loss None, lr 0.001
iter 60, train loss 13.646377563476562, val loss None, lr 0.001
iter 70, train loss 13.413948059082031, val loss None, lr 0.001
iter 80, train loss 13.365093231201172, val loss None, lr 0.001
iter 90, train loss 13.255354881286621, val loss None, lr 0.001
best loss 13.217973709106445
layer1: self_attn.v_proj
norm_0 tensor([0.5439, 0.5366, 0.5508,  ..., 0.6504, 0.5752, 0.6440], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.3213, 1.3359, 1.2393,  ..., 0.5474, 0.5635, 0.5596], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 0.9167270660400391, val loss None, lr 0.001
iter 10, train loss 0.9515395164489746, val loss None, lr 0.001
iter 20, train loss 0.8683196902275085, val loss None, lr 0.001
iter 30, train loss 0.8455976247787476, val loss None, lr 0.001
iter 40, train loss 0.8339589834213257, val loss None, lr 0.001
iter 50, train loss 0.8212147951126099, val loss None, lr 0.001
iter 60, train loss 0.8210696578025818, val loss None, lr 0.001
iter 70, train loss 0.8147401809692383, val loss None, lr 0.001
iter 80, train loss 0.8084214329719543, val loss None, lr 0.001
iter 90, train loss 0.8057594299316406, val loss None, lr 0.001
best loss 0.803149938583374
layer1: self_attn.o_proj
norm_0 tensor([0.8604, 0.8032, 0.6895,  ..., 0.2113, 0.2167, 0.2192], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.0117, 1.0215, 0.9888,  ..., 0.9995, 0.9941, 0.9839], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 0.0815332680940628, val loss None, lr 0.001
iter 10, train loss 0.07673665881156921, val loss None, lr 0.001
iter 20, train loss 0.0745716542005539, val loss None, lr 0.001
iter 30, train loss 0.07342763245105743, val loss None, lr 0.001
iter 40, train loss 0.07350338995456696, val loss None, lr 0.001
iter 50, train loss 0.07417591661214828, val loss None, lr 0.001
iter 60, train loss 0.07372230291366577, val loss None, lr 0.001
iter 70, train loss 0.07309012860059738, val loss None, lr 0.001
iter 80, train loss 0.0717211365699768, val loss None, lr 0.001
iter 90, train loss 0.07176034897565842, val loss None, lr 0.001
best loss 0.07145131379365921
layer1: mlp.gate_proj
norm_0 tensor([1.9492, 1.9355, 1.8818,  ..., 1.9443, 1.8994, 1.8916], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5796, 0.5811, 0.6069,  ..., 0.6011, 0.5981, 0.6099], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 16.576993942260742, val loss None, lr 0.001
iter 10, train loss 18.137508392333984, val loss None, lr 0.001
iter 20, train loss 17.412519454956055, val loss None, lr 0.001
iter 30, train loss 17.135644912719727, val loss None, lr 0.001
iter 40, train loss 16.980501174926758, val loss None, lr 0.001
iter 50, train loss 16.945079803466797, val loss None, lr 0.001
iter 60, train loss 16.998233795166016, val loss None, lr 0.001
iter 70, train loss 16.923709869384766, val loss None, lr 0.001
iter 80, train loss 16.975296020507812, val loss None, lr 0.001
iter 90, train loss 16.967315673828125, val loss None, lr 0.001
best loss 15.570550918579102
layer1: mlp.up_proj
norm_0 tensor([1.7627, 1.8057, 1.8271,  ..., 1.8008, 1.8066, 1.8203], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.6089, 0.6025, 0.6172,  ..., 0.6206, 0.6221, 0.6230], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 13.382257461547852, val loss None, lr 0.001
iter 10, train loss 13.87748908996582, val loss None, lr 0.001
iter 20, train loss 13.654226303100586, val loss None, lr 0.001
iter 30, train loss 13.635889053344727, val loss None, lr 0.001
iter 40, train loss 13.667500495910645, val loss None, lr 0.001
iter 50, train loss 13.683130264282227, val loss None, lr 0.001
iter 60, train loss 13.729964256286621, val loss None, lr 0.001
iter 70, train loss 13.738954544067383, val loss None, lr 0.001
iter 80, train loss 13.774079322814941, val loss None, lr 0.001
iter 90, train loss 13.81400203704834, val loss None, lr 0.001
best loss 13.249776840209961
layer1: mlp.down_proj
norm_0 tensor([1.1104, 1.1094, 1.1230,  ..., 1.1250, 1.1289, 1.1299], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.6289, 1.6357, 1.6455,  ..., 1.6387, 1.6582, 1.6318], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 0.08748472481966019, val loss None, lr 0.001
iter 10, train loss 0.244841068983078, val loss None, lr 0.001
iter 20, train loss 0.5223595499992371, val loss None, lr 0.001
iter 30, train loss 0.37519893050193787, val loss None, lr 0.001
iter 40, train loss 0.5410573482513428, val loss None, lr 0.001
iter 50, train loss 0.34311509132385254, val loss None, lr 0.001
iter 60, train loss 0.8010027408599854, val loss None, lr 0.001
iter 70, train loss 1.0492695569992065, val loss None, lr 0.001
iter 80, train loss 4.346827507019043, val loss None, lr 0.001
iter 90, train loss 2.378401279449463, val loss None, lr 0.001
best loss 0.08748472481966019
37397 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
30657 MiB free out of 48676 MiB total
after cast to cpu
37719 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
layer2: self_attn.q_proj
norm_0 tensor([1.6846, 1.7197, 1.6816,  ..., 1.7490, 1.6240, 1.6484], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5840, 0.7588, 0.9141,  ..., 1.2471, 0.3936, 1.1914], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 62.09470748901367, val loss None, lr 0.001
iter 10, train loss 62.838138580322266, val loss None, lr 0.001
iter 20, train loss 62.995506286621094, val loss None, lr 0.001
iter 30, train loss 62.896278381347656, val loss None, lr 0.001
iter 40, train loss 62.362998962402344, val loss None, lr 0.001
iter 50, train loss 62.49680709838867, val loss None, lr 0.001
iter 60, train loss 61.43412780761719, val loss None, lr 0.001
iter 70, train loss 61.477054595947266, val loss None, lr 0.001
iter 80, train loss 61.597171783447266, val loss None, lr 0.001
iter 90, train loss 62.08702087402344, val loss None, lr 0.001
best loss 56.8681526184082
layer2: self_attn.k_proj
norm_0 tensor([1.7842, 1.7520, 1.8242,  ..., 1.7949, 1.7520, 1.7822], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.3757, 0.7593, 0.8696,  ..., 1.4326, 0.3281, 1.3086], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 73.1230697631836, val loss None, lr 0.001
iter 10, train loss 70.44355010986328, val loss None, lr 0.001
iter 20, train loss 73.83193969726562, val loss None, lr 0.001
iter 30, train loss 72.49238586425781, val loss None, lr 0.001
iter 40, train loss 72.21116638183594, val loss None, lr 0.001
iter 50, train loss 71.52485656738281, val loss None, lr 0.001
iter 60, train loss 71.58109283447266, val loss None, lr 0.001
iter 70, train loss 71.31780242919922, val loss None, lr 0.001
iter 80, train loss 71.38543701171875, val loss None, lr 0.001
iter 90, train loss 71.70722198486328, val loss None, lr 0.001
best loss 65.81355285644531
layer2: self_attn.v_proj
norm_0 tensor([0.9292, 0.9331, 0.9194,  ..., 0.9048, 0.9575, 0.9600], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.0322, 1.0566, 1.0547,  ..., 0.9229, 1.0654, 1.0361], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 13.735190391540527, val loss None, lr 0.001
iter 10, train loss 14.37731647491455, val loss None, lr 0.001
iter 20, train loss 13.915281295776367, val loss None, lr 0.001
iter 30, train loss 13.919547080993652, val loss None, lr 0.001
iter 40, train loss 13.80603313446045, val loss None, lr 0.001
iter 50, train loss 13.731244087219238, val loss None, lr 0.001
iter 60, train loss 13.745625495910645, val loss None, lr 0.001
iter 70, train loss 13.724837303161621, val loss None, lr 0.001
iter 80, train loss 13.725214004516602, val loss None, lr 0.001
iter 90, train loss 13.690506935119629, val loss None, lr 0.001
best loss 13.498883247375488
layer2: self_attn.o_proj
norm_0 tensor([0.9219, 0.9819, 0.9458,  ..., 0.9478, 0.9624, 0.9248], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.9941, 1.0049, 1.0010,  ..., 0.9858, 0.9736, 0.9907], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 0.12758037447929382, val loss None, lr 0.001
iter 10, train loss 0.1278332769870758, val loss None, lr 0.001
iter 20, train loss 0.12577509880065918, val loss None, lr 0.001
iter 30, train loss 0.12497429549694061, val loss None, lr 0.001
iter 40, train loss 0.12530630826950073, val loss None, lr 0.001
iter 50, train loss 0.12440987676382065, val loss None, lr 0.001
iter 60, train loss 0.12534622848033905, val loss None, lr 0.001
iter 70, train loss 0.12458314001560211, val loss None, lr 0.001
iter 80, train loss 0.12509188055992126, val loss None, lr 0.001
iter 90, train loss 0.12464609742164612, val loss None, lr 0.001
best loss 0.12411472201347351
layer2: mlp.gate_proj
norm_0 tensor([1.9473, 1.9883, 1.9746,  ..., 1.9814, 1.9824, 1.9443], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5986, 0.6045, 0.6274,  ..., 0.6235, 0.6045, 0.5771], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 33.485755920410156, val loss None, lr 0.001
iter 10, train loss 34.549034118652344, val loss None, lr 0.001
iter 20, train loss 33.85271453857422, val loss None, lr 0.001
iter 30, train loss 33.824337005615234, val loss None, lr 0.001
iter 40, train loss 33.969200134277344, val loss None, lr 0.001
iter 50, train loss 34.00341033935547, val loss None, lr 0.001
iter 60, train loss 34.01054382324219, val loss None, lr 0.001
iter 70, train loss 34.058258056640625, val loss None, lr 0.001
iter 80, train loss 34.107913970947266, val loss None, lr 0.001
iter 90, train loss 34.18264389038086, val loss None, lr 0.001
best loss 32.92169189453125
layer2: mlp.up_proj
norm_0 tensor([1.8447, 1.8203, 1.8359,  ..., 1.8311, 1.8301, 1.8623], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.6143, 0.6196, 0.6221,  ..., 0.6240, 0.5981, 0.5825], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 27.507858276367188, val loss None, lr 0.001
iter 10, train loss 27.731985092163086, val loss None, lr 0.001
iter 20, train loss 27.83414649963379, val loss None, lr 0.001
iter 30, train loss 27.878826141357422, val loss None, lr 0.001
iter 40, train loss 27.90041732788086, val loss None, lr 0.001
iter 50, train loss 27.919992446899414, val loss None, lr 0.001
iter 60, train loss 27.91026496887207, val loss None, lr 0.001
iter 70, train loss 27.972566604614258, val loss None, lr 0.001
iter 80, train loss 28.030960083007812, val loss None, lr 0.001
iter 90, train loss 28.029361724853516, val loss None, lr 0.001
best loss 27.416967391967773
layer2: mlp.down_proj
norm_0 tensor([1.1338, 1.1387, 1.1426,  ..., 1.1465, 1.1143, 1.0684], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.6309, 1.6396, 1.6504,  ..., 1.6455, 1.6660, 1.6436], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 0.1533115804195404, val loss None, lr 0.001
iter 10, train loss 0.15356522798538208, val loss None, lr 0.001
iter 20, train loss 0.15298454463481903, val loss None, lr 0.001
iter 30, train loss 0.15223245322704315, val loss None, lr 0.001
iter 40, train loss 0.15201863646507263, val loss None, lr 0.001
iter 50, train loss 0.1519765406847, val loss None, lr 0.001
iter 60, train loss 0.15196160972118378, val loss None, lr 0.001
iter 70, train loss 0.15202952921390533, val loss None, lr 0.001
iter 80, train loss 0.1523302048444748, val loss None, lr 0.001
iter 90, train loss 0.1523340344429016, val loss None, lr 0.001
best loss 0.1518736034631729
37719 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
30379 MiB free out of 48676 MiB total
after cast to cpu
37375 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
layer3: self_attn.q_proj
norm_0 tensor([1.5566, 1.6201, 1.6104,  ..., 1.6201, 1.6006, 1.6182], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.7285, 0.8521, 0.8237,  ..., 1.6670, 1.7910, 1.8848], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 145.43572998046875, val loss None, lr 0.001
iter 10, train loss 154.4141845703125, val loss None, lr 0.001
iter 20, train loss 148.6595916748047, val loss None, lr 0.001
iter 30, train loss 144.86773681640625, val loss None, lr 0.001
iter 40, train loss 145.73843383789062, val loss None, lr 0.001
iter 50, train loss 145.81529235839844, val loss None, lr 0.001
iter 60, train loss 146.26931762695312, val loss None, lr 0.001
iter 70, train loss 145.61952209472656, val loss None, lr 0.001
iter 80, train loss 146.28668212890625, val loss None, lr 0.001
iter 90, train loss 145.07754516601562, val loss None, lr 0.001
best loss 138.330078125
layer3: self_attn.k_proj
norm_0 tensor([1.6465, 1.6738, 1.6650,  ..., 1.6475, 1.6562, 1.6846], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.6733, 0.8022, 0.7798,  ..., 1.6133, 1.7500, 1.8154], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 162.97610473632812, val loss None, lr 0.001
iter 10, train loss 170.63241577148438, val loss None, lr 0.001
iter 20, train loss 169.9893798828125, val loss None, lr 0.001
iter 30, train loss 164.556396484375, val loss None, lr 0.001
iter 40, train loss 163.9775390625, val loss None, lr 0.001
iter 50, train loss 165.08871459960938, val loss None, lr 0.001
iter 60, train loss 165.06146240234375, val loss None, lr 0.001
iter 70, train loss 163.6100311279297, val loss None, lr 0.001
iter 80, train loss 162.97152709960938, val loss None, lr 0.001
iter 90, train loss 164.2910919189453, val loss None, lr 0.001
best loss 154.24215698242188
layer3: self_attn.v_proj
norm_0 tensor([0.8999, 0.8760, 0.8857,  ..., 0.8799, 0.8950, 0.8877], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.9341, 0.9414, 0.9292,  ..., 0.4678, 0.5034, 0.4414], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 35.30034255981445, val loss None, lr 0.001
iter 10, train loss 35.81382751464844, val loss None, lr 0.001
iter 20, train loss 35.41914749145508, val loss None, lr 0.001
iter 30, train loss 35.24845504760742, val loss None, lr 0.001
iter 40, train loss 35.262332916259766, val loss None, lr 0.001
iter 50, train loss 35.29867935180664, val loss None, lr 0.001
iter 60, train loss 35.24021911621094, val loss None, lr 0.001
iter 70, train loss 35.243309020996094, val loss None, lr 0.001
iter 80, train loss 35.35696029663086, val loss None, lr 0.001
iter 90, train loss 35.38486099243164, val loss None, lr 0.001
best loss 35.078025817871094
layer3: self_attn.o_proj
norm_0 tensor([0.8149, 0.8115, 0.7832,  ..., 0.3369, 0.3589, 0.3250], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.9824, 1.0029, 0.9785,  ..., 0.9951, 0.9751, 1.0020], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 0.3065566420555115, val loss None, lr 0.001
iter 10, train loss 0.2740793228149414, val loss None, lr 0.001
iter 20, train loss 0.262952983379364, val loss None, lr 0.001
iter 30, train loss 0.25360581278800964, val loss None, lr 0.001
iter 40, train loss 0.2508770823478699, val loss None, lr 0.001
iter 50, train loss 0.24676376581192017, val loss None, lr 0.001
iter 60, train loss 0.24481335282325745, val loss None, lr 0.001
iter 70, train loss 0.2443711906671524, val loss None, lr 0.001
iter 80, train loss 0.24571926891803741, val loss None, lr 0.001
iter 90, train loss 0.2459568828344345, val loss None, lr 0.001
best loss 0.2443711906671524
layer3: mlp.gate_proj
norm_0 tensor([1.9883, 1.9893, 1.9834,  ..., 2.0176, 1.9902, 1.9932], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5977, 0.6079, 0.6357,  ..., 0.5967, 0.5039, 0.6211], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 53.261924743652344, val loss None, lr 0.001
iter 10, train loss 54.20703125, val loss None, lr 0.001
iter 20, train loss 54.05647277832031, val loss None, lr 0.001
iter 30, train loss 54.01605224609375, val loss None, lr 0.001
iter 40, train loss 54.187801361083984, val loss None, lr 0.001
iter 50, train loss 54.31703186035156, val loss None, lr 0.001
iter 60, train loss 54.364479064941406, val loss None, lr 0.001
iter 70, train loss 54.53445816040039, val loss None, lr 0.001
iter 80, train loss 54.49092102050781, val loss None, lr 0.001
iter 90, train loss 54.580482482910156, val loss None, lr 0.001
best loss 52.920135498046875
layer3: mlp.up_proj
norm_0 tensor([1.8574, 1.8525, 1.8643,  ..., 1.8350, 1.8555, 1.8564], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.6064, 0.6235, 0.6128,  ..., 0.6118, 0.5352, 0.6128], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 44.07307434082031, val loss None, lr 0.001
iter 10, train loss 44.14350891113281, val loss None, lr 0.001
iter 20, train loss 44.25180435180664, val loss None, lr 0.001
iter 30, train loss 44.411376953125, val loss None, lr 0.001
iter 40, train loss 44.464111328125, val loss None, lr 0.001
iter 50, train loss 44.49323654174805, val loss None, lr 0.001
iter 60, train loss 44.456111907958984, val loss None, lr 0.001
iter 70, train loss 44.471588134765625, val loss None, lr 0.001
iter 80, train loss 44.433319091796875, val loss None, lr 0.001
iter 90, train loss 44.40648651123047, val loss None, lr 0.001
best loss 44.07307434082031
layer3: mlp.down_proj
norm_0 tensor([1.1055, 1.1504, 1.1299,  ..., 1.1318, 0.8623, 1.1318], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.6416, 1.6357, 1.6602,  ..., 1.6182, 1.6387, 1.6445], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 0.2958294153213501, val loss None, lr 0.001
iter 10, train loss 0.2973429560661316, val loss None, lr 0.001
iter 20, train loss 0.2964603900909424, val loss None, lr 0.001
iter 30, train loss 0.29518476128578186, val loss None, lr 0.001
iter 40, train loss 0.2939245104789734, val loss None, lr 0.001
iter 50, train loss 0.29384204745292664, val loss None, lr 0.001
iter 60, train loss 0.29426413774490356, val loss None, lr 0.001
iter 70, train loss 0.29426270723342896, val loss None, lr 0.001
iter 80, train loss 0.29447126388549805, val loss None, lr 0.001
iter 90, train loss 0.29468104243278503, val loss None, lr 0.001
best loss 0.293756902217865
37375 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
29979 MiB free out of 48676 MiB total
after cast to cpu
37095 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
layer4: self_attn.q_proj
norm_0 tensor([1.6592, 1.6807, 1.6289,  ..., 1.6611, 1.7061, 1.7139], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5449, 0.8042, 0.8550,  ..., 1.3770, 1.3496, 1.3174], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 133.388916015625, val loss None, lr 0.001
iter 10, train loss 142.79916381835938, val loss None, lr 0.001
iter 20, train loss 138.01351928710938, val loss None, lr 0.001
iter 30, train loss 136.51937866210938, val loss None, lr 0.001
iter 40, train loss 136.06863403320312, val loss None, lr 0.001
iter 50, train loss 135.88983154296875, val loss None, lr 0.001
iter 60, train loss 137.38754272460938, val loss None, lr 0.001
iter 70, train loss 137.30389404296875, val loss None, lr 0.001
iter 80, train loss 136.4833984375, val loss None, lr 0.001
iter 90, train loss 136.96839904785156, val loss None, lr 0.001
best loss 128.59915161132812
layer4: self_attn.k_proj
norm_0 tensor([1.6924, 1.7266, 1.7168,  ..., 1.6689, 1.7119, 1.7129], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5698, 0.7725, 0.8120,  ..., 1.4160, 1.3164, 1.4375], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 147.00523376464844, val loss None, lr 0.001
iter 10, train loss 156.61788940429688, val loss None, lr 0.001
iter 20, train loss 153.10360717773438, val loss None, lr 0.001
iter 30, train loss 150.701171875, val loss None, lr 0.001
iter 40, train loss 150.65354919433594, val loss None, lr 0.001
iter 50, train loss 151.23960876464844, val loss None, lr 0.001
iter 60, train loss 151.38540649414062, val loss None, lr 0.001
iter 70, train loss 151.58987426757812, val loss None, lr 0.001
iter 80, train loss 152.4023895263672, val loss None, lr 0.001
iter 90, train loss 152.76844787597656, val loss None, lr 0.001
best loss 140.32723999023438
layer4: self_attn.v_proj
norm_0 tensor([0.9341, 0.9048, 0.9458,  ..., 0.9385, 0.9043, 0.9126], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.9438, 0.9316, 0.9370,  ..., 1.0049, 0.9961, 1.0156], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 34.34602355957031, val loss None, lr 0.001
iter 10, train loss 34.681156158447266, val loss None, lr 0.001
iter 20, train loss 34.468589782714844, val loss None, lr 0.001
iter 30, train loss 34.26531982421875, val loss None, lr 0.001
iter 40, train loss 34.199527740478516, val loss None, lr 0.001
iter 50, train loss 34.151756286621094, val loss None, lr 0.001
iter 60, train loss 34.196678161621094, val loss None, lr 0.001
iter 70, train loss 34.22673797607422, val loss None, lr 0.001
iter 80, train loss 34.118682861328125, val loss None, lr 0.001
iter 90, train loss 34.11186218261719, val loss None, lr 0.001
best loss 34.08332824707031
layer4: self_attn.o_proj
norm_0 tensor([0.8374, 0.8276, 0.8369,  ..., 0.9082, 0.9082, 0.9243], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.9653, 0.9951, 1.0117,  ..., 0.9888, 0.9629, 0.9980], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 0.6371036767959595, val loss None, lr 0.001
iter 10, train loss 0.5700597763061523, val loss None, lr 0.001
iter 20, train loss 0.536036491394043, val loss None, lr 0.001
iter 30, train loss 0.5157836675643921, val loss None, lr 0.001
iter 40, train loss 0.5028129816055298, val loss None, lr 0.001
iter 50, train loss 0.5004938840866089, val loss None, lr 0.001
iter 60, train loss 0.49524617195129395, val loss None, lr 0.001
iter 70, train loss 0.48971593379974365, val loss None, lr 0.001
iter 80, train loss 0.486564576625824, val loss None, lr 0.001
iter 90, train loss 0.4895326495170593, val loss None, lr 0.001
best loss 0.486297070980072
layer4: mlp.gate_proj
norm_0 tensor([2.0293, 2.0391, 2.0410,  ..., 2.0449, 2.0215, 2.0449], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5967, 0.5986, 0.5771,  ..., 0.6255, 0.6001, 0.6206], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 76.7177963256836, val loss None, lr 0.001
iter 10, train loss 78.56661987304688, val loss None, lr 0.001
iter 20, train loss 78.01104736328125, val loss None, lr 0.001
iter 30, train loss 77.73048400878906, val loss None, lr 0.001
iter 40, train loss 77.70269012451172, val loss None, lr 0.001
iter 50, train loss 77.79876708984375, val loss None, lr 0.001
iter 60, train loss 77.69529724121094, val loss None, lr 0.001
iter 70, train loss 77.82656860351562, val loss None, lr 0.001
iter 80, train loss 77.6593246459961, val loss None, lr 0.001
iter 90, train loss 77.59447479248047, val loss None, lr 0.001
best loss 76.18474578857422
layer4: mlp.up_proj
norm_0 tensor([1.8447, 1.8350, 1.8320,  ..., 1.8330, 1.8428, 1.8271], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.6138, 0.6162, 0.6216,  ..., 0.6025, 0.6216, 0.6304], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 60.33223342895508, val loss None, lr 0.001
iter 10, train loss 60.480201721191406, val loss None, lr 0.001
iter 20, train loss 60.59012222290039, val loss None, lr 0.001
iter 30, train loss 60.66950225830078, val loss None, lr 0.001
iter 40, train loss 60.68223571777344, val loss None, lr 0.001
iter 50, train loss 60.696800231933594, val loss None, lr 0.001
iter 60, train loss 60.766700744628906, val loss None, lr 0.001
iter 70, train loss 60.88869094848633, val loss None, lr 0.001
iter 80, train loss 60.87681579589844, val loss None, lr 0.001
iter 90, train loss 60.95128631591797, val loss None, lr 0.001
best loss 60.33223342895508
layer4: mlp.down_proj
norm_0 tensor([1.1270, 1.1289, 1.1396,  ..., 1.0859, 1.1279, 1.1514], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.6475, 1.6377, 1.6514,  ..., 1.6406, 1.6230, 1.6436], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 0.5745708346366882, val loss None, lr 0.001
iter 10, train loss 0.5745313167572021, val loss None, lr 0.001
iter 20, train loss 0.5747172236442566, val loss None, lr 0.001
iter 30, train loss 0.5723694562911987, val loss None, lr 0.001
iter 40, train loss 0.5730376243591309, val loss None, lr 0.001
iter 50, train loss 0.5737578868865967, val loss None, lr 0.001
iter 60, train loss 0.5723878145217896, val loss None, lr 0.001
iter 70, train loss 0.5737634897232056, val loss None, lr 0.001
iter 80, train loss 0.572838544845581, val loss None, lr 0.001
iter 90, train loss 0.573926568031311, val loss None, lr 0.001
best loss 0.5722842812538147
37095 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
29549 MiB free out of 48676 MiB total
after cast to cpu
36751 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
layer5: self_attn.q_proj
norm_0 tensor([1.6846, 1.6924, 1.6426,  ..., 1.6523, 1.6660, 1.7373], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5879, 0.6504, 0.6724,  ..., 0.9902, 1.1523, 0.9600], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 153.9870147705078, val loss None, lr 0.001
iter 10, train loss 165.0623016357422, val loss None, lr 0.001
iter 20, train loss 157.54122924804688, val loss None, lr 0.001
iter 30, train loss 155.116455078125, val loss None, lr 0.001
iter 40, train loss 156.14328002929688, val loss None, lr 0.001
iter 50, train loss 155.085205078125, val loss None, lr 0.001
iter 60, train loss 154.60269165039062, val loss None, lr 0.001
iter 70, train loss 155.06814575195312, val loss None, lr 0.001
iter 80, train loss 155.21951293945312, val loss None, lr 0.001
iter 90, train loss 154.75047302246094, val loss None, lr 0.001
best loss 148.62200927734375
layer5: self_attn.k_proj
norm_0 tensor([1.7227, 1.7930, 1.7539,  ..., 1.6982, 1.7305, 1.7275], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5361, 0.6172, 0.6533,  ..., 0.8018, 1.5273, 1.3584], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 180.77484130859375, val loss None, lr 0.001
iter 10, train loss 186.03573608398438, val loss None, lr 0.001
iter 20, train loss 188.54873657226562, val loss None, lr 0.001
iter 30, train loss 183.83258056640625, val loss None, lr 0.001
iter 40, train loss 183.38136291503906, val loss None, lr 0.001
iter 50, train loss 180.882568359375, val loss None, lr 0.001
iter 60, train loss 179.82025146484375, val loss None, lr 0.001
iter 70, train loss 179.4271240234375, val loss None, lr 0.001
iter 80, train loss 181.54066467285156, val loss None, lr 0.001
iter 90, train loss 179.320556640625, val loss None, lr 0.001
best loss 171.43148803710938
layer5: self_attn.v_proj
norm_0 tensor([0.9814, 0.9243, 0.9575,  ..., 0.9756, 0.9688, 0.9268], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.0576, 1.0322, 1.0430,  ..., 0.9473, 0.9565, 0.9619], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 41.03614807128906, val loss None, lr 0.001
iter 10, train loss 41.237552642822266, val loss None, lr 0.001
iter 20, train loss 41.08057403564453, val loss None, lr 0.001
iter 30, train loss 40.924739837646484, val loss None, lr 0.001
iter 40, train loss 40.785987854003906, val loss None, lr 0.001
iter 50, train loss 40.862369537353516, val loss None, lr 0.001
iter 60, train loss 40.71672821044922, val loss None, lr 0.001
iter 70, train loss 40.79814147949219, val loss None, lr 0.001
iter 80, train loss 40.725284576416016, val loss None, lr 0.001
iter 90, train loss 40.78529357910156, val loss None, lr 0.001
best loss 40.702911376953125
layer5: self_attn.o_proj
norm_0 tensor([0.9814, 0.9512, 0.9639,  ..., 0.8447, 0.8657, 0.8770], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.9956, 1.0107, 0.9868,  ..., 0.9888, 0.9775, 0.9995], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 0.857437014579773, val loss None, lr 0.001
iter 10, train loss 0.8372859954833984, val loss None, lr 0.001
iter 20, train loss 0.8308442831039429, val loss None, lr 0.001
iter 30, train loss 0.8290838599205017, val loss None, lr 0.001
iter 40, train loss 0.8213716745376587, val loss None, lr 0.001
iter 50, train loss 0.8187330961227417, val loss None, lr 0.001
iter 60, train loss 0.8136773705482483, val loss None, lr 0.001
iter 70, train loss 0.8133170008659363, val loss None, lr 0.001
iter 80, train loss 0.8138758540153503, val loss None, lr 0.001
iter 90, train loss 0.8168619871139526, val loss None, lr 0.001
best loss 0.8133170008659363
layer5: mlp.gate_proj
norm_0 tensor([2.0527, 2.0410, 2.0508,  ..., 2.0723, 2.0312, 2.0566], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5820, 0.6226, 0.5684,  ..., 0.6562, 0.5903, 0.5723], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 99.8784408569336, val loss None, lr 0.001
iter 10, train loss 101.9640884399414, val loss None, lr 0.001
iter 20, train loss 101.44234466552734, val loss None, lr 0.001
iter 30, train loss 101.13533782958984, val loss None, lr 0.001
iter 40, train loss 100.81788635253906, val loss None, lr 0.001
iter 50, train loss 100.74009704589844, val loss None, lr 0.001
iter 60, train loss 100.73775482177734, val loss None, lr 0.001
iter 70, train loss 100.89058685302734, val loss None, lr 0.001
iter 80, train loss 100.81642150878906, val loss None, lr 0.001
iter 90, train loss 101.06172943115234, val loss None, lr 0.001
best loss 99.50238800048828
layer5: mlp.up_proj
norm_0 tensor([1.8369, 1.8330, 1.8232,  ..., 1.8311, 1.8477, 1.8047], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.6182, 0.6274, 0.6162,  ..., 0.6235, 0.6250, 0.6138], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 77.86698913574219, val loss None, lr 0.001
iter 10, train loss 78.05978393554688, val loss None, lr 0.001
iter 20, train loss 78.26082611083984, val loss None, lr 0.001
iter 30, train loss 78.42929077148438, val loss None, lr 0.001
iter 40, train loss 78.47976684570312, val loss None, lr 0.001
iter 50, train loss 78.54037475585938, val loss None, lr 0.001
iter 60, train loss 78.56533813476562, val loss None, lr 0.001
iter 70, train loss 78.69558715820312, val loss None, lr 0.001
iter 80, train loss 78.83148193359375, val loss None, lr 0.001
iter 90, train loss 78.89935302734375, val loss None, lr 0.001
best loss 77.86698913574219
layer5: mlp.down_proj
norm_0 tensor([1.1318, 1.1416, 1.1221,  ..., 1.1436, 1.1377, 1.1289], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.6475, 1.6191, 1.6123,  ..., 1.6553, 1.6387, 1.6582], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 0.8644929528236389, val loss None, lr 0.001
iter 10, train loss 0.868330717086792, val loss None, lr 0.001
iter 20, train loss 0.8644084930419922, val loss None, lr 0.001
iter 30, train loss 0.86234450340271, val loss None, lr 0.001
iter 40, train loss 0.86129230260849, val loss None, lr 0.001
iter 50, train loss 0.8623776435852051, val loss None, lr 0.001
iter 60, train loss 0.8624802827835083, val loss None, lr 0.001
iter 70, train loss 0.8639535307884216, val loss None, lr 0.001
iter 80, train loss 0.8643448352813721, val loss None, lr 0.001
iter 90, train loss 0.8652887940406799, val loss None, lr 0.001
best loss 0.8609989881515503
36751 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
29141 MiB free out of 48676 MiB total
after cast to cpu
36311 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
layer6: self_attn.q_proj
norm_0 tensor([1.5400, 1.5742, 1.6016,  ..., 1.5332, 1.5908, 1.6064], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.8418, 0.8530, 0.8296,  ..., 1.3203, 1.2705, 1.3652], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 228.75607299804688, val loss None, lr 0.001
iter 10, train loss 248.16714477539062, val loss None, lr 0.001
iter 20, train loss 239.05258178710938, val loss None, lr 0.001
iter 30, train loss 229.213623046875, val loss None, lr 0.001
iter 40, train loss 230.09959411621094, val loss None, lr 0.001
iter 50, train loss 231.741943359375, val loss None, lr 0.001
iter 60, train loss 230.511474609375, val loss None, lr 0.001
iter 70, train loss 231.3563690185547, val loss None, lr 0.001
iter 80, train loss 231.56378173828125, val loss None, lr 0.001
iter 90, train loss 230.1990509033203, val loss None, lr 0.001
best loss 212.76988220214844
layer6: self_attn.k_proj
norm_0 tensor([1.6113, 1.6699, 1.6338,  ..., 1.6250, 1.6094, 1.6221], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.8003, 0.8091, 0.8184,  ..., 1.2295, 1.2783, 1.3213], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 252.7486114501953, val loss None, lr 0.001
iter 10, train loss 263.75677490234375, val loss None, lr 0.001
iter 20, train loss 262.0121154785156, val loss None, lr 0.001
iter 30, train loss 250.42474365234375, val loss None, lr 0.001
iter 40, train loss 251.2268829345703, val loss None, lr 0.001
iter 50, train loss 247.9901885986328, val loss None, lr 0.001
iter 60, train loss 249.17343139648438, val loss None, lr 0.001
iter 70, train loss 248.10877990722656, val loss None, lr 0.001
iter 80, train loss 249.0186767578125, val loss None, lr 0.001
iter 90, train loss 248.85540771484375, val loss None, lr 0.001
best loss 227.70639038085938
layer6: self_attn.v_proj
norm_0 tensor([0.9263, 0.8384, 0.8765,  ..., 0.9175, 0.8960, 0.8613], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.1201, 1.1016, 1.1309,  ..., 1.0840, 1.0830, 1.0801], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 58.51893997192383, val loss None, lr 0.001
iter 10, train loss 59.15177917480469, val loss None, lr 0.001
iter 20, train loss 58.82343673706055, val loss None, lr 0.001
iter 30, train loss 58.46164321899414, val loss None, lr 0.001
iter 40, train loss 58.42182159423828, val loss None, lr 0.001
iter 50, train loss 58.3463020324707, val loss None, lr 0.001
iter 60, train loss 58.34733963012695, val loss None, lr 0.001
iter 70, train loss 58.52084732055664, val loss None, lr 0.001
iter 80, train loss 58.32298278808594, val loss None, lr 0.001
iter 90, train loss 58.370845794677734, val loss None, lr 0.001
best loss 58.23634719848633
layer6: self_attn.o_proj
norm_0 tensor([0.9575, 0.9658, 0.9658,  ..., 0.9131, 0.9028, 0.9160], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.0029, 1.0332, 0.9849,  ..., 1.0117, 0.9897, 0.9814], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 1.765496015548706, val loss None, lr 0.001
iter 10, train loss 1.6486103534698486, val loss None, lr 0.001
iter 20, train loss 1.6114110946655273, val loss None, lr 0.001
iter 30, train loss 1.5978889465332031, val loss None, lr 0.001
iter 40, train loss 1.5825506448745728, val loss None, lr 0.001
iter 50, train loss 1.5711218118667603, val loss None, lr 0.001
iter 60, train loss 1.574820876121521, val loss None, lr 0.001
iter 70, train loss 1.5719646215438843, val loss None, lr 0.001
iter 80, train loss 1.577901840209961, val loss None, lr 0.001
iter 90, train loss 1.5701669454574585, val loss None, lr 0.001
best loss 1.5671346187591553
layer6: mlp.gate_proj
norm_0 tensor([2.0645, 2.0723, 2.0859,  ..., 2.0820, 2.0801, 2.0859], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.4995, 0.5005, 0.5156,  ..., 0.5415, 0.5479, 0.6479], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 125.17993927001953, val loss None, lr 0.001
iter 10, train loss 130.793701171875, val loss None, lr 0.001
iter 20, train loss 128.584716796875, val loss None, lr 0.001
iter 30, train loss 127.60472869873047, val loss None, lr 0.001
iter 40, train loss 127.35376739501953, val loss None, lr 0.001
iter 50, train loss 126.96626281738281, val loss None, lr 0.001
iter 60, train loss 126.62519836425781, val loss None, lr 0.001
iter 70, train loss 126.32229614257812, val loss None, lr 0.001
iter 80, train loss 126.52217102050781, val loss None, lr 0.001
iter 90, train loss 126.38154602050781, val loss None, lr 0.001
best loss 124.00493621826172
layer6: mlp.up_proj
norm_0 tensor([1.8457, 1.8311, 1.8223,  ..., 1.8438, 1.8291, 1.8262], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5732, 0.5552, 0.5557,  ..., 0.6118, 0.6104, 0.6094], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 92.65829467773438, val loss None, lr 0.001
iter 10, train loss 92.81161499023438, val loss None, lr 0.001
iter 20, train loss 92.82335662841797, val loss None, lr 0.001
iter 30, train loss 92.95712280273438, val loss None, lr 0.001
iter 40, train loss 93.11654663085938, val loss None, lr 0.001
iter 50, train loss 93.29055786132812, val loss None, lr 0.001
iter 60, train loss 93.31617736816406, val loss None, lr 0.001
iter 70, train loss 93.42273712158203, val loss None, lr 0.001
iter 80, train loss 93.54044342041016, val loss None, lr 0.001
iter 90, train loss 93.66960144042969, val loss None, lr 0.001
best loss 92.65829467773438
layer6: mlp.down_proj
norm_0 tensor([1.0537, 1.0254, 1.0371,  ..., 1.1191, 1.1172, 1.1006], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.6533, 1.6094, 1.6426,  ..., 1.6611, 1.6182, 1.6289], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 1.3367772102355957, val loss None, lr 0.001
iter 10, train loss 1.327805757522583, val loss None, lr 0.001
iter 20, train loss 1.3198623657226562, val loss None, lr 0.001
iter 30, train loss 1.31587553024292, val loss None, lr 0.001
iter 40, train loss 1.316819667816162, val loss None, lr 0.001
iter 50, train loss 1.3121002912521362, val loss None, lr 0.001
iter 60, train loss 1.3112601041793823, val loss None, lr 0.001
iter 70, train loss 1.3094723224639893, val loss None, lr 0.001
iter 80, train loss 1.3079477548599243, val loss None, lr 0.001
iter 90, train loss 1.3107471466064453, val loss None, lr 0.001
best loss 1.3070071935653687
36311 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
28733 MiB free out of 48676 MiB total
after cast to cpu
35903 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
layer7: self_attn.q_proj
norm_0 tensor([1.5596, 1.5742, 1.5781,  ..., 1.5889, 1.5986, 1.6016], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.4731, 0.4634, 0.4819,  ..., 1.2637, 1.7500, 1.7422], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 251.43605041503906, val loss None, lr 0.001
iter 10, train loss 273.49371337890625, val loss None, lr 0.001
iter 20, train loss 263.4604797363281, val loss None, lr 0.001
iter 30, train loss 257.81390380859375, val loss None, lr 0.001
iter 40, train loss 255.33645629882812, val loss None, lr 0.001
iter 50, train loss 254.59190368652344, val loss None, lr 0.001
iter 60, train loss 253.21502685546875, val loss None, lr 0.001
iter 70, train loss 253.3125, val loss None, lr 0.001
iter 80, train loss 251.448974609375, val loss None, lr 0.001
iter 90, train loss 253.55630493164062, val loss None, lr 0.001
best loss 231.35186767578125
layer7: self_attn.k_proj
norm_0 tensor([1.5732, 1.6455, 1.5850,  ..., 1.5645, 1.5615, 1.5967], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.4666, 0.4541, 0.4731,  ..., 1.1797, 1.4951, 1.4863], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 264.9127502441406, val loss None, lr 0.001
iter 10, train loss 282.826904296875, val loss None, lr 0.001
iter 20, train loss 276.10113525390625, val loss None, lr 0.001
iter 30, train loss 264.5404968261719, val loss None, lr 0.001
iter 40, train loss 265.0290832519531, val loss None, lr 0.001
iter 50, train loss 262.7456970214844, val loss None, lr 0.001
iter 60, train loss 262.4656982421875, val loss None, lr 0.001
iter 70, train loss 262.50408935546875, val loss None, lr 0.001
iter 80, train loss 262.4983825683594, val loss None, lr 0.001
iter 90, train loss 262.3441467285156, val loss None, lr 0.001
best loss 239.7596435546875
layer7: self_attn.v_proj
norm_0 tensor([0.9561, 0.8398, 0.8979,  ..., 0.9438, 0.9082, 0.8828], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.1172, 1.0820, 1.0469,  ..., 0.9688, 0.9741, 0.9883], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 65.78844451904297, val loss None, lr 0.001
iter 10, train loss 65.95941925048828, val loss None, lr 0.001
iter 20, train loss 65.86335754394531, val loss None, lr 0.001
iter 30, train loss 65.4361572265625, val loss None, lr 0.001
iter 40, train loss 65.47502899169922, val loss None, lr 0.001
iter 50, train loss 65.46115112304688, val loss None, lr 0.001
iter 60, train loss 65.48129272460938, val loss None, lr 0.001
iter 70, train loss 65.49676513671875, val loss None, lr 0.001
iter 80, train loss 65.48824310302734, val loss None, lr 0.001
iter 90, train loss 65.4945068359375, val loss None, lr 0.001
best loss 65.33245849609375
layer7: self_attn.o_proj
norm_0 tensor([1.0088, 0.9238, 0.9097,  ..., 0.8438, 0.8472, 0.8589], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.0029, 1.0322, 0.9668,  ..., 0.9902, 1.0176, 0.9834], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 2.4893875122070312, val loss None, lr 0.001
iter 10, train loss 2.408700704574585, val loss None, lr 0.001
iter 20, train loss 2.3823516368865967, val loss None, lr 0.001
iter 30, train loss 2.3784146308898926, val loss None, lr 0.001
iter 40, train loss 2.3669345378875732, val loss None, lr 0.001
iter 50, train loss 2.354527235031128, val loss None, lr 0.001
iter 60, train loss 2.3489811420440674, val loss None, lr 0.001
iter 70, train loss 2.3559772968292236, val loss None, lr 0.001
iter 80, train loss 2.3635449409484863, val loss None, lr 0.001
iter 90, train loss 2.3653109073638916, val loss None, lr 0.001
best loss 2.3435142040252686
layer7: mlp.gate_proj
norm_0 tensor([2.0723, 2.0605, 2.0898,  ..., 2.0801, 2.0586, 2.0879], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5020, 0.6572, 0.6641,  ..., 0.6201, 0.4226, 0.5308], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 144.8617706298828, val loss None, lr 0.001
iter 10, train loss 152.26531982421875, val loss None, lr 0.001
iter 20, train loss 149.37550354003906, val loss None, lr 0.001
iter 30, train loss 147.58355712890625, val loss None, lr 0.001
iter 40, train loss 147.00709533691406, val loss None, lr 0.001
iter 50, train loss 146.3089599609375, val loss None, lr 0.001
iter 60, train loss 145.97279357910156, val loss None, lr 0.001
iter 70, train loss 145.60479736328125, val loss None, lr 0.001
iter 80, train loss 145.817626953125, val loss None, lr 0.001
iter 90, train loss 145.40843200683594, val loss None, lr 0.001
best loss 143.46597290039062
layer7: mlp.up_proj
norm_0 tensor([1.8584, 1.8379, 1.8184,  ..., 1.8516, 1.8643, 1.8301], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5767, 0.5947, 0.5737,  ..., 0.6187, 0.5327, 0.5874], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 108.36347198486328, val loss None, lr 0.001
iter 10, train loss 108.60293579101562, val loss None, lr 0.001
iter 20, train loss 108.65859985351562, val loss None, lr 0.001
iter 30, train loss 108.76693725585938, val loss None, lr 0.001
iter 40, train loss 108.93385314941406, val loss None, lr 0.001
iter 50, train loss 109.09214782714844, val loss None, lr 0.001
iter 60, train loss 109.14836883544922, val loss None, lr 0.001
iter 70, train loss 109.21647644042969, val loss None, lr 0.001
iter 80, train loss 109.35466003417969, val loss None, lr 0.001
iter 90, train loss 109.51835632324219, val loss None, lr 0.001
best loss 108.36347198486328
layer7: mlp.down_proj
norm_0 tensor([1.0664, 1.0625, 1.0352,  ..., 1.1152, 1.0088, 1.0713], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.6602, 1.6191, 1.6016,  ..., 1.6641, 1.6426, 1.6318], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 1.7728877067565918, val loss None, lr 0.001
iter 10, train loss 1.7797982692718506, val loss None, lr 0.001
iter 20, train loss 1.7715492248535156, val loss None, lr 0.001
iter 30, train loss 1.7674939632415771, val loss None, lr 0.001
iter 40, train loss 1.7625789642333984, val loss None, lr 0.001
iter 50, train loss 1.761519193649292, val loss None, lr 0.001
iter 60, train loss 1.7619572877883911, val loss None, lr 0.001
iter 70, train loss 1.7665563821792603, val loss None, lr 0.001
iter 80, train loss 1.767913818359375, val loss None, lr 0.001
iter 90, train loss 1.7694684267044067, val loss None, lr 0.001
best loss 1.759731650352478
35903 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
28325 MiB free out of 48676 MiB total
after cast to cpu
35559 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
layer8: self_attn.q_proj
norm_0 tensor([1.5811, 1.6309, 1.5967,  ..., 1.5674, 1.5840, 1.5977], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5137, 0.6250, 0.6187,  ..., 1.4736, 1.9814, 1.9697], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 240.73731994628906, val loss None, lr 0.001
iter 10, train loss 257.74542236328125, val loss None, lr 0.001
iter 20, train loss 248.4252471923828, val loss None, lr 0.001
iter 30, train loss 245.43197631835938, val loss None, lr 0.001
iter 40, train loss 245.54141235351562, val loss None, lr 0.001
iter 50, train loss 244.87905883789062, val loss None, lr 0.001
iter 60, train loss 243.59149169921875, val loss None, lr 0.001
iter 70, train loss 244.87857055664062, val loss None, lr 0.001
iter 80, train loss 243.5644989013672, val loss None, lr 0.001
iter 90, train loss 243.4290008544922, val loss None, lr 0.001
best loss 226.78512573242188
layer8: self_attn.k_proj
norm_0 tensor([1.6221, 1.6260, 1.6396,  ..., 1.5820, 1.5830, 1.6133], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5146, 0.6313, 0.6147,  ..., 1.1768, 1.4805, 1.5146], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 249.68980407714844, val loss None, lr 0.001
iter 10, train loss 264.5599365234375, val loss None, lr 0.001
iter 20, train loss 260.01531982421875, val loss None, lr 0.001
iter 30, train loss 255.69424438476562, val loss None, lr 0.001
iter 40, train loss 251.7349395751953, val loss None, lr 0.001
iter 50, train loss 254.35316467285156, val loss None, lr 0.001
iter 60, train loss 251.1517333984375, val loss None, lr 0.001
iter 70, train loss 251.48333740234375, val loss None, lr 0.001
iter 80, train loss 252.02137756347656, val loss None, lr 0.001
iter 90, train loss 252.9301300048828, val loss None, lr 0.001
best loss 238.07870483398438
layer8: self_attn.v_proj
norm_0 tensor([0.9595, 0.8696, 0.8955,  ..., 0.9707, 0.9312, 0.9160], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.0996, 1.1016, 1.0850,  ..., 0.9624, 0.9346, 0.9497], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 67.46339416503906, val loss None, lr 0.001
iter 10, train loss 68.21322631835938, val loss None, lr 0.001
iter 20, train loss 67.52490997314453, val loss None, lr 0.001
iter 30, train loss 67.0953369140625, val loss None, lr 0.001
iter 40, train loss 67.07708740234375, val loss None, lr 0.001
iter 50, train loss 66.99768829345703, val loss None, lr 0.001
iter 60, train loss 67.045654296875, val loss None, lr 0.001
iter 70, train loss 67.0284652709961, val loss None, lr 0.001
iter 80, train loss 66.88697052001953, val loss None, lr 0.001
iter 90, train loss 66.83747863769531, val loss None, lr 0.001
best loss 66.83446502685547
layer8: self_attn.o_proj
norm_0 tensor([0.9736, 0.9629, 0.9414,  ..., 0.8193, 0.8022, 0.8066], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.0117, 1.0107, 0.9648,  ..., 0.9902, 0.9966, 0.9839], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 4.255774021148682, val loss None, lr 0.001
iter 10, train loss 4.027195930480957, val loss None, lr 0.001
iter 20, train loss 3.8663315773010254, val loss None, lr 0.001
iter 30, train loss 3.757801055908203, val loss None, lr 0.001
iter 40, train loss 3.683349132537842, val loss None, lr 0.001
iter 50, train loss 3.66786527633667, val loss None, lr 0.001
iter 60, train loss 3.6350345611572266, val loss None, lr 0.001
iter 70, train loss 3.636430501937866, val loss None, lr 0.001
iter 80, train loss 3.615492105484009, val loss None, lr 0.001
iter 90, train loss 3.6229870319366455, val loss None, lr 0.001
best loss 3.6143980026245117
layer8: mlp.gate_proj
norm_0 tensor([2.0664, 2.0273, 2.0488,  ..., 2.0664, 2.0195, 2.0566], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.7231, 0.6284, 0.5815,  ..., 0.5156, 0.5513, 0.5308], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 148.9755096435547, val loss None, lr 0.001
iter 10, train loss 155.64772033691406, val loss None, lr 0.001
iter 20, train loss 152.85113525390625, val loss None, lr 0.001
iter 30, train loss 151.22901916503906, val loss None, lr 0.001
iter 40, train loss 150.171142578125, val loss None, lr 0.001
iter 50, train loss 149.95411682128906, val loss None, lr 0.001
iter 60, train loss 149.9391632080078, val loss None, lr 0.001
iter 70, train loss 149.83143615722656, val loss None, lr 0.001
iter 80, train loss 149.54661560058594, val loss None, lr 0.001
iter 90, train loss 149.38409423828125, val loss None, lr 0.001
best loss 147.63177490234375
layer8: mlp.up_proj
norm_0 tensor([1.8564, 1.8447, 1.8408,  ..., 1.8555, 1.8818, 1.8525], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.6270, 0.6255, 0.6157,  ..., 0.6050, 0.6201, 0.6162], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 118.36595916748047, val loss None, lr 0.001
iter 10, train loss 118.8134765625, val loss None, lr 0.001
iter 20, train loss 118.91087341308594, val loss None, lr 0.001
iter 30, train loss 118.85869598388672, val loss None, lr 0.001
iter 40, train loss 118.80461120605469, val loss None, lr 0.001
iter 50, train loss 118.85810089111328, val loss None, lr 0.001
iter 60, train loss 119.13528442382812, val loss None, lr 0.001
iter 70, train loss 119.01947784423828, val loss None, lr 0.001
iter 80, train loss 118.9443130493164, val loss None, lr 0.001
iter 90, train loss 119.09014892578125, val loss None, lr 0.001
best loss 118.36595916748047
layer8: mlp.down_proj
norm_0 tensor([1.1572, 1.1436, 1.1426,  ..., 1.1152, 1.1523, 1.1123], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.6680, 1.6211, 1.5938,  ..., 1.6641, 1.6523, 1.6270], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 2.1689987182617188, val loss None, lr 0.001
iter 10, train loss 2.172210216522217, val loss None, lr 0.001
iter 20, train loss 2.1621062755584717, val loss None, lr 0.001
iter 30, train loss 2.154676914215088, val loss None, lr 0.001
iter 40, train loss 2.150257110595703, val loss None, lr 0.001
iter 50, train loss 2.147317886352539, val loss None, lr 0.001
iter 60, train loss 2.150634765625, val loss None, lr 0.001
iter 70, train loss 2.1486217975616455, val loss None, lr 0.001
iter 80, train loss 2.148111581802368, val loss None, lr 0.001
iter 90, train loss 2.149466037750244, val loss None, lr 0.001
best loss 2.146596908569336
35559 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
27917 MiB free out of 48676 MiB total
after cast to cpu
35151 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
layer9: self_attn.q_proj
norm_0 tensor([1.5449, 1.6074, 1.5713,  ..., 1.6270, 1.5781, 1.6377], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.6655, 0.7217, 0.7026,  ..., 1.2314, 1.2549, 1.3174], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 250.22975158691406, val loss None, lr 0.001
iter 10, train loss 270.90704345703125, val loss None, lr 0.001
iter 20, train loss 254.98150634765625, val loss None, lr 0.001
iter 30, train loss 252.27496337890625, val loss None, lr 0.001
iter 40, train loss 253.29672241210938, val loss None, lr 0.001
iter 50, train loss 251.55543518066406, val loss None, lr 0.001
iter 60, train loss 250.55975341796875, val loss None, lr 0.001
iter 70, train loss 252.83392333984375, val loss None, lr 0.001
iter 80, train loss 251.322998046875, val loss None, lr 0.001
iter 90, train loss 252.92376708984375, val loss None, lr 0.001
best loss 238.05972290039062
layer9: self_attn.k_proj
norm_0 tensor([1.6807, 1.6455, 1.6953,  ..., 1.6182, 1.6025, 1.6367], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5908, 0.6958, 0.6855,  ..., 1.2959, 1.3516, 1.3848], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 272.190673828125, val loss None, lr 0.001
iter 10, train loss 290.2952575683594, val loss None, lr 0.001
iter 20, train loss 281.712646484375, val loss None, lr 0.001
iter 30, train loss 272.681396484375, val loss None, lr 0.001
iter 40, train loss 276.549560546875, val loss None, lr 0.001
iter 50, train loss 273.47064208984375, val loss None, lr 0.001
iter 60, train loss 271.8414611816406, val loss None, lr 0.001
iter 70, train loss 273.59552001953125, val loss None, lr 0.001
iter 80, train loss 273.79443359375, val loss None, lr 0.001
iter 90, train loss 273.3413391113281, val loss None, lr 0.001
best loss 260.05865478515625
layer9: self_attn.v_proj
norm_0 tensor([0.9727, 0.9111, 0.8906,  ..., 0.9683, 0.9805, 0.9292], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.1680, 1.1650, 1.1729,  ..., 0.8726, 0.8779, 0.8560], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 73.13775634765625, val loss None, lr 0.001
iter 10, train loss 73.36814880371094, val loss None, lr 0.001
iter 20, train loss 72.98981475830078, val loss None, lr 0.001
iter 30, train loss 72.8384780883789, val loss None, lr 0.001
iter 40, train loss 72.61878967285156, val loss None, lr 0.001
iter 50, train loss 72.508544921875, val loss None, lr 0.001
iter 60, train loss 72.42906188964844, val loss None, lr 0.001
iter 70, train loss 72.28230285644531, val loss None, lr 0.001
iter 80, train loss 72.29417419433594, val loss None, lr 0.001
iter 90, train loss 72.30079650878906, val loss None, lr 0.001
best loss 72.20135498046875
layer9: self_attn.o_proj
norm_0 tensor([1.0342, 1.0449, 1.0400,  ..., 0.8188, 0.8184, 0.7915], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.0186, 1.0156, 0.9429,  ..., 1.0137, 0.9878, 1.0088], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 4.912703514099121, val loss None, lr 0.001
iter 10, train loss 4.604886054992676, val loss None, lr 0.001
iter 20, train loss 4.568971633911133, val loss None, lr 0.001
iter 30, train loss 4.53382682800293, val loss None, lr 0.001
iter 40, train loss 4.498623847961426, val loss None, lr 0.001
iter 50, train loss 4.508030891418457, val loss None, lr 0.001
iter 60, train loss 4.47548246383667, val loss None, lr 0.001
iter 70, train loss 4.454869747161865, val loss None, lr 0.001
iter 80, train loss 4.47534704208374, val loss None, lr 0.001
iter 90, train loss 4.452492713928223, val loss None, lr 0.001
best loss 4.447930335998535
layer9: mlp.gate_proj
norm_0 tensor([2.0605, 2.0234, 2.0059,  ..., 2.0391, 2.0254, 2.0254], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5283, 0.7275, 0.5498,  ..., 0.5586, 0.5161, 0.5200], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 154.38446044921875, val loss None, lr 0.001
iter 10, train loss 160.66201782226562, val loss None, lr 0.001
iter 20, train loss 157.17160034179688, val loss None, lr 0.001
iter 30, train loss 155.71722412109375, val loss None, lr 0.001
iter 40, train loss 155.0775909423828, val loss None, lr 0.001
iter 50, train loss 155.04115295410156, val loss None, lr 0.001
iter 60, train loss 154.46493530273438, val loss None, lr 0.001
iter 70, train loss 154.63897705078125, val loss None, lr 0.001
iter 80, train loss 154.9503173828125, val loss None, lr 0.001
iter 90, train loss 154.8912811279297, val loss None, lr 0.001
best loss 152.78012084960938
layer9: mlp.up_proj
norm_0 tensor([1.8604, 1.8662, 1.8672,  ..., 1.8809, 1.8828, 1.8701], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.6807, 0.6201, 0.6006,  ..., 0.6367, 0.6025, 0.5957], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 127.85707092285156, val loss None, lr 0.001
iter 10, train loss 128.38189697265625, val loss None, lr 0.001
iter 20, train loss 128.55101013183594, val loss None, lr 0.001
iter 30, train loss 128.46827697753906, val loss None, lr 0.001
iter 40, train loss 128.35885620117188, val loss None, lr 0.001
iter 50, train loss 128.49082946777344, val loss None, lr 0.001
iter 60, train loss 128.51593017578125, val loss None, lr 0.001
iter 70, train loss 128.65716552734375, val loss None, lr 0.001
iter 80, train loss 128.78253173828125, val loss None, lr 0.001
iter 90, train loss 128.79847717285156, val loss None, lr 0.001
best loss 127.77205657958984
layer9: mlp.down_proj
norm_0 tensor([1.2451, 1.1426, 1.1172,  ..., 1.1104, 1.0918, 1.1064], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.6816, 1.6230, 1.5801,  ..., 1.6689, 1.6455, 1.6553], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 2.540346145629883, val loss None, lr 0.001
iter 10, train loss 2.538750410079956, val loss None, lr 0.001
iter 20, train loss 2.5298614501953125, val loss None, lr 0.001
iter 30, train loss 2.5233571529388428, val loss None, lr 0.001
iter 40, train loss 2.520817518234253, val loss None, lr 0.001
iter 50, train loss 2.516366958618164, val loss None, lr 0.001
iter 60, train loss 2.519629716873169, val loss None, lr 0.001
iter 70, train loss 2.520906925201416, val loss None, lr 0.001
iter 80, train loss 2.5209250450134277, val loss None, lr 0.001
iter 90, train loss 2.5261173248291016, val loss None, lr 0.001
best loss 2.5154435634613037
35151 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
27509 MiB free out of 48676 MiB total
after cast to cpu
34743 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
layer10: self_attn.q_proj
norm_0 tensor([1.5840, 1.6006, 1.5547,  ..., 1.6133, 1.6113, 1.6143], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5054, 0.5371, 0.5088,  ..., 1.2539, 1.3711, 1.3086], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 253.67477416992188, val loss None, lr 0.001
iter 10, train loss 270.35009765625, val loss None, lr 0.001
iter 20, train loss 257.2270812988281, val loss None, lr 0.001
iter 30, train loss 254.0118408203125, val loss None, lr 0.001
iter 40, train loss 257.3756408691406, val loss None, lr 0.001
iter 50, train loss 255.41781616210938, val loss None, lr 0.001
iter 60, train loss 256.7437438964844, val loss None, lr 0.001
iter 70, train loss 255.0878448486328, val loss None, lr 0.001
iter 80, train loss 255.7403564453125, val loss None, lr 0.001
iter 90, train loss 254.26608276367188, val loss None, lr 0.001
best loss 242.31344604492188
layer10: self_attn.k_proj
norm_0 tensor([1.6543, 1.6719, 1.6875,  ..., 1.5859, 1.6025, 1.6729], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.4500, 0.5078, 0.4966,  ..., 1.3223, 1.3271, 1.2959], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 279.6641540527344, val loss None, lr 0.001
iter 10, train loss 298.552978515625, val loss None, lr 0.001
iter 20, train loss 293.6697692871094, val loss None, lr 0.001
iter 30, train loss 287.1566467285156, val loss None, lr 0.001
iter 40, train loss 288.06365966796875, val loss None, lr 0.001
iter 50, train loss 286.0129089355469, val loss None, lr 0.001
iter 60, train loss 285.9190368652344, val loss None, lr 0.001
iter 70, train loss 286.6849060058594, val loss None, lr 0.001
iter 80, train loss 285.5949401855469, val loss None, lr 0.001
iter 90, train loss 286.7376708984375, val loss None, lr 0.001
best loss 268.9780578613281
layer10: self_attn.v_proj
norm_0 tensor([0.9683, 0.9048, 0.8916,  ..., 0.9570, 0.9429, 0.9282], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.2246, 1.2041, 1.2119,  ..., 1.0098, 0.9971, 1.0068], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 73.40159606933594, val loss None, lr 0.001
iter 10, train loss 73.83623504638672, val loss None, lr 0.001
iter 20, train loss 73.40765380859375, val loss None, lr 0.001
iter 30, train loss 73.19621276855469, val loss None, lr 0.001
iter 40, train loss 72.96629333496094, val loss None, lr 0.001
iter 50, train loss 72.68579864501953, val loss None, lr 0.001
iter 60, train loss 72.86314392089844, val loss None, lr 0.001
iter 70, train loss 72.84503173828125, val loss None, lr 0.001
iter 80, train loss 72.6948013305664, val loss None, lr 0.001
iter 90, train loss 72.6727294921875, val loss None, lr 0.001
best loss 72.60881042480469
layer10: self_attn.o_proj
norm_0 tensor([1.0889, 1.0732, 1.0840,  ..., 0.8955, 0.8984, 0.8911], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.0137, 0.9883, 0.9497,  ..., 0.9912, 0.9956, 0.9736], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 7.974893093109131, val loss None, lr 0.001
iter 10, train loss 7.520105361938477, val loss None, lr 0.001
iter 20, train loss 7.269577980041504, val loss None, lr 0.001
iter 30, train loss 6.994724750518799, val loss None, lr 0.001
iter 40, train loss 6.815705299377441, val loss None, lr 0.001
iter 50, train loss 6.727828025817871, val loss None, lr 0.001
iter 60, train loss 6.690667629241943, val loss None, lr 0.001
iter 70, train loss 6.642268657684326, val loss None, lr 0.001
iter 80, train loss 6.614989280700684, val loss None, lr 0.001
iter 90, train loss 6.631063461303711, val loss None, lr 0.001
best loss 6.5733842849731445
layer10: mlp.gate_proj
norm_0 tensor([2.0371, 1.9961, 2.0020,  ..., 2.0371, 1.9932, 2.0195], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.6074, 0.6484, 0.5117,  ..., 0.5186, 0.4858, 0.6157], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 160.90463256835938, val loss None, lr 0.001
iter 10, train loss 169.23599243164062, val loss None, lr 0.001
iter 20, train loss 165.51768493652344, val loss None, lr 0.001
iter 30, train loss 164.0183563232422, val loss None, lr 0.001
iter 40, train loss 163.30264282226562, val loss None, lr 0.001
iter 50, train loss 162.6247100830078, val loss None, lr 0.001
iter 60, train loss 162.6004180908203, val loss None, lr 0.001
iter 70, train loss 161.96994018554688, val loss None, lr 0.001
iter 80, train loss 161.6612091064453, val loss None, lr 0.001
iter 90, train loss 161.5294647216797, val loss None, lr 0.001
best loss 159.3275146484375
layer10: mlp.up_proj
norm_0 tensor([1.8994, 1.8828, 1.8857,  ..., 1.8867, 1.9229, 1.8984], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.6636, 0.6587, 0.5776,  ..., 0.5815, 0.5786, 0.5996], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 135.83433532714844, val loss None, lr 0.001
iter 10, train loss 136.79006958007812, val loss None, lr 0.001
iter 20, train loss 136.9812469482422, val loss None, lr 0.001
iter 30, train loss 137.00852966308594, val loss None, lr 0.001
iter 40, train loss 136.8931884765625, val loss None, lr 0.001
iter 50, train loss 136.76824951171875, val loss None, lr 0.001
iter 60, train loss 136.67477416992188, val loss None, lr 0.001
iter 70, train loss 136.45724487304688, val loss None, lr 0.001
iter 80, train loss 136.4817352294922, val loss None, lr 0.001
iter 90, train loss 136.78599548339844, val loss None, lr 0.001
best loss 135.37008666992188
layer10: mlp.down_proj
norm_0 tensor([1.2129, 1.2090, 1.0928,  ..., 1.1016, 1.0967, 1.0996], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.6562, 1.6104, 1.5859,  ..., 1.6670, 1.6445, 1.6562], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 3.1506314277648926, val loss None, lr 0.001
iter 10, train loss 3.1553096771240234, val loss None, lr 0.001
iter 20, train loss 3.1272850036621094, val loss None, lr 0.001
iter 30, train loss 3.0946974754333496, val loss None, lr 0.001
iter 40, train loss 3.0946874618530273, val loss None, lr 0.001
iter 50, train loss 3.0721263885498047, val loss None, lr 0.001
iter 60, train loss 3.0605366230010986, val loss None, lr 0.001
iter 70, train loss 3.0541038513183594, val loss None, lr 0.001
iter 80, train loss 3.0557570457458496, val loss None, lr 0.001
iter 90, train loss 3.056192398071289, val loss None, lr 0.001
best loss 3.0497524738311768
34743 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
27101 MiB free out of 48676 MiB total
after cast to cpu
34333 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
layer11: self_attn.q_proj
norm_0 tensor([1.5352, 1.5703, 1.5000,  ..., 1.5029, 1.5508, 1.5420], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.6152, 0.5410, 0.5269,  ..., 1.3330, 1.2666, 1.3496], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 289.6383056640625, val loss None, lr 0.001
iter 10, train loss 314.5087585449219, val loss None, lr 0.001
iter 20, train loss 301.68743896484375, val loss None, lr 0.001
iter 30, train loss 294.05975341796875, val loss None, lr 0.001
iter 40, train loss 297.5512390136719, val loss None, lr 0.001
iter 50, train loss 293.23114013671875, val loss None, lr 0.001
iter 60, train loss 292.4377136230469, val loss None, lr 0.001
iter 70, train loss 294.44580078125, val loss None, lr 0.001
iter 80, train loss 294.84271240234375, val loss None, lr 0.001
iter 90, train loss 295.02459716796875, val loss None, lr 0.001
best loss 273.25177001953125
layer11: self_attn.k_proj
norm_0 tensor([1.4922, 1.5254, 1.5713,  ..., 1.5029, 1.4570, 1.5527], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5928, 0.5405, 0.5244,  ..., 1.2598, 1.2256, 1.2422], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 293.3273010253906, val loss None, lr 0.001
iter 10, train loss 316.06524658203125, val loss None, lr 0.001
iter 20, train loss 311.06207275390625, val loss None, lr 0.001
iter 30, train loss 302.2336120605469, val loss None, lr 0.001
iter 40, train loss 301.6845703125, val loss None, lr 0.001
iter 50, train loss 300.37188720703125, val loss None, lr 0.001
iter 60, train loss 301.6539306640625, val loss None, lr 0.001
iter 70, train loss 301.7325439453125, val loss None, lr 0.001
iter 80, train loss 299.383544921875, val loss None, lr 0.001
iter 90, train loss 300.80670166015625, val loss None, lr 0.001
best loss 278.3755187988281
layer11: self_attn.v_proj
norm_0 tensor([0.9839, 0.9263, 0.9023,  ..., 1.0098, 0.9917, 0.9473], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.9438, 0.9624, 0.9556,  ..., 1.0645, 1.0928, 1.0693], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 99.54254913330078, val loss None, lr 0.001
iter 10, train loss 99.76063537597656, val loss None, lr 0.001
iter 20, train loss 99.42999267578125, val loss None, lr 0.001
iter 30, train loss 98.74971008300781, val loss None, lr 0.001
iter 40, train loss 98.6415786743164, val loss None, lr 0.001
iter 50, train loss 98.68948364257812, val loss None, lr 0.001
iter 60, train loss 98.64928436279297, val loss None, lr 0.001
iter 70, train loss 98.39022064208984, val loss None, lr 0.001
iter 80, train loss 98.36228942871094, val loss None, lr 0.001
iter 90, train loss 98.30023193359375, val loss None, lr 0.001
best loss 98.21873474121094
layer11: self_attn.o_proj
norm_0 tensor([0.8975, 0.9155, 0.8994,  ..., 0.9780, 0.9990, 0.9839], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.0078, 0.9941, 0.9434,  ..., 0.9971, 0.9868, 0.9946], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 8.382554054260254, val loss None, lr 0.001
iter 10, train loss 8.257294654846191, val loss None, lr 0.001
iter 20, train loss 8.301128387451172, val loss None, lr 0.001
iter 30, train loss 8.232047080993652, val loss None, lr 0.001
iter 40, train loss 8.246397972106934, val loss None, lr 0.001
iter 50, train loss 8.096332550048828, val loss None, lr 0.001
iter 60, train loss 8.116963386535645, val loss None, lr 0.001
iter 70, train loss 8.044793128967285, val loss None, lr 0.001
iter 80, train loss 8.070452690124512, val loss None, lr 0.001
iter 90, train loss 8.010231018066406, val loss None, lr 0.001
best loss 7.988231658935547
layer11: mlp.gate_proj
norm_0 tensor([2.0176, 2.0156, 2.0137,  ..., 2.0137, 2.0098, 1.9883], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.6592, 0.6074, 0.5933,  ..., 0.6416, 0.4954, 0.5454], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 172.40966796875, val loss None, lr 0.001
iter 10, train loss 181.03567504882812, val loss None, lr 0.001
iter 20, train loss 177.8431854248047, val loss None, lr 0.001
iter 30, train loss 176.35450744628906, val loss None, lr 0.001
iter 40, train loss 175.35427856445312, val loss None, lr 0.001
iter 50, train loss 174.6053009033203, val loss None, lr 0.001
iter 60, train loss 173.62948608398438, val loss None, lr 0.001
iter 70, train loss 173.34242248535156, val loss None, lr 0.001
iter 80, train loss 172.6734619140625, val loss None, lr 0.001
iter 90, train loss 173.0760498046875, val loss None, lr 0.001
best loss 170.55496215820312
layer11: mlp.up_proj
norm_0 tensor([1.9180, 1.8945, 1.8945,  ..., 1.9150, 1.9199, 1.9229], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.6187, 0.6519, 0.6460,  ..., 0.5723, 0.5889, 0.5908], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 149.79286193847656, val loss None, lr 0.001
iter 10, train loss 150.7362060546875, val loss None, lr 0.001
iter 20, train loss 151.0875701904297, val loss None, lr 0.001
iter 30, train loss 150.70782470703125, val loss None, lr 0.001
iter 40, train loss 150.66207885742188, val loss None, lr 0.001
iter 50, train loss 150.4754638671875, val loss None, lr 0.001
iter 60, train loss 150.57846069335938, val loss None, lr 0.001
iter 70, train loss 150.50830078125, val loss None, lr 0.001
iter 80, train loss 150.4114227294922, val loss None, lr 0.001
iter 90, train loss 150.5746307373047, val loss None, lr 0.001
best loss 149.79286193847656
layer11: mlp.down_proj
norm_0 tensor([1.1641, 1.1953, 1.2109,  ..., 1.0713, 1.1064, 1.1221], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.6807, 1.6084, 1.5986,  ..., 1.6611, 1.6416, 1.6543], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 3.3703296184539795, val loss None, lr 0.001
iter 10, train loss 3.3747315406799316, val loss None, lr 0.001
iter 20, train loss 3.3520150184631348, val loss None, lr 0.001
iter 30, train loss 3.3374722003936768, val loss None, lr 0.001
iter 40, train loss 3.3351404666900635, val loss None, lr 0.001
iter 50, train loss 3.3326075077056885, val loss None, lr 0.001
iter 60, train loss 3.3291492462158203, val loss None, lr 0.001
iter 70, train loss 3.3274786472320557, val loss None, lr 0.001
iter 80, train loss 3.3296611309051514, val loss None, lr 0.001
iter 90, train loss 3.337613582611084, val loss None, lr 0.001
best loss 3.326819896697998
34333 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
26693 MiB free out of 48676 MiB total
after cast to cpu
33925 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
layer12: self_attn.q_proj
norm_0 tensor([1.5273, 1.5771, 1.5420,  ..., 1.5244, 1.5674, 1.5654], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5688, 0.6431, 0.6504,  ..., 1.0703, 1.0967, 0.9834], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 295.88775634765625, val loss None, lr 0.001
iter 10, train loss 316.8731384277344, val loss None, lr 0.001
iter 20, train loss 304.02667236328125, val loss None, lr 0.001
iter 30, train loss 299.3857116699219, val loss None, lr 0.001
iter 40, train loss 300.1458435058594, val loss None, lr 0.001
iter 50, train loss 300.0270080566406, val loss None, lr 0.001
iter 60, train loss 298.697509765625, val loss None, lr 0.001
iter 70, train loss 299.8208923339844, val loss None, lr 0.001
iter 80, train loss 298.3275146484375, val loss None, lr 0.001
iter 90, train loss 300.2601318359375, val loss None, lr 0.001
best loss 281.82012939453125
layer12: self_attn.k_proj
norm_0 tensor([1.6348, 1.6309, 1.6162,  ..., 1.5889, 1.5713, 1.6162], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5381, 0.6201, 0.6494,  ..., 1.0996, 1.1416, 1.0020], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 328.5665283203125, val loss None, lr 0.001
iter 10, train loss 352.615234375, val loss None, lr 0.001
iter 20, train loss 342.25299072265625, val loss None, lr 0.001
iter 30, train loss 332.57244873046875, val loss None, lr 0.001
iter 40, train loss 333.76995849609375, val loss None, lr 0.001
iter 50, train loss 334.1871032714844, val loss None, lr 0.001
iter 60, train loss 332.8476867675781, val loss None, lr 0.001
iter 70, train loss 332.88751220703125, val loss None, lr 0.001
iter 80, train loss 333.5738220214844, val loss None, lr 0.001
iter 90, train loss 334.5915832519531, val loss None, lr 0.001
best loss 310.0274963378906
layer12: self_attn.v_proj
norm_0 tensor([0.9805, 0.8818, 0.9146,  ..., 0.9917, 0.9771, 0.9478], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.9761, 0.9873, 0.9897,  ..., 0.8320, 0.8545, 0.8218], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 96.7445068359375, val loss None, lr 0.001
iter 10, train loss 97.23340606689453, val loss None, lr 0.001
iter 20, train loss 96.77142333984375, val loss None, lr 0.001
iter 30, train loss 96.5680160522461, val loss None, lr 0.001
iter 40, train loss 96.38896179199219, val loss None, lr 0.001
iter 50, train loss 96.34404754638672, val loss None, lr 0.001
iter 60, train loss 96.18270874023438, val loss None, lr 0.001
iter 70, train loss 96.38304138183594, val loss None, lr 0.001
iter 80, train loss 96.22978210449219, val loss None, lr 0.001
iter 90, train loss 96.10906982421875, val loss None, lr 0.001
best loss 95.95069885253906
layer12: self_attn.o_proj
norm_0 tensor([0.9019, 0.9160, 0.9263,  ..., 0.8096, 0.8281, 0.8232], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.0186, 1.0127, 0.9609,  ..., 1.0098, 0.9951, 1.0020], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 8.692168235778809, val loss None, lr 0.001
iter 10, train loss 8.415837287902832, val loss None, lr 0.001
iter 20, train loss 8.44070053100586, val loss None, lr 0.001
iter 30, train loss 8.35010051727295, val loss None, lr 0.001
iter 40, train loss 8.228636741638184, val loss None, lr 0.001
iter 50, train loss 8.223518371582031, val loss None, lr 0.001
iter 60, train loss 8.194517135620117, val loss None, lr 0.001
iter 70, train loss 8.159993171691895, val loss None, lr 0.001
iter 80, train loss 8.14456558227539, val loss None, lr 0.001
iter 90, train loss 8.160161018371582, val loss None, lr 0.001
best loss 8.129647254943848
layer12: mlp.gate_proj
norm_0 tensor([1.9873, 1.9941, 1.9922,  ..., 1.9883, 1.9795, 2.0000], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.7197, 0.5757, 0.6562,  ..., 0.5669, 0.5142, 0.5156], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 182.38925170898438, val loss None, lr 0.001
iter 10, train loss 190.68995666503906, val loss None, lr 0.001
iter 20, train loss 187.09109497070312, val loss None, lr 0.001
iter 30, train loss 186.20880126953125, val loss None, lr 0.001
iter 40, train loss 184.9903106689453, val loss None, lr 0.001
iter 50, train loss 184.49264526367188, val loss None, lr 0.001
iter 60, train loss 183.65310668945312, val loss None, lr 0.001
iter 70, train loss 183.25799560546875, val loss None, lr 0.001
iter 80, train loss 182.79641723632812, val loss None, lr 0.001
iter 90, train loss 182.78465270996094, val loss None, lr 0.001
best loss 180.837890625
layer12: mlp.up_proj
norm_0 tensor([1.9453, 1.9141, 1.9209,  ..., 1.9414, 1.9346, 1.9307], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.6260, 0.6333, 0.5864,  ..., 0.6069, 0.5874, 0.5762], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 163.14483642578125, val loss None, lr 0.001
iter 10, train loss 163.69949340820312, val loss None, lr 0.001
iter 20, train loss 164.1337432861328, val loss None, lr 0.001
iter 30, train loss 164.21653747558594, val loss None, lr 0.001
iter 40, train loss 164.13619995117188, val loss None, lr 0.001
iter 50, train loss 163.84898376464844, val loss None, lr 0.001
iter 60, train loss 164.16336059570312, val loss None, lr 0.001
iter 70, train loss 163.9463348388672, val loss None, lr 0.001
iter 80, train loss 163.95794677734375, val loss None, lr 0.001
iter 90, train loss 163.87869262695312, val loss None, lr 0.001
best loss 163.14483642578125
layer12: mlp.down_proj
norm_0 tensor([1.2061, 1.2090, 1.1094,  ..., 1.1533, 1.1279, 1.1035], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.6914, 1.6230, 1.5947,  ..., 1.6689, 1.6689, 1.6318], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 3.7516632080078125, val loss None, lr 0.001
iter 10, train loss 3.7746663093566895, val loss None, lr 0.001
iter 20, train loss 3.767897605895996, val loss None, lr 0.001
iter 30, train loss 3.778132438659668, val loss None, lr 0.001
iter 40, train loss 3.7747042179107666, val loss None, lr 0.001
iter 50, train loss 3.773378372192383, val loss None, lr 0.001
iter 60, train loss 3.791914939880371, val loss None, lr 0.001
iter 70, train loss 3.7966880798339844, val loss None, lr 0.001
iter 80, train loss 3.7987210750579834, val loss None, lr 0.001
iter 90, train loss 3.8091201782226562, val loss None, lr 0.001
best loss 3.7516632080078125
33925 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
26285 MiB free out of 48676 MiB total
after cast to cpu
33517 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
layer13: self_attn.q_proj
norm_0 tensor([1.5020, 1.5752, 1.5205,  ..., 1.5469, 1.5420, 1.5430], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.6719, 0.6899, 0.7017,  ..., 1.1709, 1.1445, 1.1143], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 307.3148193359375, val loss None, lr 0.001
iter 10, train loss 326.71160888671875, val loss None, lr 0.001
iter 20, train loss 309.2354431152344, val loss None, lr 0.001
iter 30, train loss 306.5003967285156, val loss None, lr 0.001
iter 40, train loss 307.6092529296875, val loss None, lr 0.001
iter 50, train loss 308.0234375, val loss None, lr 0.001
iter 60, train loss 305.6647644042969, val loss None, lr 0.001
iter 70, train loss 305.2076416015625, val loss None, lr 0.001
iter 80, train loss 305.9677734375, val loss None, lr 0.001
iter 90, train loss 306.2828369140625, val loss None, lr 0.001
best loss 288.3516845703125
layer13: self_attn.k_proj
norm_0 tensor([1.6123, 1.5781, 1.5908,  ..., 1.5596, 1.5391, 1.5684], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5093, 0.6797, 0.6953,  ..., 1.2012, 1.2188, 1.1230], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 327.8361511230469, val loss None, lr 0.001
iter 10, train loss 351.84893798828125, val loss None, lr 0.001
iter 20, train loss 337.6601867675781, val loss None, lr 0.001
iter 30, train loss 327.48486328125, val loss None, lr 0.001
iter 40, train loss 328.85003662109375, val loss None, lr 0.001
iter 50, train loss 326.64898681640625, val loss None, lr 0.001
iter 60, train loss 328.1259765625, val loss None, lr 0.001
iter 70, train loss 329.3936462402344, val loss None, lr 0.001
iter 80, train loss 327.0560302734375, val loss None, lr 0.001
iter 90, train loss 325.77667236328125, val loss None, lr 0.001
best loss 307.3000793457031
layer13: self_attn.v_proj
norm_0 tensor([1.0107, 0.9507, 0.9663,  ..., 0.9980, 1.0049, 0.9756], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.9341, 0.9478, 0.9473,  ..., 0.9712, 0.9614, 0.9858], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 106.88170623779297, val loss None, lr 0.001
iter 10, train loss 106.72053527832031, val loss None, lr 0.001
iter 20, train loss 106.49415588378906, val loss None, lr 0.001
iter 30, train loss 106.69719696044922, val loss None, lr 0.001
iter 40, train loss 106.73133087158203, val loss None, lr 0.001
iter 50, train loss 106.38076782226562, val loss None, lr 0.001
iter 60, train loss 106.29185485839844, val loss None, lr 0.001
iter 70, train loss 106.37638854980469, val loss None, lr 0.001
iter 80, train loss 106.25888061523438, val loss None, lr 0.001
iter 90, train loss 106.25701141357422, val loss None, lr 0.001
best loss 106.0888900756836
layer13: self_attn.o_proj
norm_0 tensor([0.8975, 0.9448, 0.9434,  ..., 0.9521, 0.9253, 0.9536], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.9922, 1.0107, 0.9492,  ..., 0.9976, 0.9951, 0.9917], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 8.692206382751465, val loss None, lr 0.001
iter 10, train loss 8.273488998413086, val loss None, lr 0.001
iter 20, train loss 8.283926963806152, val loss None, lr 0.001
iter 30, train loss 8.218344688415527, val loss None, lr 0.001
iter 40, train loss 8.142853736877441, val loss None, lr 0.001
iter 50, train loss 8.071053504943848, val loss None, lr 0.001
iter 60, train loss 8.065375328063965, val loss None, lr 0.001
iter 70, train loss 7.999049186706543, val loss None, lr 0.001
iter 80, train loss 7.992883682250977, val loss None, lr 0.001
iter 90, train loss 8.025115013122559, val loss None, lr 0.001
best loss 7.962367534637451
layer13: mlp.gate_proj
norm_0 tensor([1.9912, 1.9756, 1.9883,  ..., 1.9971, 2.0098, 1.9844], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.4390, 0.5815, 0.5342,  ..., 0.5488, 0.5723, 0.5757], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 193.19064331054688, val loss None, lr 0.001
iter 10, train loss 202.26220703125, val loss None, lr 0.001
iter 20, train loss 197.7857666015625, val loss None, lr 0.001
iter 30, train loss 195.5033721923828, val loss None, lr 0.001
iter 40, train loss 194.23556518554688, val loss None, lr 0.001
iter 50, train loss 193.47885131835938, val loss None, lr 0.001
iter 60, train loss 192.66131591796875, val loss None, lr 0.001
iter 70, train loss 192.89453125, val loss None, lr 0.001
iter 80, train loss 193.08880615234375, val loss None, lr 0.001
iter 90, train loss 192.84161376953125, val loss None, lr 0.001
best loss 191.23724365234375
layer13: mlp.up_proj
norm_0 tensor([1.9521, 1.9463, 1.9248,  ..., 1.9443, 1.9375, 1.9648], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5557, 0.5957, 0.6099,  ..., 0.5972, 0.5747, 0.6196], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 177.29844665527344, val loss None, lr 0.001
iter 10, train loss 177.87290954589844, val loss None, lr 0.001
iter 20, train loss 178.1895751953125, val loss None, lr 0.001
iter 30, train loss 178.81723022460938, val loss None, lr 0.001
iter 40, train loss 178.66754150390625, val loss None, lr 0.001
iter 50, train loss 178.57275390625, val loss None, lr 0.001
iter 60, train loss 178.51693725585938, val loss None, lr 0.001
iter 70, train loss 178.54034423828125, val loss None, lr 0.001
iter 80, train loss 178.46630859375, val loss None, lr 0.001
iter 90, train loss 178.6578369140625, val loss None, lr 0.001
best loss 177.1890411376953
layer13: mlp.down_proj
norm_0 tensor([1.0791, 1.1406, 1.1748,  ..., 1.1504, 1.0996, 1.1953], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.6523, 1.6123, 1.5850,  ..., 1.6445, 1.6318, 1.6611], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 4.632449150085449, val loss None, lr 0.001
iter 10, train loss 4.607898235321045, val loss None, lr 0.001
iter 20, train loss 4.573877811431885, val loss None, lr 0.001
iter 30, train loss 4.548999309539795, val loss None, lr 0.001
iter 40, train loss 4.536091327667236, val loss None, lr 0.001
iter 50, train loss 4.5229692459106445, val loss None, lr 0.001
iter 60, train loss 4.51470422744751, val loss None, lr 0.001
iter 70, train loss 4.515053749084473, val loss None, lr 0.001
iter 80, train loss 4.515117645263672, val loss None, lr 0.001
iter 90, train loss 4.509890079498291, val loss None, lr 0.001
best loss 4.507930755615234
33517 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
25877 MiB free out of 48676 MiB total
after cast to cpu
33109 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
layer14: self_attn.q_proj
norm_0 tensor([1.5146, 1.5508, 1.5156,  ..., 1.5625, 1.5400, 1.5449], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.7290, 0.9404, 0.9341,  ..., 0.9673, 1.0361, 0.9829], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 321.9073181152344, val loss None, lr 0.001
iter 10, train loss 346.9134521484375, val loss None, lr 0.001
iter 20, train loss 329.02276611328125, val loss None, lr 0.001
iter 30, train loss 322.1051330566406, val loss None, lr 0.001
iter 40, train loss 322.5132751464844, val loss None, lr 0.001
iter 50, train loss 321.8959045410156, val loss None, lr 0.001
iter 60, train loss 321.12542724609375, val loss None, lr 0.001
iter 70, train loss 323.80084228515625, val loss None, lr 0.001
iter 80, train loss 323.3758850097656, val loss None, lr 0.001
iter 90, train loss 321.0032653808594, val loss None, lr 0.001
best loss 300.00982666015625
layer14: self_attn.k_proj
norm_0 tensor([1.6133, 1.5752, 1.5879,  ..., 1.5352, 1.5273, 1.5488], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.6665, 0.9277, 0.9243,  ..., 0.9937, 1.0176, 0.9570], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 352.046875, val loss None, lr 0.001
iter 10, train loss 377.7576599121094, val loss None, lr 0.001
iter 20, train loss 364.4711608886719, val loss None, lr 0.001
iter 30, train loss 356.480712890625, val loss None, lr 0.001
iter 40, train loss 354.04498291015625, val loss None, lr 0.001
iter 50, train loss 354.4078674316406, val loss None, lr 0.001
iter 60, train loss 354.4710693359375, val loss None, lr 0.001
iter 70, train loss 355.73077392578125, val loss None, lr 0.001
iter 80, train loss 354.6539306640625, val loss None, lr 0.001
iter 90, train loss 353.66778564453125, val loss None, lr 0.001
best loss 323.6965637207031
layer14: self_attn.v_proj
norm_0 tensor([0.9697, 0.9165, 0.9658,  ..., 0.9917, 0.9927, 0.9868], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.9492, 0.9385, 0.9473,  ..., 1.1113, 1.1074, 1.1270], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 108.3416748046875, val loss None, lr 0.001
iter 10, train loss 107.82971954345703, val loss None, lr 0.001
iter 20, train loss 107.49407196044922, val loss None, lr 0.001
iter 30, train loss 107.35676574707031, val loss None, lr 0.001
iter 40, train loss 107.02421569824219, val loss None, lr 0.001
iter 50, train loss 107.06890106201172, val loss None, lr 0.001
iter 60, train loss 107.06959533691406, val loss None, lr 0.001
iter 70, train loss 107.40827941894531, val loss None, lr 0.001
iter 80, train loss 106.80876159667969, val loss None, lr 0.001
iter 90, train loss 106.87850189208984, val loss None, lr 0.001
best loss 106.66243743896484
layer14: self_attn.o_proj
norm_0 tensor([0.8921, 0.8745, 0.8906,  ..., 1.0439, 1.0264, 1.0498], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.9810, 0.9897, 0.9609,  ..., 1.0146, 0.9961, 1.0293], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 12.310384750366211, val loss None, lr 0.001
iter 10, train loss 11.84507942199707, val loss None, lr 0.001
iter 20, train loss 11.594184875488281, val loss None, lr 0.001
iter 30, train loss 11.378371238708496, val loss None, lr 0.001
iter 40, train loss 11.207840919494629, val loss None, lr 0.001
iter 50, train loss 11.078749656677246, val loss None, lr 0.001
iter 60, train loss 11.0186185836792, val loss None, lr 0.001
iter 70, train loss 10.998387336730957, val loss None, lr 0.001
iter 80, train loss 10.997615814208984, val loss None, lr 0.001
iter 90, train loss 10.96815013885498, val loss None, lr 0.001
best loss 10.887550354003906
layer14: mlp.gate_proj
norm_0 tensor([1.9795, 1.9805, 1.9893,  ..., 1.9951, 1.9756, 1.9717], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5684, 0.6548, 0.5732,  ..., 0.6045, 0.5820, 0.5410], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 209.1988067626953, val loss None, lr 0.001
iter 10, train loss 219.5481719970703, val loss None, lr 0.001
iter 20, train loss 213.7230224609375, val loss None, lr 0.001
iter 30, train loss 212.18460083007812, val loss None, lr 0.001
iter 40, train loss 211.0585479736328, val loss None, lr 0.001
iter 50, train loss 211.0112762451172, val loss None, lr 0.001
iter 60, train loss 210.3386688232422, val loss None, lr 0.001
iter 70, train loss 210.1193084716797, val loss None, lr 0.001
iter 80, train loss 209.83717346191406, val loss None, lr 0.001
iter 90, train loss 209.3287811279297, val loss None, lr 0.001
best loss 206.6634979248047
layer14: mlp.up_proj
norm_0 tensor([1.9697, 1.9395, 1.9385,  ..., 1.9443, 1.9668, 1.9580], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.6133, 0.6011, 0.6069,  ..., 0.6299, 0.5903, 0.5781], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 192.50999450683594, val loss None, lr 0.001
iter 10, train loss 193.48365783691406, val loss None, lr 0.001
iter 20, train loss 193.85816955566406, val loss None, lr 0.001
iter 30, train loss 194.03114318847656, val loss None, lr 0.001
iter 40, train loss 193.85983276367188, val loss None, lr 0.001
iter 50, train loss 194.0816192626953, val loss None, lr 0.001
iter 60, train loss 194.36349487304688, val loss None, lr 0.001
iter 70, train loss 194.50927734375, val loss None, lr 0.001
iter 80, train loss 194.34967041015625, val loss None, lr 0.001
iter 90, train loss 194.4346160888672, val loss None, lr 0.001
best loss 192.48208618164062
layer14: mlp.down_proj
norm_0 tensor([1.1758, 1.1631, 1.1572,  ..., 1.2100, 1.1436, 1.1152], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.6904, 1.6191, 1.5908,  ..., 1.6602, 1.6436, 1.6416], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 5.149151802062988, val loss None, lr 0.001
iter 10, train loss 5.168508529663086, val loss None, lr 0.001
iter 20, train loss 5.155340194702148, val loss None, lr 0.001
iter 30, train loss 5.146152973175049, val loss None, lr 0.001
iter 40, train loss 5.138250350952148, val loss None, lr 0.001
iter 50, train loss 5.134203910827637, val loss None, lr 0.001
iter 60, train loss 5.1339545249938965, val loss None, lr 0.001
iter 70, train loss 5.135983943939209, val loss None, lr 0.001
iter 80, train loss 5.140180587768555, val loss None, lr 0.001
iter 90, train loss 5.14481782913208, val loss None, lr 0.001
best loss 5.126903057098389
33109 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
25469 MiB free out of 48676 MiB total
after cast to cpu
32701 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
layer15: self_attn.q_proj
norm_0 tensor([1.4932, 1.5342, 1.4893,  ..., 1.5107, 1.5303, 1.5342], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5806, 0.6270, 0.6035,  ..., 1.2744, 1.2510, 1.2686], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 302.09765625, val loss None, lr 0.001
iter 10, train loss 320.4405517578125, val loss None, lr 0.001
iter 20, train loss 303.69158935546875, val loss None, lr 0.001
iter 30, train loss 297.9458312988281, val loss None, lr 0.001
iter 40, train loss 297.4917907714844, val loss None, lr 0.001
iter 50, train loss 298.3206481933594, val loss None, lr 0.001
iter 60, train loss 297.3565673828125, val loss None, lr 0.001
iter 70, train loss 297.52362060546875, val loss None, lr 0.001
iter 80, train loss 299.4521484375, val loss None, lr 0.001
iter 90, train loss 298.35662841796875, val loss None, lr 0.001
best loss 281.6033935546875
layer15: self_attn.k_proj
norm_0 tensor([1.6230, 1.5781, 1.5586,  ..., 1.5215, 1.5791, 1.5371], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5669, 0.6113, 0.5835,  ..., 1.2627, 1.2861, 1.3330], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 337.351318359375, val loss None, lr 0.001
iter 10, train loss 362.56866455078125, val loss None, lr 0.001
iter 20, train loss 355.7379455566406, val loss None, lr 0.001
iter 30, train loss 342.9360046386719, val loss None, lr 0.001
iter 40, train loss 339.67974853515625, val loss None, lr 0.001
iter 50, train loss 339.65185546875, val loss None, lr 0.001
iter 60, train loss 338.3500061035156, val loss None, lr 0.001
iter 70, train loss 338.5076904296875, val loss None, lr 0.001
iter 80, train loss 341.54644775390625, val loss None, lr 0.001
iter 90, train loss 339.48431396484375, val loss None, lr 0.001
best loss 309.7843933105469
layer15: self_attn.v_proj
norm_0 tensor([1.0156, 0.9771, 0.9902,  ..., 1.0430, 1.0137, 1.0068], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.0195, 1.0264, 1.0195,  ..., 0.9795, 0.9761, 0.9629], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 112.62069702148438, val loss None, lr 0.001
iter 10, train loss 112.49534606933594, val loss None, lr 0.001
iter 20, train loss 112.28544616699219, val loss None, lr 0.001
iter 30, train loss 111.78639221191406, val loss None, lr 0.001
iter 40, train loss 111.35389709472656, val loss None, lr 0.001
iter 50, train loss 111.27119445800781, val loss None, lr 0.001
iter 60, train loss 111.4656982421875, val loss None, lr 0.001
iter 70, train loss 111.43099212646484, val loss None, lr 0.001
iter 80, train loss 111.38267517089844, val loss None, lr 0.001
iter 90, train loss 111.45780944824219, val loss None, lr 0.001
best loss 111.21349334716797
layer15: self_attn.o_proj
norm_0 tensor([1.0000, 1.0098, 1.0049,  ..., 0.9663, 0.9692, 0.9546], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.9834, 0.9985, 0.9790,  ..., 1.0117, 0.9805, 1.0205], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 10.907017707824707, val loss None, lr 0.001
iter 10, train loss 10.827316284179688, val loss None, lr 0.001
iter 20, train loss 10.767756462097168, val loss None, lr 0.001
iter 30, train loss 10.665695190429688, val loss None, lr 0.001
iter 40, train loss 10.662161827087402, val loss None, lr 0.001
iter 50, train loss 10.640006065368652, val loss None, lr 0.001
iter 60, train loss 10.713214874267578, val loss None, lr 0.001
iter 70, train loss 10.667806625366211, val loss None, lr 0.001
iter 80, train loss 10.676244735717773, val loss None, lr 0.001
iter 90, train loss 10.649465560913086, val loss None, lr 0.001
best loss 10.607720375061035
layer15: mlp.gate_proj
norm_0 tensor([1.9961, 2.0000, 2.0059,  ..., 2.0059, 2.0000, 2.0000], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5493, 0.6406, 0.6123,  ..., 0.5264, 0.5767, 0.5229], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 229.54742431640625, val loss None, lr 0.001
iter 10, train loss 241.16500854492188, val loss None, lr 0.001
iter 20, train loss 236.0081024169922, val loss None, lr 0.001
iter 30, train loss 234.2373809814453, val loss None, lr 0.001
iter 40, train loss 233.9069366455078, val loss None, lr 0.001
iter 50, train loss 233.15428161621094, val loss None, lr 0.001
iter 60, train loss 232.13111877441406, val loss None, lr 0.001
iter 70, train loss 231.96920776367188, val loss None, lr 0.001
iter 80, train loss 231.56642150878906, val loss None, lr 0.001
iter 90, train loss 231.64125061035156, val loss None, lr 0.001
best loss 226.98764038085938
layer15: mlp.up_proj
norm_0 tensor([1.9668, 1.9443, 1.9502,  ..., 1.9609, 1.9629, 1.9482], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.6147, 0.6255, 0.6577,  ..., 0.5459, 0.5649, 0.5571], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 211.60890197753906, val loss None, lr 0.001
iter 10, train loss 213.1795654296875, val loss None, lr 0.001
iter 20, train loss 213.49542236328125, val loss None, lr 0.001
iter 30, train loss 213.40106201171875, val loss None, lr 0.001
iter 40, train loss 213.6591796875, val loss None, lr 0.001
iter 50, train loss 213.47821044921875, val loss None, lr 0.001
iter 60, train loss 213.65771484375, val loss None, lr 0.001
iter 70, train loss 213.6640625, val loss None, lr 0.001
iter 80, train loss 213.6818389892578, val loss None, lr 0.001
iter 90, train loss 213.7227783203125, val loss None, lr 0.001
best loss 211.60890197753906
layer15: mlp.down_proj
norm_0 tensor([1.2051, 1.1855, 1.3145,  ..., 1.0889, 1.1104, 1.0977], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.6289, 1.6133, 1.6055,  ..., 1.6436, 1.6494, 1.6436], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 6.477618217468262, val loss None, lr 0.001
iter 10, train loss 6.488422870635986, val loss None, lr 0.001
iter 20, train loss 6.443301200866699, val loss None, lr 0.001
iter 30, train loss 6.416740417480469, val loss None, lr 0.001
iter 40, train loss 6.408239364624023, val loss None, lr 0.001
iter 50, train loss 6.403791904449463, val loss None, lr 0.001
iter 60, train loss 6.396803855895996, val loss None, lr 0.001
iter 70, train loss 6.394731521606445, val loss None, lr 0.001
iter 80, train loss 6.397216796875, val loss None, lr 0.001
iter 90, train loss 6.394106864929199, val loss None, lr 0.001
best loss 6.387502193450928
32701 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
25061 MiB free out of 48676 MiB total
after cast to cpu
32293 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
layer16: self_attn.q_proj
norm_0 tensor([1.4893, 1.4844, 1.4600,  ..., 1.4814, 1.5146, 1.4990], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.7651, 0.8101, 0.8564,  ..., 1.0957, 1.0938, 1.0938], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 316.8189392089844, val loss None, lr 0.001
iter 10, train loss 335.6387634277344, val loss None, lr 0.001
iter 20, train loss 320.4115905761719, val loss None, lr 0.001
iter 30, train loss 313.63275146484375, val loss None, lr 0.001
iter 40, train loss 313.9012756347656, val loss None, lr 0.001
iter 50, train loss 315.4091491699219, val loss None, lr 0.001
iter 60, train loss 313.3706359863281, val loss None, lr 0.001
iter 70, train loss 314.53753662109375, val loss None, lr 0.001
iter 80, train loss 312.903076171875, val loss None, lr 0.001
iter 90, train loss 315.55523681640625, val loss None, lr 0.001
best loss 294.5308837890625
layer16: self_attn.k_proj
norm_0 tensor([1.5898, 1.5801, 1.5420,  ..., 1.5215, 1.5137, 1.5518], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.7666, 0.7998, 0.8311,  ..., 1.1182, 1.0986, 1.1484], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 355.4478454589844, val loss None, lr 0.001
iter 10, train loss 378.2843322753906, val loss None, lr 0.001
iter 20, train loss 368.7886657714844, val loss None, lr 0.001
iter 30, train loss 357.1917724609375, val loss None, lr 0.001
iter 40, train loss 356.5268859863281, val loss None, lr 0.001
iter 50, train loss 351.6860656738281, val loss None, lr 0.001
iter 60, train loss 351.3704528808594, val loss None, lr 0.001
iter 70, train loss 353.948486328125, val loss None, lr 0.001
iter 80, train loss 351.3017578125, val loss None, lr 0.001
iter 90, train loss 352.6253662109375, val loss None, lr 0.001
best loss 323.57958984375
layer16: self_attn.v_proj
norm_0 tensor([1.0615, 1.0146, 1.0693,  ..., 1.0898, 1.0518, 1.0322], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.2354, 1.2041, 1.2158,  ..., 1.0371, 1.0479, 1.0527], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 128.20468139648438, val loss None, lr 0.001
iter 10, train loss 127.49758911132812, val loss None, lr 0.001
iter 20, train loss 127.51712036132812, val loss None, lr 0.001
iter 30, train loss 126.79304504394531, val loss None, lr 0.001
iter 40, train loss 126.91841125488281, val loss None, lr 0.001
iter 50, train loss 126.504638671875, val loss None, lr 0.001
iter 60, train loss 126.32211303710938, val loss None, lr 0.001
iter 70, train loss 126.41814422607422, val loss None, lr 0.001
iter 80, train loss 126.62632751464844, val loss None, lr 0.001
iter 90, train loss 126.50759887695312, val loss None, lr 0.001
best loss 126.20022583007812
layer16: self_attn.o_proj
norm_0 tensor([1.2393, 1.2080, 1.2275,  ..., 1.0664, 1.0781, 1.0684], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.9907, 0.9927, 0.9731,  ..., 1.0098, 0.9839, 0.9854], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 13.483257293701172, val loss None, lr 0.001
iter 10, train loss 13.355647087097168, val loss None, lr 0.001
iter 20, train loss 13.374603271484375, val loss None, lr 0.001
iter 30, train loss 13.291839599609375, val loss None, lr 0.001
iter 40, train loss 13.232461929321289, val loss None, lr 0.001
iter 50, train loss 13.260549545288086, val loss None, lr 0.001
iter 60, train loss 13.266691207885742, val loss None, lr 0.001
iter 70, train loss 13.247965812683105, val loss None, lr 0.001
iter 80, train loss 13.23727798461914, val loss None, lr 0.001
iter 90, train loss 13.328079223632812, val loss None, lr 0.001
best loss 13.232461929321289
layer16: mlp.gate_proj
norm_0 tensor([1.9951, 2.0078, 2.0254,  ..., 2.0195, 2.0273, 2.0137], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5884, 0.5054, 0.5322,  ..., 0.5488, 0.6777, 0.8716], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 269.19085693359375, val loss None, lr 0.001
iter 10, train loss 285.3585205078125, val loss None, lr 0.001
iter 20, train loss 277.726806640625, val loss None, lr 0.001
iter 30, train loss 276.29595947265625, val loss None, lr 0.001
iter 40, train loss 272.9442443847656, val loss None, lr 0.001
iter 50, train loss 273.3562927246094, val loss None, lr 0.001
iter 60, train loss 272.4686279296875, val loss None, lr 0.001
iter 70, train loss 272.6474609375, val loss None, lr 0.001
iter 80, train loss 272.4345397949219, val loss None, lr 0.001
iter 90, train loss 272.5145568847656, val loss None, lr 0.001
best loss 264.1056823730469
layer16: mlp.up_proj
norm_0 tensor([1.9805, 1.9512, 1.9395,  ..., 1.9551, 1.9570, 1.9482], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.6250, 0.5552, 0.6064,  ..., 0.6055, 0.5176, 0.6606], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 241.5882568359375, val loss None, lr 0.001
iter 10, train loss 242.84152221679688, val loss None, lr 0.001
iter 20, train loss 243.44927978515625, val loss None, lr 0.001
iter 30, train loss 244.1660614013672, val loss None, lr 0.001
iter 40, train loss 244.12429809570312, val loss None, lr 0.001
iter 50, train loss 244.1607208251953, val loss None, lr 0.001
iter 60, train loss 244.2548065185547, val loss None, lr 0.001
iter 70, train loss 244.37484741210938, val loss None, lr 0.001
iter 80, train loss 244.09756469726562, val loss None, lr 0.001
iter 90, train loss 244.36256408691406, val loss None, lr 0.001
best loss 241.22402954101562
layer16: mlp.down_proj
norm_0 tensor([1.1953, 1.0918, 1.1787,  ..., 1.1670, 0.9341, 1.2900], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.6504, 1.6055, 1.6289,  ..., 1.6279, 1.6436, 1.6221], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 8.543350219726562, val loss None, lr 0.001
iter 10, train loss 8.566137313842773, val loss None, lr 0.001
iter 20, train loss 8.540643692016602, val loss None, lr 0.001
iter 30, train loss 8.532180786132812, val loss None, lr 0.001
iter 40, train loss 8.514227867126465, val loss None, lr 0.001
iter 50, train loss 8.514360427856445, val loss None, lr 0.001
iter 60, train loss 8.518112182617188, val loss None, lr 0.001
iter 70, train loss 8.51805305480957, val loss None, lr 0.001
iter 80, train loss 8.513236999511719, val loss None, lr 0.001
iter 90, train loss 8.524913787841797, val loss None, lr 0.001
best loss 8.502421379089355
32293 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
24653 MiB free out of 48676 MiB total
after cast to cpu
31885 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
layer17: self_attn.q_proj
norm_0 tensor([1.4795, 1.4707, 1.4639,  ..., 1.5068, 1.5332, 1.4980], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.4700, 0.4636, 0.4636,  ..., 1.3955, 1.4902, 1.3623], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 331.5037536621094, val loss None, lr 0.001
iter 10, train loss 358.68231201171875, val loss None, lr 0.001
iter 20, train loss 338.9327392578125, val loss None, lr 0.001
iter 30, train loss 330.1971130371094, val loss None, lr 0.001
iter 40, train loss 328.4251708984375, val loss None, lr 0.001
iter 50, train loss 328.50958251953125, val loss None, lr 0.001
iter 60, train loss 328.309814453125, val loss None, lr 0.001
iter 70, train loss 329.3372802734375, val loss None, lr 0.001
iter 80, train loss 328.71502685546875, val loss None, lr 0.001
iter 90, train loss 329.14898681640625, val loss None, lr 0.001
best loss 306.52239990234375
layer17: self_attn.k_proj
norm_0 tensor([1.5576, 1.5840, 1.5215,  ..., 1.4814, 1.5000, 1.5342], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.4421, 0.4617, 0.4624,  ..., 1.4277, 1.0381, 1.4160], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 365.3417053222656, val loss None, lr 0.001
iter 10, train loss 393.49072265625, val loss None, lr 0.001
iter 20, train loss 381.5689392089844, val loss None, lr 0.001
iter 30, train loss 369.20684814453125, val loss None, lr 0.001
iter 40, train loss 362.6997375488281, val loss None, lr 0.001
iter 50, train loss 363.4766845703125, val loss None, lr 0.001
iter 60, train loss 361.13916015625, val loss None, lr 0.001
iter 70, train loss 357.7182312011719, val loss None, lr 0.001
iter 80, train loss 359.54083251953125, val loss None, lr 0.001
iter 90, train loss 358.22613525390625, val loss None, lr 0.001
best loss 330.6647644042969
layer17: self_attn.v_proj
norm_0 tensor([1.0449, 1.0146, 1.0801,  ..., 1.0645, 1.0703, 1.0439], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.9409, 0.9824, 0.9521,  ..., 1.0010, 0.9766, 1.0088], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 135.6123504638672, val loss None, lr 0.001
iter 10, train loss 135.685791015625, val loss None, lr 0.001
iter 20, train loss 135.37478637695312, val loss None, lr 0.001
iter 30, train loss 134.85903930664062, val loss None, lr 0.001
iter 40, train loss 134.9873046875, val loss None, lr 0.001
iter 50, train loss 134.9449920654297, val loss None, lr 0.001
iter 60, train loss 134.6536865234375, val loss None, lr 0.001
iter 70, train loss 134.89573669433594, val loss None, lr 0.001
iter 80, train loss 134.87525939941406, val loss None, lr 0.001
iter 90, train loss 134.95460510253906, val loss None, lr 0.001
best loss 134.6170196533203
layer17: self_attn.o_proj
norm_0 tensor([1.0244, 1.0225, 0.9854,  ..., 1.0234, 0.9883, 1.0273], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.0020, 1.0020, 0.9790,  ..., 1.0176, 1.0029, 1.0137], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 9.752521514892578, val loss None, lr 0.001
iter 10, train loss 9.541794776916504, val loss None, lr 0.001
iter 20, train loss 9.358041763305664, val loss None, lr 0.001
iter 30, train loss 9.26237678527832, val loss None, lr 0.001
iter 40, train loss 9.175200462341309, val loss None, lr 0.001
iter 50, train loss 9.250673294067383, val loss None, lr 0.001
iter 60, train loss 9.20997428894043, val loss None, lr 0.001
iter 70, train loss 9.193754196166992, val loss None, lr 0.001
iter 80, train loss 9.191539764404297, val loss None, lr 0.001
iter 90, train loss 9.291121482849121, val loss None, lr 0.001
best loss 9.15002727508545
layer17: mlp.gate_proj
norm_0 tensor([2.0137, 2.0352, 2.0410,  ..., 2.0469, 2.0312, 2.0312], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5410, 0.5557, 0.5542,  ..., 0.5640, 0.6533, 0.5396], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 303.790283203125, val loss None, lr 0.001
iter 10, train loss 319.77764892578125, val loss None, lr 0.001
iter 20, train loss 317.039794921875, val loss None, lr 0.001
iter 30, train loss 314.49462890625, val loss None, lr 0.001
iter 40, train loss 311.6716003417969, val loss None, lr 0.001
iter 50, train loss 310.55511474609375, val loss None, lr 0.001
iter 60, train loss 309.2296142578125, val loss None, lr 0.001
iter 70, train loss 308.30108642578125, val loss None, lr 0.001
iter 80, train loss 307.61376953125, val loss None, lr 0.001
iter 90, train loss 306.568115234375, val loss None, lr 0.001
best loss 298.8487243652344
layer17: mlp.up_proj
norm_0 tensor([1.9707, 1.9473, 1.9355,  ..., 1.9434, 1.9570, 1.9453], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.6118, 0.6030, 0.5854,  ..., 0.6094, 0.6396, 0.5801], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 265.9812316894531, val loss None, lr 0.001
iter 10, train loss 266.4231262207031, val loss None, lr 0.001
iter 20, train loss 267.3423767089844, val loss None, lr 0.001
iter 30, train loss 268.1445617675781, val loss None, lr 0.001
iter 40, train loss 269.0317687988281, val loss None, lr 0.001
iter 50, train loss 269.216796875, val loss None, lr 0.001
iter 60, train loss 269.433837890625, val loss None, lr 0.001
iter 70, train loss 269.29229736328125, val loss None, lr 0.001
iter 80, train loss 269.7574768066406, val loss None, lr 0.001
iter 90, train loss 270.2113037109375, val loss None, lr 0.001
best loss 265.5191650390625
layer17: mlp.down_proj
norm_0 tensor([1.1807, 1.1660, 1.1338,  ..., 1.1768, 1.2031, 1.1309], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.6416, 1.6201, 1.6396,  ..., 1.6279, 1.6738, 1.6338], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 9.235962867736816, val loss None, lr 0.001
iter 10, train loss 9.22262191772461, val loss None, lr 0.001
iter 20, train loss 9.168745040893555, val loss None, lr 0.001
iter 30, train loss 9.131498336791992, val loss None, lr 0.001
iter 40, train loss 9.0931396484375, val loss None, lr 0.001
iter 50, train loss 9.074711799621582, val loss None, lr 0.001
iter 60, train loss 9.058622360229492, val loss None, lr 0.001
iter 70, train loss 9.038785934448242, val loss None, lr 0.001
iter 80, train loss 9.035359382629395, val loss None, lr 0.001
iter 90, train loss 9.017943382263184, val loss None, lr 0.001
best loss 9.014326095581055
31885 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
24245 MiB free out of 48676 MiB total
after cast to cpu
31477 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
layer18: self_attn.q_proj
norm_0 tensor([1.4463, 1.4893, 1.4805,  ..., 1.4541, 1.4648, 1.4648], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.7310, 0.8086, 0.8174,  ..., 1.3447, 1.3838, 1.3779], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 362.05328369140625, val loss None, lr 0.001
iter 10, train loss 386.3497009277344, val loss None, lr 0.001
iter 20, train loss 362.92291259765625, val loss None, lr 0.001
iter 30, train loss 349.3280944824219, val loss None, lr 0.001
iter 40, train loss 347.32183837890625, val loss None, lr 0.001
iter 50, train loss 347.32763671875, val loss None, lr 0.001
iter 60, train loss 347.28533935546875, val loss None, lr 0.001
iter 70, train loss 348.358154296875, val loss None, lr 0.001
iter 80, train loss 346.6258544921875, val loss None, lr 0.001
iter 90, train loss 345.8872985839844, val loss None, lr 0.001
best loss 330.011474609375
layer18: self_attn.k_proj
norm_0 tensor([1.5254, 1.5303, 1.4717,  ..., 1.4941, 1.4668, 1.4463], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.7163, 0.7930, 0.8096,  ..., 1.4365, 1.5371, 1.5498], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 391.59259033203125, val loss None, lr 0.001
iter 10, train loss 415.1654357910156, val loss None, lr 0.001
iter 20, train loss 398.81536865234375, val loss None, lr 0.001
iter 30, train loss 382.039306640625, val loss None, lr 0.001
iter 40, train loss 376.43865966796875, val loss None, lr 0.001
iter 50, train loss 374.3327331542969, val loss None, lr 0.001
iter 60, train loss 376.06365966796875, val loss None, lr 0.001
iter 70, train loss 374.8103332519531, val loss None, lr 0.001
iter 80, train loss 373.0449523925781, val loss None, lr 0.001
iter 90, train loss 374.6556091308594, val loss None, lr 0.001
best loss 352.346923828125
layer18: self_attn.v_proj
norm_0 tensor([1.0791, 1.0596, 1.0859,  ..., 1.1260, 1.1191, 1.1211], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.1074, 1.1133, 1.1289,  ..., 0.9028, 0.9150, 0.9268], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 164.068115234375, val loss None, lr 0.001
iter 10, train loss 163.86500549316406, val loss None, lr 0.001
iter 20, train loss 163.6876983642578, val loss None, lr 0.001
iter 30, train loss 163.45001220703125, val loss None, lr 0.001
iter 40, train loss 163.65811157226562, val loss None, lr 0.001
iter 50, train loss 163.67127990722656, val loss None, lr 0.001
iter 60, train loss 163.3314666748047, val loss None, lr 0.001
iter 70, train loss 163.29833984375, val loss None, lr 0.001
iter 80, train loss 163.84996032714844, val loss None, lr 0.001
iter 90, train loss 163.9871826171875, val loss None, lr 0.001
best loss 163.18792724609375
layer18: self_attn.o_proj
norm_0 tensor([1.1689, 1.1865, 1.1924,  ..., 0.9775, 0.9907, 0.9966], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.9873, 0.9946, 0.9844,  ..., 1.0000, 1.0010, 0.9917], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 8.459556579589844, val loss None, lr 0.001
iter 10, train loss 8.23733901977539, val loss None, lr 0.001
iter 20, train loss 8.184452056884766, val loss None, lr 0.001
iter 30, train loss 8.162997245788574, val loss None, lr 0.001
iter 40, train loss 8.047407150268555, val loss None, lr 0.001
iter 50, train loss 7.953636169433594, val loss None, lr 0.001
iter 60, train loss 7.947299003601074, val loss None, lr 0.001
iter 70, train loss 7.901212215423584, val loss None, lr 0.001
iter 80, train loss 7.909736633300781, val loss None, lr 0.001
iter 90, train loss 7.897165298461914, val loss None, lr 0.001
best loss 7.890370845794678
layer18: mlp.gate_proj
norm_0 tensor([2.0293, 2.0410, 2.0410,  ..., 2.0645, 2.0430, 2.0488], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5918, 0.5991, 0.5757,  ..., 0.7612, 0.6367, 0.6006], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 343.42425537109375, val loss None, lr 0.001
iter 10, train loss 359.35003662109375, val loss None, lr 0.001
iter 20, train loss 356.29229736328125, val loss None, lr 0.001
iter 30, train loss 354.99127197265625, val loss None, lr 0.001
iter 40, train loss 354.5256042480469, val loss None, lr 0.001
iter 50, train loss 353.73138427734375, val loss None, lr 0.001
iter 60, train loss 351.6315002441406, val loss None, lr 0.001
iter 70, train loss 350.2743835449219, val loss None, lr 0.001
iter 80, train loss 349.0950012207031, val loss None, lr 0.001
iter 90, train loss 349.3298645019531, val loss None, lr 0.001
best loss 338.89239501953125
layer18: mlp.up_proj
norm_0 tensor([1.9561, 1.9521, 1.9531,  ..., 1.9365, 1.9541, 1.9434], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.6191, 0.6401, 0.6274,  ..., 0.6147, 0.6201, 0.6216], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 291.82647705078125, val loss None, lr 0.001
iter 10, train loss 292.4576110839844, val loss None, lr 0.001
iter 20, train loss 293.033935546875, val loss None, lr 0.001
iter 30, train loss 293.2290344238281, val loss None, lr 0.001
iter 40, train loss 294.06640625, val loss None, lr 0.001
iter 50, train loss 294.63311767578125, val loss None, lr 0.001
iter 60, train loss 294.40216064453125, val loss None, lr 0.001
iter 70, train loss 295.1358642578125, val loss None, lr 0.001
iter 80, train loss 295.7479248046875, val loss None, lr 0.001
iter 90, train loss 296.14892578125, val loss None, lr 0.001
best loss 291.82647705078125
layer18: mlp.down_proj
norm_0 tensor([1.1943, 1.2393, 1.1982,  ..., 1.1797, 1.1914, 1.1836], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.6309, 1.6201, 1.6191,  ..., 1.6230, 1.6641, 1.6348], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 10.569709777832031, val loss None, lr 0.001
iter 10, train loss 10.573328018188477, val loss None, lr 0.001
iter 20, train loss 10.527451515197754, val loss None, lr 0.001
iter 30, train loss 10.472208023071289, val loss None, lr 0.001
iter 40, train loss 10.426450729370117, val loss None, lr 0.001
iter 50, train loss 10.399096488952637, val loss None, lr 0.001
iter 60, train loss 10.382315635681152, val loss None, lr 0.001
iter 70, train loss 10.355195999145508, val loss None, lr 0.001
iter 80, train loss 10.34833812713623, val loss None, lr 0.001
iter 90, train loss 10.35179328918457, val loss None, lr 0.001
best loss 10.343156814575195
31477 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
23837 MiB free out of 48676 MiB total
after cast to cpu
31069 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
layer19: self_attn.q_proj
norm_0 tensor([1.4170, 1.4453, 1.4180,  ..., 1.4248, 1.4531, 1.4580], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.6357, 0.7725, 0.7656,  ..., 1.3174, 1.2842, 1.2930], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 342.87353515625, val loss None, lr 0.001
iter 10, train loss 366.0221862792969, val loss None, lr 0.001
iter 20, train loss 345.9569396972656, val loss None, lr 0.001
iter 30, train loss 341.44390869140625, val loss None, lr 0.001
iter 40, train loss 335.9600524902344, val loss None, lr 0.001
iter 50, train loss 336.2127380371094, val loss None, lr 0.001
iter 60, train loss 335.4700927734375, val loss None, lr 0.001
iter 70, train loss 334.3083190917969, val loss None, lr 0.001
iter 80, train loss 335.9520263671875, val loss None, lr 0.001
iter 90, train loss 336.3960876464844, val loss None, lr 0.001
best loss 317.15509033203125
layer19: self_attn.k_proj
norm_0 tensor([1.4941, 1.5205, 1.4795,  ..., 1.4336, 1.4473, 1.4717], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5762, 0.7686, 0.7637,  ..., 1.3730, 1.3975, 1.3691], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 370.595703125, val loss None, lr 0.001
iter 10, train loss 395.634765625, val loss None, lr 0.001
iter 20, train loss 383.7339782714844, val loss None, lr 0.001
iter 30, train loss 370.6038818359375, val loss None, lr 0.001
iter 40, train loss 366.12542724609375, val loss None, lr 0.001
iter 50, train loss 362.51141357421875, val loss None, lr 0.001
iter 60, train loss 363.79766845703125, val loss None, lr 0.001
iter 70, train loss 365.99530029296875, val loss None, lr 0.001
iter 80, train loss 364.1109313964844, val loss None, lr 0.001
iter 90, train loss 362.2047119140625, val loss None, lr 0.001
best loss 337.0743408203125
layer19: self_attn.v_proj
norm_0 tensor([1.0986, 1.0859, 1.1182,  ..., 1.1279, 1.1064, 1.0957], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.0381, 1.0420, 1.0430,  ..., 1.0039, 0.9912, 0.9868], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 166.1918487548828, val loss None, lr 0.001
iter 10, train loss 165.43499755859375, val loss None, lr 0.001
iter 20, train loss 165.26776123046875, val loss None, lr 0.001
iter 30, train loss 164.99774169921875, val loss None, lr 0.001
iter 40, train loss 164.75924682617188, val loss None, lr 0.001
iter 50, train loss 165.44927978515625, val loss None, lr 0.001
iter 60, train loss 165.09158325195312, val loss None, lr 0.001
iter 70, train loss 165.09693908691406, val loss None, lr 0.001
iter 80, train loss 165.0037841796875, val loss None, lr 0.001
iter 90, train loss 165.14181518554688, val loss None, lr 0.001
best loss 164.65512084960938
layer19: self_attn.o_proj
norm_0 tensor([1.1250, 1.1182, 1.1289,  ..., 1.0898, 1.0742, 1.0723], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.9971, 0.9946, 0.9863,  ..., 1.0146, 1.0020, 0.9941], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 8.895318984985352, val loss None, lr 0.001
iter 10, train loss 8.43305492401123, val loss None, lr 0.001
iter 20, train loss 8.422727584838867, val loss None, lr 0.001
iter 30, train loss 8.353312492370605, val loss None, lr 0.001
iter 40, train loss 8.253827095031738, val loss None, lr 0.001
iter 50, train loss 8.251306533813477, val loss None, lr 0.001
iter 60, train loss 8.236552238464355, val loss None, lr 0.001
iter 70, train loss 8.230722427368164, val loss None, lr 0.001
iter 80, train loss 8.234704971313477, val loss None, lr 0.001
iter 90, train loss 8.28493881225586, val loss None, lr 0.001
best loss 8.206988334655762
layer19: mlp.gate_proj
norm_0 tensor([2.0488, 2.0527, 2.0625,  ..., 2.0684, 2.0508, 2.0605], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5234, 0.5581, 0.5713,  ..., 0.5615, 0.5903, 0.5181], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 363.35308837890625, val loss None, lr 0.001
iter 10, train loss 378.60150146484375, val loss None, lr 0.001
iter 20, train loss 377.07373046875, val loss None, lr 0.001
iter 30, train loss 375.1502685546875, val loss None, lr 0.001
iter 40, train loss 373.7983703613281, val loss None, lr 0.001
iter 50, train loss 372.4696044921875, val loss None, lr 0.001
iter 60, train loss 371.248291015625, val loss None, lr 0.001
iter 70, train loss 370.26690673828125, val loss None, lr 0.001
iter 80, train loss 371.31671142578125, val loss None, lr 0.001
iter 90, train loss 372.0002746582031, val loss None, lr 0.001
best loss 360.0721435546875
layer19: mlp.up_proj
norm_0 tensor([1.9551, 1.9395, 1.9385,  ..., 1.9365, 1.9551, 1.9414], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5293, 0.5942, 0.6035,  ..., 0.5835, 0.6016, 0.5757], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 311.7584533691406, val loss None, lr 0.001
iter 10, train loss 312.12060546875, val loss None, lr 0.001
iter 20, train loss 313.0640869140625, val loss None, lr 0.001
iter 30, train loss 313.68487548828125, val loss None, lr 0.001
iter 40, train loss 313.75140380859375, val loss None, lr 0.001
iter 50, train loss 313.88409423828125, val loss None, lr 0.001
iter 60, train loss 314.12786865234375, val loss None, lr 0.001
iter 70, train loss 314.39312744140625, val loss None, lr 0.001
iter 80, train loss 315.26873779296875, val loss None, lr 0.001
iter 90, train loss 315.3353271484375, val loss None, lr 0.001
best loss 311.74908447265625
layer19: mlp.down_proj
norm_0 tensor([1.0137, 1.1533, 1.1680,  ..., 1.1416, 1.1641, 1.1152], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.6338, 1.6182, 1.6416,  ..., 1.6455, 1.6562, 1.6279], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 11.291705131530762, val loss None, lr 0.001
iter 10, train loss 11.315741539001465, val loss None, lr 0.001
iter 20, train loss 11.258569717407227, val loss None, lr 0.001
iter 30, train loss 11.210762023925781, val loss None, lr 0.001
iter 40, train loss 11.18044376373291, val loss None, lr 0.001
iter 50, train loss 11.143524169921875, val loss None, lr 0.001
iter 60, train loss 11.13110637664795, val loss None, lr 0.001
iter 70, train loss 11.153207778930664, val loss None, lr 0.001
iter 80, train loss 11.13308048248291, val loss None, lr 0.001
iter 90, train loss 11.144322395324707, val loss None, lr 0.001
best loss 11.125991821289062
31069 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
23429 MiB free out of 48676 MiB total
after cast to cpu
30661 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
layer20: self_attn.q_proj
norm_0 tensor([1.4395, 1.4912, 1.4180,  ..., 1.4355, 1.4502, 1.4541], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.3748, 0.4128, 0.4402,  ..., 1.1904, 1.2539, 1.0312], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 354.19757080078125, val loss None, lr 0.001
iter 10, train loss 378.60272216796875, val loss None, lr 0.001
iter 20, train loss 361.3248291015625, val loss None, lr 0.001
iter 30, train loss 347.9599609375, val loss None, lr 0.001
iter 40, train loss 346.97467041015625, val loss None, lr 0.001
iter 50, train loss 347.3675842285156, val loss None, lr 0.001
iter 60, train loss 346.276123046875, val loss None, lr 0.001
iter 70, train loss 345.58087158203125, val loss None, lr 0.001
iter 80, train loss 345.3811950683594, val loss None, lr 0.001
iter 90, train loss 348.7784423828125, val loss None, lr 0.001
best loss 326.647705078125
layer20: self_attn.k_proj
norm_0 tensor([1.5039, 1.5273, 1.4980,  ..., 1.4531, 1.4551, 1.4834], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.3584, 0.3821, 0.3943,  ..., 1.2578, 1.3662, 1.3652], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 381.3771667480469, val loss None, lr 0.001
iter 10, train loss 408.6920471191406, val loss None, lr 0.001
iter 20, train loss 394.7860107421875, val loss None, lr 0.001
iter 30, train loss 384.248779296875, val loss None, lr 0.001
iter 40, train loss 378.4476318359375, val loss None, lr 0.001
iter 50, train loss 375.83673095703125, val loss None, lr 0.001
iter 60, train loss 372.49554443359375, val loss None, lr 0.001
iter 70, train loss 373.6690673828125, val loss None, lr 0.001
iter 80, train loss 375.3462219238281, val loss None, lr 0.001
iter 90, train loss 375.77227783203125, val loss None, lr 0.001
best loss 349.5030822753906
layer20: self_attn.v_proj
norm_0 tensor([1.1191, 1.0830, 1.1377,  ..., 1.1338, 1.1348, 1.1270], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.9707, 0.9888, 0.9868,  ..., 0.9263, 0.9385, 0.9614], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 171.34397888183594, val loss None, lr 0.001
iter 10, train loss 171.1686248779297, val loss None, lr 0.001
iter 20, train loss 170.5113525390625, val loss None, lr 0.001
iter 30, train loss 170.40841674804688, val loss None, lr 0.001
iter 40, train loss 170.24546813964844, val loss None, lr 0.001
iter 50, train loss 170.08668518066406, val loss None, lr 0.001
iter 60, train loss 170.03231811523438, val loss None, lr 0.001
iter 70, train loss 169.95635986328125, val loss None, lr 0.001
iter 80, train loss 170.05104064941406, val loss None, lr 0.001
iter 90, train loss 169.86691284179688, val loss None, lr 0.001
best loss 169.79942321777344
layer20: self_attn.o_proj
norm_0 tensor([1.0674, 1.0762, 1.0830,  ..., 1.0527, 1.0527, 1.0654], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.0127, 1.0049, 0.9897,  ..., 0.9941, 0.9941, 1.0234], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 13.508757591247559, val loss None, lr 0.001
iter 10, train loss 12.146504402160645, val loss None, lr 0.001
iter 20, train loss 11.464555740356445, val loss None, lr 0.001
iter 30, train loss 10.96044635772705, val loss None, lr 0.001
iter 40, train loss 10.74107837677002, val loss None, lr 0.001
iter 50, train loss 10.601289749145508, val loss None, lr 0.001
iter 60, train loss 10.56185531616211, val loss None, lr 0.001
iter 70, train loss 10.507718086242676, val loss None, lr 0.001
iter 80, train loss 10.448436737060547, val loss None, lr 0.001
iter 90, train loss 10.374213218688965, val loss None, lr 0.001
best loss 10.354215621948242
layer20: mlp.gate_proj
norm_0 tensor([2.0840, 2.0762, 2.0762,  ..., 2.0781, 2.0684, 2.0410], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.6143, 0.5889, 0.5723,  ..., 0.6572, 0.5854, 0.5718], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 393.7664794921875, val loss None, lr 0.001
iter 10, train loss 410.94512939453125, val loss None, lr 0.001
iter 20, train loss 410.33673095703125, val loss None, lr 0.001
iter 30, train loss 407.6432800292969, val loss None, lr 0.001
iter 40, train loss 406.5127868652344, val loss None, lr 0.001
iter 50, train loss 406.0745849609375, val loss None, lr 0.001
iter 60, train loss 404.8559875488281, val loss None, lr 0.001
iter 70, train loss 404.02276611328125, val loss None, lr 0.001
iter 80, train loss 404.6425476074219, val loss None, lr 0.001
iter 90, train loss 405.88623046875, val loss None, lr 0.001
best loss 389.321044921875
layer20: mlp.up_proj
norm_0 tensor([1.9189, 1.9297, 1.9297,  ..., 1.9404, 1.9395, 1.9697], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.6250, 0.6016, 0.6074,  ..., 0.5781, 0.5952, 0.6099], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 334.44488525390625, val loss None, lr 0.001
iter 10, train loss 334.9599609375, val loss None, lr 0.001
iter 20, train loss 336.64862060546875, val loss None, lr 0.001
iter 30, train loss 338.49505615234375, val loss None, lr 0.001
iter 40, train loss 339.5569763183594, val loss None, lr 0.001
iter 50, train loss 340.13714599609375, val loss None, lr 0.001
iter 60, train loss 340.50897216796875, val loss None, lr 0.001
iter 70, train loss 340.9018249511719, val loss None, lr 0.001
iter 80, train loss 341.4302062988281, val loss None, lr 0.001
iter 90, train loss 341.6377868652344, val loss None, lr 0.001
best loss 334.1961669921875
layer20: mlp.down_proj
norm_0 tensor([1.1963, 1.1641, 1.1738,  ..., 1.1289, 1.1533, 1.1768], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.6328, 1.6299, 1.5996,  ..., 1.6387, 1.6143, 1.6260], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 13.837039947509766, val loss None, lr 0.001
iter 10, train loss 13.871869087219238, val loss None, lr 0.001
iter 20, train loss 13.810517311096191, val loss None, lr 0.001
iter 30, train loss 13.829607963562012, val loss None, lr 0.001
iter 40, train loss 13.804328918457031, val loss None, lr 0.001
iter 50, train loss 13.800652503967285, val loss None, lr 0.001
iter 60, train loss 13.805116653442383, val loss None, lr 0.001
iter 70, train loss 13.839154243469238, val loss None, lr 0.001
iter 80, train loss 13.83942985534668, val loss None, lr 0.001
iter 90, train loss 13.85150146484375, val loss None, lr 0.001
best loss 13.795693397521973
30661 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
23021 MiB free out of 48676 MiB total
after cast to cpu
30253 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
layer21: self_attn.q_proj
norm_0 tensor([1.4141, 1.4355, 1.4150,  ..., 1.3877, 1.4229, 1.3867], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.6099, 0.6714, 0.6670,  ..., 1.2812, 1.0244, 1.3604], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 364.7615966796875, val loss None, lr 0.001
iter 10, train loss 388.8818359375, val loss None, lr 0.001
iter 20, train loss 371.3271179199219, val loss None, lr 0.001
iter 30, train loss 361.8466796875, val loss None, lr 0.001
iter 40, train loss 361.5069580078125, val loss None, lr 0.001
iter 50, train loss 360.4684143066406, val loss None, lr 0.001
iter 60, train loss 359.74603271484375, val loss None, lr 0.001
iter 70, train loss 361.3185119628906, val loss None, lr 0.001
iter 80, train loss 360.4950866699219, val loss None, lr 0.001
iter 90, train loss 362.4718933105469, val loss None, lr 0.001
best loss 344.7853088378906
layer21: self_attn.k_proj
norm_0 tensor([1.4551, 1.4717, 1.4316,  ..., 1.4121, 1.4375, 1.4316], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5571, 0.6606, 0.6641,  ..., 1.4189, 1.1855, 1.3262], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 386.13970947265625, val loss None, lr 0.001
iter 10, train loss 412.1426086425781, val loss None, lr 0.001
iter 20, train loss 385.50689697265625, val loss None, lr 0.001
iter 30, train loss 381.4500732421875, val loss None, lr 0.001
iter 40, train loss 377.4333801269531, val loss None, lr 0.001
iter 50, train loss 378.697509765625, val loss None, lr 0.001
iter 60, train loss 378.010498046875, val loss None, lr 0.001
iter 70, train loss 381.0968017578125, val loss None, lr 0.001
iter 80, train loss 379.02020263671875, val loss None, lr 0.001
iter 90, train loss 380.65704345703125, val loss None, lr 0.001
best loss 363.62762451171875
layer21: self_attn.v_proj
norm_0 tensor([1.1396, 1.1367, 1.1572,  ..., 1.1680, 1.1572, 1.1758], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.9434, 0.9448, 0.9663,  ..., 1.0039, 1.0078, 0.9932], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 202.39346313476562, val loss None, lr 0.001
iter 10, train loss 201.7493133544922, val loss None, lr 0.001
iter 20, train loss 201.34217834472656, val loss None, lr 0.001
iter 30, train loss 201.6984405517578, val loss None, lr 0.001
iter 40, train loss 201.75450134277344, val loss None, lr 0.001
iter 50, train loss 201.61578369140625, val loss None, lr 0.001
iter 60, train loss 201.6688232421875, val loss None, lr 0.001
iter 70, train loss 201.76263427734375, val loss None, lr 0.001
iter 80, train loss 201.52053833007812, val loss None, lr 0.001
iter 90, train loss 201.75509643554688, val loss None, lr 0.001
best loss 201.13360595703125
layer21: self_attn.o_proj
norm_0 tensor([1.0898, 1.0791, 1.1064,  ..., 1.1357, 1.1426, 1.1260], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.0186, 1.0312, 0.9946,  ..., 1.0088, 0.9761, 1.0010], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 10.296520233154297, val loss None, lr 0.001
iter 10, train loss 9.550138473510742, val loss None, lr 0.001
iter 20, train loss 9.387618064880371, val loss None, lr 0.001
iter 30, train loss 9.17741870880127, val loss None, lr 0.001
iter 40, train loss 9.080202102661133, val loss None, lr 0.001
iter 50, train loss 8.991690635681152, val loss None, lr 0.001
iter 60, train loss 8.942197799682617, val loss None, lr 0.001
iter 70, train loss 8.905606269836426, val loss None, lr 0.001
iter 80, train loss 8.903420448303223, val loss None, lr 0.001
iter 90, train loss 8.886574745178223, val loss None, lr 0.001
best loss 8.866999626159668
layer21: mlp.gate_proj
norm_0 tensor([2.0898, 2.0723, 2.0703,  ..., 2.0879, 2.0684, 2.0742], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5762, 0.6357, 0.8628,  ..., 0.6323, 0.5581, 0.6060], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 418.4781494140625, val loss None, lr 0.001
iter 10, train loss 436.9237976074219, val loss None, lr 0.001
iter 20, train loss 436.1583251953125, val loss None, lr 0.001
iter 30, train loss 433.46087646484375, val loss None, lr 0.001
iter 40, train loss 433.55242919921875, val loss None, lr 0.001
iter 50, train loss 433.7169189453125, val loss None, lr 0.001
iter 60, train loss 432.5389709472656, val loss None, lr 0.001
iter 70, train loss 433.0866394042969, val loss None, lr 0.001
iter 80, train loss 433.733154296875, val loss None, lr 0.001
iter 90, train loss 434.344482421875, val loss None, lr 0.001
best loss 414.83123779296875
layer21: mlp.up_proj
norm_0 tensor([1.9199, 1.9414, 1.9492,  ..., 1.9453, 1.9502, 1.9482], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.6128, 0.6001, 0.5967,  ..., 0.6216, 0.5693, 0.6104], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 351.50408935546875, val loss None, lr 0.001
iter 10, train loss 352.0054931640625, val loss None, lr 0.001
iter 20, train loss 353.10479736328125, val loss None, lr 0.001
iter 30, train loss 354.29052734375, val loss None, lr 0.001
iter 40, train loss 354.9384460449219, val loss None, lr 0.001
iter 50, train loss 355.63739013671875, val loss None, lr 0.001
iter 60, train loss 356.1131896972656, val loss None, lr 0.001
iter 70, train loss 357.48345947265625, val loss None, lr 0.001
iter 80, train loss 358.0454406738281, val loss None, lr 0.001
iter 90, train loss 357.90386962890625, val loss None, lr 0.001
best loss 351.4502868652344
layer21: mlp.down_proj
norm_0 tensor([1.1758, 1.1504, 1.1396,  ..., 1.1865, 1.0908, 1.1748], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.6426, 1.6338, 1.6162,  ..., 1.6455, 1.6689, 1.6455], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 14.427471160888672, val loss None, lr 0.001
iter 10, train loss 14.366310119628906, val loss None, lr 0.001
iter 20, train loss 14.267069816589355, val loss None, lr 0.001
iter 30, train loss 14.189302444458008, val loss None, lr 0.001
iter 40, train loss 14.103267669677734, val loss None, lr 0.001
iter 50, train loss 14.088231086730957, val loss None, lr 0.001
iter 60, train loss 14.065526008605957, val loss None, lr 0.001
iter 70, train loss 14.063004493713379, val loss None, lr 0.001
iter 80, train loss 14.06646728515625, val loss None, lr 0.001
iter 90, train loss 14.071807861328125, val loss None, lr 0.001
best loss 14.05481243133545
30253 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
22613 MiB free out of 48676 MiB total
after cast to cpu
29845 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
layer22: self_attn.q_proj
norm_0 tensor([1.4336, 1.4648, 1.4492,  ..., 1.4375, 1.4297, 1.4365], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.0156, 1.0264, 1.0361,  ..., 1.3164, 1.3232, 1.5078], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 382.37841796875, val loss None, lr 0.001
iter 10, train loss 416.28546142578125, val loss None, lr 0.001
iter 20, train loss 391.420166015625, val loss None, lr 0.001
iter 30, train loss 383.16387939453125, val loss None, lr 0.001
iter 40, train loss 385.3652038574219, val loss None, lr 0.001
iter 50, train loss 383.478759765625, val loss None, lr 0.001
iter 60, train loss 381.5450744628906, val loss None, lr 0.001
iter 70, train loss 381.18035888671875, val loss None, lr 0.001
iter 80, train loss 382.87445068359375, val loss None, lr 0.001
iter 90, train loss 380.89581298828125, val loss None, lr 0.001
best loss 365.2654113769531
layer22: self_attn.k_proj
norm_0 tensor([1.4746, 1.4775, 1.4902,  ..., 1.4658, 1.4785, 1.4824], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.0010, 1.0156, 1.0244,  ..., 0.8042, 1.3096, 1.3477], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 410.4793395996094, val loss None, lr 0.001
iter 10, train loss 436.9076843261719, val loss None, lr 0.001
iter 20, train loss 417.76837158203125, val loss None, lr 0.001
iter 30, train loss 411.63079833984375, val loss None, lr 0.001
iter 40, train loss 408.7106018066406, val loss None, lr 0.001
iter 50, train loss 405.8583068847656, val loss None, lr 0.001
iter 60, train loss 407.5152587890625, val loss None, lr 0.001
iter 70, train loss 406.7281494140625, val loss None, lr 0.001
iter 80, train loss 409.75250244140625, val loss None, lr 0.001
iter 90, train loss 406.4259033203125, val loss None, lr 0.001
best loss 387.19580078125
layer22: self_attn.v_proj
norm_0 tensor([1.1533, 1.1396, 1.1543,  ..., 1.1416, 1.1611, 1.1689], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.0938, 1.0869, 1.0977,  ..., 1.0674, 1.0605, 1.0586], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 209.05001831054688, val loss None, lr 0.001
iter 10, train loss 208.96456909179688, val loss None, lr 0.001
iter 20, train loss 208.39967346191406, val loss None, lr 0.001
iter 30, train loss 207.9641876220703, val loss None, lr 0.001
iter 40, train loss 207.80966186523438, val loss None, lr 0.001
iter 50, train loss 207.95285034179688, val loss None, lr 0.001
iter 60, train loss 208.06910705566406, val loss None, lr 0.001
iter 70, train loss 207.73736572265625, val loss None, lr 0.001
iter 80, train loss 207.85154724121094, val loss None, lr 0.001
iter 90, train loss 207.69744873046875, val loss None, lr 0.001
best loss 207.2157745361328
layer22: self_attn.o_proj
norm_0 tensor([1.2227, 1.2178, 1.2207,  ..., 1.1592, 1.1973, 1.1738], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.9922, 0.9956, 1.0020,  ..., 0.9902, 1.0010, 0.9966], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 56.39710235595703, val loss None, lr 0.001
iter 10, train loss 38.42435073852539, val loss None, lr 0.001
iter 20, train loss 31.041969299316406, val loss None, lr 0.001
iter 30, train loss 21.971843719482422, val loss None, lr 0.001
iter 40, train loss 18.095849990844727, val loss None, lr 0.001
iter 50, train loss 15.540428161621094, val loss None, lr 0.001
iter 60, train loss 15.185778617858887, val loss None, lr 0.001
iter 70, train loss 14.960532188415527, val loss None, lr 0.001
iter 80, train loss 14.772171974182129, val loss None, lr 0.001
iter 90, train loss 14.587909698486328, val loss None, lr 0.001
best loss 14.497032165527344
layer22: mlp.gate_proj
norm_0 tensor([2.1055, 2.0918, 2.1211,  ..., 2.0996, 2.0859, 2.0898], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.6201, 0.5845, 0.5884,  ..., 0.5752, 0.6777, 0.5820], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 427.93231201171875, val loss None, lr 0.001
iter 10, train loss 443.36871337890625, val loss None, lr 0.001
iter 20, train loss 442.487060546875, val loss None, lr 0.001
iter 30, train loss 442.70281982421875, val loss None, lr 0.001
iter 40, train loss 442.54998779296875, val loss None, lr 0.001
iter 50, train loss 441.55438232421875, val loss None, lr 0.001
iter 60, train loss 439.7804870605469, val loss None, lr 0.001
iter 70, train loss 440.8828125, val loss None, lr 0.001
iter 80, train loss 441.69561767578125, val loss None, lr 0.001
iter 90, train loss 442.0770263671875, val loss None, lr 0.001
best loss 424.17523193359375
layer22: mlp.up_proj
norm_0 tensor([1.9238, 1.9365, 1.9082,  ..., 1.9385, 1.9424, 1.9453], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.6074, 0.5898, 0.6133,  ..., 0.6416, 0.6157, 0.6045], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 355.6418151855469, val loss None, lr 0.001
iter 10, train loss 356.1891784667969, val loss None, lr 0.001
iter 20, train loss 357.212890625, val loss None, lr 0.001
iter 30, train loss 359.0277404785156, val loss None, lr 0.001
iter 40, train loss 359.85308837890625, val loss None, lr 0.001
iter 50, train loss 360.623046875, val loss None, lr 0.001
iter 60, train loss 361.0523681640625, val loss None, lr 0.001
iter 70, train loss 361.4400634765625, val loss None, lr 0.001
iter 80, train loss 362.4149475097656, val loss None, lr 0.001
iter 90, train loss 362.8603515625, val loss None, lr 0.001
best loss 355.6418151855469
layer22: mlp.down_proj
norm_0 tensor([1.1670, 1.1377, 1.1826,  ..., 1.2158, 1.1846, 1.1650], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.6611, 1.6270, 1.6455,  ..., 1.6406, 1.6670, 1.6357], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 14.959738731384277, val loss None, lr 0.001
iter 10, train loss 15.00796127319336, val loss None, lr 0.001
iter 20, train loss 14.959174156188965, val loss None, lr 0.001
iter 30, train loss 14.932117462158203, val loss None, lr 0.001
iter 40, train loss 14.933401107788086, val loss None, lr 0.001
iter 50, train loss 14.92885971069336, val loss None, lr 0.001
iter 60, train loss 14.936450004577637, val loss None, lr 0.001
iter 70, train loss 14.95401382446289, val loss None, lr 0.001
iter 80, train loss 14.984113693237305, val loss None, lr 0.001
iter 90, train loss 14.993311882019043, val loss None, lr 0.001
best loss 14.91543960571289
29845 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
22205 MiB free out of 48676 MiB total
after cast to cpu
29435 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
layer23: self_attn.q_proj
norm_0 tensor([1.4346, 1.4453, 1.4326,  ..., 1.4609, 1.4014, 1.4336], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5103, 0.5107, 0.5371,  ..., 1.1738, 1.3018, 1.1387], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 417.90374755859375, val loss None, lr 0.001
iter 10, train loss 432.14959716796875, val loss None, lr 0.001
iter 20, train loss 413.3480224609375, val loss None, lr 0.001
iter 30, train loss 409.09661865234375, val loss None, lr 0.001
iter 40, train loss 406.418212890625, val loss None, lr 0.001
iter 50, train loss 407.9627990722656, val loss None, lr 0.001
iter 60, train loss 410.55029296875, val loss None, lr 0.001
iter 70, train loss 408.4840087890625, val loss None, lr 0.001
iter 80, train loss 411.5479431152344, val loss None, lr 0.001
iter 90, train loss 411.04510498046875, val loss None, lr 0.001
best loss 400.957275390625
layer23: self_attn.k_proj
norm_0 tensor([1.4512, 1.4863, 1.4678,  ..., 1.4688, 1.4492, 1.4697], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5093, 0.5063, 0.5337,  ..., 1.0703, 0.9331, 1.0977], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 433.01214599609375, val loss None, lr 0.001
iter 10, train loss 464.35809326171875, val loss None, lr 0.001
iter 20, train loss 439.4329833984375, val loss None, lr 0.001
iter 30, train loss 429.55615234375, val loss None, lr 0.001
iter 40, train loss 429.944091796875, val loss None, lr 0.001
iter 50, train loss 428.0818786621094, val loss None, lr 0.001
iter 60, train loss 430.9626770019531, val loss None, lr 0.001
iter 70, train loss 430.29827880859375, val loss None, lr 0.001
iter 80, train loss 431.4356689453125, val loss None, lr 0.001
iter 90, train loss 429.1825256347656, val loss None, lr 0.001
best loss 415.7100830078125
layer23: self_attn.v_proj
norm_0 tensor([1.2002, 1.1748, 1.2148,  ..., 1.1846, 1.2295, 1.2295], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.9097, 0.8989, 0.9121,  ..., 0.9551, 0.9502, 0.9492], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 254.20266723632812, val loss None, lr 0.001
iter 10, train loss 253.862548828125, val loss None, lr 0.001
iter 20, train loss 253.3670196533203, val loss None, lr 0.001
iter 30, train loss 253.0570526123047, val loss None, lr 0.001
iter 40, train loss 252.9399871826172, val loss None, lr 0.001
iter 50, train loss 253.1378173828125, val loss None, lr 0.001
iter 60, train loss 253.63392639160156, val loss None, lr 0.001
iter 70, train loss 253.61541748046875, val loss None, lr 0.001
iter 80, train loss 253.47279357910156, val loss None, lr 0.001
iter 90, train loss 253.65927124023438, val loss None, lr 0.001
best loss 252.86801147460938
layer23: self_attn.o_proj
norm_0 tensor([1.0889, 1.0791, 1.0947,  ..., 1.1396, 1.1250, 1.1348], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.0059, 1.0078, 0.9873,  ..., 0.9961, 0.9990, 0.9941], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 12.099872589111328, val loss None, lr 0.001
iter 10, train loss 11.950121879577637, val loss None, lr 0.001
iter 20, train loss 11.797758102416992, val loss None, lr 0.001
iter 30, train loss 11.698797225952148, val loss None, lr 0.001
iter 40, train loss 11.680926322937012, val loss None, lr 0.001
iter 50, train loss 11.653303146362305, val loss None, lr 0.001
iter 60, train loss 11.556726455688477, val loss None, lr 0.001
iter 70, train loss 11.549074172973633, val loss None, lr 0.001
iter 80, train loss 11.543829917907715, val loss None, lr 0.001
iter 90, train loss 11.526095390319824, val loss None, lr 0.001
best loss 11.52581787109375
layer23: mlp.gate_proj
norm_0 tensor([2.0957, 2.0938, 2.0859,  ..., 2.1016, 2.0859, 2.1035], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5854, 0.5957, 0.6162,  ..., 0.6245, 0.5713, 0.5938], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 468.76666259765625, val loss None, lr 0.001
iter 10, train loss 480.86181640625, val loss None, lr 0.001
iter 20, train loss 483.38043212890625, val loss None, lr 0.001
iter 30, train loss 483.10186767578125, val loss None, lr 0.001
iter 40, train loss 481.8397216796875, val loss None, lr 0.001
iter 50, train loss 481.5379638671875, val loss None, lr 0.001
iter 60, train loss 482.84881591796875, val loss None, lr 0.001
iter 70, train loss 482.517578125, val loss None, lr 0.001
iter 80, train loss 483.9510192871094, val loss None, lr 0.001
iter 90, train loss 485.44989013671875, val loss None, lr 0.001
best loss 465.8262939453125
layer23: mlp.up_proj
norm_0 tensor([1.9434, 1.9414, 1.9482,  ..., 1.9385, 1.9551, 1.9414], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.6162, 0.6108, 0.6108,  ..., 0.5962, 0.6040, 0.6133], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 392.3151550292969, val loss None, lr 0.001
iter 10, train loss 392.3066711425781, val loss None, lr 0.001
iter 20, train loss 392.8473205566406, val loss None, lr 0.001
iter 30, train loss 393.59356689453125, val loss None, lr 0.001
iter 40, train loss 394.95574951171875, val loss None, lr 0.001
iter 50, train loss 395.5705261230469, val loss None, lr 0.001
iter 60, train loss 395.9339294433594, val loss None, lr 0.001
iter 70, train loss 396.1771240234375, val loss None, lr 0.001
iter 80, train loss 396.9680480957031, val loss None, lr 0.001
iter 90, train loss 397.8188171386719, val loss None, lr 0.001
best loss 392.30462646484375
layer23: mlp.down_proj
norm_0 tensor([1.1904, 1.1797, 1.1846,  ..., 1.1572, 1.1680, 1.1875], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.6455, 1.6436, 1.6299,  ..., 1.6309, 1.6523, 1.6592], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 17.00755500793457, val loss None, lr 0.001
iter 10, train loss 17.032039642333984, val loss None, lr 0.001
iter 20, train loss 16.98625946044922, val loss None, lr 0.001
iter 30, train loss 16.955360412597656, val loss None, lr 0.001
iter 40, train loss 16.93084716796875, val loss None, lr 0.001
iter 50, train loss 16.889240264892578, val loss None, lr 0.001
iter 60, train loss 16.888431549072266, val loss None, lr 0.001
iter 70, train loss 16.86567497253418, val loss None, lr 0.001
iter 80, train loss 16.85858154296875, val loss None, lr 0.001
iter 90, train loss 16.86563491821289, val loss None, lr 0.001
best loss 16.855974197387695
29435 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
21797 MiB free out of 48676 MiB total
after cast to cpu
29027 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
layer24: self_attn.q_proj
norm_0 tensor([1.3877, 1.3955, 1.3955,  ..., 1.4209, 1.4150, 1.4092], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.9824, 0.9819, 1.0146,  ..., 1.1738, 1.2207, 1.2129], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 381.3890380859375, val loss None, lr 0.001
iter 10, train loss 400.19757080078125, val loss None, lr 0.001
iter 20, train loss 382.3557434082031, val loss None, lr 0.001
iter 30, train loss 378.55902099609375, val loss None, lr 0.001
iter 40, train loss 378.0317687988281, val loss None, lr 0.001
iter 50, train loss 377.14031982421875, val loss None, lr 0.001
iter 60, train loss 378.45684814453125, val loss None, lr 0.001
iter 70, train loss 380.6500549316406, val loss None, lr 0.001
iter 80, train loss 378.1128845214844, val loss None, lr 0.001
iter 90, train loss 377.9300231933594, val loss None, lr 0.001
best loss 364.4424743652344
layer24: self_attn.k_proj
norm_0 tensor([1.4111, 1.4404, 1.4209,  ..., 1.3926, 1.3711, 1.3906], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.9712, 0.9785, 1.0078,  ..., 1.1924, 1.2432, 1.2480], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 402.2735595703125, val loss None, lr 0.001
iter 10, train loss 422.0841979980469, val loss None, lr 0.001
iter 20, train loss 404.04095458984375, val loss None, lr 0.001
iter 30, train loss 398.5588073730469, val loss None, lr 0.001
iter 40, train loss 396.3482666015625, val loss None, lr 0.001
iter 50, train loss 397.9754638671875, val loss None, lr 0.001
iter 60, train loss 395.9583740234375, val loss None, lr 0.001
iter 70, train loss 397.9559020996094, val loss None, lr 0.001
iter 80, train loss 400.07623291015625, val loss None, lr 0.001
iter 90, train loss 401.36419677734375, val loss None, lr 0.001
best loss 382.4722900390625
layer24: self_attn.v_proj
norm_0 tensor([1.2119, 1.1797, 1.1826,  ..., 1.2158, 1.1904, 1.2080], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.0342, 1.0391, 1.0469,  ..., 0.9829, 0.9883, 0.9868], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 241.85980224609375, val loss None, lr 0.001
iter 10, train loss 241.11660766601562, val loss None, lr 0.001
iter 20, train loss 240.3296356201172, val loss None, lr 0.001
iter 30, train loss 240.50357055664062, val loss None, lr 0.001
iter 40, train loss 240.8118438720703, val loss None, lr 0.001
iter 50, train loss 240.87582397460938, val loss None, lr 0.001
iter 60, train loss 240.98141479492188, val loss None, lr 0.001
iter 70, train loss 240.76577758789062, val loss None, lr 0.001
iter 80, train loss 240.53475952148438, val loss None, lr 0.001
iter 90, train loss 240.4129638671875, val loss None, lr 0.001
best loss 239.99195861816406
layer24: self_attn.o_proj
norm_0 tensor([1.2158, 1.2217, 1.2324,  ..., 1.1572, 1.1641, 1.1611], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.0166, 1.0166, 1.0146,  ..., 1.0166, 1.0156, 1.0049], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 27.576250076293945, val loss None, lr 0.001
iter 10, train loss 22.528772354125977, val loss None, lr 0.001
iter 20, train loss 18.922298431396484, val loss None, lr 0.001
iter 30, train loss 16.942623138427734, val loss None, lr 0.001
iter 40, train loss 16.464113235473633, val loss None, lr 0.001
iter 50, train loss 16.232227325439453, val loss None, lr 0.001
iter 60, train loss 16.04879379272461, val loss None, lr 0.001
iter 70, train loss 15.946695327758789, val loss None, lr 0.001
iter 80, train loss 15.886462211608887, val loss None, lr 0.001
iter 90, train loss 15.81812858581543, val loss None, lr 0.001
best loss 15.81812858581543
layer24: mlp.gate_proj
norm_0 tensor([2.0957, 2.0996, 2.1133,  ..., 2.1172, 2.0742, 2.0898], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5962, 0.5820, 0.5894,  ..., 0.7324, 0.5840, 0.5908], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 486.9532775878906, val loss None, lr 0.001
iter 10, train loss 500.8472900390625, val loss None, lr 0.001
iter 20, train loss 500.239013671875, val loss None, lr 0.001
iter 30, train loss 498.73919677734375, val loss None, lr 0.001
iter 40, train loss 498.7732238769531, val loss None, lr 0.001
iter 50, train loss 502.2044677734375, val loss None, lr 0.001
iter 60, train loss 502.77301025390625, val loss None, lr 0.001
iter 70, train loss 501.2147521972656, val loss None, lr 0.001
iter 80, train loss 503.4996643066406, val loss None, lr 0.001
iter 90, train loss 504.24462890625, val loss None, lr 0.001
best loss 484.056396484375
layer24: mlp.up_proj
norm_0 tensor([1.9482, 1.9463, 1.9375,  ..., 1.9424, 1.9717, 1.9648], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.6069, 0.6074, 0.6108,  ..., 0.6602, 0.6094, 0.6113], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 409.71270751953125, val loss None, lr 0.001
iter 10, train loss 410.1762390136719, val loss None, lr 0.001
iter 20, train loss 411.60748291015625, val loss None, lr 0.001
iter 30, train loss 412.7370300292969, val loss None, lr 0.001
iter 40, train loss 413.9615478515625, val loss None, lr 0.001
iter 50, train loss 414.6318359375, val loss None, lr 0.001
iter 60, train loss 414.3860778808594, val loss None, lr 0.001
iter 70, train loss 414.5887451171875, val loss None, lr 0.001
iter 80, train loss 415.43011474609375, val loss None, lr 0.001
iter 90, train loss 415.9525146484375, val loss None, lr 0.001
best loss 409.71270751953125
layer24: mlp.down_proj
norm_0 tensor([1.1787, 1.1797, 1.1846,  ..., 1.2139, 1.1807, 1.1875], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.6465, 1.6338, 1.6123,  ..., 1.6299, 1.6709, 1.6494], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 17.70675277709961, val loss None, lr 0.001
iter 10, train loss 17.742053985595703, val loss None, lr 0.001
iter 20, train loss 17.67222785949707, val loss None, lr 0.001
iter 30, train loss 17.648801803588867, val loss None, lr 0.001
iter 40, train loss 17.62884521484375, val loss None, lr 0.001
iter 50, train loss 17.621593475341797, val loss None, lr 0.001
iter 60, train loss 17.625408172607422, val loss None, lr 0.001
iter 70, train loss 17.628280639648438, val loss None, lr 0.001
iter 80, train loss 17.610591888427734, val loss None, lr 0.001
iter 90, train loss 17.621082305908203, val loss None, lr 0.001
best loss 17.610591888427734
29027 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
21389 MiB free out of 48676 MiB total
after cast to cpu
28619 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
layer25: self_attn.q_proj
norm_0 tensor([1.4170, 1.3965, 1.4229,  ..., 1.4102, 1.3906, 1.4141], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.3994, 0.3853, 0.4221,  ..., 1.1709, 1.2480, 1.1719], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 434.20819091796875, val loss None, lr 0.001
iter 10, train loss 450.1622619628906, val loss None, lr 0.001
iter 20, train loss 435.8812255859375, val loss None, lr 0.001
iter 30, train loss 433.85919189453125, val loss None, lr 0.001
iter 40, train loss 434.7669982910156, val loss None, lr 0.001
iter 50, train loss 432.3171691894531, val loss None, lr 0.001
iter 60, train loss 433.61322021484375, val loss None, lr 0.001
iter 70, train loss 436.5401306152344, val loss None, lr 0.001
iter 80, train loss 434.9581298828125, val loss None, lr 0.001
iter 90, train loss 436.373291015625, val loss None, lr 0.001
best loss 421.29400634765625
layer25: self_attn.k_proj
norm_0 tensor([1.4189, 1.4688, 1.4131,  ..., 1.4209, 1.3965, 1.4219], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.3999, 0.3755, 0.4084,  ..., 1.1504, 1.2510, 1.2012], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 451.7890930175781, val loss None, lr 0.001
iter 10, train loss 477.3236083984375, val loss None, lr 0.001
iter 20, train loss 447.0487976074219, val loss None, lr 0.001
iter 30, train loss 447.56170654296875, val loss None, lr 0.001
iter 40, train loss 444.24566650390625, val loss None, lr 0.001
iter 50, train loss 446.1426696777344, val loss None, lr 0.001
iter 60, train loss 447.91790771484375, val loss None, lr 0.001
iter 70, train loss 446.28253173828125, val loss None, lr 0.001
iter 80, train loss 449.89874267578125, val loss None, lr 0.001
iter 90, train loss 451.61376953125, val loss None, lr 0.001
best loss 433.8714294433594
layer25: self_attn.v_proj
norm_0 tensor([1.2549, 1.2246, 1.2295,  ..., 1.2441, 1.2461, 1.2500], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.0566, 1.0508, 1.0664,  ..., 1.0342, 1.0488, 1.0332], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 295.99969482421875, val loss None, lr 0.001
iter 10, train loss 295.368896484375, val loss None, lr 0.001
iter 20, train loss 294.1507263183594, val loss None, lr 0.001
iter 30, train loss 294.58367919921875, val loss None, lr 0.001
iter 40, train loss 294.6944580078125, val loss None, lr 0.001
iter 50, train loss 294.86700439453125, val loss None, lr 0.001
iter 60, train loss 294.80499267578125, val loss None, lr 0.001
iter 70, train loss 295.3043212890625, val loss None, lr 0.001
iter 80, train loss 295.2083435058594, val loss None, lr 0.001
iter 90, train loss 295.7630615234375, val loss None, lr 0.001
best loss 294.1507263183594
layer25: self_attn.o_proj
norm_0 tensor([1.2656, 1.2500, 1.2812,  ..., 1.2715, 1.2812, 1.2803], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.0146, 1.0244, 1.0039,  ..., 0.9951, 1.0215, 1.0186], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 15.754212379455566, val loss None, lr 0.001
iter 10, train loss 16.44565200805664, val loss None, lr 0.001
iter 20, train loss 16.304729461669922, val loss None, lr 0.001
iter 30, train loss 16.15226173400879, val loss None, lr 0.001
iter 40, train loss 16.347192764282227, val loss None, lr 0.001
iter 50, train loss 16.253726959228516, val loss None, lr 0.001
iter 60, train loss 16.304744720458984, val loss None, lr 0.001
iter 70, train loss 16.168365478515625, val loss None, lr 0.001
iter 80, train loss 16.207931518554688, val loss None, lr 0.001
iter 90, train loss 16.05997085571289, val loss None, lr 0.001
best loss 15.47213363647461
layer25: mlp.gate_proj
norm_0 tensor([2.0938, 2.1016, 2.1172,  ..., 2.1074, 2.0918, 2.1094], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5811, 0.7798, 0.5840,  ..., 0.5850, 0.6807, 0.6221], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 524.3232421875, val loss None, lr 0.001
iter 10, train loss 540.5745849609375, val loss None, lr 0.001
iter 20, train loss 543.5469970703125, val loss None, lr 0.001
iter 30, train loss 539.4510498046875, val loss None, lr 0.001
iter 40, train loss 537.825927734375, val loss None, lr 0.001
iter 50, train loss 537.4512939453125, val loss None, lr 0.001
iter 60, train loss 538.305419921875, val loss None, lr 0.001
iter 70, train loss 538.0768432617188, val loss None, lr 0.001
iter 80, train loss 537.9091796875, val loss None, lr 0.001
iter 90, train loss 539.939697265625, val loss None, lr 0.001
best loss 521.0003051757812
layer25: mlp.up_proj
norm_0 tensor([1.9619, 1.9541, 1.9443,  ..., 1.9619, 1.9629, 1.9561], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.6021, 0.5732, 0.6108,  ..., 0.6089, 0.6016, 0.6147], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 437.5265808105469, val loss None, lr 0.001
iter 10, train loss 437.4796142578125, val loss None, lr 0.001
iter 20, train loss 438.17095947265625, val loss None, lr 0.001
iter 30, train loss 440.15081787109375, val loss None, lr 0.001
iter 40, train loss 440.9712829589844, val loss None, lr 0.001
iter 50, train loss 441.51641845703125, val loss None, lr 0.001
iter 60, train loss 442.44818115234375, val loss None, lr 0.001
iter 70, train loss 442.5414123535156, val loss None, lr 0.001
iter 80, train loss 442.6396179199219, val loss None, lr 0.001
iter 90, train loss 444.0401306152344, val loss None, lr 0.001
best loss 437.3699645996094
layer25: mlp.down_proj
norm_0 tensor([1.1787, 1.1006, 1.1943,  ..., 1.1865, 1.1553, 1.1885], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.6377, 1.6230, 1.6387,  ..., 1.6523, 1.6602, 1.6357], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 18.458723068237305, val loss None, lr 0.001
iter 10, train loss 18.480510711669922, val loss None, lr 0.001
iter 20, train loss 18.423452377319336, val loss None, lr 0.001
iter 30, train loss 18.36532211303711, val loss None, lr 0.001
iter 40, train loss 18.383007049560547, val loss None, lr 0.001
iter 50, train loss 18.38518524169922, val loss None, lr 0.001
iter 60, train loss 18.37063217163086, val loss None, lr 0.001
iter 70, train loss 18.373445510864258, val loss None, lr 0.001
iter 80, train loss 18.372297286987305, val loss None, lr 0.001
iter 90, train loss 18.366111755371094, val loss None, lr 0.001
best loss 18.35541343688965
28619 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
20981 MiB free out of 48676 MiB total
after cast to cpu
28211 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
layer26: self_attn.q_proj
norm_0 tensor([1.3867, 1.3984, 1.4092,  ..., 1.3887, 1.3691, 1.4180], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.8169, 0.8052, 0.8237,  ..., 0.5854, 1.2822, 1.2910], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 414.9117126464844, val loss None, lr 0.001
iter 10, train loss 441.0644836425781, val loss None, lr 0.001
iter 20, train loss 423.1085205078125, val loss None, lr 0.001
iter 30, train loss 417.9508972167969, val loss None, lr 0.001
iter 40, train loss 416.97967529296875, val loss None, lr 0.001
iter 50, train loss 413.8265686035156, val loss None, lr 0.001
iter 60, train loss 412.88677978515625, val loss None, lr 0.001
iter 70, train loss 416.21038818359375, val loss None, lr 0.001
iter 80, train loss 414.3089904785156, val loss None, lr 0.001
iter 90, train loss 417.68243408203125, val loss None, lr 0.001
best loss 395.61749267578125
layer26: self_attn.k_proj
norm_0 tensor([1.3887, 1.4287, 1.3926,  ..., 1.4004, 1.3945, 1.3818], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.8066, 0.8022, 0.8164,  ..., 1.2861, 1.2861, 1.2402], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 443.227783203125, val loss None, lr 0.001
iter 10, train loss 470.38507080078125, val loss None, lr 0.001
iter 20, train loss 453.0783386230469, val loss None, lr 0.001
iter 30, train loss 441.15533447265625, val loss None, lr 0.001
iter 40, train loss 439.36700439453125, val loss None, lr 0.001
iter 50, train loss 436.34014892578125, val loss None, lr 0.001
iter 60, train loss 435.61370849609375, val loss None, lr 0.001
iter 70, train loss 436.8169250488281, val loss None, lr 0.001
iter 80, train loss 437.4600830078125, val loss None, lr 0.001
iter 90, train loss 436.73760986328125, val loss None, lr 0.001
best loss 413.2093505859375
layer26: self_attn.v_proj
norm_0 tensor([1.2695, 1.2607, 1.2549,  ..., 1.2920, 1.2363, 1.2383], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.0547, 1.0449, 1.0547,  ..., 1.0107, 0.9946, 0.9990], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 296.91119384765625, val loss None, lr 0.001
iter 10, train loss 296.2390441894531, val loss None, lr 0.001
iter 20, train loss 295.1336669921875, val loss None, lr 0.001
iter 30, train loss 295.2489318847656, val loss None, lr 0.001
iter 40, train loss 294.68438720703125, val loss None, lr 0.001
iter 50, train loss 294.44415283203125, val loss None, lr 0.001
iter 60, train loss 293.5934143066406, val loss None, lr 0.001
iter 70, train loss 293.9137878417969, val loss None, lr 0.001
iter 80, train loss 293.6869201660156, val loss None, lr 0.001
iter 90, train loss 293.56744384765625, val loss None, lr 0.001
best loss 293.14813232421875
layer26: self_attn.o_proj
norm_0 tensor([1.3115, 1.3018, 1.3115,  ..., 1.2676, 1.2451, 1.2510], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.9995, 0.9941, 0.9927,  ..., 1.0010, 1.0010, 0.9844], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 49.636558532714844, val loss None, lr 0.001
iter 10, train loss 35.582923889160156, val loss None, lr 0.001
iter 20, train loss 27.471694946289062, val loss None, lr 0.001
iter 30, train loss 22.421459197998047, val loss None, lr 0.001
iter 40, train loss 20.624740600585938, val loss None, lr 0.001
iter 50, train loss 20.184879302978516, val loss None, lr 0.001
iter 60, train loss 19.848962783813477, val loss None, lr 0.001
iter 70, train loss 19.72649383544922, val loss None, lr 0.001
iter 80, train loss 19.672502517700195, val loss None, lr 0.001
iter 90, train loss 19.643224716186523, val loss None, lr 0.001
best loss 19.581398010253906
layer26: mlp.gate_proj
norm_0 tensor([2.1113, 2.0859, 2.1035,  ..., 2.1113, 2.0840, 2.1074], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5879, 0.5894, 0.5073,  ..., 0.5898, 0.5825, 0.6504], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 545.6718139648438, val loss None, lr 0.001
iter 10, train loss 569.7052612304688, val loss None, lr 0.001
iter 20, train loss 571.4818115234375, val loss None, lr 0.001
iter 30, train loss 571.9364624023438, val loss None, lr 0.001
iter 40, train loss 568.318603515625, val loss None, lr 0.001
iter 50, train loss 566.623291015625, val loss None, lr 0.001
iter 60, train loss 568.0188598632812, val loss None, lr 0.001
iter 70, train loss 569.3074951171875, val loss None, lr 0.001
iter 80, train loss 566.41455078125, val loss None, lr 0.001
iter 90, train loss 567.9613647460938, val loss None, lr 0.001
best loss 540.2881469726562
layer26: mlp.up_proj
norm_0 tensor([1.9570, 1.9814, 1.9775,  ..., 1.9756, 1.9824, 1.9707], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.6172, 0.6187, 0.5669,  ..., 0.6133, 0.6094, 0.6104], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 461.0416259765625, val loss None, lr 0.001
iter 10, train loss 461.56134033203125, val loss None, lr 0.001
iter 20, train loss 462.83905029296875, val loss None, lr 0.001
iter 30, train loss 465.5962829589844, val loss None, lr 0.001
iter 40, train loss 466.9915771484375, val loss None, lr 0.001
iter 50, train loss 466.6068420410156, val loss None, lr 0.001
iter 60, train loss 467.1945495605469, val loss None, lr 0.001
iter 70, train loss 469.14630126953125, val loss None, lr 0.001
iter 80, train loss 469.67889404296875, val loss None, lr 0.001
iter 90, train loss 470.34881591796875, val loss None, lr 0.001
best loss 461.02838134765625
layer26: mlp.down_proj
norm_0 tensor([1.2051, 1.2080, 1.1055,  ..., 1.2041, 1.1904, 1.1768], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.6514, 1.6426, 1.6309,  ..., 1.6543, 1.6719, 1.6475], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 19.90035629272461, val loss None, lr 0.001
iter 10, train loss 19.925403594970703, val loss None, lr 0.001
iter 20, train loss 19.899215698242188, val loss None, lr 0.001
iter 30, train loss 19.90797996520996, val loss None, lr 0.001
iter 40, train loss 19.919971466064453, val loss None, lr 0.001
iter 50, train loss 19.96017074584961, val loss None, lr 0.001
iter 60, train loss 19.951404571533203, val loss None, lr 0.001
iter 70, train loss 19.932376861572266, val loss None, lr 0.001
iter 80, train loss 19.952980041503906, val loss None, lr 0.001
iter 90, train loss 19.97508430480957, val loss None, lr 0.001
best loss 19.887073516845703
28211 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
20573 MiB free out of 48676 MiB total
after cast to cpu
27803 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
layer27: self_attn.q_proj
norm_0 tensor([1.4111, 1.4785, 1.4375,  ..., 1.4502, 1.4502, 1.4775], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.9072, 0.9258, 0.9243,  ..., 1.0654, 1.0371, 0.6768], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 458.7215576171875, val loss None, lr 0.001
iter 10, train loss 484.359130859375, val loss None, lr 0.001
iter 20, train loss 456.2622375488281, val loss None, lr 0.001
iter 30, train loss 449.2684326171875, val loss None, lr 0.001
iter 40, train loss 448.57672119140625, val loss None, lr 0.001
iter 50, train loss 448.5136413574219, val loss None, lr 0.001
iter 60, train loss 447.09326171875, val loss None, lr 0.001
iter 70, train loss 444.36492919921875, val loss None, lr 0.001
iter 80, train loss 446.6878662109375, val loss None, lr 0.001
iter 90, train loss 445.49822998046875, val loss None, lr 0.001
best loss 430.7391357421875
layer27: self_attn.k_proj
norm_0 tensor([1.4434, 1.4600, 1.4590,  ..., 1.4561, 1.4580, 1.4375], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.8877, 0.9116, 0.9131,  ..., 1.0752, 1.0342, 1.0830], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 492.854248046875, val loss None, lr 0.001
iter 10, train loss 518.4536743164062, val loss None, lr 0.001
iter 20, train loss 488.212646484375, val loss None, lr 0.001
iter 30, train loss 476.9271240234375, val loss None, lr 0.001
iter 40, train loss 469.05694580078125, val loss None, lr 0.001
iter 50, train loss 466.17730712890625, val loss None, lr 0.001
iter 60, train loss 463.2620544433594, val loss None, lr 0.001
iter 70, train loss 467.35418701171875, val loss None, lr 0.001
iter 80, train loss 466.4954833984375, val loss None, lr 0.001
iter 90, train loss 468.2008056640625, val loss None, lr 0.001
best loss 447.793212890625
layer27: self_attn.v_proj
norm_0 tensor([1.2715, 1.2510, 1.2793,  ..., 1.2666, 1.2422, 1.2607], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.0166, 1.0137, 1.0107,  ..., 1.1543, 1.1465, 1.1748], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 302.93377685546875, val loss None, lr 0.001
iter 10, train loss 302.43524169921875, val loss None, lr 0.001
iter 20, train loss 302.7394714355469, val loss None, lr 0.001
iter 30, train loss 302.8372802734375, val loss None, lr 0.001
iter 40, train loss 302.399169921875, val loss None, lr 0.001
iter 50, train loss 303.576904296875, val loss None, lr 0.001
iter 60, train loss 304.0657958984375, val loss None, lr 0.001
iter 70, train loss 303.8200378417969, val loss None, lr 0.001
iter 80, train loss 303.078369140625, val loss None, lr 0.001
iter 90, train loss 303.3211975097656, val loss None, lr 0.001
best loss 302.3719177246094
layer27: self_attn.o_proj
norm_0 tensor([1.2764, 1.2842, 1.2754,  ..., 1.5234, 1.4912, 1.5518], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.9780, 1.0078, 1.0020,  ..., 1.0273, 1.0264, 1.0049], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 18.403757095336914, val loss None, lr 0.001
iter 10, train loss 16.99125099182129, val loss None, lr 0.001
iter 20, train loss 16.52456283569336, val loss None, lr 0.001
iter 30, train loss 15.781237602233887, val loss None, lr 0.001
iter 40, train loss 15.441534996032715, val loss None, lr 0.001
iter 50, train loss 15.231768608093262, val loss None, lr 0.001
iter 60, train loss 15.145528793334961, val loss None, lr 0.001
iter 70, train loss 15.099642753601074, val loss None, lr 0.001
iter 80, train loss 15.037275314331055, val loss None, lr 0.001
iter 90, train loss 15.036477088928223, val loss None, lr 0.001
best loss 15.003284454345703
layer27: mlp.gate_proj
norm_0 tensor([2.1191, 2.1113, 2.0977,  ..., 2.1191, 2.0957, 2.1250], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5542, 0.5698, 0.5684,  ..., 0.5898, 0.5835, 0.5747], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 586.1134033203125, val loss None, lr 0.001
iter 10, train loss 610.1032104492188, val loss None, lr 0.001
iter 20, train loss 615.1968383789062, val loss None, lr 0.001
iter 30, train loss 614.0293579101562, val loss None, lr 0.001
iter 40, train loss 614.028564453125, val loss None, lr 0.001
iter 50, train loss 610.00390625, val loss None, lr 0.001
iter 60, train loss 608.47314453125, val loss None, lr 0.001
iter 70, train loss 610.95263671875, val loss None, lr 0.001
iter 80, train loss 609.2811889648438, val loss None, lr 0.001
iter 90, train loss 608.0941162109375, val loss None, lr 0.001
best loss 578.6490478515625
layer27: mlp.up_proj
norm_0 tensor([1.9707, 1.9766, 1.9795,  ..., 1.9707, 1.9590, 1.9639], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.6104, 0.6157, 0.6143,  ..., 0.6162, 0.6138, 0.6060], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 495.191650390625, val loss None, lr 0.001
iter 10, train loss 496.2254638671875, val loss None, lr 0.001
iter 20, train loss 498.18060302734375, val loss None, lr 0.001
iter 30, train loss 500.93634033203125, val loss None, lr 0.001
iter 40, train loss 502.10943603515625, val loss None, lr 0.001
iter 50, train loss 502.46826171875, val loss None, lr 0.001
iter 60, train loss 502.11273193359375, val loss None, lr 0.001
iter 70, train loss 502.358642578125, val loss None, lr 0.001
iter 80, train loss 502.1217346191406, val loss None, lr 0.001
iter 90, train loss 503.57684326171875, val loss None, lr 0.001
best loss 494.2006530761719
layer27: mlp.down_proj
norm_0 tensor([1.2051, 1.2148, 1.2158,  ..., 1.2227, 1.2139, 1.1934], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.6436, 1.6357, 1.6455,  ..., 1.6436, 1.6680, 1.6650], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 22.846073150634766, val loss None, lr 0.001
iter 10, train loss 22.900339126586914, val loss None, lr 0.001
iter 20, train loss 22.86186981201172, val loss None, lr 0.001
iter 30, train loss 22.92377281188965, val loss None, lr 0.001
iter 40, train loss 22.883058547973633, val loss None, lr 0.001
iter 50, train loss 22.909236907958984, val loss None, lr 0.001
iter 60, train loss 22.90396499633789, val loss None, lr 0.001
iter 70, train loss 22.935386657714844, val loss None, lr 0.001
iter 80, train loss 22.8981876373291, val loss None, lr 0.001
iter 90, train loss 22.948081970214844, val loss None, lr 0.001
best loss 22.846073150634766
27803 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
20163 MiB free out of 48676 MiB total
after cast to cpu
27395 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
layer28: self_attn.q_proj
norm_0 tensor([1.3896, 1.4053, 1.4189,  ..., 1.3926, 1.3818, 1.4053], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.4944, 0.5146, 0.5117,  ..., 1.1074, 1.1670, 1.2334], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 456.8793640136719, val loss None, lr 0.001
iter 10, train loss 489.3057556152344, val loss None, lr 0.001
iter 20, train loss 463.38592529296875, val loss None, lr 0.001
iter 30, train loss 446.04296875, val loss None, lr 0.001
iter 40, train loss 445.16265869140625, val loss None, lr 0.001
iter 50, train loss 444.7200622558594, val loss None, lr 0.001
iter 60, train loss 443.65045166015625, val loss None, lr 0.001
iter 70, train loss 442.1190185546875, val loss None, lr 0.001
iter 80, train loss 442.9435729980469, val loss None, lr 0.001
iter 90, train loss 442.2293701171875, val loss None, lr 0.001
best loss 423.79144287109375
layer28: self_attn.k_proj
norm_0 tensor([1.4238, 1.4307, 1.4199,  ..., 1.4238, 1.4033, 1.4395], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5518, 0.5249, 0.5312,  ..., 1.2432, 1.2236, 1.2285], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 487.317138671875, val loss None, lr 0.001
iter 10, train loss 519.920654296875, val loss None, lr 0.001
iter 20, train loss 493.9028015136719, val loss None, lr 0.001
iter 30, train loss 479.7347412109375, val loss None, lr 0.001
iter 40, train loss 474.13604736328125, val loss None, lr 0.001
iter 50, train loss 467.4287414550781, val loss None, lr 0.001
iter 60, train loss 465.9388427734375, val loss None, lr 0.001
iter 70, train loss 464.2218933105469, val loss None, lr 0.001
iter 80, train loss 467.9017333984375, val loss None, lr 0.001
iter 90, train loss 467.73504638671875, val loss None, lr 0.001
best loss 441.0004577636719
layer28: self_attn.v_proj
norm_0 tensor([1.2852, 1.2939, 1.3252,  ..., 1.2988, 1.3174, 1.3135], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.1309, 1.1387, 1.1641,  ..., 1.0996, 1.1113, 1.1045], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 334.34857177734375, val loss None, lr 0.001
iter 10, train loss 334.1017761230469, val loss None, lr 0.001
iter 20, train loss 334.82208251953125, val loss None, lr 0.001
iter 30, train loss 335.24530029296875, val loss None, lr 0.001
iter 40, train loss 334.44915771484375, val loss None, lr 0.001
iter 50, train loss 335.0006103515625, val loss None, lr 0.001
iter 60, train loss 336.6089172363281, val loss None, lr 0.001
iter 70, train loss 333.925537109375, val loss None, lr 0.001
iter 80, train loss 334.73724365234375, val loss None, lr 0.001
iter 90, train loss 335.12298583984375, val loss None, lr 0.001
best loss 333.607177734375
layer28: self_attn.o_proj
norm_0 tensor([1.5029, 1.4824, 1.5049,  ..., 1.4678, 1.4609, 1.4609], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.9937, 1.0020, 1.0000,  ..., 1.0322, 1.0430, 1.0068], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 28.2691707611084, val loss None, lr 0.001
iter 10, train loss 27.82850456237793, val loss None, lr 0.001
iter 20, train loss 27.3494873046875, val loss None, lr 0.001
iter 30, train loss 26.87496566772461, val loss None, lr 0.001
iter 40, train loss 27.087080001831055, val loss None, lr 0.001
iter 50, train loss 27.18366050720215, val loss None, lr 0.001
iter 60, train loss 26.7115535736084, val loss None, lr 0.001
iter 70, train loss 26.774879455566406, val loss None, lr 0.001
iter 80, train loss 26.620773315429688, val loss None, lr 0.001
iter 90, train loss 26.466121673583984, val loss None, lr 0.001
best loss 26.380836486816406
layer28: mlp.gate_proj
norm_0 tensor([2.1055, 2.1094, 2.1113,  ..., 2.1152, 2.0996, 2.1133], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5732, 0.5693, 0.5732,  ..., 0.5791, 0.5791, 0.5576], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 617.2517700195312, val loss None, lr 0.001
iter 10, train loss 647.23974609375, val loss None, lr 0.001
iter 20, train loss 641.5679931640625, val loss None, lr 0.001
iter 30, train loss 641.56005859375, val loss None, lr 0.001
iter 40, train loss 641.349365234375, val loss None, lr 0.001
iter 50, train loss 641.5957641601562, val loss None, lr 0.001
iter 60, train loss 636.4212646484375, val loss None, lr 0.001
iter 70, train loss 637.5193481445312, val loss None, lr 0.001
iter 80, train loss 636.0184326171875, val loss None, lr 0.001
iter 90, train loss 633.50341796875, val loss None, lr 0.001
best loss 608.4021606445312
layer28: mlp.up_proj
norm_0 tensor([1.9922, 1.9785, 1.9844,  ..., 1.9854, 1.9727, 1.9932], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.6123, 0.6172, 0.6143,  ..., 0.6187, 0.6123, 0.6128], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 544.378662109375, val loss None, lr 0.001
iter 10, train loss 552.2278442382812, val loss None, lr 0.001
iter 20, train loss 546.2552490234375, val loss None, lr 0.001
iter 30, train loss 546.1001586914062, val loss None, lr 0.001
iter 40, train loss 545.9830932617188, val loss None, lr 0.001
iter 50, train loss 546.626708984375, val loss None, lr 0.001
iter 60, train loss 548.092529296875, val loss None, lr 0.001
iter 70, train loss 549.5669555664062, val loss None, lr 0.001
iter 80, train loss 549.9298706054688, val loss None, lr 0.001
iter 90, train loss 549.802490234375, val loss None, lr 0.001
best loss 540.3026123046875
layer28: mlp.down_proj
norm_0 tensor([1.2217, 1.2285, 1.2178,  ..., 1.2363, 1.2178, 1.2188], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.6611, 1.6377, 1.6562,  ..., 1.6406, 1.6826, 1.6348], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 27.558334350585938, val loss None, lr 0.001
iter 10, train loss 27.60926055908203, val loss None, lr 0.001
iter 20, train loss 27.567523956298828, val loss None, lr 0.001
iter 30, train loss 27.615795135498047, val loss None, lr 0.001
iter 40, train loss 27.57830047607422, val loss None, lr 0.001
iter 50, train loss 27.618989944458008, val loss None, lr 0.001
iter 60, train loss 27.59566879272461, val loss None, lr 0.001
iter 70, train loss 27.583078384399414, val loss None, lr 0.001
iter 80, train loss 27.574176788330078, val loss None, lr 0.001
iter 90, train loss 27.570693969726562, val loss None, lr 0.001
best loss 27.508899688720703
27395 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
19755 MiB free out of 48676 MiB total
after cast to cpu
26987 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
layer29: self_attn.q_proj
norm_0 tensor([1.3789, 1.3936, 1.3721,  ..., 1.3564, 1.3232, 1.3398], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.6392, 0.6831, 0.7266,  ..., 1.0986, 1.2461, 0.9692], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 415.0982666015625, val loss None, lr 0.001
iter 10, train loss 443.61865234375, val loss None, lr 0.001
iter 20, train loss 421.7574157714844, val loss None, lr 0.001
iter 30, train loss 407.4882507324219, val loss None, lr 0.001
iter 40, train loss 401.9139404296875, val loss None, lr 0.001
iter 50, train loss 402.5413818359375, val loss None, lr 0.001
iter 60, train loss 401.224609375, val loss None, lr 0.001
iter 70, train loss 404.15997314453125, val loss None, lr 0.001
iter 80, train loss 402.75128173828125, val loss None, lr 0.001
iter 90, train loss 405.2242126464844, val loss None, lr 0.001
best loss 377.63897705078125
layer29: self_attn.k_proj
norm_0 tensor([1.3682, 1.4170, 1.3818,  ..., 1.4092, 1.3447, 1.3857], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.6626, 0.6821, 0.7129,  ..., 1.1348, 1.3066, 1.3779], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 450.1985168457031, val loss None, lr 0.001
iter 10, train loss 479.5103759765625, val loss None, lr 0.001
iter 20, train loss 459.6424560546875, val loss None, lr 0.001
iter 30, train loss 442.1175231933594, val loss None, lr 0.001
iter 40, train loss 427.8238830566406, val loss None, lr 0.001
iter 50, train loss 425.60821533203125, val loss None, lr 0.001
iter 60, train loss 421.6997985839844, val loss None, lr 0.001
iter 70, train loss 426.174072265625, val loss None, lr 0.001
iter 80, train loss 423.452392578125, val loss None, lr 0.001
iter 90, train loss 425.49444580078125, val loss None, lr 0.001
best loss 396.1522216796875
layer29: self_attn.v_proj
norm_0 tensor([1.3330, 1.2891, 1.3330,  ..., 1.2949, 1.3076, 1.3145], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.9766, 0.9478, 0.9673,  ..., 0.9668, 0.9727, 0.9717], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 314.323974609375, val loss None, lr 0.001
iter 10, train loss 313.67626953125, val loss None, lr 0.001
iter 20, train loss 313.36114501953125, val loss None, lr 0.001
iter 30, train loss 313.88360595703125, val loss None, lr 0.001
iter 40, train loss 314.285400390625, val loss None, lr 0.001
iter 50, train loss 314.49468994140625, val loss None, lr 0.001
iter 60, train loss 313.648193359375, val loss None, lr 0.001
iter 70, train loss 314.20098876953125, val loss None, lr 0.001
iter 80, train loss 314.78326416015625, val loss None, lr 0.001
iter 90, train loss 313.531494140625, val loss None, lr 0.001
best loss 312.874267578125
layer29: self_attn.o_proj
norm_0 tensor([1.2744, 1.2568, 1.2705,  ..., 1.2578, 1.2646, 1.2598], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.0117, 0.9854, 0.9849,  ..., 1.0127, 1.0352, 1.0098], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 33.19264221191406, val loss None, lr 0.001
iter 10, train loss 27.879058837890625, val loss None, lr 0.001
iter 20, train loss 26.93911361694336, val loss None, lr 0.001
iter 30, train loss 26.14385223388672, val loss None, lr 0.001
iter 40, train loss 25.638912200927734, val loss None, lr 0.001
iter 50, train loss 25.357412338256836, val loss None, lr 0.001
iter 60, train loss 25.35194206237793, val loss None, lr 0.001
iter 70, train loss 25.197784423828125, val loss None, lr 0.001
iter 80, train loss 25.123815536499023, val loss None, lr 0.001
iter 90, train loss 24.93429183959961, val loss None, lr 0.001
best loss 24.87242317199707
layer29: mlp.gate_proj
norm_0 tensor([2.1191, 2.1035, 2.1074,  ..., 2.1055, 2.0938, 2.1074], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5664, 0.5767, 0.6104,  ..., 0.5674, 0.5713, 0.5664], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 645.6410522460938, val loss None, lr 0.001
iter 10, train loss 682.6766357421875, val loss None, lr 0.001
iter 20, train loss 668.05712890625, val loss None, lr 0.001
iter 30, train loss 666.7654418945312, val loss None, lr 0.001
iter 40, train loss 661.5560302734375, val loss None, lr 0.001
iter 50, train loss 662.5973510742188, val loss None, lr 0.001
iter 60, train loss 656.7001342773438, val loss None, lr 0.001
iter 70, train loss 656.0020751953125, val loss None, lr 0.001
iter 80, train loss 655.00146484375, val loss None, lr 0.001
iter 90, train loss 653.0237426757812, val loss None, lr 0.001
best loss 629.2884521484375
layer29: mlp.up_proj
norm_0 tensor([2.0000, 2.0293, 2.0215,  ..., 2.0156, 2.0078, 2.0234], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.6172, 0.6235, 0.6426,  ..., 0.6147, 0.6089, 0.6167], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 575.564453125, val loss None, lr 0.001
iter 10, train loss 597.4091796875, val loss None, lr 0.001
iter 20, train loss 570.61669921875, val loss None, lr 0.001
iter 30, train loss 567.073486328125, val loss None, lr 0.001
iter 40, train loss 567.8783569335938, val loss None, lr 0.001
iter 50, train loss 567.2110595703125, val loss None, lr 0.001
iter 60, train loss 567.2894897460938, val loss None, lr 0.001
iter 70, train loss 568.244873046875, val loss None, lr 0.001
iter 80, train loss 567.7935180664062, val loss None, lr 0.001
iter 90, train loss 568.12890625, val loss None, lr 0.001
best loss 565.2167358398438
layer29: mlp.down_proj
norm_0 tensor([1.2285, 1.2559, 1.2871,  ..., 1.2324, 1.2061, 1.2393], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.6162, 1.6123, 1.6396,  ..., 1.6416, 1.6826, 1.6348], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 33.32902908325195, val loss None, lr 0.001
iter 10, train loss 33.462440490722656, val loss None, lr 0.001
iter 20, train loss 33.56292724609375, val loss None, lr 0.001
iter 30, train loss 33.52940368652344, val loss None, lr 0.001
iter 40, train loss 33.48939514160156, val loss None, lr 0.001
iter 50, train loss 33.439727783203125, val loss None, lr 0.001
iter 60, train loss 33.38860321044922, val loss None, lr 0.001
iter 70, train loss 33.35470199584961, val loss None, lr 0.001
iter 80, train loss 33.26386260986328, val loss None, lr 0.001
iter 90, train loss 33.25973892211914, val loss None, lr 0.001
best loss 33.16017150878906
26987 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
19347 MiB free out of 48676 MiB total
after cast to cpu
26579 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
layer30: self_attn.q_proj
norm_0 tensor([1.3730, 1.4258, 1.3994,  ..., 1.3545, 1.3359, 1.3438], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.7368, 0.7397, 0.7373,  ..., 1.0498, 1.0098, 1.0273], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 442.7083740234375, val loss None, lr 0.001
iter 10, train loss 476.39764404296875, val loss None, lr 0.001
iter 20, train loss 453.79461669921875, val loss None, lr 0.001
iter 30, train loss 438.5274658203125, val loss None, lr 0.001
iter 40, train loss 427.2748718261719, val loss None, lr 0.001
iter 50, train loss 427.5125427246094, val loss None, lr 0.001
iter 60, train loss 429.98529052734375, val loss None, lr 0.001
iter 70, train loss 428.1734924316406, val loss None, lr 0.001
iter 80, train loss 429.0774230957031, val loss None, lr 0.001
iter 90, train loss 427.0107421875, val loss None, lr 0.001
best loss 401.78106689453125
layer30: self_attn.k_proj
norm_0 tensor([1.4023, 1.4180, 1.3975,  ..., 1.3867, 1.3604, 1.3975], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.7329, 0.7285, 0.7402,  ..., 1.0410, 1.0596, 1.0371], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 476.87109375, val loss None, lr 0.001
iter 10, train loss 515.848388671875, val loss None, lr 0.001
iter 20, train loss 488.81805419921875, val loss None, lr 0.001
iter 30, train loss 458.9067077636719, val loss None, lr 0.001
iter 40, train loss 449.34417724609375, val loss None, lr 0.001
iter 50, train loss 448.3720703125, val loss None, lr 0.001
iter 60, train loss 447.2295227050781, val loss None, lr 0.001
iter 70, train loss 447.0700378417969, val loss None, lr 0.001
iter 80, train loss 444.84588623046875, val loss None, lr 0.001
iter 90, train loss 447.8506164550781, val loss None, lr 0.001
best loss 417.8321533203125
layer30: self_attn.v_proj
norm_0 tensor([1.3516, 1.3242, 1.3369,  ..., 1.3467, 1.3623, 1.3652], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.0098, 1.0420, 1.0059,  ..., 1.1006, 1.0938, 1.1279], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 351.808837890625, val loss None, lr 0.001
iter 10, train loss 354.0298767089844, val loss None, lr 0.001
iter 20, train loss 354.18389892578125, val loss None, lr 0.001
iter 30, train loss 354.77130126953125, val loss None, lr 0.001
iter 40, train loss 354.84075927734375, val loss None, lr 0.001
iter 50, train loss 356.1792907714844, val loss None, lr 0.001
iter 60, train loss 356.4512939453125, val loss None, lr 0.001
iter 70, train loss 356.7076721191406, val loss None, lr 0.001
iter 80, train loss 355.9427490234375, val loss None, lr 0.001
iter 90, train loss 356.30718994140625, val loss None, lr 0.001
best loss 351.808837890625
layer30: self_attn.o_proj
norm_0 tensor([1.3750, 1.4141, 1.3652,  ..., 1.4678, 1.4893, 1.4814], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.0010, 0.9873, 0.9775,  ..., 1.0000, 1.0088, 1.0098], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 38.016761779785156, val loss None, lr 0.001
iter 10, train loss 31.730972290039062, val loss None, lr 0.001
iter 20, train loss 29.4166259765625, val loss None, lr 0.001
iter 30, train loss 28.781234741210938, val loss None, lr 0.001
iter 40, train loss 27.873104095458984, val loss None, lr 0.001
iter 50, train loss 27.740982055664062, val loss None, lr 0.001
iter 60, train loss 27.774566650390625, val loss None, lr 0.001
iter 70, train loss 27.30139923095703, val loss None, lr 0.001
iter 80, train loss 27.422128677368164, val loss None, lr 0.001
iter 90, train loss 27.44124984741211, val loss None, lr 0.001
best loss 27.186294555664062
layer30: mlp.gate_proj
norm_0 tensor([2.1562, 2.1406, 2.1602,  ..., 2.1289, 2.1211, 2.1367], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5620, 0.5479, 0.5527,  ..., 0.6050, 0.7075, 0.5610], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 724.2023315429688, val loss None, lr 0.001
iter 10, train loss 770.8690795898438, val loss None, lr 0.001
iter 20, train loss 712.8331909179688, val loss None, lr 0.001
iter 30, train loss 719.685302734375, val loss None, lr 0.001
iter 40, train loss 713.8568725585938, val loss None, lr 0.001
iter 50, train loss 712.9349975585938, val loss None, lr 0.001
iter 60, train loss 707.7225341796875, val loss None, lr 0.001
iter 70, train loss 712.626708984375, val loss None, lr 0.001
iter 80, train loss 705.1973266601562, val loss None, lr 0.001
iter 90, train loss 711.3703002929688, val loss None, lr 0.001
best loss 672.2405395507812
layer30: mlp.up_proj
norm_0 tensor([2.0352, 2.0371, 2.0508,  ..., 2.0391, 2.0508, 2.0254], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.6172, 0.6138, 0.6118,  ..., 0.5757, 0.6182, 0.6245], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 637.973388671875, val loss None, lr 0.001
iter 10, train loss 673.6409301757812, val loss None, lr 0.001
iter 20, train loss 625.848388671875, val loss None, lr 0.001
iter 30, train loss 608.532470703125, val loss None, lr 0.001
iter 40, train loss 606.7366333007812, val loss None, lr 0.001
iter 50, train loss 606.98388671875, val loss None, lr 0.001
iter 60, train loss 601.8628540039062, val loss None, lr 0.001
iter 70, train loss 608.17626953125, val loss None, lr 0.001
iter 80, train loss 611.7363891601562, val loss None, lr 0.001
iter 90, train loss 609.1489868164062, val loss None, lr 0.001
best loss 592.7469482421875
layer30: mlp.down_proj
norm_0 tensor([1.2627, 1.2412, 1.2354,  ..., 1.1279, 1.0615, 1.2676], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.6152, 1.6250, 1.6465,  ..., 1.6309, 1.6963, 1.6270], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 47.821189880371094, val loss None, lr 0.001
iter 10, train loss 48.59349822998047, val loss None, lr 0.001
iter 20, train loss 48.296085357666016, val loss None, lr 0.001
iter 30, train loss 48.04576110839844, val loss None, lr 0.001
iter 40, train loss 48.04705810546875, val loss None, lr 0.001
iter 50, train loss 47.82646942138672, val loss None, lr 0.001
iter 60, train loss 47.47736358642578, val loss None, lr 0.001
iter 70, train loss 47.83347702026367, val loss None, lr 0.001
iter 80, train loss 47.45561218261719, val loss None, lr 0.001
iter 90, train loss 47.233978271484375, val loss None, lr 0.001
best loss 47.222564697265625
26579 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
18939 MiB free out of 48676 MiB total
after cast to cpu
26171 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
layer31: self_attn.q_proj
norm_0 tensor([1.3574, 1.4004, 1.4141,  ..., 1.4287, 1.3535, 1.3486], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.6294, 0.7041, 0.7632,  ..., 1.1475, 1.1475, 1.1602], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 340.9993896484375, val loss None, lr 0.001
iter 10, train loss 362.3575134277344, val loss None, lr 0.001
iter 20, train loss 344.15191650390625, val loss None, lr 0.001
iter 30, train loss 327.4039001464844, val loss None, lr 0.001
iter 40, train loss 322.1881103515625, val loss None, lr 0.001
iter 50, train loss 320.7735900878906, val loss None, lr 0.001
iter 60, train loss 321.5979309082031, val loss None, lr 0.001
iter 70, train loss 321.990234375, val loss None, lr 0.001
iter 80, train loss 321.3848876953125, val loss None, lr 0.001
iter 90, train loss 321.999755859375, val loss None, lr 0.001
best loss 290.78265380859375
layer31: self_attn.k_proj
norm_0 tensor([1.4404, 1.4561, 1.4365,  ..., 1.4336, 1.4102, 1.4043], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.6567, 0.6768, 0.7300,  ..., 1.1465, 1.1113, 1.1416], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 402.1640625, val loss None, lr 0.001
iter 10, train loss 410.99859619140625, val loss None, lr 0.001
iter 20, train loss 404.26080322265625, val loss None, lr 0.001
iter 30, train loss 377.4686279296875, val loss None, lr 0.001
iter 40, train loss 366.8742980957031, val loss None, lr 0.001
iter 50, train loss 359.0146484375, val loss None, lr 0.001
iter 60, train loss 362.8719177246094, val loss None, lr 0.001
iter 70, train loss 366.0496826171875, val loss None, lr 0.001
iter 80, train loss 360.38043212890625, val loss None, lr 0.001
iter 90, train loss 362.3641052246094, val loss None, lr 0.001
best loss 321.1018371582031
layer31: self_attn.v_proj
norm_0 tensor([1.1836, 1.2168, 1.2959,  ..., 1.1934, 1.2607, 1.2217], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.9331, 0.9346, 0.9429,  ..., 0.9785, 0.9932, 0.9839], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 205.40093994140625, val loss None, lr 0.001
iter 10, train loss 205.3140869140625, val loss None, lr 0.001
iter 20, train loss 206.7327423095703, val loss None, lr 0.001
iter 30, train loss 206.54525756835938, val loss None, lr 0.001
iter 40, train loss 205.69772338867188, val loss None, lr 0.001
iter 50, train loss 205.58692932128906, val loss None, lr 0.001
iter 60, train loss 205.49234008789062, val loss None, lr 0.001
iter 70, train loss 205.59906005859375, val loss None, lr 0.001
iter 80, train loss 204.89193725585938, val loss None, lr 0.001
iter 90, train loss 205.97763061523438, val loss None, lr 0.001
best loss 204.49887084960938
layer31: self_attn.o_proj
norm_0 tensor([1.2305, 1.2275, 1.2314,  ..., 1.2520, 1.2607, 1.2422], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.9502, 0.9829, 0.9736,  ..., 0.9819, 1.0146, 0.9907], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 213.15231323242188, val loss None, lr 0.001
iter 10, train loss 158.93560791015625, val loss None, lr 0.001
iter 20, train loss 138.63784790039062, val loss None, lr 0.001
iter 30, train loss 115.34166717529297, val loss None, lr 0.001
iter 40, train loss 99.79200744628906, val loss None, lr 0.001
iter 50, train loss 91.7818374633789, val loss None, lr 0.001
iter 60, train loss 86.36632537841797, val loss None, lr 0.001
iter 70, train loss 83.53477478027344, val loss None, lr 0.001
iter 80, train loss 84.06546020507812, val loss None, lr 0.001
iter 90, train loss 83.35636901855469, val loss None, lr 0.001
best loss 82.58187866210938
layer31: mlp.gate_proj
norm_0 tensor([2.1680, 2.2246, 2.2109,  ..., 2.2520, 2.2559, 2.2422], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5762, 0.5664, 0.5405,  ..., 0.5767, 0.9365, 0.5073], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 680.5545654296875, val loss None, lr 0.001
iter 10, train loss 693.78369140625, val loss None, lr 0.001
iter 20, train loss 655.9824829101562, val loss None, lr 0.001
iter 30, train loss 639.7108154296875, val loss None, lr 0.001
iter 40, train loss 646.3148193359375, val loss None, lr 0.001
iter 50, train loss 642.1633911132812, val loss None, lr 0.001
iter 60, train loss 634.8461303710938, val loss None, lr 0.001
iter 70, train loss 640.1317138671875, val loss None, lr 0.001
iter 80, train loss 633.9400634765625, val loss None, lr 0.001
iter 90, train loss 637.7955322265625, val loss None, lr 0.001
best loss 582.6943969726562
layer31: mlp.up_proj
norm_0 tensor([2.0586, 2.0996, 2.0996,  ..., 2.1133, 2.1641, 2.0840], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.6226, 0.6172, 0.5361,  ..., 0.6191, 0.5962, 0.5386], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 661.6909790039062, val loss None, lr 0.001
iter 10, train loss 663.2140502929688, val loss None, lr 0.001
iter 20, train loss 632.9309692382812, val loss None, lr 0.001
iter 30, train loss 587.4644775390625, val loss None, lr 0.001
iter 40, train loss 571.2376708984375, val loss None, lr 0.001
iter 50, train loss 569.3974609375, val loss None, lr 0.001
iter 60, train loss 565.5116577148438, val loss None, lr 0.001
iter 70, train loss 566.7125244140625, val loss None, lr 0.001
iter 80, train loss 565.0458984375, val loss None, lr 0.001
iter 90, train loss 566.3781127929688, val loss None, lr 0.001
best loss 538.6964111328125
layer31: mlp.down_proj
norm_0 tensor([1.3008, 1.2646, 1.0684,  ..., 1.2900, 1.0322, 1.0645], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.6172, 1.6699, 1.6533,  ..., 1.6094, 1.6836, 1.5586], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 101.33438110351562, val loss None, lr 0.001
iter 10, train loss 108.9704818725586, val loss None, lr 0.001
iter 20, train loss 100.52444458007812, val loss None, lr 0.001
iter 30, train loss 94.74349975585938, val loss None, lr 0.001
iter 40, train loss 90.58065795898438, val loss None, lr 0.001
iter 50, train loss 88.55928039550781, val loss None, lr 0.001
iter 60, train loss 88.09069061279297, val loss None, lr 0.001
iter 70, train loss 87.11477661132812, val loss None, lr 0.001
iter 80, train loss 85.78346252441406, val loss None, lr 0.001
iter 90, train loss 85.52936553955078, val loss None, lr 0.001
best loss 84.76824188232422
26171 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
18531 MiB free out of 48676 MiB total
after cast to cpu
25763 MiB free out of 48676 MiB total
Total bits: 12995657728.0 Total params: 6476005376
average bits per value: 2.0067397992227978
total time taken: 14521.558909416199
Loading tokenizer for meta-llama/Llama-2-7b-hf
/home/lliu/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Dataset: wikitext2
Evaluating ...
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
Perplexity: 7.444556
