wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: m6481. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /data/lliu/huffman/wandb/run-20250101_170326-uck2ffxu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run scarlet-fire-56
wandb: ‚≠êÔ∏è View project at https://wandb.ai/m6481/compression_no_finetune
wandb: üöÄ View run at https://wandb.ai/m6481/compression_no_finetune/runs/uck2ffxu
Namespace(models_to_compress=['meta-llama/Llama-2-7b-hf', 'meta-llama/Llama-2-13b-hf', 'meta-llama/Llama-2-70b-hf', 'meta-llama/Meta-Llama-3-8B', 'meta-llama/Meta-Llama-3-70B'], seqlens=[4096, 4096, 4096, 8192, 8192], batch_size=1, hessian_path='/data/lliu/huffman/models/{model_name}/hessians_new/pajama/128/', save_path='/data/lliu/huffman/models/{model_name}/compressed', self_attn_compression_algorithm='quantize', mlp_compression_algorithm='quantize', devices=['cuda:5', 'cuda:6', 'cuda:4', 'cuda:7'], yaml_path='/data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml', self_attn_yaml_path=None, mlp_yaml_path=None, use_already_done=False, use_wandb=True, wandb_project='compression_no_finetune')
  0%|          | 0/1848 [00:00<?, ?it/s]  0%|          | 1/1848 [01:40<51:19:05, 100.02s/it]  0%|          | 2/1848 [01:55<25:38:44, 50.01s/it]   0%|          | 3/1848 [03:10<31:28:53, 61.43s/it]  0%|          | 4/1848 [03:25<22:04:37, 43.10s/it]  0%|          | 6/1848 [04:35<19:51:29, 38.81s/it]n_commands 1848
sample command python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_3/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_3/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_3/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_3/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_3/mlp.gate_proj/compressed.log 2>&1 &
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_3/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_3/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_3/mlp.up_proj/compressed.log 2>&1 &
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_3/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_3/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_3/self_attn.q_proj/compressed.log 2>&1 &
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_3/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_3/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_3/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf/layer_3/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_3/self_attn.q_proj/compressed.log
best_loss 112.32766723632812 running bpv: 2.014652
COMMANDS_FINISHED 1 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_3/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_3/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_3/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_3/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_3/self_attn.k_proj/compressed.log
best_loss 122.52682495117188 running bpv: 2.014652
COMMANDS_FINISHED 2 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_3/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_3/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_3/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_3/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_3/self_attn.o_proj/compressed.log
best_loss 0.24364137649536133 running bpv: 2.014652
COMMANDS_FINISHED 3 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_3/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_3/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_3/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_3/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_3/mlp.gate_proj/compressed.log
best_loss 49.43342208862305 running bpv: 2.012043
COMMANDS_FINISHED 4 n_commands 1848
meta-llama/Llama-2-7b-hf/layer_3/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_3/mlp.up_proj/compressed.log
best_loss 41.891727447509766 running bpv: 2.011109
COMMANDS_FINISHED 5 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_5/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_5/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_5/mlp.gate_proj/compressed.log 2>&1 &
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_5/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_5/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_5/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_3/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_3/self_attn.v_proj/compressed.log
best_loss 31.879804611206055 running bpv: 2.011487
COMMANDS_FINISHED 6 n_commands 1848
  0%|          | 7/1848 [05:10<19:18:47, 37.77s/it]  0%|          | 8/1848 [06:05<21:47:09, 42.62s/it]  0%|          | 9/1848 [06:40<20:39:25, 40.44s/it]  1%|          | 11/1848 [07:00<13:38:15, 26.73s/it]  1%|          | 12/1848 [07:35<14:39:30, 28.74s/it]  1%|          | 13/1848 [08:20<16:47:24, 32.94s/it]  1%|          | 14/1848 [09:55<25:17:55, 49.66s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_5/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_5/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_5/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_3/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_3/mlp.down_proj/compressed.log
best_loss 0.8539788126945496 running bpv: 2.010608
COMMANDS_FINISHED 7 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_5/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_5/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_5/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_5/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_5/self_attn.q_proj/compressed.log
best_loss 121.28446960449219 running bpv: 2.010917
COMMANDS_FINISHED 8 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_5/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_5/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_5/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_5/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_5/mlp.gate_proj/compressed.log
best_loss 88.11199951171875 running bpv: 2.010612
COMMANDS_FINISHED 9 n_commands 1848
meta-llama/Llama-2-7b-hf/layer_5/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_5/self_attn.k_proj/compressed.log
best_loss 132.9383544921875 running bpv: 2.010854
COMMANDS_FINISHED 10 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_5/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_5/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_5/mlp.down_proj/compressed.log 2>&1 &
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_5/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_5/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_5/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_5/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_5/mlp.up_proj/compressed.log
best_loss 69.80905151367188 running bpv: 2.010616
COMMANDS_FINISHED 11 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_8/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_8/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_8/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_5/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_5/self_attn.o_proj/compressed.log
best_loss 0.8941723704338074 running bpv: 2.010813
COMMANDS_FINISHED 12 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_8/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_8/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_8/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_5/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_5/self_attn.v_proj/compressed.log
best_loss 36.62811279296875 running bpv: 2.010992
COMMANDS_FINISHED 13 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_8/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_8/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_8/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_5/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_5/mlp.down_proj/compressed.log
best_loss 2.309817314147949 running bpv: 2.010608
COMMANDS_FINISHED 14 n_commands 1848
meta-llama/Llama-2-7b-hf/layer_8/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_8/self_attn.q_proj/compressed.log
best_loss 191.25511169433594 running bpv: 2.010769
COMMANDS_FINISHED 15 n_commands 1848
  1%|          | 16/1848 [10:15<16:29:42, 32.41s/it]  1%|          | 17/1848 [10:50<16:47:54, 33.03s/it]  1%|          | 18/1848 [11:25<17:02:37, 33.53s/it]  1%|          | 19/1848 [11:40<14:32:11, 28.61s/it]  1%|          | 20/1848 [12:25<16:49:03, 33.12s/it]  1%|          | 21/1848 [13:30<21:22:20, 42.11s/it]  1%|          | 22/1848 [13:55<18:52:07, 37.20s/it]  1%|          | 23/1848 [14:40<20:00:36, 39.47s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_8/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_8/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_8/self_attn.k_proj/compressed.log 2>&1 &
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_8/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_8/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_8/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_8/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_8/mlp.gate_proj/compressed.log
best_loss 131.2567901611328 running bpv: 2.01061
COMMANDS_FINISHED 16 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_8/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_8/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_8/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_8/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_8/mlp.up_proj/compressed.log
best_loss 106.80435180664062 running bpv: 2.01048
COMMANDS_FINISHED 17 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_8/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_8/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_8/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_8/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_8/self_attn.k_proj/compressed.log
best_loss 193.88648986816406 running bpv: 2.010612
COMMANDS_FINISHED 18 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_12/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_12/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_12/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_8/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_8/self_attn.o_proj/compressed.log
best_loss 2.943741798400879 running bpv: 2.010737
COMMANDS_FINISHED 19 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_12/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_12/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_12/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_8/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_8/self_attn.v_proj/compressed.log
best_loss 59.480464935302734 running bpv: 2.010854
COMMANDS_FINISHED 20 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_12/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_12/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_12/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_8/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_8/mlp.down_proj/compressed.log
best_loss 5.640310287475586 running bpv: 2.010608
COMMANDS_FINISHED 21 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_12/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_12/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_12/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_12/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_12/self_attn.q_proj/compressed.log
best_loss 240.7495574951172 running bpv: 2.010716
COMMANDS_FINISHED 22 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_12/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_12/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_12/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_12/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_12/mlp.gate_proj/compressed.log
best_loss 162.8937225341797 running bpv: 2.01061
COMMANDS_FINISHED 23 n_commands   1%|‚ñè         | 24/1848 [14:55<16:21:35, 32.29s/it]  1%|‚ñè         | 25/1848 [15:10<13:45:52, 27.18s/it]  1%|‚ñè         | 26/1848 [15:25<11:55:40, 23.57s/it]  1%|‚ñè         | 27/1848 [16:30<18:09:47, 35.91s/it]  2%|‚ñè         | 28/1848 [17:55<25:33:42, 50.56s/it]  2%|‚ñè         | 29/1848 [18:10<20:10:39, 39.93s/it]  2%|‚ñè         | 30/1848 [18:25<16:23:59, 32.47s/it]  2%|‚ñè         | 31/1848 [18:40<13:45:01, 27.24s/it]1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_12/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_12/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_12/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_12/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_12/mlp.up_proj/compressed.log
best_loss 148.902099609375 running bpv: 2.010516
COMMANDS_FINISHED 24 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_12/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_12/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_12/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_12/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_12/self_attn.k_proj/compressed.log
best_loss 257.9990539550781 running bpv: 2.010611
COMMANDS_FINISHED 25 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_1/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_1/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_1/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_12/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_12/self_attn.o_proj/compressed.log
best_loss 6.915116310119629 running bpv: 2.010702
COMMANDS_FINISHED 26 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_1/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_1/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_1/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_12/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_12/self_attn.v_proj/compressed.log
best_loss 86.48735046386719 running bpv: 2.010789
COMMANDS_FINISHED 27 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_1/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_1/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_1/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_12/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_12/mlp.down_proj/compressed.log
best_loss 10.196646690368652 running bpv: 2.010608
COMMANDS_FINISHED 28 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_1/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_1/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_1/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_1/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_1/self_attn.q_proj/compressed.log
best_loss 6.516097068786621 running bpv: 2.01069
COMMANDS_FINISHED 29 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_1/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_1/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_1/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_1/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_1/mlp.gate_proj/compressed.log
best_loss 12.91602897644043 running bpv: 2.010609
COMMANDS_FINISHED 30 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_1/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_1/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_1/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_1/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_1/mlp.up_proj/compressed.log
best_loss 11.325397491455078 running bpv: 2.010536
COMMANDS_FINISHED 31 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_1/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_1/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_1/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
  2%|‚ñè         | 32/1848 [19:25<16:25:40, 32.57s/it]  2%|‚ñè         | 33/1848 [19:40<13:45:54, 27.30s/it]  2%|‚ñè         | 34/1848 [20:15<14:55:16, 29.61s/it]  2%|‚ñè         | 35/1848 [21:40<23:16:44, 46.22s/it]  2%|‚ñè         | 36/1848 [21:55<18:33:12, 36.86s/it]  2%|‚ñè         | 37/1848 [22:40<19:46:20, 39.30s/it]  2%|‚ñè         | 38/1848 [22:55<16:05:48, 32.02s/it]  2%|‚ñè         | 39/1848 [23:10<13:31:26, 26.91s/it]  2%|‚ñè         | 40/1848 [23:25<11:43:21, 23.34s/it]meta-llama/Llama-2-7b-hf/layer_1/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_1/self_attn.k_proj/compressed.log
best_loss 6.616118431091309 running bpv: 2.01061
COMMANDS_FINISHED 32 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_7/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_7/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_7/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_1/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_1/self_attn.o_proj/compressed.log
best_loss 0.0628652349114418 running bpv: 2.010682
COMMANDS_FINISHED 33 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_7/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_7/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_7/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_1/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_1/self_attn.v_proj/compressed.log
best_loss 0.6918058395385742 running bpv: 2.010751
COMMANDS_FINISHED 34 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_7/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_7/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_7/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_1/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_1/mlp.down_proj/compressed.log
best_loss 0.21109314262866974 running bpv: 2.010608
COMMANDS_FINISHED 35 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_7/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_7/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_7/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_7/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_7/self_attn.q_proj/compressed.log
best_loss 194.13951110839844 running bpv: 2.010674
COMMANDS_FINISHED 36 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_7/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_7/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_7/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_7/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_7/mlp.gate_proj/compressed.log
best_loss 126.69721221923828 running bpv: 2.010609
COMMANDS_FINISHED 37 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_7/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_7/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_7/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_7/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_7/mlp.up_proj/compressed.log
best_loss 97.25100708007812 running bpv: 2.010549
COMMANDS_FINISHED 38 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_7/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_7/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_7/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_7/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_7/self_attn.k_proj/compressed.log
best_loss 196.28851318359375 running bpv: 2.01061
COMMANDS_FINISHED 39 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_0/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_0/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_0/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_7/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_7/self_attn.o_proj/compressed.log
best_loss 1.8761067390441895 running bpv: 2.010669
COMMANDS_FINISHED 40 n_commands 1848
  2%|‚ñè         | 41/1848 [24:30<17:59:24, 35.84s/it]  2%|‚ñè         | 42/1848 [25:55<25:22:47, 50.59s/it]  2%|‚ñè         | 44/1848 [26:25<17:07:17, 34.17s/it]  2%|‚ñè         | 45/1848 [26:40<14:43:58, 29.42s/it]  2%|‚ñè         | 46/1848 [27:25<16:45:57, 33.49s/it]  3%|‚ñé         | 47/1848 [27:40<14:14:26, 28.47s/it]  3%|‚ñé         | 48/1848 [28:05<13:44:55, 27.50s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_0/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_0/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_0/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_7/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_7/self_attn.v_proj/compressed.log
best_loss 58.0523567199707 running bpv: 2.010726
COMMANDS_FINISHED 41 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_0/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_0/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_0/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_7/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_7/mlp.down_proj/compressed.log
best_loss 4.567765235900879 running bpv: 2.010608
COMMANDS_FINISHED 42 n_commands 1848
meta-llama/Llama-2-7b-hf/layer_0/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_0/self_attn.q_proj/compressed.log
best_loss 0.36603060364723206 running bpv: 2.010663
COMMANDS_FINISHED 43 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_0/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_0/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_0/self_attn.k_proj/compressed.log 2>&1 &
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_0/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_0/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_0/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_0/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_0/mlp.gate_proj/compressed.log
best_loss 2.9977715015411377 running bpv: 2.010609
COMMANDS_FINISHED 44 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_0/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_0/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_0/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_0/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_0/mlp.up_proj/compressed.log
best_loss 2.8641300201416016 running bpv: 2.010558
COMMANDS_FINISHED 45 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_0/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_0/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_0/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_0/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_0/self_attn.k_proj/compressed.log
best_loss 0.2853046655654907 running bpv: 2.01061
COMMANDS_FINISHED 46 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_31/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_31/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_31/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_0/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_0/self_attn.o_proj/compressed.log
best_loss 0.005404493305832148 running bpv: 2.01066
COMMANDS_FINISHED 47 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_31/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_31/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_31/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_0/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_0/self_attn.v_proj/compressed.log
best_loss 0.05582483485341072 running bpv: 2.010708
COMMANDS_FINISHED 48 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_31/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_31/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_31/self_attn.q_proj/compressed.log 2>&1 &
  3%|‚ñé         | 49/1848 [29:40<23:22:36, 46.78s/it]  3%|‚ñé         | 51/1848 [30:40<19:35:17, 39.24s/it]  3%|‚ñé         | 52/1848 [30:55<16:38:22, 33.35s/it]  3%|‚ñé         | 53/1848 [31:10<14:16:23, 28.63s/it]  3%|‚ñé         | 55/1848 [32:20<15:36:17, 31.33s/it]  3%|‚ñé         | 56/1848 [33:55<22:57:54, 46.14s/it]meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_0/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_0/mlp.down_proj/compressed.log
best_loss 0.02371348813176155 running bpv: 2.010608
COMMANDS_FINISHED 49 n_commands 1848
meta-llama/Llama-2-7b-hf/layer_31/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_31/self_attn.q_proj/compressed.log
best_loss 270.0703430175781 running bpv: 2.010655
COMMANDS_FINISHED 50 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_31/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_31/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_31/self_attn.k_proj/compressed.log 2>&1 &
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_31/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_31/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_31/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_31/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_31/mlp.gate_proj/compressed.log
best_loss 585.796630859375 running bpv: 2.010608
COMMANDS_FINISHED 51 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_31/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_31/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_31/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_31/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_31/mlp.up_proj/compressed.log
best_loss 518.9898681640625 running bpv: 2.010565
COMMANDS_FINISHED 52 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_31/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_31/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_31/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_31/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_31/self_attn.k_proj/compressed.log
best_loss 292.4201354980469 running bpv: 2.010609
COMMANDS_FINISHED 53 n_commands 1848
meta-llama/Llama-2-7b-hf/layer_31/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_31/self_attn.o_proj/compressed.log
best_loss 42.25667953491211 running bpv: 2.010653
COMMANDS_FINISHED 54 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_21/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_21/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_21/mlp.gate_proj/compressed.log 2>&1 &
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_21/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_21/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_21/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_31/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_31/self_attn.v_proj/compressed.log
best_loss 200.59046936035156 running bpv: 2.010695
COMMANDS_FINISHED 55 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_21/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_21/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_21/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_31/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_31/mlp.down_proj/compressed.log
best_loss 284.68499755859375 running bpv: 2.010608
COMMANDS_FINISHED 56 n_commands 1848
meta-llama/Llama-2-7b-hf/layer_21/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_21/self_attn.q_proj/compressed.log
best_loss 308.9530334472656 running bpv: 2.010649
COMMANDS_FINISHED 57 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_21/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_21/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_21/self_attn.k_proj/compressed.log 2>&1 &
  3%|‚ñé         | 58/1848 [34:25<16:45:43, 33.71s/it]  3%|‚ñé         | 59/1848 [34:40<14:41:25, 29.56s/it]  3%|‚ñé         | 60/1848 [35:25<16:31:41, 33.28s/it]  3%|‚ñé         | 62/1848 [36:15<14:50:16, 29.91s/it]  3%|‚ñé         | 63/1848 [37:40<20:59:17, 42.33s/it]  4%|‚ñé         | 65/1848 [38:40<18:34:25, 37.50s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_21/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_21/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_21/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_21/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_21/mlp.gate_proj/compressed.log
best_loss 399.2372131347656 running bpv: 2.010608
COMMANDS_FINISHED 58 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_21/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_21/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_21/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_21/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_21/mlp.up_proj/compressed.log
best_loss 341.3741760253906 running bpv: 2.01057
COMMANDS_FINISHED 59 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_21/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_21/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_21/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_21/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_21/self_attn.k_proj/compressed.log
best_loss 317.4786071777344 running bpv: 2.010609
COMMANDS_FINISHED 60 n_commands 1848
meta-llama/Llama-2-7b-hf/layer_21/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_21/self_attn.o_proj/compressed.log
best_loss 11.116890907287598 running bpv: 2.010648
COMMANDS_FINISHED 61 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_26/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_26/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_26/mlp.gate_proj/compressed.log 2>&1 &
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_26/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_26/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_26/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_21/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_21/self_attn.v_proj/compressed.log
best_loss 194.8931121826172 running bpv: 2.010685
COMMANDS_FINISHED 62 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_26/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_26/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_26/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_21/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_21/mlp.down_proj/compressed.log
best_loss 43.497642517089844 running bpv: 2.010608
COMMANDS_FINISHED 63 n_commands 1848
meta-llama/Llama-2-7b-hf/layer_26/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_26/self_attn.q_proj/compressed.log
best_loss 363.3192138671875 running bpv: 2.010645
COMMANDS_FINISHED 64 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_26/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_26/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_26/self_attn.k_proj/compressed.log 2>&1 &
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_26/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_26/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_26/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_26/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_26/mlp.gate_proj/compressed.log
best_loss 537.7118530273438 running bpv: 2.010608
COMMANDS_FINISHED 65 n_commands 1848
  4%|‚ñé         | 66/1848 [38:55<16:07:43, 32.58s/it]  4%|‚ñé         | 67/1848 [39:10<14:02:59, 28.40s/it]  4%|‚ñé         | 69/1848 [40:30<16:21:20, 33.10s/it]  4%|‚ñç         | 70/1848 [41:55<22:05:48, 44.74s/it]  4%|‚ñç         | 71/1848 [42:10<18:31:22, 37.53s/it]  4%|‚ñç         | 72/1848 [42:35<16:55:21, 34.30s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_26/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_26/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_26/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_26/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_26/mlp.up_proj/compressed.log
best_loss 462.7989807128906 running bpv: 2.010574
COMMANDS_FINISHED 66 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_26/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_26/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_26/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_26/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_26/self_attn.k_proj/compressed.log
best_loss 371.9248962402344 running bpv: 2.010609
COMMANDS_FINISHED 67 n_commands 1848
meta-llama/Llama-2-7b-hf/layer_26/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_26/self_attn.o_proj/compressed.log
best_loss 21.499774932861328 running bpv: 2.010644
COMMANDS_FINISHED 68 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_14/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_14/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_14/mlp.gate_proj/compressed.log 2>&1 &
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_14/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_14/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_14/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_26/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_26/self_attn.v_proj/compressed.log
best_loss 289.975830078125 running bpv: 2.010678
COMMANDS_FINISHED 69 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_14/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_14/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_14/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_26/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_26/mlp.down_proj/compressed.log
best_loss 64.91046905517578 running bpv: 2.010608
COMMANDS_FINISHED 70 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_14/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_14/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_14/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_14/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_14/self_attn.q_proj/compressed.log
best_loss 257.5028076171875 running bpv: 2.010641
COMMANDS_FINISHED 71 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_14/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_14/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_14/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_14/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_14/mlp.gate_proj/compressed.log
best_loss 188.02056884765625 running bpv: 2.010608
COMMANDS_FINISHED 72 n_commands 1848
meta-llama/Llama-2-7b-hf/layer_14/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_14/mlp.up_proj/compressed.log
best_loss 177.93357849121094 running bpv: 2.010577
COMMANDS_FINISHED 73 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_14/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_14/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_14/mlp.down_proj/compressed.log 2>&1 &
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_14/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_14/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_14/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
  4%|‚ñç         | 74/1848 [43:25<14:57:44, 30.36s/it]  4%|‚ñç         | 75/1848 [43:40<13:11:46, 26.79s/it]  4%|‚ñç         | 76/1848 [44:05<12:58:10, 26.35s/it]  4%|‚ñç         | 77/1848 [45:40<21:49:51, 44.38s/it]  4%|‚ñç         | 78/1848 [45:55<17:52:46, 36.37s/it]  4%|‚ñç         | 79/1848 [46:40<19:03:37, 38.79s/it]  4%|‚ñç         | 80/1848 [46:55<15:42:32, 31.99s/it]  4%|‚ñç         | 81/1848 [47:10<13:16:57, 27.06s/it]  4%|‚ñç         | 82/1848 [47:25<11:32:32, 23.53s/it]meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_14/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_14/self_attn.k_proj/compressed.log
best_loss 269.93011474609375 running bpv: 2.010609
COMMANDS_FINISHED 74 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_15/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_15/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_15/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_14/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_14/self_attn.o_proj/compressed.log
best_loss 9.668866157531738 running bpv: 2.01064
COMMANDS_FINISHED 75 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_15/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_15/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_15/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_14/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_14/self_attn.v_proj/compressed.log
best_loss 97.36930847167969 running bpv: 2.010671
COMMANDS_FINISHED 76 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_15/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_15/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_15/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_15/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_15/self_attn.q_proj/compressed.log
best_loss 240.59933471679688 running bpv: 2.010701
COMMANDS_FINISHED 77 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_15/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_15/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_15/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_14/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_14/mlp.down_proj/compressed.log
best_loss 14.621866226196289 running bpv: 2.010638
COMMANDS_FINISHED 78 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_15/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_15/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_15/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_15/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_15/mlp.gate_proj/compressed.log
best_loss 207.4346160888672 running bpv: 2.010608
COMMANDS_FINISHED 79 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_15/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_15/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_15/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_15/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_15/mlp.up_proj/compressed.log
best_loss 196.49502563476562 running bpv: 2.01058
COMMANDS_FINISHED 80 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_15/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_15/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_15/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_15/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_15/self_attn.k_proj/compressed.log
best_loss 260.69268798828125 running bpv: 2.010609
COMMANDS_FINISHED 81 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_4/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_4/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_4/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_15/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_15/self_attn.o_proj/compressed.log
best_loss 10.165802001953125 running bpv: 2.010637
COMMANDS_FINISHED 82 n_commands 1848
  4%|‚ñç         | 83/1848 [48:20<16:05:22, 32.82s/it]  5%|‚ñç         | 84/1848 [49:55<25:07:01, 51.26s/it]  5%|‚ñç         | 86/1848 [50:25<16:57:01, 34.63s/it]  5%|‚ñç         | 87/1848 [50:40<14:34:18, 29.79s/it]  5%|‚ñç         | 88/1848 [51:25<16:30:10, 33.76s/it]  5%|‚ñç         | 89/1848 [51:40<14:00:28, 28.67s/it]  5%|‚ñç         | 90/1848 [52:15<14:51:52, 30.44s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_4/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_4/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_4/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_15/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_15/self_attn.v_proj/compressed.log
best_loss 101.61927795410156 running bpv: 2.010666
COMMANDS_FINISHED 83 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_4/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_4/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_4/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_15/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_15/mlp.down_proj/compressed.log
best_loss 18.33635139465332 running bpv: 2.010608
COMMANDS_FINISHED 84 n_commands 1848
meta-llama/Llama-2-7b-hf/layer_4/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_4/self_attn.q_proj/compressed.log
best_loss 107.25277709960938 running bpv: 2.010635
COMMANDS_FINISHED 85 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_4/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_4/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_4/self_attn.k_proj/compressed.log 2>&1 &
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_4/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_4/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_4/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_4/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_4/mlp.gate_proj/compressed.log
best_loss 69.46524047851562 running bpv: 2.010608
COMMANDS_FINISHED 86 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_4/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_4/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_4/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_4/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_4/mlp.up_proj/compressed.log
best_loss 55.66865158081055 running bpv: 2.010582
COMMANDS_FINISHED 87 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_4/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_4/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_4/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_4/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_4/self_attn.k_proj/compressed.log
best_loss 112.94963836669922 running bpv: 2.010609
COMMANDS_FINISHED 88 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_27/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_27/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_27/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_4/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_4/self_attn.o_proj/compressed.log
best_loss 0.536568284034729 running bpv: 2.010635
COMMANDS_FINISHED 89 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_27/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_27/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_27/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_4/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_4/self_attn.v_proj/compressed.log
best_loss 31.05900764465332 running bpv: 2.010661
COMMANDS_FINISHED 90 n_commands 1848
  5%|‚ñç         | 91/1848 [53:40<22:27:10, 46.00s/it]  5%|‚ñå         | 93/1848 [54:40<18:55:30, 38.82s/it]  5%|‚ñå         | 94/1848 [54:55<16:05:48, 33.04s/it]  5%|‚ñå         | 95/1848 [55:10<13:49:33, 28.39s/it]  5%|‚ñå         | 96/1848 [55:25<12:03:56, 24.79s/it]  5%|‚ñå         | 97/1848 [56:30<17:29:19, 35.96s/it]  5%|‚ñå         | 98/1848 [57:55<24:15:00, 49.89s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_27/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_27/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_27/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_27/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_27/self_attn.q_proj/compressed.log
best_loss 403.30023193359375 running bpv: 2.010687
COMMANDS_FINISHED 91 n_commands 1848
meta-llama/Llama-2-7b-hf/layer_4/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_4/mlp.down_proj/compressed.log
best_loss 1.5733578205108643 running bpv: 2.010633
COMMANDS_FINISHED 92 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_27/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_27/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_27/self_attn.k_proj/compressed.log 2>&1 &
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_27/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_27/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_27/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_27/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_27/mlp.gate_proj/compressed.log
best_loss 572.5789794921875 running bpv: 2.010608
COMMANDS_FINISHED 93 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_27/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_27/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_27/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_27/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_27/mlp.up_proj/compressed.log
best_loss 496.35308837890625 running bpv: 2.010584
COMMANDS_FINISHED 94 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_27/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_27/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_27/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_27/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_27/self_attn.k_proj/compressed.log
best_loss 413.33197021484375 running bpv: 2.010609
COMMANDS_FINISHED 95 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_2/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_2/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_2/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_27/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_27/self_attn.o_proj/compressed.log
best_loss 19.273479461669922 running bpv: 2.010633
COMMANDS_FINISHED 96 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_2/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_2/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_2/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_27/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_27/self_attn.v_proj/compressed.log
best_loss 302.3028869628906 running bpv: 2.010657
COMMANDS_FINISHED 97 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_2/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_2/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_2/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_27/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_27/mlp.down_proj/compressed.log
best_loss 74.24616241455078 running bpv: 2.010608
COMMANDS_FINISHED 98 n_commands 1848
  5%|‚ñå         | 99/1848 [58:10<19:20:44, 39.82s/it]  5%|‚ñå         | 100/1848 [58:25<15:49:03, 32.58s/it]  5%|‚ñå         | 101/1848 [58:40<13:17:57, 27.41s/it]  6%|‚ñå         | 102/1848 [59:25<15:49:06, 32.62s/it]  6%|‚ñå         | 103/1848 [59:40<13:16:22, 27.38s/it]  6%|‚ñå         | 104/1848 [1:00:15<14:21:58, 29.66s/it]  6%|‚ñå         | 105/1848 [1:01:40<22:21:39, 46.18s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_2/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_2/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_2/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_2/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_2/self_attn.q_proj/compressed.log
best_loss 41.967960357666016 running bpv: 2.010631
COMMANDS_FINISHED 99 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_2/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_2/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_2/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_2/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_2/mlp.gate_proj/compressed.log
best_loss 29.31281852722168 running bpv: 2.010608
COMMANDS_FINISHED 100 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_2/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_2/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_2/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_2/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_2/mlp.up_proj/compressed.log
best_loss 25.18877601623535 running bpv: 2.010585
COMMANDS_FINISHED 101 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_2/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_2/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_2/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_2/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_2/self_attn.k_proj/compressed.log
best_loss 46.972328186035156 running bpv: 2.010608
COMMANDS_FINISHED 102 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_28/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_28/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_28/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_2/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_2/self_attn.o_proj/compressed.log
best_loss 0.14825434982776642 running bpv: 2.010631
COMMANDS_FINISHED 103 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_28/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_28/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_28/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_2/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_2/self_attn.v_proj/compressed.log
best_loss 11.816667556762695 running bpv: 2.010654
COMMANDS_FINISHED 104 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_28/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_28/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_28/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_2/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_2/mlp.down_proj/compressed.log
best_loss 0.42012080550193787 running bpv: 2.010608
COMMANDS_FINISHED 105 n_commands 1848
meta-llama/Llama-2-7b-hf/layer_28/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_28/self_attn.q_proj/compressed.log
best_loss 397.3584289550781 running bpv: 2.01063
COMMANDS_FINISHED 106 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_28/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_28/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_28/self_attn.k_proj/compressed.log 2>&1 &
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_28/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_28/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_28/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
  6%|‚ñå         | 107/1848 [1:02:40<18:43:58, 38.74s/it]  6%|‚ñå         | 108/1848 [1:02:55<15:53:01, 32.86s/it]  6%|‚ñå         | 109/1848 [1:03:10<13:37:17, 28.20s/it]  6%|‚ñå         | 110/1848 [1:03:25<11:52:58, 24.61s/it]  6%|‚ñå         | 111/1848 [1:04:30<17:19:34, 35.91s/it]  6%|‚ñå         | 112/1848 [1:05:55<24:04:29, 49.92s/it]  6%|‚ñå         | 114/1848 [1:06:25<16:29:29, 34.24s/it]  6%|‚ñå         | 115/1848 [1:06:40<14:13:58, 29.57s/it]meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_28/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_28/mlp.gate_proj/compressed.log
best_loss 595.8999633789062 running bpv: 2.010608
COMMANDS_FINISHED 107 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_28/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_28/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_28/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_28/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_28/mlp.up_proj/compressed.log
best_loss 532.6217041015625 running bpv: 2.010587
COMMANDS_FINISHED 108 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_28/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_28/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_28/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_28/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_28/self_attn.k_proj/compressed.log
best_loss 407.24334716796875 running bpv: 2.010608
COMMANDS_FINISHED 109 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_18/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_18/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_18/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_28/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_28/self_attn.o_proj/compressed.log
best_loss 26.552471160888672 running bpv: 2.01063
COMMANDS_FINISHED 110 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_18/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_18/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_18/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_28/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_28/self_attn.v_proj/compressed.log
best_loss 334.60162353515625 running bpv: 2.010651
COMMANDS_FINISHED 111 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_18/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_18/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_18/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_28/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_28/mlp.down_proj/compressed.log
best_loss 90.18383026123047 running bpv: 2.010608
COMMANDS_FINISHED 112 n_commands 1848
meta-llama/Llama-2-7b-hf/layer_18/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_18/self_attn.q_proj/compressed.log
best_loss 286.10357666015625 running bpv: 2.010628
COMMANDS_FINISHED 113 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_18/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_18/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_18/self_attn.k_proj/compressed.log 2>&1 &
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_18/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_18/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_18/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_18/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_18/mlp.gate_proj/compressed.log
best_loss 316.8416442871094 running bpv: 2.010608
COMMANDS_FINISHED 114 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_18/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_18/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_18/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_18/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_18/mlp.up_proj/compressed.log
best_loss 280.7430114746094 running bpv: 2.010588
COMMANDS_FINISHED 115 n_commands 1848
  6%|‚ñã         | 116/1848 [1:07:25<16:08:18, 33.54s/it]  6%|‚ñã         | 118/1848 [1:08:05<13:21:40, 27.80s/it]  6%|‚ñã         | 119/1848 [1:09:40<20:51:23, 43.43s/it]  7%|‚ñã         | 121/1848 [1:10:40<18:15:46, 38.07s/it]  7%|‚ñã         | 122/1848 [1:10:55<15:47:57, 32.95s/it]  7%|‚ñã         | 123/1848 [1:11:10<13:43:13, 28.63s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_18/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_18/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_18/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_18/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_18/self_attn.k_proj/compressed.log
best_loss 301.54583740234375 running bpv: 2.010608
COMMANDS_FINISHED 116 n_commands 1848
meta-llama/Llama-2-7b-hf/layer_18/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_18/self_attn.o_proj/compressed.log
best_loss 10.608203887939453 running bpv: 2.010628
COMMANDS_FINISHED 117 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_25/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_25/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_25/mlp.gate_proj/compressed.log 2>&1 &
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_25/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_25/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_25/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_18/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_18/self_attn.v_proj/compressed.log
best_loss 154.01303100585938 running bpv: 2.010648
COMMANDS_FINISHED 118 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_25/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_25/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_25/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_18/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_18/mlp.down_proj/compressed.log
best_loss 31.79154396057129 running bpv: 2.010608
COMMANDS_FINISHED 119 n_commands 1848
meta-llama/Llama-2-7b-hf/layer_25/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_25/self_attn.q_proj/compressed.log
best_loss 391.51904296875 running bpv: 2.010627
COMMANDS_FINISHED 120 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_25/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_25/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_25/self_attn.k_proj/compressed.log 2>&1 &
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_25/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_25/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_25/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_25/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_25/mlp.gate_proj/compressed.log
best_loss 506.6246032714844 running bpv: 2.010608
COMMANDS_FINISHED 121 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_25/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_25/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_25/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_25/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_25/mlp.up_proj/compressed.log
best_loss 434.90582275390625 running bpv: 2.010589
COMMANDS_FINISHED 122 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_25/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_25/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_25/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_25/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_25/self_attn.k_proj/compressed.log
best_loss 395.7146911621094 running bpv: 2.010608
COMMANDS_FINISHED 123 n_commands 1848
  7%|‚ñã         | 124/1848 [1:11:25<12:02:33, 25.15s/it]  7%|‚ñã         | 125/1848 [1:12:30<17:08:30, 35.82s/it]  7%|‚ñã         | 126/1848 [1:13:55<23:38:26, 49.42s/it]  7%|‚ñã         | 127/1848 [1:14:10<18:58:00, 39.67s/it]  7%|‚ñã         | 128/1848 [1:14:25<15:33:38, 32.57s/it]  7%|‚ñã         | 129/1848 [1:14:40<13:06:24, 27.45s/it]  7%|‚ñã         | 130/1848 [1:15:25<15:33:46, 32.61s/it]  7%|‚ñã         | 131/1848 [1:15:40<13:04:13, 27.40s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_30/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_30/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_30/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_25/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_25/self_attn.o_proj/compressed.log
best_loss 13.131237983703613 running bpv: 2.010627
COMMANDS_FINISHED 124 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_30/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_30/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_30/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_25/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_25/self_attn.v_proj/compressed.log
best_loss 294.0787353515625 running bpv: 2.010646
COMMANDS_FINISHED 125 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_30/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_30/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_30/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_25/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_25/mlp.down_proj/compressed.log
best_loss 58.50953674316406 running bpv: 2.010608
COMMANDS_FINISHED 126 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_30/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_30/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_30/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_30/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_30/self_attn.q_proj/compressed.log
best_loss 381.9583740234375 running bpv: 2.010626
COMMANDS_FINISHED 127 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_30/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_30/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_30/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_30/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_30/mlp.gate_proj/compressed.log
best_loss 654.3162841796875 running bpv: 2.010608
COMMANDS_FINISHED 128 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_30/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_30/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_30/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_30/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_30/mlp.up_proj/compressed.log
best_loss 585.7929077148438 running bpv: 2.01059
COMMANDS_FINISHED 129 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_30/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_30/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_30/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_30/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_30/self_attn.k_proj/compressed.log
best_loss 393.1234436035156 running bpv: 2.010608
COMMANDS_FINISHED 130 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_29/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_29/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_29/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_30/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_30/self_attn.o_proj/compressed.log
best_loss 33.642051696777344 running bpv: 2.010626
COMMANDS_FINISHED 131 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_29/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_29/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_29/mlp.up_proj/compressed.log 2>&1 &
  7%|‚ñã         | 132/1848 [1:16:15<14:08:21, 29.66s/it]  7%|‚ñã         | 133/1848 [1:17:40<21:59:10, 46.15s/it]  7%|‚ñã         | 134/1848 [1:17:55<17:32:46, 36.85s/it]  7%|‚ñã         | 135/1848 [1:18:40<18:41:45, 39.29s/it]  7%|‚ñã         | 136/1848 [1:18:55<15:13:43, 32.02s/it]  7%|‚ñã         | 137/1848 [1:19:10<12:47:50, 26.93s/it]  7%|‚ñã         | 138/1848 [1:19:25<11:05:34, 23.35s/it]  8%|‚ñä         | 139/1848 [1:20:30<17:00:50, 35.84s/it]  8%|‚ñä         | 140/1848 [1:21:55<23:59:55, 50.58s/it]meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_30/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_30/self_attn.v_proj/compressed.log
best_loss 352.8522033691406 running bpv: 2.010644
COMMANDS_FINISHED 132 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_29/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_29/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_29/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_30/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_30/mlp.down_proj/compressed.log
best_loss 151.99533081054688 running bpv: 2.010608
COMMANDS_FINISHED 133 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_29/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_29/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_29/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_29/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_29/self_attn.q_proj/compressed.log
best_loss 351.9922790527344 running bpv: 2.010625
COMMANDS_FINISHED 134 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_29/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_29/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_29/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_29/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_29/mlp.gate_proj/compressed.log
best_loss 621.0830078125 running bpv: 2.010608
COMMANDS_FINISHED 135 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_29/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_29/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_29/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_29/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_29/mlp.up_proj/compressed.log
best_loss 561.5757446289062 running bpv: 2.010591
COMMANDS_FINISHED 136 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_29/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_29/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_29/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_29/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_29/self_attn.k_proj/compressed.log
best_loss 361.2091369628906 running bpv: 2.010608
COMMANDS_FINISHED 137 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_16/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_16/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_16/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_29/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_29/self_attn.o_proj/compressed.log
best_loss 24.44049072265625 running bpv: 2.010625
COMMANDS_FINISHED 138 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_16/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_16/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_16/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_29/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_29/self_attn.v_proj/compressed.log
best_loss 313.60357666015625 running bpv: 2.010642
COMMANDS_FINISHED 139 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_16/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_16/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_16/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_29/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_29/mlp.down_proj/compressed.log
best_loss 109.30132293701172 running bpv: 2.010608
COMMANDS_FINISHED 140 n_commands 1848
  8%|‚ñä         | 141/1848 [1:22:10<18:55:32, 39.91s/it]  8%|‚ñä         | 142/1848 [1:22:25<15:22:28, 32.44s/it]  8%|‚ñä         | 143/1848 [1:22:40<12:53:18, 27.21s/it]  8%|‚ñä         | 144/1848 [1:23:26<15:24:27, 32.55s/it]  8%|‚ñä         | 145/1848 [1:23:41<12:54:32, 27.29s/it]  8%|‚ñä         | 146/1848 [1:24:06<12:34:39, 26.60s/it]  8%|‚ñä         | 147/1848 [1:25:41<22:16:00, 47.13s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_16/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_16/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_16/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_16/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_16/self_attn.q_proj/compressed.log
best_loss 253.4866943359375 running bpv: 2.010624
COMMANDS_FINISHED 141 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_16/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_16/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_16/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_16/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_16/mlp.gate_proj/compressed.log
best_loss 241.6373291015625 running bpv: 2.010608
COMMANDS_FINISHED 142 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_16/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_16/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_16/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_16/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_16/mlp.up_proj/compressed.log
best_loss 225.00425720214844 running bpv: 2.010592
COMMANDS_FINISHED 143 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_16/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_16/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_16/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_16/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_16/self_attn.k_proj/compressed.log
best_loss 270.81134033203125 running bpv: 2.010608
COMMANDS_FINISHED 144 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_13/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_13/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_13/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_16/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_16/self_attn.o_proj/compressed.log
best_loss 13.430116653442383 running bpv: 2.010624
COMMANDS_FINISHED 145 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_13/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_13/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_13/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_16/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_16/self_attn.v_proj/compressed.log
best_loss 117.02839660644531 running bpv: 2.010641
COMMANDS_FINISHED 146 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_13/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_13/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_13/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_16/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_16/mlp.down_proj/compressed.log
best_loss 24.68352699279785 running bpv: 2.010608
COMMANDS_FINISHED 147 n_commands 1848
meta-llama/Llama-2-7b-hf/layer_13/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_13/self_attn.q_proj/compressed.log
best_loss 246.3892822265625 running bpv: 2.010624
COMMANDS_FINISHED 148 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_13/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_13/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_13/self_attn.k_proj/compressed.log 2>&1 &
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_13/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_13/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_13/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
  8%|‚ñä         | 149/1848 [1:26:41<18:30:41, 39.22s/it]  8%|‚ñä         | 150/1848 [1:26:56<15:40:06, 33.22s/it]  8%|‚ñä         | 151/1848 [1:27:11<13:24:48, 28.46s/it]  8%|‚ñä         | 153/1848 [1:28:21<14:43:01, 31.26s/it]  8%|‚ñä         | 154/1848 [1:29:56<21:43:42, 46.18s/it]  8%|‚ñä         | 156/1848 [1:30:26<15:49:57, 33.69s/it]  8%|‚ñä         | 157/1848 [1:30:41<13:52:14, 29.53s/it]meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_13/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_13/mlp.gate_proj/compressed.log
best_loss 173.1314697265625 running bpv: 2.010608
COMMANDS_FINISHED 149 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_13/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_13/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_13/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_13/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_13/mlp.up_proj/compressed.log
best_loss 162.82406616210938 running bpv: 2.010593
COMMANDS_FINISHED 150 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_13/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_13/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_13/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_13/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_13/self_attn.k_proj/compressed.log
best_loss 256.6785888671875 running bpv: 2.010608
COMMANDS_FINISHED 151 n_commands 1848
meta-llama/Llama-2-7b-hf/layer_13/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_13/self_attn.o_proj/compressed.log
best_loss 7.756601333618164 running bpv: 2.010624
COMMANDS_FINISHED 152 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_10/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_10/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_10/mlp.gate_proj/compressed.log 2>&1 &
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_10/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_10/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_10/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_13/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_13/self_attn.v_proj/compressed.log
best_loss 96.61712646484375 running bpv: 2.010639
COMMANDS_FINISHED 153 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_10/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_10/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_10/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_13/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_13/mlp.down_proj/compressed.log
best_loss 12.640562057495117 running bpv: 2.010608
COMMANDS_FINISHED 154 n_commands 1848
meta-llama/Llama-2-7b-hf/layer_10/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_10/self_attn.q_proj/compressed.log
best_loss 206.27691650390625 running bpv: 2.010623
COMMANDS_FINISHED 155 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_10/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_10/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_10/self_attn.k_proj/compressed.log 2>&1 &
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_10/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_10/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_10/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_10/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_10/mlp.gate_proj/compressed.log
best_loss 144.21078491210938 running bpv: 2.010608
COMMANDS_FINISHED 156 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_10/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_10/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_10/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_10/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_10/mlp.up_proj/compressed.log
best_loss 124.62789916992188 running bpv: 2.010593
COMMANDS_FINISHED 157 n_commands 1848
  9%|‚ñä         | 158/1848 [1:31:26<15:36:54, 33.26s/it]  9%|‚ñä         | 159/1848 [1:31:41<13:24:39, 28.58s/it]  9%|‚ñä         | 160/1848 [1:32:16<14:12:36, 30.31s/it]  9%|‚ñä         | 161/1848 [1:33:41<21:17:55, 45.45s/it]  9%|‚ñâ         | 162/1848 [1:33:56<17:14:42, 36.82s/it]  9%|‚ñâ         | 163/1848 [1:34:41<18:20:20, 39.18s/it]  9%|‚ñâ         | 164/1848 [1:34:56<15:01:47, 32.13s/it]  9%|‚ñâ         | 165/1848 [1:35:11<12:39:59, 27.09s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_10/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_10/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_10/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_10/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_10/self_attn.k_proj/compressed.log
best_loss 220.53269958496094 running bpv: 2.010608
COMMANDS_FINISHED 158 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_19/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_19/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_19/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_10/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_10/self_attn.o_proj/compressed.log
best_loss 6.174777507781982 running bpv: 2.010623
COMMANDS_FINISHED 159 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_19/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_19/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_19/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_10/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_10/self_attn.v_proj/compressed.log
best_loss 65.67193603515625 running bpv: 2.010638
COMMANDS_FINISHED 160 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_19/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_19/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_19/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_10/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_10/mlp.down_proj/compressed.log
best_loss 7.830939292907715 running bpv: 2.010608
COMMANDS_FINISHED 161 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_19/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_19/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_19/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_19/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_19/self_attn.q_proj/compressed.log
best_loss 279.0536804199219 running bpv: 2.010622
COMMANDS_FINISHED 162 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_19/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_19/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_19/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_19/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_19/mlp.gate_proj/compressed.log
best_loss 341.4483947753906 running bpv: 2.010608
COMMANDS_FINISHED 163 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_19/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_19/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_19/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_19/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_19/mlp.up_proj/compressed.log
best_loss 300.86920166015625 running bpv: 2.010594
COMMANDS_FINISHED 164 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_19/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_19/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_19/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_19/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_19/self_attn.k_proj/compressed.log
best_loss 292.7369384765625 running bpv: 2.010608
COMMANDS_FINISHED 165 n_commands 1848
  9%|‚ñâ         | 166/1848 [1:35:26<10:59:16, 23.52s/it]  9%|‚ñâ         | 167/1848 [1:36:31<16:44:13, 35.84s/it]  9%|‚ñâ         | 168/1848 [1:37:56<23:33:48, 50.49s/it]  9%|‚ñâ         | 169/1848 [1:38:11<18:36:28, 39.90s/it]  9%|‚ñâ         | 170/1848 [1:38:26<15:07:39, 32.46s/it]  9%|‚ñâ         | 171/1848 [1:38:41<12:41:08, 27.23s/it]  9%|‚ñâ         | 172/1848 [1:39:26<15:09:24, 32.56s/it]  9%|‚ñâ         | 173/1848 [1:39:41<12:42:02, 27.30s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_6/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_6/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_6/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_19/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_19/self_attn.o_proj/compressed.log
best_loss 10.021682739257812 running bpv: 2.010622
COMMANDS_FINISHED 166 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_6/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_6/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_6/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_19/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_19/self_attn.v_proj/compressed.log
best_loss 156.53829956054688 running bpv: 2.010636
COMMANDS_FINISHED 167 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_6/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_6/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_6/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_19/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_19/mlp.down_proj/compressed.log
best_loss 35.231529235839844 running bpv: 2.010608
COMMANDS_FINISHED 168 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_6/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_6/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_6/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_6/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_6/self_attn.q_proj/compressed.log
best_loss 178.04071044921875 running bpv: 2.010622
COMMANDS_FINISHED 169 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_6/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_6/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_6/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_6/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_6/mlp.gate_proj/compressed.log
best_loss 110.373046875 running bpv: 2.010608
COMMANDS_FINISHED 170 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_6/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_6/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_6/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_6/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_6/mlp.up_proj/compressed.log
best_loss 83.9266586303711 running bpv: 2.010594
COMMANDS_FINISHED 171 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_6/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_6/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_6/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_6/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_6/self_attn.k_proj/compressed.log
best_loss 185.38763427734375 running bpv: 2.010608
COMMANDS_FINISHED 172 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_11/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_11/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_11/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_6/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_6/self_attn.o_proj/compressed.log
best_loss 1.2577875852584839 running bpv: 2.010622
COMMANDS_FINISHED 173 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_11/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_11/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_11/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
  9%|‚ñâ         | 174/1848 [1:40:16<13:46:04, 29.61s/it]  9%|‚ñâ         | 175/1848 [1:41:41<21:28:45, 46.22s/it] 10%|‚ñâ         | 176/1848 [1:41:56<17:07:08, 36.86s/it] 10%|‚ñâ         | 177/1848 [1:42:41<18:14:35, 39.30s/it] 10%|‚ñâ         | 178/1848 [1:42:56<14:51:06, 32.02s/it] 10%|‚ñâ         | 179/1848 [1:43:11<12:28:38, 26.91s/it] 10%|‚ñâ         | 180/1848 [1:43:26<10:48:53, 23.34s/it] 10%|‚ñâ         | 181/1848 [1:44:31<16:35:47, 35.84s/it] 10%|‚ñâ         | 182/1848 [1:45:56<23:24:45, 50.59s/it]meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_6/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_6/self_attn.v_proj/compressed.log
best_loss 51.76110076904297 running bpv: 2.010635
COMMANDS_FINISHED 174 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_11/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_11/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_11/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_6/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_6/mlp.down_proj/compressed.log
best_loss 3.40524959564209 running bpv: 2.010608
COMMANDS_FINISHED 175 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_11/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_11/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_11/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_11/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_11/self_attn.q_proj/compressed.log
best_loss 231.41769409179688 running bpv: 2.010621
COMMANDS_FINISHED 176 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_11/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_11/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_11/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_11/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_11/mlp.gate_proj/compressed.log
best_loss 154.31858825683594 running bpv: 2.010608
COMMANDS_FINISHED 177 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_11/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_11/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_11/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_11/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_11/mlp.up_proj/compressed.log
best_loss 136.99771118164062 running bpv: 2.010595
COMMANDS_FINISHED 178 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_11/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_11/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_11/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_11/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_11/self_attn.k_proj/compressed.log
best_loss 231.53173828125 running bpv: 2.010608
COMMANDS_FINISHED 179 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_20/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_20/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_20/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_11/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_11/self_attn.o_proj/compressed.log
best_loss 6.377998352050781 running bpv: 2.010621
COMMANDS_FINISHED 180 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_20/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_20/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_20/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_11/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_11/self_attn.v_proj/compressed.log
best_loss 88.64358520507812 running bpv: 2.010634
COMMANDS_FINISHED 181 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_20/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_20/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_20/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_11/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_11/mlp.down_proj/compressed.log
best_loss 8.862733840942383 running bpv: 2.010608
COMMANDS_FINISHED 182 n_commands 1848
 10%|‚ñâ         | 183/1848 [1:46:11<18:27:40, 39.92s/it] 10%|‚ñâ         | 184/1848 [1:46:26<14:59:44, 32.44s/it] 10%|‚ñà         | 185/1848 [1:46:41<12:34:13, 27.21s/it] 10%|‚ñà         | 186/1848 [1:47:26<15:01:39, 32.55s/it] 10%|‚ñà         | 187/1848 [1:47:41<12:35:24, 27.29s/it] 10%|‚ñà         | 188/1848 [1:48:16<13:39:01, 29.60s/it] 10%|‚ñà         | 189/1848 [1:49:41<21:18:08, 46.23s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_20/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_20/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_20/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_20/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_20/self_attn.q_proj/compressed.log
best_loss 287.87615966796875 running bpv: 2.01062
COMMANDS_FINISHED 183 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_20/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_20/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_20/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_20/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_20/mlp.gate_proj/compressed.log
best_loss 370.33929443359375 running bpv: 2.010608
COMMANDS_FINISHED 184 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_20/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_20/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_20/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_20/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_20/mlp.up_proj/compressed.log
best_loss 322.443603515625 running bpv: 2.010595
COMMANDS_FINISHED 185 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_20/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_20/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_20/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_20/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_20/self_attn.k_proj/compressed.log
best_loss 301.86676025390625 running bpv: 2.010608
COMMANDS_FINISHED 186 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_23/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_23/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_23/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_20/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_20/self_attn.o_proj/compressed.log
best_loss 13.759693145751953 running bpv: 2.010621
COMMANDS_FINISHED 187 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_23/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_23/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_23/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_20/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_20/self_attn.v_proj/compressed.log
best_loss 162.9111785888672 running bpv: 2.010633
COMMANDS_FINISHED 188 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_23/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_23/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_23/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_20/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_20/mlp.down_proj/compressed.log
best_loss 42.207576751708984 running bpv: 2.010608
COMMANDS_FINISHED 189 n_commands 1848
meta-llama/Llama-2-7b-hf/layer_23/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_23/self_attn.q_proj/compressed.log
best_loss 363.3808898925781 running bpv: 2.01062
COMMANDS_FINISHED 190 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_23/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_23/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_23/self_attn.k_proj/compressed.log 2>&1 &
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_23/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_23/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_23/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
 10%|‚ñà         | 191/1848 [1:50:41<17:49:51, 38.74s/it] 10%|‚ñà         | 192/1848 [1:50:56<15:06:48, 32.86s/it] 10%|‚ñà         | 193/1848 [1:51:11<12:57:29, 28.19s/it] 10%|‚ñà         | 194/1848 [1:51:26<11:18:10, 24.60s/it] 11%|‚ñà         | 195/1848 [1:52:31<16:29:18, 35.91s/it] 11%|‚ñà         | 196/1848 [1:53:56<22:54:49, 49.93s/it] 11%|‚ñà         | 197/1848 [1:54:11<18:15:29, 39.81s/it] 11%|‚ñà         | 198/1848 [1:54:26<14:55:06, 32.55s/it] 11%|‚ñà         | 199/1848 [1:54:41<12:32:22, 27.38s/it]meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_23/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_23/mlp.gate_proj/compressed.log
best_loss 452.44000244140625 running bpv: 2.010608
COMMANDS_FINISHED 191 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_23/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_23/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_23/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_23/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_23/mlp.up_proj/compressed.log
best_loss 386.1899719238281 running bpv: 2.010596
COMMANDS_FINISHED 192 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_23/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_23/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_23/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_23/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_23/self_attn.k_proj/compressed.log
best_loss 371.1566467285156 running bpv: 2.010608
COMMANDS_FINISHED 193 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_17/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_17/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_17/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_23/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_23/self_attn.o_proj/compressed.log
best_loss 13.302600860595703 running bpv: 2.01062
COMMANDS_FINISHED 194 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_17/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_17/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_17/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_23/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_23/self_attn.v_proj/compressed.log
best_loss 248.58941650390625 running bpv: 2.010632
COMMANDS_FINISHED 195 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_17/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_17/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_17/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_23/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_23/mlp.down_proj/compressed.log
best_loss 51.89737319946289 running bpv: 2.010608
COMMANDS_FINISHED 196 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_17/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_17/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_17/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_17/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_17/self_attn.q_proj/compressed.log
best_loss 268.37554931640625 running bpv: 2.01062
COMMANDS_FINISHED 197 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_17/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_17/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_17/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_17/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_17/mlp.gate_proj/compressed.log
best_loss 276.5735778808594 running bpv: 2.010608
COMMANDS_FINISHED 198 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_17/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_17/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_17/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_17/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_17/mlp.up_proj/compressed.log
best_loss 251.37319946289062 running bpv: 2.010596
COMMANDS_FINISHED 199 n_commands 1848
 11%|‚ñà         | 200/1848 [1:55:26<14:55:28, 32.60s/it] 11%|‚ñà         | 201/1848 [1:55:41<12:31:14, 27.37s/it] 11%|‚ñà         | 202/1848 [1:56:06<12:11:28, 26.66s/it] 11%|‚ñà         | 203/1848 [1:57:31<20:08:57, 44.10s/it] 11%|‚ñà         | 204/1848 [1:57:46<16:09:47, 35.39s/it] 11%|‚ñà         | 205/1848 [1:58:41<18:50:00, 41.27s/it] 11%|‚ñà         | 206/1848 [1:58:56<15:14:01, 33.40s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_17/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_17/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_17/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_17/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_17/self_attn.k_proj/compressed.log
best_loss 282.61614990234375 running bpv: 2.010608
COMMANDS_FINISHED 200 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_9/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_9/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_9/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_17/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_17/self_attn.o_proj/compressed.log
best_loss 9.753259658813477 running bpv: 2.01062
COMMANDS_FINISHED 201 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_9/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_9/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_9/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_17/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_17/self_attn.v_proj/compressed.log
best_loss 125.42306518554688 running bpv: 2.010631
COMMANDS_FINISHED 202 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_9/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_9/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_9/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_9/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_9/self_attn.q_proj/compressed.log
best_loss 201.29061889648438 running bpv: 2.010643
COMMANDS_FINISHED 203 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_9/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_9/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_9/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_17/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_17/mlp.down_proj/compressed.log
best_loss 27.123397827148438 running bpv: 2.010619
COMMANDS_FINISHED 204 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_9/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_9/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_9/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_9/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_9/mlp.gate_proj/compressed.log
best_loss 138.5034637451172 running bpv: 2.010608
COMMANDS_FINISHED 205 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_9/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_9/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_9/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_9/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_9/self_attn.k_proj/compressed.log
best_loss 212.59571838378906 running bpv: 2.010619
COMMANDS_FINISHED 206 n_commands 1848
meta-llama/Llama-2-7b-hf/layer_9/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_9/mlp.up_proj/compressed.log
best_loss 116.50347900390625 running bpv: 2.010608
COMMANDS_FINISHED 207 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_9/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_9/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_9/self_attn.v_proj/compressed.log 2>&1 &
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_22/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_22/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_22/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
 11%|‚ñà‚ñè        | 208/1848 [1:59:16<10:18:00, 22.61s/it] 11%|‚ñà‚ñè        | 209/1848 [2:00:21<15:04:36, 33.12s/it] 11%|‚ñà‚ñè        | 210/1848 [2:01:46<21:14:26, 46.68s/it] 11%|‚ñà‚ñè        | 211/1848 [2:02:01<17:18:39, 38.07s/it] 11%|‚ñà‚ñè        | 212/1848 [2:02:16<14:22:04, 31.62s/it] 12%|‚ñà‚ñè        | 213/1848 [2:02:31<12:12:17, 26.87s/it] 12%|‚ñà‚ñè        | 214/1848 [2:03:16<14:34:56, 32.13s/it] 12%|‚ñà‚ñè        | 215/1848 [2:03:31<12:17:59, 27.12s/it] 12%|‚ñà‚ñè        | 216/1848 [2:03:56<12:00:38, 26.49s/it]meta-llama/Llama-2-7b-hf/layer_9/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_9/self_attn.o_proj/compressed.log
best_loss 4.0911455154418945 running bpv: 2.010619
COMMANDS_FINISHED 208 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_22/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_22/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_22/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_9/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_9/self_attn.v_proj/compressed.log
best_loss 65.13645935058594 running bpv: 2.010631
COMMANDS_FINISHED 209 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_22/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_22/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_22/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_22/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_22/self_attn.q_proj/compressed.log
best_loss 330.46392822265625 running bpv: 2.010642
COMMANDS_FINISHED 210 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_22/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_22/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_22/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_9/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_9/mlp.down_proj/compressed.log
best_loss 6.7013349533081055 running bpv: 2.010619
COMMANDS_FINISHED 211 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_22/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_22/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_22/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_22/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_22/mlp.gate_proj/compressed.log
best_loss 427.835693359375 running bpv: 2.010608
COMMANDS_FINISHED 212 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_22/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_22/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_22/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_22/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_22/mlp.up_proj/compressed.log
best_loss 360.9086608886719 running bpv: 2.010597
COMMANDS_FINISHED 213 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_22/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_22/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_22/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_22/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_22/self_attn.k_proj/compressed.log
best_loss 341.1171875 running bpv: 2.010608
COMMANDS_FINISHED 214 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_24/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_24/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_24/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_22/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_22/self_attn.o_proj/compressed.log
best_loss 17.1939640045166 running bpv: 2.010619
COMMANDS_FINISHED 215 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_24/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_24/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_24/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_22/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_22/self_attn.v_proj/compressed.log
best_loss 202.0686492919922 running bpv: 2.01063
COMMANDS_FINISHED 216 n_commands 1848
 12%|‚ñà‚ñè        | 217/1848 [2:05:31<21:12:16, 46.80s/it] 12%|‚ñà‚ñè        | 219/1848 [2:06:31<17:41:36, 39.10s/it] 12%|‚ñà‚ñè        | 220/1848 [2:06:46<14:59:39, 33.16s/it] 12%|‚ñà‚ñè        | 221/1848 [2:07:01<12:50:50, 28.43s/it] 12%|‚ñà‚ñè        | 223/1848 [2:08:11<14:05:57, 31.24s/it] 12%|‚ñà‚ñè        | 224/1848 [2:09:46<20:48:45, 46.14s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_24/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_24/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_24/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_24/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_24/self_attn.q_proj/compressed.log
best_loss 332.237060546875 running bpv: 2.010641
COMMANDS_FINISHED 217 n_commands 1848
meta-llama/Llama-2-7b-hf/layer_22/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_22/mlp.down_proj/compressed.log
best_loss 49.28509521484375 running bpv: 2.010618
COMMANDS_FINISHED 218 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_24/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_24/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_24/self_attn.k_proj/compressed.log 2>&1 &
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_24/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_24/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_24/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_24/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_24/mlp.gate_proj/compressed.log
best_loss 476.72027587890625 running bpv: 2.010608
COMMANDS_FINISHED 219 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_24/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_24/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_24/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_24/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_24/mlp.up_proj/compressed.log
best_loss 406.8534240722656 running bpv: 2.010597
COMMANDS_FINISHED 220 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_24/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_24/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_24/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_24/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_24/self_attn.k_proj/compressed.log
best_loss 338.40740966796875 running bpv: 2.010608
COMMANDS_FINISHED 221 n_commands 1848
meta-llama/Llama-2-7b-hf/layer_24/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_24/self_attn.o_proj/compressed.log
best_loss 17.620500564575195 running bpv: 2.010619
COMMANDS_FINISHED 222 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_3/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_3/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_3/mlp.gate_proj/compressed.log 2>&1 &
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_3/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_3/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_3/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_24/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_24/self_attn.v_proj/compressed.log
best_loss 235.66339111328125 running bpv: 2.010629
COMMANDS_FINISHED 223 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_3/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_3/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_3/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_24/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_24/mlp.down_proj/compressed.log
best_loss 55.164093017578125 running bpv: 2.010608
COMMANDS_FINISHED 224 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_3/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_3/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_3/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
done with meta-llama/Llama-2-7b-hf
done with {'meta-llama/Llama-2-7b-hf/layer_3/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_3/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_3/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_3/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_3/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_3/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_3/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_3/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_3/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_3/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_3/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_3/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_3/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_3/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_5/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_5/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_5/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_5/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_5/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_5/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_5/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_5/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_5/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_5/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_5/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_5/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_5/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_5/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_8/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_8/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_8/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_8/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_8/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_8/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_8/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_8/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_8/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_8/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_8/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_8/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_8/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_8/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_12/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_12/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_12/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_12/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_12/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_12/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_12/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_12/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_12/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_12/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_12/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_12/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_12/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_12/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_1/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_1/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_1/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_1/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_1/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_1/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_1/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_1/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_1/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_1/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_1/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_1/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_1/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_1/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_7/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_7/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_7/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_7/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_7/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_7/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_7/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_7/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_7/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_7/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_7/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_7/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_7/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_7/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_0/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_0/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_0/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_0/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_0/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_0/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_0/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_0/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_0/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_0/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_0/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_0/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_0/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_0/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_31/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_31/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_31/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_31/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_31/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_31/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_31/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_31/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_31/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_31/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_31/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_31/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_31/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_31/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_21/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_21/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_21/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_21/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_21/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_21/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_21/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_21/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_21/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_21/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_21/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_21/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_21/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_21/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_26/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_26/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_26/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_26/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_26/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_26/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_26/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_26/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_26/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_26/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_26/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_26/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_26/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_26/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_14/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_14/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_14/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_14/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_14/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_14/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_14/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_14/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_14/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_14/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_14/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_14/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_15/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_15/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_14/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_14/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_15/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_15/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_15/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_15/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_15/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_15/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_15/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_15/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_15/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_15/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_15/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_15/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_4/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_4/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_4/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_4/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_4/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_4/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_4/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_4/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_4/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_4/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_4/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_4/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_27/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_27/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_4/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_4/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_27/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_27/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_27/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_27/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_27/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_27/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_27/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_27/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_27/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_27/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_27/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_27/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_2/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_2/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_2/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_2/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_2/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_2/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_2/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_2/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_2/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_2/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_2/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_2/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_2/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_2/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_28/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_28/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_28/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_28/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_28/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_28/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_28/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_28/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_28/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_28/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_28/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_28/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_28/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_28/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_18/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_18/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_18/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_18/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_18/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_18/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_18/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_18/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_18/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_18/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_18/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_18/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_18/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_18/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_25/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_25/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_25/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_25/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_25/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_25/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_25/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_25/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_25/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_25/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_25/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_25/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_25/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_25/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_30/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_30/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_30/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_30/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_30/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_30/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_30/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_30/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_30/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_30/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_30/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_30/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_30/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_30/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_29/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_29/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_29/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_29/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_29/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_29/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_29/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_29/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_29/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_29/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_29/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_29/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_29/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_29/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_16/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_16/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_16/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_16/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_16/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_16/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_16/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_16/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_16/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_16/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_16/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_16/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_16/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_16/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_13/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_13/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_13/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_13/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_13/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_13/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_13/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_13/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_13/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_13/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_13/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_13/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_13/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_13/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_10/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_10/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_10/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_10/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_10/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_10/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_10/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_10/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_10/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_10/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_10/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_10/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_10/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_10/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_19/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_19/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_19/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_19/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_19/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_19/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_19/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_19/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_19/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_19/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_19/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_19/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_19/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_19/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_6/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_6/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_6/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_6/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_6/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_6/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_6/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_6/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_6/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_6/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_6/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_6/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_6/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_6/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_11/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_11/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_11/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_11/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_11/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_11/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_11/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_11/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_11/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_11/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_11/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_11/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_11/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_11/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_20/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_20/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_20/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_20/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_20/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_20/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_20/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_20/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_20/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_20/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_20/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_20/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_20/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_20/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_23/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_23/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_23/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_23/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_23/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_23/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_23/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_23/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_23/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_23/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_23/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_23/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_23/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_23/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_17/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_17/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_17/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_17/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_17/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_17/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_17/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_17/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_17/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_17/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_17/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_17/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_9/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_9/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_17/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_17/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_9/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_9/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_9/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_9/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_9/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_9/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_9/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_9/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_9/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_9/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_22/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_22/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_9/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_9/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_22/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_22/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_22/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_22/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_22/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_22/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_22/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_22/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_22/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_22/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_24/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_24/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_22/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_22/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_24/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_24/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_24/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_24/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_24/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_24/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_24/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_24/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_24/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_24/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_24/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-7b-hf/layer_24/mlp.down_proj/compressed.pt'} 12%|‚ñà‚ñè        | 225/1848 [2:10:11<18:25:16, 40.86s/it] 12%|‚ñà‚ñè        | 226/1848 [2:11:51<25:25:43, 56.44s/it] 12%|‚ñà‚ñè        | 227/1848 [2:12:06<20:18:46, 45.11s/it] 12%|‚ñà‚ñè        | 229/1848 [2:12:26<13:21:51, 29.72s/it] 12%|‚ñà‚ñè        | 230/1848 [2:14:11<21:25:40, 47.68s/it] 12%|‚ñà‚ñé        | 231/1848 [2:16:16<30:14:36, 67.33s/it] 13%|‚ñà‚ñé        | 232/1848 [2:16:41<25:09:50, 56.06s/it]
/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/checkpoints.yaml
perplexity_inference_command:
 python -u perplexity_eval.py --base_model meta-llama/Llama-2-7b-hf --seqlen 4096 --checkpoint_list_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/checkpoints.yaml --log_wandb --wandb_project compression_no_finetune --wandb_id uck2ffxu
meta-llama/Llama-2-13b-hf/layer_3/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_3/self_attn.q_proj/compressed.log
best_loss 53.1224365234375 running bpv: 2.010603
COMMANDS_FINISHED 225 n_commands 1849
running: nohup python -u perplexity_eval.py --base_model meta-llama/Llama-2-7b-hf --seqlen 4096 --checkpoint_list_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/checkpoints.yaml --log_wandb --wandb_project compression_no_finetune --wandb_id uck2ffxu --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/scarlet-fire-56/ppl_eval.log 2>&1 &
meta-llama/Llama-2-13b-hf
eval is done
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_3/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_3/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_3/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_3/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_3/self_attn.k_proj/compressed.log
best_loss 60.43235397338867 running bpv: 2.010598
COMMANDS_FINISHED 227 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_3/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_3/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_3/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_3/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_3/mlp.gate_proj/compressed.log
best_loss 54.23960494995117 running bpv: 2.010542
COMMANDS_FINISHED 228 n_commands 1849
meta-llama/Llama-2-13b-hf/layer_3/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_3/mlp.up_proj/compressed.log
best_loss 46.03487014770508 running bpv: 2.010488
COMMANDS_FINISHED 229 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_3/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_3/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_3/self_attn.v_proj/compressed.log 2>&1 &
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_5/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_5/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_5/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_3/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_3/self_attn.o_proj/compressed.log
best_loss 0.456948846578598 running bpv: 2.010484
COMMANDS_FINISHED 230 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_5/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_5/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_5/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_3/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_3/self_attn.v_proj/compressed.log
best_loss 16.366701126098633 running bpv: 2.01048
COMMANDS_FINISHED 231 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_5/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_5/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_5/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_5/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_5/self_attn.q_proj/compressed.log
best_loss 127.60209655761719 running bpv: 2.010475
COMMANDS_FINISHED 232 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_5/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_5/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_5/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_3/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_3/mlp.down_proj/compressed.log
best_loss 64.88185119628906 running bpv: 2.010425
COMMANDS_FINISHED 233 13%|‚ñà‚ñé        | 233/1848 [2:17:06<21:18:32, 47.50s/it] 13%|‚ñà‚ñé        | 234/1848 [2:17:21<17:10:47, 38.32s/it] 13%|‚ñà‚ñé        | 235/1848 [2:18:26<20:36:28, 45.99s/it] 13%|‚ñà‚ñé        | 236/1848 [2:18:41<16:33:17, 36.97s/it] 13%|‚ñà‚ñé        | 237/1848 [2:19:26<17:36:04, 39.33s/it] 13%|‚ñà‚ñé        | 238/1848 [2:21:31<28:55:04, 64.66s/it] 13%|‚ñà‚ñé        | 239/1848 [2:21:56<23:38:15, 52.89s/it] 13%|‚ñà‚ñé        | 240/1848 [2:23:21<27:53:47, 62.45s/it] n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_5/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_5/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_5/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_5/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_5/mlp.gate_proj/compressed.log
best_loss 99.08880615234375 running bpv: 2.010373
COMMANDS_FINISHED 234 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_5/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_5/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_5/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_5/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_5/mlp.up_proj/compressed.log
best_loss 81.50370025634766 running bpv: 2.010323
COMMANDS_FINISHED 235 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_5/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_5/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_5/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_5/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_5/self_attn.k_proj/compressed.log
best_loss 125.24456787109375 running bpv: 2.01032
COMMANDS_FINISHED 236 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_8/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_8/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_8/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_5/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_5/self_attn.o_proj/compressed.log
best_loss 1.1377114057540894 running bpv: 2.010316
COMMANDS_FINISHED 237 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_8/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_8/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_8/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_5/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_5/self_attn.v_proj/compressed.log
best_loss 45.1345100402832 running bpv: 2.010313
COMMANDS_FINISHED 238 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_8/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_8/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_8/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_8/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_8/self_attn.q_proj/compressed.log
best_loss 188.285888671875 running bpv: 2.010309
COMMANDS_FINISHED 239 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_8/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_8/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_8/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_5/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_5/mlp.down_proj/compressed.log
best_loss 3.4261465072631836 running bpv: 2.010262
COMMANDS_FINISHED 240 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_8/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_8/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_8/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_8/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_8/mlp.gate_proj/compressed.log
best_loss 164.4126739501953 running bpv: 2.010215
COMMANDS_FINISHED 241 n_commands 1849
 13%|‚ñà‚ñé        | 241/1848 [2:23:36<21:33:25, 48.29s/it] 13%|‚ñà‚ñé        | 243/1848 [2:24:06<14:41:59, 32.97s/it] 13%|‚ñà‚ñé        | 244/1848 [2:25:41<21:31:48, 48.32s/it] 13%|‚ñà‚ñé        | 245/1848 [2:27:46<30:26:06, 68.35s/it] 13%|‚ñà‚ñé        | 246/1848 [2:28:21<26:23:04, 59.29s/it] 13%|‚ñà‚ñé        | 247/1848 [2:28:36<20:51:39, 46.91s/it] 13%|‚ñà‚ñé        | 248/1848 [2:29:01<18:04:10, 40.66s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_8/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_8/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_8/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_8/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_8/self_attn.k_proj/compressed.log
best_loss 195.38671875 running bpv: 2.010212
COMMANDS_FINISHED 242 n_commands 1849
meta-llama/Llama-2-13b-hf/layer_8/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_8/mlp.up_proj/compressed.log
best_loss 134.5661163330078 running bpv: 2.010166
COMMANDS_FINISHED 243 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_8/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_8/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_8/self_attn.v_proj/compressed.log 2>&1 &
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_39/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_39/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_39/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_8/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_8/self_attn.o_proj/compressed.log
best_loss 4.063199043273926 running bpv: 2.010163
COMMANDS_FINISHED 244 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_39/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_39/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_39/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_8/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_8/self_attn.v_proj/compressed.log
best_loss 74.94720458984375 running bpv: 2.01016
COMMANDS_FINISHED 245 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_39/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_39/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_39/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_39/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_39/self_attn.q_proj/compressed.log
best_loss 275.9280090332031 running bpv: 2.010157
COMMANDS_FINISHED 246 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_39/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_39/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_39/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_8/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_8/mlp.down_proj/compressed.log
best_loss 8.793144226074219 running bpv: 2.010114
COMMANDS_FINISHED 247 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_39/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_39/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_39/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_39/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_39/mlp.gate_proj/compressed.log
best_loss 670.8037719726562 running bpv: 2.01007
COMMANDS_FINISHED 248 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_39/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_39/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_39/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_39/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_39/mlp.up_proj/compressed.log
best_loss 570.7372436523438 running bpv: 2.010027
COMMANDS_FINISHED 249 n_commands 1849
 13%|‚ñà‚ñé        | 249/1848 [2:29:46<18:37:05, 41.92s/it] 14%|‚ñà‚ñé        | 250/1848 [2:30:31<19:00:30, 42.82s/it] 14%|‚ñà‚ñé        | 251/1848 [2:31:06<17:58:27, 40.52s/it] 14%|‚ñà‚ñé        | 252/1848 [2:33:11<29:03:54, 65.56s/it] 14%|‚ñà‚ñé        | 253/1848 [2:33:26<22:23:04, 50.52s/it] 14%|‚ñà‚ñé        | 254/1848 [2:34:41<25:36:14, 57.83s/it] 14%|‚ñà‚ñç        | 255/1848 [2:35:16<22:34:17, 51.01s/it] 14%|‚ñà‚ñç        | 256/1848 [2:35:31<17:47:41, 40.24s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_39/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_39/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_39/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_39/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_39/self_attn.k_proj/compressed.log
best_loss 284.7082214355469 running bpv: 2.010024
COMMANDS_FINISHED 250 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_12/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_12/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_12/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_39/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_39/self_attn.o_proj/compressed.log
best_loss 93.27365112304688 running bpv: 2.010022
COMMANDS_FINISHED 251 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_12/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_12/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_12/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_39/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_39/self_attn.v_proj/compressed.log
best_loss 289.5940856933594 running bpv: 2.01002
COMMANDS_FINISHED 252 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_12/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_12/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_12/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_12/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_12/self_attn.q_proj/compressed.log
best_loss 243.3356475830078 running bpv: 2.010018
COMMANDS_FINISHED 253 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_12/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_12/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_12/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_39/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_39/mlp.down_proj/compressed.log
best_loss 295.00927734375 running bpv: 2.009977
COMMANDS_FINISHED 254 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_12/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_12/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_12/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_12/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_12/mlp.gate_proj/compressed.log
best_loss 195.92845153808594 running bpv: 2.009936
COMMANDS_FINISHED 255 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_12/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_12/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_12/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_12/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_12/self_attn.k_proj/compressed.log
best_loss 253.77915954589844 running bpv: 2.009935
COMMANDS_FINISHED 256 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_12/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_12/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_12/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_12/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_12/mlp.up_proj/compressed.log
best_loss 182.85533142089844 running bpv: 2.009894
COMMANDS_FINISHED 257 n_commands 1849
meta-llama/Llama-2-13b-hf/layer_12/self_attn.o_proj is done
reading log  14%|‚ñà‚ñç        | 258/1848 [2:37:21<20:46:42, 47.05s/it] 14%|‚ñà‚ñç        | 259/1848 [2:39:26<29:17:16, 66.35s/it] 14%|‚ñà‚ñç        | 260/1848 [2:39:41<23:21:01, 52.94s/it] 14%|‚ñà‚ñç        | 261/1848 [2:40:26<22:23:08, 50.78s/it] 14%|‚ñà‚ñç        | 262/1848 [2:40:41<17:57:47, 40.77s/it] 14%|‚ñà‚ñç        | 263/1848 [2:41:36<19:44:27, 44.84s/it] 14%|‚ñà‚ñç        | 264/1848 [2:41:51<15:55:33, 36.20s/it]/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_12/self_attn.o_proj/compressed.log
best_loss 11.020336151123047 running bpv: 2.009893
COMMANDS_FINISHED 258 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_1/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_1/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_1/mlp.gate_proj/compressed.log 2>&1 &
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_1/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_1/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_1/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_12/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_12/self_attn.v_proj/compressed.log
best_loss 105.55152893066406 running bpv: 2.009891
COMMANDS_FINISHED 259 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_1/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_1/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_1/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_1/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_1/self_attn.q_proj/compressed.log
best_loss 6.579348564147949 running bpv: 2.009889
COMMANDS_FINISHED 260 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_1/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_1/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_1/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_12/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_12/mlp.down_proj/compressed.log
best_loss 15.257170677185059 running bpv: 2.009852
COMMANDS_FINISHED 261 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_1/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_1/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_1/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_1/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_1/mlp.gate_proj/compressed.log
best_loss 12.317741394042969 running bpv: 2.009813
COMMANDS_FINISHED 262 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_1/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_1/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_1/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_1/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_1/mlp.up_proj/compressed.log
best_loss 10.953301429748535 running bpv: 2.009776
COMMANDS_FINISHED 263 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_1/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_1/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_1/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_1/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_1/self_attn.k_proj/compressed.log
best_loss 6.955574035644531 running bpv: 2.009775
COMMANDS_FINISHED 264 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_7/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_7/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_7/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_1/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_1/self_attn.o_proj/compressed.log
best_loss 0.0974542498588562 running bpv: 2.009773
COMMANDS_FINISHED 265 n_commands 1849
 14%|‚ñà‚ñç        | 265/1848 [2:42:46<18:20:14, 41.70s/it] 14%|‚ñà‚ñç        | 266/1848 [2:44:51<29:07:14, 66.27s/it] 14%|‚ñà‚ñç        | 267/1848 [2:45:27<25:02:00, 57.00s/it] 15%|‚ñà‚ñç        | 268/1848 [2:46:32<26:03:46, 59.38s/it] 15%|‚ñà‚ñç        | 269/1848 [2:46:47<20:14:30, 46.15s/it] 15%|‚ñà‚ñç        | 270/1848 [2:47:02<16:09:02, 36.85s/it] 15%|‚ñà‚ñç        | 271/1848 [2:47:37<15:53:58, 36.30s/it] 15%|‚ñà‚ñç        | 272/1848 [2:48:52<20:57:49, 47.89s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_7/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_7/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_7/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_1/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_1/self_attn.v_proj/compressed.log
best_loss 1.5800458192825317 running bpv: 2.009772
COMMANDS_FINISHED 266 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_7/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_7/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_7/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_7/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_7/self_attn.q_proj/compressed.log
best_loss 156.36599731445312 running bpv: 2.009771
COMMANDS_FINISHED 267 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_7/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_7/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_7/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_1/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_1/mlp.down_proj/compressed.log
best_loss 0.28582441806793213 running bpv: 2.009736
COMMANDS_FINISHED 268 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_7/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_7/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_7/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_7/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_7/mlp.gate_proj/compressed.log
best_loss 145.3167724609375 running bpv: 2.0097
COMMANDS_FINISHED 269 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_7/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_7/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_7/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_7/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_7/mlp.up_proj/compressed.log
best_loss 115.94062042236328 running bpv: 2.009664
COMMANDS_FINISHED 270 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_7/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_7/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_7/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_7/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_7/self_attn.k_proj/compressed.log
best_loss 161.99087524414062 running bpv: 2.009664
COMMANDS_FINISHED 271 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_33/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_33/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_33/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_7/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_7/self_attn.o_proj/compressed.log
best_loss 3.0512895584106445 running bpv: 2.009663
COMMANDS_FINISHED 272 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_33/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_33/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_33/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_7/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_7/self_attn.v_proj/compressed.log
best_loss 58.64621353149414 running bpv: 2.009662
COMMANDS_FINISHED 273 n_commands 1849
 15%|‚ñà‚ñç        | 273/1848 [2:50:57<31:03:31, 70.99s/it] 15%|‚ñà‚ñç        | 274/1848 [2:51:22<25:00:48, 57.21s/it] 15%|‚ñà‚ñç        | 275/1848 [2:51:57<22:05:21, 50.55s/it] 15%|‚ñà‚ñç        | 276/1848 [2:52:32<20:02:23, 45.89s/it] 15%|‚ñà‚ñç        | 277/1848 [2:52:57<17:17:37, 39.63s/it] 15%|‚ñà‚ñå        | 278/1848 [2:53:22<15:22:12, 35.24s/it] 15%|‚ñà‚ñå        | 279/1848 [2:54:37<20:33:32, 47.17s/it] 15%|‚ñà‚ñå        | 280/1848 [2:56:42<30:42:58, 70.52s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_33/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_33/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_33/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_33/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_33/self_attn.q_proj/compressed.log
best_loss 419.1004333496094 running bpv: 2.009661
COMMANDS_FINISHED 274 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_33/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_33/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_33/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_7/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_7/mlp.down_proj/compressed.log
best_loss 6.868077754974365 running bpv: 2.009628
COMMANDS_FINISHED 275 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_33/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_33/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_33/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_33/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_33/mlp.gate_proj/compressed.log
best_loss 698.9303588867188 running bpv: 2.009594
COMMANDS_FINISHED 276 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_33/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_33/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_33/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_33/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_33/mlp.up_proj/compressed.log
best_loss 603.845458984375 running bpv: 2.009561
COMMANDS_FINISHED 277 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_33/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_33/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_33/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_33/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_33/self_attn.k_proj/compressed.log
best_loss 430.7882385253906 running bpv: 2.00956
COMMANDS_FINISHED 278 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_34/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_34/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_34/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_33/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_33/self_attn.o_proj/compressed.log
best_loss 19.462024688720703 running bpv: 2.00956
COMMANDS_FINISHED 279 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_34/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_34/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_34/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_33/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_33/self_attn.v_proj/compressed.log
best_loss 346.39459228515625 running bpv: 2.009559
COMMANDS_FINISHED 280 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_34/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_34/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_34/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_34/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_34/self_attn.q_proj/compressed.log
best_loss 423.15020751953125 running bpv: 2.009559
 15%|‚ñà‚ñå        | 281/1848 [2:56:57<23:26:51, 53.87s/it] 15%|‚ñà‚ñå        | 282/1848 [2:57:52<23:34:53, 54.21s/it] 15%|‚ñà‚ñå        | 283/1848 [2:58:17<19:45:29, 45.45s/it] 15%|‚ñà‚ñå        | 284/1848 [2:58:42<17:04:52, 39.32s/it] 15%|‚ñà‚ñå        | 285/1848 [2:58:57<13:54:14, 32.02s/it] 15%|‚ñà‚ñå        | 286/1848 [3:00:22<20:47:32, 47.92s/it] 16%|‚ñà‚ñå        | 287/1848 [3:02:27<30:48:26, 71.05s/it] 16%|‚ñà‚ñå        | 288/1848 [3:02:42<23:30:07, 54.24s/it]COMMANDS_FINISHED 281 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_34/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_34/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_34/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_33/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_33/mlp.down_proj/compressed.log
best_loss 88.76944732666016 running bpv: 2.009528
COMMANDS_FINISHED 282 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_34/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_34/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_34/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_34/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_34/mlp.gate_proj/compressed.log
best_loss 720.1103515625 running bpv: 2.009496
COMMANDS_FINISHED 283 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_34/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_34/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_34/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_34/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_34/mlp.up_proj/compressed.log
best_loss 637.3717041015625 running bpv: 2.009465
COMMANDS_FINISHED 284 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_34/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_34/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_34/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_34/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_34/self_attn.k_proj/compressed.log
best_loss 434.8938903808594 running bpv: 2.009464
COMMANDS_FINISHED 285 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_0/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_0/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_0/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_34/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_34/self_attn.o_proj/compressed.log
best_loss 26.323436737060547 running bpv: 2.009464
COMMANDS_FINISHED 286 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_0/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_0/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_0/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_34/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_34/self_attn.v_proj/compressed.log
best_loss 380.5462646484375 running bpv: 2.009464
COMMANDS_FINISHED 287 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_0/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_0/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_0/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_0/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_0/self_attn.q_proj/compressed.log
best_loss 0.5211895108222961 running bpv: 2.009464
COMMANDS_FINISHED 288 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_0/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_0/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_0/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_34/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_34/mlp.down_proj/compressed.log
best_loss 99.24510192871094 running bpv: 2.009434
COMMANDS_FINISHED 289 n_commands 1849
 16%|‚ñà‚ñå        | 289/1848 [3:03:37<23:35:15, 54.47s/it] 16%|‚ñà‚ñå        | 290/1848 [3:03:52<18:26:56, 42.63s/it] 16%|‚ñà‚ñå        | 291/1848 [3:04:27<17:26:54, 40.34s/it] 16%|‚ñà‚ñå        | 292/1848 [3:04:42<14:09:06, 32.74s/it] 16%|‚ñà‚ñå        | 293/1848 [3:05:57<19:37:12, 45.42s/it] 16%|‚ñà‚ñå        | 294/1848 [3:08:02<29:54:52, 69.30s/it] 16%|‚ñà‚ñå        | 295/1848 [3:08:37<25:27:26, 59.01s/it] 16%|‚ñà‚ñå        | 296/1848 [3:09:22<23:37:46, 54.81s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_0/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_0/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_0/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_0/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_0/mlp.gate_proj/compressed.log
best_loss 2.259347677230835 running bpv: 2.009404
COMMANDS_FINISHED 290 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_0/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_0/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_0/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_0/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_0/mlp.up_proj/compressed.log
best_loss 2.0919041633605957 running bpv: 2.009375
COMMANDS_FINISHED 291 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_0/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_0/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_0/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_0/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_0/self_attn.k_proj/compressed.log
best_loss 0.23475229740142822 running bpv: 2.009375
COMMANDS_FINISHED 292 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_32/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_32/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_32/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_0/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_0/self_attn.o_proj/compressed.log
best_loss 0.006003833841532469 running bpv: 2.009375
COMMANDS_FINISHED 293 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_32/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_32/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_32/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_0/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_0/self_attn.v_proj/compressed.log
best_loss 0.03111320547759533 running bpv: 2.009375
COMMANDS_FINISHED 294 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_32/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_32/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_32/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_32/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_32/self_attn.q_proj/compressed.log
best_loss 455.0344543457031 running bpv: 2.009375
COMMANDS_FINISHED 295 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_32/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_32/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_32/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_0/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_0/mlp.down_proj/compressed.log
best_loss 0.036247655749320984 running bpv: 2.009347
COMMANDS_FINISHED 296 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_32/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_32/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_32/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_32/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_32/mlp.gate_proj/compressed.log
best_loss 679.33544921875 running bpv: 2.009319
COMMANDS_FINISHED 297 n_commands 1849
 16%|‚ñà‚ñå        | 297/1848 [3:09:37<18:28:10, 42.87s/it] 16%|‚ñà‚ñå        | 298/1848 [3:10:02<16:09:01, 37.51s/it] 16%|‚ñà‚ñå        | 299/1848 [3:10:47<17:06:27, 39.76s/it] 16%|‚ñà‚ñå        | 300/1848 [3:11:42<19:03:49, 44.33s/it] 16%|‚ñà‚ñã        | 301/1848 [3:13:47<29:27:09, 68.54s/it] 16%|‚ñà‚ñã        | 302/1848 [3:14:22<25:06:48, 58.48s/it] 16%|‚ñà‚ñã        | 303/1848 [3:14:57<22:04:31, 51.44s/it] 16%|‚ñà‚ñã        | 304/1848 [3:15:42<21:14:01, 49.51s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_32/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_32/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_32/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_32/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_32/mlp.up_proj/compressed.log
best_loss 580.239990234375 running bpv: 2.009291
COMMANDS_FINISHED 298 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_32/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_32/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_32/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_32/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_32/self_attn.k_proj/compressed.log
best_loss 463.6034851074219 running bpv: 2.009291
COMMANDS_FINISHED 299 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_31/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_31/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_31/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_32/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_32/self_attn.o_proj/compressed.log
best_loss 20.13448143005371 running bpv: 2.009291
COMMANDS_FINISHED 300 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_31/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_31/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_31/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_32/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_32/self_attn.v_proj/compressed.log
best_loss 388.5054016113281 running bpv: 2.009291
COMMANDS_FINISHED 301 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_31/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_31/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_31/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_31/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_31/self_attn.q_proj/compressed.log
best_loss 433.54736328125 running bpv: 2.009292
COMMANDS_FINISHED 302 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_31/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_31/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_31/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_32/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_32/mlp.down_proj/compressed.log
best_loss 83.5928726196289 running bpv: 2.009265
COMMANDS_FINISHED 303 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_31/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_31/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_31/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_31/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_31/mlp.gate_proj/compressed.log
best_loss 656.03271484375 running bpv: 2.009238
COMMANDS_FINISHED 304 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_31/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_31/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_31/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_31/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_31/mlp.up_proj/compressed.log
best_loss 553.5203857421875 running bpv: 2.009212
COMMANDS_FINISHED 305 n_commands 1849
 17%|‚ñà‚ñã        | 305/1848 [3:15:57<16:47:00, 39.16s/it] 17%|‚ñà‚ñã        | 306/1848 [3:16:22<14:57:15, 34.91s/it] 17%|‚ñà‚ñã        | 307/1848 [3:17:47<21:22:41, 49.94s/it] 17%|‚ñà‚ñã        | 308/1848 [3:19:52<30:59:54, 72.46s/it] 17%|‚ñà‚ñã        | 310/1848 [3:20:52<22:35:10, 52.87s/it] 17%|‚ñà‚ñã        | 311/1848 [3:21:17<19:37:20, 45.96s/it] 17%|‚ñà‚ñã        | 312/1848 [3:22:02<19:30:11, 45.71s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_31/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_31/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_31/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_31/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_31/self_attn.k_proj/compressed.log
best_loss 450.1387023925781 running bpv: 2.009212
COMMANDS_FINISHED 306 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_21/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_21/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_21/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_31/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_31/self_attn.o_proj/compressed.log
best_loss 20.24744415283203 running bpv: 2.009213
COMMANDS_FINISHED 307 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_21/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_21/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_21/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_31/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_31/self_attn.v_proj/compressed.log
best_loss 336.6954650878906 running bpv: 2.009213
COMMANDS_FINISHED 308 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_21/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_21/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_21/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_21/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_21/self_attn.q_proj/compressed.log
best_loss 309.86407470703125 running bpv: 2.009214
COMMANDS_FINISHED 309 n_commands 1849
meta-llama/Llama-2-13b-hf/layer_31/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_31/mlp.down_proj/compressed.log
best_loss 78.11799621582031 running bpv: 2.009189
COMMANDS_FINISHED 310 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_21/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_21/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_21/self_attn.k_proj/compressed.log 2>&1 &
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_21/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_21/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_21/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_21/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_21/mlp.gate_proj/compressed.log
best_loss 412.9654541015625 running bpv: 2.009163
COMMANDS_FINISHED 311 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_21/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_21/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_21/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_21/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_21/mlp.up_proj/compressed.log
best_loss 366.4824523925781 running bpv: 2.009138
COMMANDS_FINISHED 312 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_21/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_21/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_21/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_21/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_21/self_attn.k_proj/compressed.log
best_loss 322.92193603515625 running bpv: 2.009138
COMMANDS_FINISHED 313 n_commands 1849
meta-llama/Llama-2-13b-hf/layer_21/self_attn.o_proj is done
reading log  17%|‚ñà‚ñã        | 314/1848 [3:23:22<18:26:18, 43.27s/it] 17%|‚ñà‚ñã        | 315/1848 [3:25:27<26:34:19, 62.40s/it] 17%|‚ñà‚ñã        | 316/1848 [3:25:42<21:30:05, 50.53s/it] 17%|‚ñà‚ñã        | 317/1848 [3:26:57<24:13:55, 56.98s/it] 17%|‚ñà‚ñã        | 319/1848 [3:27:27<16:32:32, 38.95s/it] 17%|‚ñà‚ñã        | 320/1848 [3:27:42<14:08:44, 33.33s/it]/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_21/self_attn.o_proj/compressed.log
best_loss 24.641399383544922 running bpv: 2.009139
COMMANDS_FINISHED 314 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_26/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_26/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_26/mlp.gate_proj/compressed.log 2>&1 &
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_26/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_26/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_26/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_21/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_21/self_attn.v_proj/compressed.log
best_loss 189.29347229003906 running bpv: 2.00914
COMMANDS_FINISHED 315 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_26/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_26/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_26/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_26/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_26/self_attn.q_proj/compressed.log
best_loss 370.041015625 running bpv: 2.00914
COMMANDS_FINISHED 316 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_26/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_26/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_26/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_21/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_21/mlp.down_proj/compressed.log
best_loss 55.22259521484375 running bpv: 2.009116
COMMANDS_FINISHED 317 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_26/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_26/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_26/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_26/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_26/mlp.gate_proj/compressed.log
best_loss 542.1161499023438 running bpv: 2.009092
COMMANDS_FINISHED 318 n_commands 1849
meta-llama/Llama-2-13b-hf/layer_26/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_26/mlp.up_proj/compressed.log
best_loss 453.97979736328125 running bpv: 2.009068
COMMANDS_FINISHED 319 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_26/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_26/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_26/mlp.down_proj/compressed.log 2>&1 &
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_26/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_26/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_26/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_26/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_26/self_attn.k_proj/compressed.log
best_loss 379.9730529785156 running bpv: 2.009069
COMMANDS_FINISHED 320 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_14/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_14/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_14/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_26/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_26/self_attn.o_proj/compressed.log
best_loss 19.949474334716797 running bpv: 2.00907
COMMANDS_FINISHED 321 n_commands 1849
 17%|‚ñà‚ñã        | 321/1848 [3:29:07<19:38:34, 46.31s/it] 17%|‚ñà‚ñã        | 322/1848 [3:31:12<28:26:23, 67.09s/it] 17%|‚ñà‚ñã        | 323/1848 [3:31:57<25:51:31, 61.04s/it] 18%|‚ñà‚ñä        | 324/1848 [3:32:22<21:33:03, 50.91s/it] 18%|‚ñà‚ñä        | 325/1848 [3:32:37<17:11:00, 40.62s/it] 18%|‚ñà‚ñä        | 326/1848 [3:33:12<16:28:58, 38.99s/it] 18%|‚ñà‚ñä        | 327/1848 [3:34:07<18:27:27, 43.69s/it] 18%|‚ñà‚ñä        | 328/1848 [3:34:42<17:21:48, 41.12s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_14/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_14/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_14/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_26/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_26/self_attn.v_proj/compressed.log
best_loss 282.05291748046875 running bpv: 2.00907
COMMANDS_FINISHED 322 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_14/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_14/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_14/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_14/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_14/self_attn.q_proj/compressed.log
best_loss 283.28948974609375 running bpv: 2.009071
COMMANDS_FINISHED 323 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_14/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_14/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_14/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_26/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_26/mlp.down_proj/compressed.log
best_loss 66.67710876464844 running bpv: 2.009049
COMMANDS_FINISHED 324 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_14/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_14/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_14/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_14/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_14/mlp.gate_proj/compressed.log
best_loss 225.4476318359375 running bpv: 2.009025
COMMANDS_FINISHED 325 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_14/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_14/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_14/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_14/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_14/mlp.up_proj/compressed.log
best_loss 218.1425018310547 running bpv: 2.009002
COMMANDS_FINISHED 326 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_14/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_14/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_14/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_14/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_14/self_attn.k_proj/compressed.log
best_loss 295.5626220703125 running bpv: 2.009003
COMMANDS_FINISHED 327 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_15/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_15/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_15/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_14/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_14/self_attn.o_proj/compressed.log
best_loss 15.74996566772461 running bpv: 2.009004
COMMANDS_FINISHED 328 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_15/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_15/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_15/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_14/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_14/self_attn.v_proj/compressed.log
best_loss 126.48492431640625 running bpv: 2.009005
COMMANDS_FINISHED 329 n_commands 1849
 18%|‚ñà‚ñä        | 329/1848 [3:36:47<27:51:13, 66.01s/it] 18%|‚ñà‚ñä        | 330/1848 [3:37:12<22:41:18, 53.81s/it] 18%|‚ñà‚ñä        | 331/1848 [3:38:07<22:49:28, 54.17s/it] 18%|‚ñà‚ñä        | 332/1848 [3:38:52<21:39:25, 51.43s/it] 18%|‚ñà‚ñä        | 333/1848 [3:39:07<17:03:23, 40.53s/it] 18%|‚ñà‚ñä        | 334/1848 [3:39:22<13:49:51, 32.89s/it] 18%|‚ñà‚ñä        | 335/1848 [3:40:57<21:38:39, 51.50s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_15/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_15/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_15/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_15/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_15/self_attn.q_proj/compressed.log
best_loss 266.84515380859375 running bpv: 2.009006
COMMANDS_FINISHED 330 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_15/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_15/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_15/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_14/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_14/mlp.down_proj/compressed.log
best_loss 21.240861892700195 running bpv: 2.008984
COMMANDS_FINISHED 331 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_15/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_15/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_15/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_15/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_15/mlp.gate_proj/compressed.log
best_loss 244.17776489257812 running bpv: 2.008962
COMMANDS_FINISHED 332 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_15/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_15/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_15/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_15/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_15/self_attn.k_proj/compressed.log
best_loss 285.4037170410156 running bpv: 2.008963
COMMANDS_FINISHED 333 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_15/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_15/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_15/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_15/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_15/mlp.up_proj/compressed.log
best_loss 237.3103790283203 running bpv: 2.008941
COMMANDS_FINISHED 334 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_4/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_4/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_4/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_15/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_15/self_attn.o_proj/compressed.log
best_loss 18.832427978515625 running bpv: 2.008942
COMMANDS_FINISHED 335 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_4/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_4/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_4/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_15/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_15/self_attn.v_proj/compressed.log
best_loss 131.76324462890625 running bpv: 2.008943
COMMANDS_FINISHED 336 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_4/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_4/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_4/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_4/self_attn.q_proj is done
reading log  18%|‚ñà‚ñä        | 336/1848 [3:43:02<30:53:02, 73.53s/it] 18%|‚ñà‚ñä        | 338/1848 [3:44:02<22:25:15, 53.45s/it] 18%|‚ñà‚ñä        | 339/1848 [3:44:17<18:24:42, 43.92s/it] 18%|‚ñà‚ñä        | 340/1848 [3:45:02<18:31:05, 44.21s/it] 18%|‚ñà‚ñä        | 341/1848 [3:45:17<15:10:53, 36.27s/it] 19%|‚ñà‚ñä        | 342/1848 [3:46:22<18:32:09, 44.31s/it] 19%|‚ñà‚ñä        | 343/1848 [3:48:27<28:09:30, 67.36s/it] 19%|‚ñà‚ñä        | 344/1848 [3:49:02<24:13:25, 57.98s/it]/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_4/self_attn.q_proj/compressed.log
best_loss 113.05928039550781 running bpv: 2.008944
COMMANDS_FINISHED 337 n_commands 1849
meta-llama/Llama-2-13b-hf/layer_15/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_15/mlp.down_proj/compressed.log
best_loss 24.557130813598633 running bpv: 2.008924
COMMANDS_FINISHED 338 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_4/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_4/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_4/self_attn.k_proj/compressed.log 2>&1 &
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_4/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_4/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_4/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_4/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_4/mlp.gate_proj/compressed.log
best_loss 74.62548065185547 running bpv: 2.008903
COMMANDS_FINISHED 339 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_4/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_4/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_4/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_4/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_4/mlp.up_proj/compressed.log
best_loss 62.961788177490234 running bpv: 2.008882
COMMANDS_FINISHED 340 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_4/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_4/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_4/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_4/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_4/self_attn.k_proj/compressed.log
best_loss 115.82968139648438 running bpv: 2.008883
COMMANDS_FINISHED 341 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_27/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_27/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_27/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_4/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_4/self_attn.o_proj/compressed.log
best_loss 0.7335690855979919 running bpv: 2.008884
COMMANDS_FINISHED 342 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_27/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_27/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_27/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_4/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_4/self_attn.v_proj/compressed.log
best_loss 37.012603759765625 running bpv: 2.008885
COMMANDS_FINISHED 343 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_27/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_27/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_27/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_27/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_27/self_attn.q_proj/compressed.log
best_loss 420.9500427246094 running bpv: 2.008886
COMMANDS_FINISHED 344 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_27/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_27/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_27/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_4/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_4/mlp.down_proj/compressed.log
best_loss 2.1497416496276855 running bpv: 2.008867
COMMANDS_FINISHED 345 n_commands 1849
 19%|‚ñà‚ñä        | 345/1848 [3:49:57<23:50:39, 57.11s/it] 19%|‚ñà‚ñä        | 346/1848 [3:50:12<18:38:52, 44.70s/it] 19%|‚ñà‚ñâ        | 347/1848 [3:50:37<16:12:07, 38.86s/it] 19%|‚ñà‚ñâ        | 348/1848 [3:51:12<15:42:49, 37.71s/it] 19%|‚ñà‚ñâ        | 349/1848 [3:52:17<19:05:35, 45.85s/it] 19%|‚ñà‚ñâ        | 350/1848 [3:54:22<28:55:16, 69.50s/it] 19%|‚ñà‚ñâ        | 351/1848 [3:54:47<23:22:01, 56.19s/it] 19%|‚ñà‚ñâ        | 352/1848 [3:55:32<21:57:35, 52.84s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_27/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_27/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_27/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_27/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_27/mlp.gate_proj/compressed.log
best_loss 567.6290283203125 running bpv: 2.008846
COMMANDS_FINISHED 346 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_27/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_27/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_27/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_27/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_27/mlp.up_proj/compressed.log
best_loss 473.5431823730469 running bpv: 2.008826
COMMANDS_FINISHED 347 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_27/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_27/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_27/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_27/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_27/self_attn.k_proj/compressed.log
best_loss 429.83062744140625 running bpv: 2.008827
COMMANDS_FINISHED 348 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_2/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_2/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_2/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_27/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_27/self_attn.o_proj/compressed.log
best_loss 16.4290714263916 running bpv: 2.008829
COMMANDS_FINISHED 349 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_2/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_2/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_2/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_27/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_27/self_attn.v_proj/compressed.log
best_loss 316.6716003417969 running bpv: 2.00883
COMMANDS_FINISHED 350 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_2/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_2/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_2/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_2/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_2/self_attn.q_proj/compressed.log
best_loss 36.827392578125 running bpv: 2.008831
COMMANDS_FINISHED 351 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_2/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_2/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_2/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_27/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_27/mlp.down_proj/compressed.log
best_loss 68.2752914428711 running bpv: 2.008812
COMMANDS_FINISHED 352 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_2/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_2/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_2/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_2/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_2/mlp.gate_proj/compressed.log
best_loss 32.93649673461914 running bpv: 2.008793
COMMANDS_FINISHED 353 n_commands 1849
 19%|‚ñà‚ñâ        | 353/1848 [3:56:07<19:43:33, 47.50s/it] 19%|‚ñà‚ñâ        | 354/1848 [3:56:22<15:40:16, 37.76s/it] 19%|‚ñà‚ñâ        | 355/1848 [3:56:47<14:04:29, 33.94s/it] 19%|‚ñà‚ñâ        | 356/1848 [3:58:12<20:24:44, 49.25s/it] 19%|‚ñà‚ñâ        | 357/1848 [4:00:17<29:48:31, 71.97s/it] 19%|‚ñà‚ñâ        | 358/1848 [4:00:32<22:43:01, 54.89s/it] 19%|‚ñà‚ñâ        | 359/1848 [4:01:17<21:28:34, 51.92s/it] 19%|‚ñà‚ñâ        | 360/1848 [4:01:42<18:07:27, 43.85s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_2/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_2/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_2/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_2/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_2/mlp.up_proj/compressed.log
best_loss 28.065248489379883 running bpv: 2.008774
COMMANDS_FINISHED 354 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_2/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_2/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_2/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_2/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_2/self_attn.k_proj/compressed.log
best_loss 39.88235092163086 running bpv: 2.008775
COMMANDS_FINISHED 355 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_36/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_36/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_36/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_2/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_2/self_attn.o_proj/compressed.log
best_loss 0.23060572147369385 running bpv: 2.008776
COMMANDS_FINISHED 356 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_36/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_36/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_36/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_2/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_2/self_attn.v_proj/compressed.log
best_loss 10.765594482421875 running bpv: 2.008777
COMMANDS_FINISHED 357 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_36/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_36/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_36/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_36/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_36/self_attn.q_proj/compressed.log
best_loss 373.7840881347656 running bpv: 2.008779
COMMANDS_FINISHED 358 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_36/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_36/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_36/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_2/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_2/mlp.down_proj/compressed.log
best_loss 0.7298557758331299 running bpv: 2.008761
COMMANDS_FINISHED 359 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_36/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_36/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_36/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_36/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_36/mlp.gate_proj/compressed.log
best_loss 770.8426513671875 running bpv: 2.008742
COMMANDS_FINISHED 360 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_36/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_36/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_36/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_36/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_36/mlp.up_proj/compressed.log
best_loss 708.0986328125 running bpv: 2.008723
COMMANDS_FINISHED 361 n_commands 1849
 20%|‚ñà‚ñâ        | 361/1848 [4:02:27<18:15:20, 44.20s/it] 20%|‚ñà‚ñâ        | 362/1848 [4:02:42<14:37:43, 35.44s/it] 20%|‚ñà‚ñâ        | 363/1848 [4:03:47<18:16:40, 44.31s/it] 20%|‚ñà‚ñâ        | 364/1848 [4:05:52<28:14:44, 68.52s/it] 20%|‚ñà‚ñâ        | 365/1848 [4:06:07<21:36:47, 52.47s/it] 20%|‚ñà‚ñâ        | 366/1848 [4:07:22<24:22:57, 59.23s/it] 20%|‚ñà‚ñâ        | 367/1848 [4:07:37<18:54:29, 45.96s/it] 20%|‚ñà‚ñâ        | 368/1848 [4:07:52<15:04:38, 36.67s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_36/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_36/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_36/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_36/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_36/self_attn.k_proj/compressed.log
best_loss 389.73419189453125 running bpv: 2.008725
COMMANDS_FINISHED 362 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_28/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_28/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_28/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_36/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_36/self_attn.o_proj/compressed.log
best_loss 40.80598449707031 running bpv: 2.008726
COMMANDS_FINISHED 363 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_28/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_28/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_28/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_36/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_36/self_attn.v_proj/compressed.log
best_loss 356.02911376953125 running bpv: 2.008727
COMMANDS_FINISHED 364 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_28/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_28/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_28/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_28/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_28/self_attn.q_proj/compressed.log
best_loss 417.9053039550781 running bpv: 2.008729
COMMANDS_FINISHED 365 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_28/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_28/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_28/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_36/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_36/mlp.down_proj/compressed.log
best_loss 132.67579650878906 running bpv: 2.008711
COMMANDS_FINISHED 366 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_28/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_28/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_28/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_28/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_28/mlp.gate_proj/compressed.log
best_loss 582.91796875 running bpv: 2.008694
COMMANDS_FINISHED 367 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_28/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_28/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_28/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_28/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_28/mlp.up_proj/compressed.log
best_loss 487.3921203613281 running bpv: 2.008676
COMMANDS_FINISHED 368 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_28/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_28/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_28/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_28/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_28/self_attn.k_proj/compressed.log
best_loss 425.8831481933594 running bpv: 2.008677
COMMANDS_FINISHED 369 n_commands 1849
 20%|‚ñà‚ñâ        | 369/1848 [4:08:07<12:23:46, 30.17s/it] 20%|‚ñà‚ñà        | 370/1848 [4:09:42<20:22:25, 49.62s/it] 20%|‚ñà‚ñà        | 371/1848 [4:11:47<29:38:19, 72.24s/it] 20%|‚ñà‚ñà        | 372/1848 [4:12:22<25:02:19, 61.07s/it] 20%|‚ñà‚ñà        | 373/1848 [4:12:47<20:35:19, 50.25s/it] 20%|‚ñà‚ñà        | 374/1848 [4:13:02<16:14:44, 39.68s/it] 20%|‚ñà‚ñà        | 375/1848 [4:13:47<16:53:19, 41.28s/it] 20%|‚ñà‚ñà        | 376/1848 [4:14:32<17:20:06, 42.40s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_18/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_18/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_18/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_28/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_28/self_attn.o_proj/compressed.log
best_loss 26.243324279785156 running bpv: 2.008679
COMMANDS_FINISHED 370 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_18/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_18/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_18/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_28/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_28/self_attn.v_proj/compressed.log
best_loss 319.4544982910156 running bpv: 2.00868
COMMANDS_FINISHED 371 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_18/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_18/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_18/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_18/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_18/self_attn.q_proj/compressed.log
best_loss 308.69842529296875 running bpv: 2.008681
COMMANDS_FINISHED 372 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_18/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_18/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_18/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_28/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_28/mlp.down_proj/compressed.log
best_loss 68.91473388671875 running bpv: 2.008665
COMMANDS_FINISHED 373 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_18/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_18/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_18/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_18/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_18/mlp.gate_proj/compressed.log
best_loss 328.4075927734375 running bpv: 2.008647
COMMANDS_FINISHED 374 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_18/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_18/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_18/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_18/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_18/mlp.up_proj/compressed.log
best_loss 303.911865234375 running bpv: 2.00863
COMMANDS_FINISHED 375 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_18/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_18/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_18/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_18/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_18/self_attn.k_proj/compressed.log
best_loss 324.5490417480469 running bpv: 2.008632
COMMANDS_FINISHED 376 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_25/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_25/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_25/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_18/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_18/self_attn.o_proj/compressed.log
best_loss 15.892072677612305 running bpv: 2.008633
COMMANDS_FINISHED 377 n_commands 1849
 20%|‚ñà‚ñà        | 377/1848 [4:15:07<16:25:03, 40.18s/it] 20%|‚ñà‚ñà        | 378/1848 [4:17:12<26:47:54, 65.63s/it] 21%|‚ñà‚ñà        | 379/1848 [4:17:37<21:48:26, 53.44s/it] 21%|‚ñà‚ñà        | 380/1848 [4:18:42<23:12:27, 56.91s/it] 21%|‚ñà‚ñà        | 381/1848 [4:19:17<20:30:49, 50.34s/it] 21%|‚ñà‚ñà        | 382/1848 [4:19:32<16:10:58, 39.74s/it] 21%|‚ñà‚ñà        | 383/1848 [4:19:47<13:09:08, 32.32s/it] 21%|‚ñà‚ñà        | 384/1848 [4:21:22<20:47:29, 51.13s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_25/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_25/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_25/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_18/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_18/self_attn.v_proj/compressed.log
best_loss 170.39923095703125 running bpv: 2.008635
COMMANDS_FINISHED 378 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_25/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_25/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_25/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_25/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_25/self_attn.q_proj/compressed.log
best_loss 370.8851013183594 running bpv: 2.008636
COMMANDS_FINISHED 379 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_25/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_25/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_25/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_18/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_18/mlp.down_proj/compressed.log
best_loss 35.80217742919922 running bpv: 2.00862
COMMANDS_FINISHED 380 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_25/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_25/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_25/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_25/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_25/mlp.gate_proj/compressed.log
best_loss 521.6195678710938 running bpv: 2.008604
COMMANDS_FINISHED 381 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_25/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_25/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_25/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_25/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_25/self_attn.k_proj/compressed.log
best_loss 380.77606201171875 running bpv: 2.008605
COMMANDS_FINISHED 382 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_25/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_25/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_25/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_25/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_25/mlp.up_proj/compressed.log
best_loss 439.0030822753906 running bpv: 2.008589
COMMANDS_FINISHED 383 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_37/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_37/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_37/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_25/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_25/self_attn.o_proj/compressed.log
best_loss 17.573041915893555 running bpv: 2.00859
COMMANDS_FINISHED 384 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_37/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_37/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_37/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_25/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_25/self_attn.v_proj/compressed.log
best_loss 275.72845458984375 running bpv: 2.008592
COMMANDS_FINISHED 385 n_commands 1849
 21%|‚ñà‚ñà        | 385/1848 [4:23:27<29:47:06, 73.29s/it] 21%|‚ñà‚ñà        | 386/1848 [4:23:42<22:39:48, 55.81s/it] 21%|‚ñà‚ñà        | 387/1848 [4:24:27<21:19:59, 52.57s/it] 21%|‚ñà‚ñà        | 388/1848 [4:24:43<16:44:54, 41.30s/it] 21%|‚ñà‚ñà        | 389/1848 [4:25:28<17:11:17, 42.41s/it] 21%|‚ñà‚ñà        | 390/1848 [4:25:43<13:50:48, 34.19s/it] 21%|‚ñà‚ñà        | 391/1848 [4:26:48<17:34:45, 43.44s/it] 21%|‚ñà‚ñà        | 392/1848 [4:28:53<27:27:54, 67.91s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_37/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_37/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_37/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_37/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_37/self_attn.q_proj/compressed.log
best_loss 335.856201171875 running bpv: 2.008593
COMMANDS_FINISHED 386 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_37/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_37/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_37/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_25/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_25/mlp.down_proj/compressed.log
best_loss 63.71583557128906 running bpv: 2.008578
COMMANDS_FINISHED 387 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_37/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_37/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_37/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_37/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_37/mlp.gate_proj/compressed.log
best_loss 777.447509765625 running bpv: 2.008562
COMMANDS_FINISHED 388 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_37/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_37/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_37/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_37/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_37/mlp.up_proj/compressed.log
best_loss 718.8955078125 running bpv: 2.008546
COMMANDS_FINISHED 389 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_37/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_37/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_37/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_37/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_37/self_attn.k_proj/compressed.log
best_loss 337.1485595703125 running bpv: 2.008547
COMMANDS_FINISHED 390 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_30/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_30/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_30/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_37/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_37/self_attn.o_proj/compressed.log
best_loss 46.089149475097656 running bpv: 2.008549
COMMANDS_FINISHED 391 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_30/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_30/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_30/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_37/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_37/self_attn.v_proj/compressed.log
best_loss 357.63836669921875 running bpv: 2.008551
COMMANDS_FINISHED 392 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_30/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_30/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_30/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_30/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_30/self_attn.q_proj/compressed.log
best_loss 478.0395202636719 running bpv: 2.008552
COMMANDS_FINISHED 393 n_commands 1849
 21%|‚ñà‚ñà‚ñè       | 393/1848 [4:29:28<23:27:25, 58.04s/it] 21%|‚ñà‚ñà‚ñè       | 394/1848 [4:30:23<23:04:25, 57.13s/it] 21%|‚ñà‚ñà‚ñè       | 395/1848 [4:30:38<17:57:25, 44.49s/it] 21%|‚ñà‚ñà‚ñè       | 396/1848 [4:30:53<14:22:36, 35.65s/it] 21%|‚ñà‚ñà‚ñè       | 397/1848 [4:31:38<15:29:56, 38.45s/it] 22%|‚ñà‚ñà‚ñè       | 398/1848 [4:32:43<18:41:49, 46.42s/it] 22%|‚ñà‚ñà‚ñè       | 399/1848 [4:34:48<28:10:27, 70.00s/it] 22%|‚ñà‚ñà‚ñè       | 400/1848 [4:35:13<22:43:32, 56.50s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_30/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_30/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_30/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_37/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_37/mlp.down_proj/compressed.log
best_loss 156.91957092285156 running bpv: 2.008537
COMMANDS_FINISHED 394 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_30/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_30/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_30/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_30/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_30/mlp.gate_proj/compressed.log
best_loss 632.8765258789062 running bpv: 2.008522
COMMANDS_FINISHED 395 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_30/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_30/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_30/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_30/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_30/mlp.up_proj/compressed.log
best_loss 532.7077026367188 running bpv: 2.008506
COMMANDS_FINISHED 396 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_30/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_30/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_30/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_30/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_30/self_attn.k_proj/compressed.log
best_loss 484.9186706542969 running bpv: 2.008508
COMMANDS_FINISHED 397 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_29/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_29/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_29/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_30/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_30/self_attn.o_proj/compressed.log
best_loss 18.40155792236328 running bpv: 2.00851
COMMANDS_FINISHED 398 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_29/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_29/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_29/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_30/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_30/self_attn.v_proj/compressed.log
best_loss 400.0123291015625 running bpv: 2.008511
COMMANDS_FINISHED 399 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_29/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_29/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_29/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_29/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_29/self_attn.q_proj/compressed.log
best_loss 442.5615234375 running bpv: 2.008513
COMMANDS_FINISHED 400 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_29/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_29/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_29/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_30/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_30/mlp.down_proj/compressed.log
best_loss 75.72935485839844 running bpv: 2.008498
COMMANDS_FINISHED 401 n_commands 1849
 22%|‚ñà‚ñà‚ñè       | 401/1848 [4:35:48<20:07:05, 50.05s/it] 22%|‚ñà‚ñà‚ñè       | 402/1848 [4:36:33<19:29:46, 48.54s/it] 22%|‚ñà‚ñà‚ñè       | 403/1848 [4:36:48<15:26:41, 38.48s/it] 22%|‚ñà‚ñà‚ñè       | 404/1848 [4:37:23<15:00:59, 37.44s/it] 22%|‚ñà‚ñà‚ñè       | 405/1848 [4:38:38<19:31:27, 48.71s/it] 22%|‚ñà‚ñà‚ñè       | 406/1848 [4:40:43<28:40:48, 71.60s/it] 22%|‚ñà‚ñà‚ñè       | 408/1848 [4:41:43<20:57:40, 52.40s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_29/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_29/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_29/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_29/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_29/mlp.gate_proj/compressed.log
best_loss 602.961669921875 running bpv: 2.008484
COMMANDS_FINISHED 402 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_29/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_29/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_29/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_29/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_29/mlp.up_proj/compressed.log
best_loss 508.0683898925781 running bpv: 2.008469
COMMANDS_FINISHED 403 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_29/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_29/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_29/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_29/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_29/self_attn.k_proj/compressed.log
best_loss 455.90814208984375 running bpv: 2.00847
COMMANDS_FINISHED 404 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_16/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_16/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_16/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_29/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_29/self_attn.o_proj/compressed.log
best_loss 17.061853408813477 running bpv: 2.008472
COMMANDS_FINISHED 405 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_16/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_16/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_16/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_29/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_29/self_attn.v_proj/compressed.log
best_loss 333.83990478515625 running bpv: 2.008474
COMMANDS_FINISHED 406 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_16/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_16/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_16/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_29/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_29/mlp.down_proj/compressed.log
best_loss 72.19507598876953 running bpv: 2.00846
COMMANDS_FINISHED 407 n_commands 1849
meta-llama/Llama-2-13b-hf/layer_16/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_16/self_attn.q_proj/compressed.log
best_loss 295.4585876464844 running bpv: 2.008461
COMMANDS_FINISHED 408 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_16/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_16/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_16/self_attn.k_proj/compressed.log 2>&1 &
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_16/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_16/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_16/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_16/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_16/mlp.gate_proj/compressed.log
best_loss 269.2110595703125 running bpv: 2.008447
COMMANDS_FINISHED 409 n_commands 1849
 22%|‚ñà‚ñà‚ñè       | 409/1848 [4:42:18<19:13:21, 48.09s/it] 22%|‚ñà‚ñà‚ñè       | 410/1848 [4:42:43<16:47:51, 42.05s/it] 22%|‚ñà‚ñà‚ñè       | 411/1848 [4:42:58<13:50:57, 34.70s/it] 22%|‚ñà‚ñà‚ñè       | 412/1848 [4:44:23<19:27:22, 48.78s/it] 22%|‚ñà‚ñà‚ñè       | 413/1848 [4:46:28<28:07:18, 70.55s/it] 22%|‚ñà‚ñà‚ñè       | 414/1848 [4:46:43<21:41:27, 54.45s/it] 22%|‚ñà‚ñà‚ñè       | 415/1848 [4:47:38<21:44:24, 54.62s/it] 23%|‚ñà‚ñà‚ñé       | 416/1848 [4:47:53<17:04:42, 42.93s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_16/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_16/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_16/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_16/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_16/mlp.up_proj/compressed.log
best_loss 260.59368896484375 running bpv: 2.008433
COMMANDS_FINISHED 410 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_16/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_16/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_16/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_16/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_16/self_attn.k_proj/compressed.log
best_loss 310.3056945800781 running bpv: 2.008434
COMMANDS_FINISHED 411 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_13/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_13/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_13/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_16/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_16/self_attn.o_proj/compressed.log
best_loss 19.088428497314453 running bpv: 2.008436
COMMANDS_FINISHED 412 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_13/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_13/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_13/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_16/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_16/self_attn.v_proj/compressed.log
best_loss 148.66763305664062 running bpv: 2.008438
COMMANDS_FINISHED 413 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_13/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_13/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_13/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_13/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_13/self_attn.q_proj/compressed.log
best_loss 278.955078125 running bpv: 2.008439
COMMANDS_FINISHED 414 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_13/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_13/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_13/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_16/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_16/mlp.down_proj/compressed.log
best_loss 30.6907958984375 running bpv: 2.008426
COMMANDS_FINISHED 415 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_13/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_13/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_13/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_13/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_13/mlp.gate_proj/compressed.log
best_loss 209.4438934326172 running bpv: 2.008412
COMMANDS_FINISHED 416 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_13/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_13/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_13/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_13/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_13/mlp.up_proj/compressed.log
best_loss 199.34805297851562 running bpv: 2.008398
COMMANDS_FINISHED 417 n_commands 1849
 23%|‚ñà‚ñà‚ñé       | 417/1848 [4:48:28<16:07:57, 40.59s/it] 23%|‚ñà‚ñà‚ñé       | 418/1848 [4:48:53<14:16:49, 35.95s/it] 23%|‚ñà‚ñà‚ñé       | 419/1848 [4:49:58<17:42:37, 44.62s/it] 23%|‚ñà‚ñà‚ñé       | 420/1848 [4:52:03<27:13:32, 68.64s/it] 23%|‚ñà‚ñà‚ñé       | 421/1848 [4:52:28<22:01:59, 55.58s/it] 23%|‚ñà‚ñà‚ñé       | 422/1848 [4:53:23<21:56:57, 55.41s/it] 23%|‚ñà‚ñà‚ñé       | 423/1848 [4:53:48<18:19:41, 46.30s/it] 23%|‚ñà‚ñà‚ñé       | 424/1848 [4:54:03<14:36:18, 36.92s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_13/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_13/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_13/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_13/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_13/self_attn.k_proj/compressed.log
best_loss 279.47998046875 running bpv: 2.0084
COMMANDS_FINISHED 418 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_10/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_10/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_10/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_13/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_13/self_attn.o_proj/compressed.log
best_loss 12.82209587097168 running bpv: 2.008401
COMMANDS_FINISHED 419 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_10/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_10/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_10/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_13/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_13/self_attn.v_proj/compressed.log
best_loss 131.0183563232422 running bpv: 2.008403
COMMANDS_FINISHED 420 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_10/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_10/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_10/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_10/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_10/self_attn.q_proj/compressed.log
best_loss 228.26121520996094 running bpv: 2.008405
COMMANDS_FINISHED 421 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_10/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_10/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_10/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_13/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_13/mlp.down_proj/compressed.log
best_loss 18.02561378479004 running bpv: 2.008392
COMMANDS_FINISHED 422 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_10/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_10/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_10/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_10/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_10/mlp.gate_proj/compressed.log
best_loss 183.69154357910156 running bpv: 2.008378
COMMANDS_FINISHED 423 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_10/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_10/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_10/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_10/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_10/mlp.up_proj/compressed.log
best_loss 162.11517333984375 running bpv: 2.008365
COMMANDS_FINISHED 424 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_10/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_10/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_10/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_10/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_10/self_attn.k_proj/compressed.log
best_loss 237.2767333984375 running bpv: 2.008366
COMMANDS_FINISHED 425 n_commands 1849
 23%|‚ñà‚ñà‚ñé       | 425/1848 [4:54:28<13:10:57, 33.35s/it] 23%|‚ñà‚ñà‚ñé       | 426/1848 [4:55:53<19:17:31, 48.84s/it] 23%|‚ñà‚ñà‚ñé       | 427/1848 [4:57:58<28:17:43, 71.68s/it] 23%|‚ñà‚ñà‚ñé       | 428/1848 [4:58:13<21:34:11, 54.68s/it] 23%|‚ñà‚ñà‚ñé       | 429/1848 [4:58:58<20:24:38, 51.78s/it] 23%|‚ñà‚ñà‚ñé       | 430/1848 [4:59:23<17:13:57, 43.75s/it] 23%|‚ñà‚ñà‚ñé       | 431/1848 [4:59:58<16:11:17, 41.13s/it] 23%|‚ñà‚ñà‚ñé       | 432/1848 [5:00:13<13:05:40, 33.29s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_19/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_19/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_19/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_10/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_10/self_attn.o_proj/compressed.log
best_loss 8.333536148071289 running bpv: 2.008368
COMMANDS_FINISHED 426 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_19/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_19/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_19/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_10/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_10/self_attn.v_proj/compressed.log
best_loss 90.87319946289062 running bpv: 2.00837
COMMANDS_FINISHED 427 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_19/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_19/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_19/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_19/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_19/self_attn.q_proj/compressed.log
best_loss 306.61376953125 running bpv: 2.008372
COMMANDS_FINISHED 428 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_19/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_19/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_19/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_10/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_10/mlp.down_proj/compressed.log
best_loss 12.319294929504395 running bpv: 2.008359
COMMANDS_FINISHED 429 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_19/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_19/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_19/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_19/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_19/mlp.gate_proj/compressed.log
best_loss 355.4185791015625 running bpv: 2.008346
COMMANDS_FINISHED 430 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_19/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_19/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_19/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_19/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_19/mlp.up_proj/compressed.log
best_loss 324.1293029785156 running bpv: 2.008333
COMMANDS_FINISHED 431 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_19/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_19/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_19/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_19/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_19/self_attn.k_proj/compressed.log
best_loss 318.01873779296875 running bpv: 2.008335
COMMANDS_FINISHED 432 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_6/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_6/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_6/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_19/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_19/self_attn.o_proj/compressed.log
best_loss 17.20932388305664 running bpv: 2.008336
COMMANDS_FINISHED 433 n_commands 1849
 23%|‚ñà‚ñà‚ñé       | 433/1848 [5:01:28<18:00:16, 45.81s/it] 23%|‚ñà‚ñà‚ñé       | 434/1848 [5:03:33<27:19:29, 69.57s/it] 24%|‚ñà‚ñà‚ñé       | 435/1848 [5:03:58<22:03:30, 56.20s/it] 24%|‚ñà‚ñà‚ñé       | 436/1848 [5:04:53<21:54:09, 55.84s/it] 24%|‚ñà‚ñà‚ñé       | 437/1848 [5:05:08<17:05:07, 43.59s/it] 24%|‚ñà‚ñà‚ñé       | 438/1848 [5:05:33<14:53:22, 38.02s/it] 24%|‚ñà‚ñà‚ñç       | 439/1848 [5:05:58<13:21:05, 34.11s/it] 24%|‚ñà‚ñà‚ñç       | 440/1848 [5:07:13<18:08:26, 46.38s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_6/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_6/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_6/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_19/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_19/self_attn.v_proj/compressed.log
best_loss 176.51104736328125 running bpv: 2.008338
COMMANDS_FINISHED 434 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_6/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_6/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_6/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_6/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_6/self_attn.q_proj/compressed.log
best_loss 157.69754028320312 running bpv: 2.00834
COMMANDS_FINISHED 435 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_6/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_6/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_6/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_19/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_19/mlp.down_proj/compressed.log
best_loss 40.090606689453125 running bpv: 2.008328
COMMANDS_FINISHED 436 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_6/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_6/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_6/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_6/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_6/mlp.gate_proj/compressed.log
best_loss 125.69383239746094 running bpv: 2.008315
COMMANDS_FINISHED 437 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_6/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_6/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_6/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_6/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_6/mlp.up_proj/compressed.log
best_loss 100.75638580322266 running bpv: 2.008302
COMMANDS_FINISHED 438 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_6/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_6/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_6/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_6/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_6/self_attn.k_proj/compressed.log
best_loss 163.14389038085938 running bpv: 2.008304
COMMANDS_FINISHED 439 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_11/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_11/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_11/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_6/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_6/self_attn.o_proj/compressed.log
best_loss 1.820237636566162 running bpv: 2.008306
COMMANDS_FINISHED 440 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_11/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_11/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_11/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_6/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_6/self_attn.v_proj/compressed.log
best_loss 58.39838409423828 running bpv: 2.008307
COMMANDS_FINISHED 441 n_commands 1849
 24%|‚ñà‚ñà‚ñç       | 441/1848 [5:09:18<27:20:50, 69.97s/it] 24%|‚ñà‚ñà‚ñç       | 442/1848 [5:09:43<22:03:34, 56.48s/it] 24%|‚ñà‚ñà‚ñç       | 443/1848 [5:10:28<20:42:00, 53.04s/it] 24%|‚ñà‚ñà‚ñç       | 444/1848 [5:10:53<17:24:19, 44.63s/it] 24%|‚ñà‚ñà‚ñç       | 445/1848 [5:11:18<15:05:55, 38.74s/it] 24%|‚ñà‚ñà‚ñç       | 446/1848 [5:11:43<13:28:59, 34.62s/it] 24%|‚ñà‚ñà‚ñç       | 447/1848 [5:12:58<18:11:20, 46.74s/it] 24%|‚ñà‚ñà‚ñç       | 448/1848 [5:15:03<27:18:30, 70.22s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_11/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_11/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_11/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_11/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_11/self_attn.q_proj/compressed.log
best_loss 224.3614501953125 running bpv: 2.008309
COMMANDS_FINISHED 442 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_11/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_11/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_11/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_6/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_6/mlp.down_proj/compressed.log
best_loss 5.086723327636719 running bpv: 2.008297
COMMANDS_FINISHED 443 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_11/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_11/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_11/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_11/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_11/mlp.gate_proj/compressed.log
best_loss 186.2613067626953 running bpv: 2.008285
COMMANDS_FINISHED 444 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_11/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_11/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_11/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_11/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_11/mlp.up_proj/compressed.log
best_loss 170.336669921875 running bpv: 2.008273
COMMANDS_FINISHED 445 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_11/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_11/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_11/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_11/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_11/self_attn.k_proj/compressed.log
best_loss 235.7552490234375 running bpv: 2.008275
COMMANDS_FINISHED 446 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_20/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_20/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_20/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_11/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_11/self_attn.o_proj/compressed.log
best_loss 9.771909713745117 running bpv: 2.008276
COMMANDS_FINISHED 447 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_20/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_20/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_20/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_11/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_11/self_attn.v_proj/compressed.log
best_loss 86.9627914428711 running bpv: 2.008278
COMMANDS_FINISHED 448 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_20/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_20/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_20/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_20/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_20/self_attn.q_proj/compressed.log
best_loss 289.09503173828125 running bpv: 2.00828
 24%|‚ñà‚ñà‚ñç       | 449/1848 [5:15:18<20:51:06, 53.66s/it] 24%|‚ñà‚ñà‚ñç       | 450/1848 [5:16:13<20:59:39, 54.06s/it] 24%|‚ñà‚ñà‚ñç       | 451/1848 [5:16:38<17:35:47, 45.35s/it] 24%|‚ñà‚ñà‚ñç       | 452/1848 [5:17:13<16:22:52, 42.24s/it] 25%|‚ñà‚ñà‚ñç       | 453/1848 [5:17:28<13:12:10, 34.07s/it] 25%|‚ñà‚ñà‚ñç       | 454/1848 [5:18:43<17:56:57, 46.35s/it] 25%|‚ñà‚ñà‚ñç       | 455/1848 [5:20:48<27:04:03, 69.95s/it] 25%|‚ñà‚ñà‚ñç       | 456/1848 [5:21:03<20:40:27, 53.47s/it]COMMANDS_FINISHED 449 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_20/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_20/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_20/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_11/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_11/mlp.down_proj/compressed.log
best_loss 13.795598983764648 running bpv: 2.008268
COMMANDS_FINISHED 450 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_20/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_20/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_20/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_20/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_20/mlp.gate_proj/compressed.log
best_loss 376.8356018066406 running bpv: 2.008256
COMMANDS_FINISHED 451 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_20/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_20/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_20/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_20/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_20/mlp.up_proj/compressed.log
best_loss 341.66473388671875 running bpv: 2.008245
COMMANDS_FINISHED 452 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_20/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_20/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_20/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_20/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_20/self_attn.k_proj/compressed.log
best_loss 303.81060791015625 running bpv: 2.008246
COMMANDS_FINISHED 453 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_23/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_23/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_23/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_20/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_20/self_attn.o_proj/compressed.log
best_loss 19.13610076904297 running bpv: 2.008248
COMMANDS_FINISHED 454 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_23/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_23/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_23/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_20/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_20/self_attn.v_proj/compressed.log
best_loss 163.28213500976562 running bpv: 2.00825
COMMANDS_FINISHED 455 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_23/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_23/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_23/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_23/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_23/self_attn.q_proj/compressed.log
best_loss 333.1888427734375 running bpv: 2.008252
COMMANDS_FINISHED 456 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_23/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_23/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_23/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_20/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_20/mlp.down_proj/compressed.log
best_loss 44.74059295654297 running bpv: 2.00824
COMMANDS_FINISHED 457 n_commands 1849
 25%|‚ñà‚ñà‚ñç       | 457/1848 [5:22:08<21:59:49, 56.93s/it] 25%|‚ñà‚ñà‚ñç       | 458/1848 [5:22:23<17:07:30, 44.35s/it] 25%|‚ñà‚ñà‚ñç       | 459/1848 [5:22:58<16:01:52, 41.55s/it] 25%|‚ñà‚ñà‚ñç       | 460/1848 [5:23:13<12:56:57, 33.59s/it] 25%|‚ñà‚ñà‚ñç       | 461/1848 [5:24:28<17:43:41, 46.01s/it] 25%|‚ñà‚ñà‚ñå       | 462/1848 [5:26:33<26:50:24, 69.71s/it] 25%|‚ñà‚ñà‚ñå       | 463/1848 [5:27:08<22:48:53, 59.30s/it] 25%|‚ñà‚ñà‚ñå       | 464/1848 [5:27:53<21:08:59, 55.01s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_23/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_23/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_23/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_23/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_23/mlp.gate_proj/compressed.log
best_loss 473.2320556640625 running bpv: 2.008229
COMMANDS_FINISHED 458 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_23/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_23/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_23/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_23/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_23/mlp.up_proj/compressed.log
best_loss 404.66387939453125 running bpv: 2.008217
COMMANDS_FINISHED 459 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_23/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_23/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_23/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_23/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_23/self_attn.k_proj/compressed.log
best_loss 344.7528991699219 running bpv: 2.008219
COMMANDS_FINISHED 460 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_35/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_35/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_35/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_23/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_23/self_attn.o_proj/compressed.log
best_loss 17.110332489013672 running bpv: 2.008221
COMMANDS_FINISHED 461 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_35/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_35/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_35/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_23/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_23/self_attn.v_proj/compressed.log
best_loss 235.62109375 running bpv: 2.008223
COMMANDS_FINISHED 462 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_35/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_35/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_35/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_35/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_35/self_attn.q_proj/compressed.log
best_loss 393.0472412109375 running bpv: 2.008224
COMMANDS_FINISHED 463 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_35/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_35/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_35/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_23/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_23/mlp.down_proj/compressed.log
best_loss 59.42518997192383 running bpv: 2.008213
COMMANDS_FINISHED 464 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_35/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_35/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_35/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_35/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_35/mlp.gate_proj/compressed.log
best_loss 744.649658203125 running bpv: 2.008202
COMMANDS_FINISHED 465 n_commands 1849
 25%|‚ñà‚ñà‚ñå       | 465/1848 [5:28:08<16:31:24, 43.01s/it] 25%|‚ñà‚ñà‚ñå       | 466/1848 [5:28:33<14:26:17, 37.61s/it] 25%|‚ñà‚ñà‚ñå       | 467/1848 [5:29:08<14:07:41, 36.83s/it] 25%|‚ñà‚ñà‚ñå       | 468/1848 [5:30:13<17:21:30, 45.28s/it] 25%|‚ñà‚ñà‚ñå       | 469/1848 [5:32:18<26:30:30, 69.20s/it] 25%|‚ñà‚ñà‚ñå       | 470/1848 [5:32:43<21:24:50, 55.94s/it] 25%|‚ñà‚ñà‚ñå       | 471/1848 [5:33:28<20:08:36, 52.66s/it] 26%|‚ñà‚ñà‚ñå       | 472/1848 [5:34:03<18:06:15, 47.37s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_35/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_35/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_35/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_35/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_35/mlp.up_proj/compressed.log
best_loss 674.7061157226562 running bpv: 2.008191
COMMANDS_FINISHED 466 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_35/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_35/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_35/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_35/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_35/self_attn.k_proj/compressed.log
best_loss 405.17584228515625 running bpv: 2.008193
COMMANDS_FINISHED 467 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_38/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_38/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_38/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_35/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_35/self_attn.o_proj/compressed.log
best_loss 26.5930233001709 running bpv: 2.008195
COMMANDS_FINISHED 468 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_38/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_38/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_38/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_35/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_35/self_attn.v_proj/compressed.log
best_loss 360.4745178222656 running bpv: 2.008196
COMMANDS_FINISHED 469 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_38/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_38/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_38/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_38/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_38/self_attn.q_proj/compressed.log
best_loss 324.0082702636719 running bpv: 2.008198
COMMANDS_FINISHED 470 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_38/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_38/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_38/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_35/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_35/mlp.down_proj/compressed.log
best_loss 112.70238494873047 running bpv: 2.008188
COMMANDS_FINISHED 471 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_38/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_38/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_38/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_38/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_38/mlp.gate_proj/compressed.log
best_loss 788.2267456054688 running bpv: 2.008177
COMMANDS_FINISHED 472 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_38/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_38/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_38/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_38/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_38/mlp.up_proj/compressed.log
best_loss 696.6744384765625 running bpv: 2.008166
COMMANDS_FINISHED 473 n_commands 1849
 26%|‚ñà‚ñà‚ñå       | 473/1848 [5:34:18<14:23:00, 37.66s/it] 26%|‚ñà‚ñà‚ñå       | 474/1848 [5:34:43<12:55:27, 33.86s/it] 26%|‚ñà‚ñà‚ñå       | 475/1848 [5:36:08<18:46:01, 49.21s/it] 26%|‚ñà‚ñà‚ñå       | 476/1848 [5:38:13<27:25:15, 71.95s/it] 26%|‚ñà‚ñà‚ñå       | 477/1848 [5:38:28<20:53:42, 54.87s/it] 26%|‚ñà‚ñà‚ñå       | 478/1848 [5:39:13<19:45:15, 51.91s/it] 26%|‚ñà‚ñà‚ñå       | 479/1848 [5:39:38<16:40:14, 43.84s/it] 26%|‚ñà‚ñà‚ñå       | 480/1848 [5:40:13<15:39:06, 41.19s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_38/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_38/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_38/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_38/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_38/self_attn.k_proj/compressed.log
best_loss 329.7085876464844 running bpv: 2.008167
COMMANDS_FINISHED 474 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_17/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_17/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_17/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_38/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_38/self_attn.o_proj/compressed.log
best_loss 71.00900268554688 running bpv: 2.008169
COMMANDS_FINISHED 475 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_17/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_17/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_17/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_38/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_38/self_attn.v_proj/compressed.log
best_loss 386.0357360839844 running bpv: 2.008171
COMMANDS_FINISHED 476 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_17/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_17/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_17/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_17/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_17/self_attn.q_proj/compressed.log
best_loss 293.9764404296875 running bpv: 2.008173
COMMANDS_FINISHED 477 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_17/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_17/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_17/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_38/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_38/mlp.down_proj/compressed.log
best_loss 194.73583984375 running bpv: 2.008163
COMMANDS_FINISHED 478 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_17/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_17/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_17/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_17/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_17/mlp.gate_proj/compressed.log
best_loss 299.6954040527344 running bpv: 2.008152
COMMANDS_FINISHED 479 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_17/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_17/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_17/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_17/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_17/mlp.up_proj/compressed.log
best_loss 283.359619140625 running bpv: 2.008141
COMMANDS_FINISHED 480 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_17/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_17/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_17/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_17/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_17/self_attn.k_proj/compressed.log
best_loss 309.7267150878906 running bpv: 2.008143
COMMANDS_FINISHED 481 n_commands 1849
 26%|‚ñà‚ñà‚ñå       | 481/1848 [5:40:28<12:39:27, 33.33s/it] 26%|‚ñà‚ñà‚ñå       | 482/1848 [5:41:43<17:23:32, 45.84s/it] 26%|‚ñà‚ñà‚ñå       | 483/1848 [5:43:48<26:23:10, 69.59s/it] 26%|‚ñà‚ñà‚ñå       | 484/1848 [5:44:13<21:17:57, 56.21s/it] 26%|‚ñà‚ñà‚ñå       | 485/1848 [5:45:08<21:08:47, 55.85s/it] 26%|‚ñà‚ñà‚ñã       | 486/1848 [5:45:23<16:29:41, 43.60s/it] 26%|‚ñà‚ñà‚ñã       | 487/1848 [5:45:48<14:22:27, 38.02s/it] 26%|‚ñà‚ñà‚ñã       | 488/1848 [5:46:13<12:53:19, 34.12s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_9/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_9/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_9/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_17/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_17/self_attn.o_proj/compressed.log
best_loss 17.769956588745117 running bpv: 2.008145
COMMANDS_FINISHED 482 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_9/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_9/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_9/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_17/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_17/self_attn.v_proj/compressed.log
best_loss 153.7455291748047 running bpv: 2.008147
COMMANDS_FINISHED 483 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_9/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_9/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_9/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_9/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_9/self_attn.q_proj/compressed.log
best_loss 230.11932373046875 running bpv: 2.008148
COMMANDS_FINISHED 484 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_9/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_9/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_9/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_17/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_17/mlp.down_proj/compressed.log
best_loss 33.68016815185547 running bpv: 2.008138
COMMANDS_FINISHED 485 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_9/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_9/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_9/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_9/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_9/mlp.gate_proj/compressed.log
best_loss 179.27581787109375 running bpv: 2.008128
COMMANDS_FINISHED 486 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_9/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_9/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_9/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_9/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_9/mlp.up_proj/compressed.log
best_loss 152.74160766601562 running bpv: 2.008118
COMMANDS_FINISHED 487 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_9/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_9/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_9/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_9/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_9/self_attn.k_proj/compressed.log
best_loss 230.73965454101562 running bpv: 2.008119
COMMANDS_FINISHED 488 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_22/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_22/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_22/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_9/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_9/self_attn.o_proj/compressed.log
best_loss 5.620550155639648 running bpv: 2.008121
COMMANDS_FINISHED 489 n_commands 1849
 26%|‚ñà‚ñà‚ñã       | 489/1848 [5:47:28<17:30:37, 46.39s/it] 27%|‚ñà‚ñà‚ñã       | 490/1848 [5:49:33<26:23:44, 69.97s/it] 27%|‚ñà‚ñà‚ñã       | 491/1848 [5:50:08<22:25:19, 59.48s/it] 27%|‚ñà‚ñà‚ñã       | 492/1848 [5:50:43<19:38:23, 52.14s/it] 27%|‚ñà‚ñà‚ñã       | 493/1848 [5:51:08<16:33:41, 44.00s/it] 27%|‚ñà‚ñà‚ñã       | 494/1848 [5:51:33<14:24:21, 38.30s/it] 27%|‚ñà‚ñà‚ñã       | 495/1848 [5:52:18<15:09:05, 40.31s/it] 27%|‚ñà‚ñà‚ñã       | 496/1848 [5:53:13<16:47:44, 44.72s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_22/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_22/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_22/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_9/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_9/self_attn.v_proj/compressed.log
best_loss 91.90483093261719 running bpv: 2.008123
COMMANDS_FINISHED 490 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_22/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_22/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_22/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_22/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_22/self_attn.q_proj/compressed.log
best_loss 343.7789306640625 running bpv: 2.008125
COMMANDS_FINISHED 491 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_22/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_22/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_22/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_9/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_9/mlp.down_proj/compressed.log
best_loss 10.403076171875 running bpv: 2.008115
COMMANDS_FINISHED 492 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_22/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_22/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_22/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_22/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_22/mlp.gate_proj/compressed.log
best_loss 446.1928405761719 running bpv: 2.008105
COMMANDS_FINISHED 493 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_22/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_22/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_22/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_22/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_22/mlp.up_proj/compressed.log
best_loss 388.4912109375 running bpv: 2.008095
COMMANDS_FINISHED 494 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_22/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_22/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_22/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_22/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_22/self_attn.k_proj/compressed.log
best_loss 358.70550537109375 running bpv: 2.008097
COMMANDS_FINISHED 495 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_24/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_24/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_24/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_22/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_22/self_attn.o_proj/compressed.log
best_loss 16.89639663696289 running bpv: 2.008098
COMMANDS_FINISHED 496 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_24/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_24/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_24/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_22/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_22/self_attn.v_proj/compressed.log
best_loss 237.94509887695312 running bpv: 2.0081
COMMANDS_FINISHED 497 n_commands 1849
 27%|‚ñà‚ñà‚ñã       | 497/1848 [5:55:18<25:49:22, 68.81s/it] 27%|‚ñà‚ñà‚ñã       | 498/1848 [5:55:43<20:52:33, 55.67s/it] 27%|‚ñà‚ñà‚ñã       | 499/1848 [5:56:28<19:39:42, 52.47s/it] 27%|‚ñà‚ñà‚ñã       | 500/1848 [5:57:13<18:48:32, 50.23s/it] 27%|‚ñà‚ñà‚ñã       | 501/1848 [5:57:28<14:50:27, 39.66s/it] 27%|‚ñà‚ñà‚ñã       | 502/1848 [5:57:43<12:03:51, 32.27s/it] 27%|‚ñà‚ñà‚ñã       | 503/1848 [5:59:18<19:05:16, 51.09s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_24/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_24/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_24/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_24/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_24/self_attn.q_proj/compressed.log
best_loss 345.40850830078125 running bpv: 2.008102
COMMANDS_FINISHED 498 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_24/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_24/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_24/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_22/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_22/mlp.down_proj/compressed.log
best_loss 57.64418411254883 running bpv: 2.008092
COMMANDS_FINISHED 499 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_24/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_24/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_24/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_24/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_24/mlp.gate_proj/compressed.log
best_loss 494.1996765136719 running bpv: 2.008083
COMMANDS_FINISHED 500 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_24/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_24/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_24/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_24/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_24/mlp.up_proj/compressed.log
best_loss 418.732177734375 running bpv: 2.008073
COMMANDS_FINISHED 501 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_24/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_24/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_24/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_24/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_24/self_attn.k_proj/compressed.log
best_loss 355.76885986328125 running bpv: 2.008074
COMMANDS_FINISHED 502 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/hessians_new/pajama/128/layer_3/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_3/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_3/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_24/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_24/self_attn.o_proj/compressed.log
best_loss 16.300413131713867 running bpv: 2.008076
COMMANDS_FINISHED 503 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/hessians_new/pajama/128/layer_3/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_3/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_3/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_24/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_24/self_attn.v_proj/compressed.log
best_loss 243.96865844726562 running bpv: 2.008078
COMMANDS_FINISHED 504 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/hessians_new/pajama/128/layer_3/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_3/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_3/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_24/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_24/mlp.down_proj/compressed.log
best_loss 27%|‚ñà‚ñà‚ñã       | 504/1848 [6:01:23<27:21:11, 73.27s/it] 60.38478088378906 running bpv: 2.008069
COMMANDS_FINISHED 505 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/hessians_new/pajama/128/layer_3/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_3/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_3/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
done with meta-llama/Llama-2-13b-hf
done with {'meta-llama/Llama-2-13b-hf/layer_3/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_3/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_3/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_3/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_3/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_3/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_3/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_3/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_3/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_3/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_3/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_3/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_5/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_5/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_3/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_3/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_5/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_5/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_5/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_5/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_5/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_5/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_5/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_5/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_5/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_5/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_8/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_8/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_5/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_5/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_8/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_8/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_8/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_8/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_8/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_8/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_8/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_8/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_8/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_8/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_39/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_39/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_8/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_8/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_39/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_39/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_39/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_39/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_39/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_39/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_39/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_39/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_39/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_39/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_12/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_12/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_39/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_39/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_12/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_12/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_12/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_12/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_12/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_12/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_12/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_12/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_12/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_12/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_1/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_1/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_12/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_12/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_1/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_1/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_1/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_1/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_1/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_1/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_1/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_1/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_1/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_1/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_7/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_7/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_1/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_1/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_7/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_7/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_7/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_7/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_7/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_7/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_7/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_7/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_7/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_7/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_33/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_33/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_7/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_7/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_33/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_33/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_33/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_33/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_33/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_33/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_33/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_33/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_33/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_33/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_34/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_34/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_33/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_33/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_34/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_34/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_34/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_34/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_34/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_34/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_34/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_34/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_34/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_34/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_0/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_0/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_34/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_34/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_0/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_0/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_0/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_0/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_0/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_0/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_0/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_0/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_0/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_0/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_32/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_32/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_0/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_0/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_32/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_32/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_32/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_32/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_32/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_32/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_32/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_32/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_32/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_32/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_31/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_31/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_32/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_32/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_31/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_31/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_31/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_31/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_31/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_31/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_31/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_31/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_31/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_31/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_21/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_21/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_31/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_31/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_21/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_21/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_21/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_21/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_21/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_21/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_21/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_21/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_21/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_21/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_26/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_26/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_21/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_21/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_26/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_26/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_26/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_26/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_26/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_26/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_26/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_26/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_26/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_26/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_14/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_14/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_26/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_26/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_14/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_14/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_14/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_14/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_14/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_14/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_14/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_14/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_14/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_14/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_15/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_15/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_14/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_14/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_15/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_15/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_15/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_15/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_15/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_15/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_15/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_15/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_15/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_15/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_4/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_4/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_15/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_15/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_4/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_4/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_4/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_4/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_4/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_4/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_4/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_4/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_4/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_4/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_27/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_27/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_4/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_4/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_27/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_27/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_27/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_27/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_27/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_27/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_27/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_27/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_27/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_27/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_2/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_2/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_27/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_27/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_2/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_2/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_2/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_2/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_2/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_2/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_2/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_2/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_2/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_2/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_36/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_36/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_2/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_2/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_36/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_36/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_36/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_36/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_36/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_36/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_36/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_36/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_36/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_36/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_28/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_28/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_36/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_36/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_28/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_28/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_28/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_28/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_28/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_28/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_28/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_28/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_28/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_28/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_18/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_18/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_28/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_28/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_18/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_18/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_18/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_18/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_18/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_18/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_18/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_18/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_18/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_18/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_25/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_25/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_18/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_18/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_25/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_25/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_25/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_25/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_25/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_25/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_25/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_25/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_25/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_25/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_37/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_37/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_25/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_25/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_37/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_37/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_37/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_37/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_37/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_37/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_37/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_37/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_37/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_37/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_30/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_30/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_37/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_37/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_30/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_30/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_30/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_30/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_30/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_30/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_30/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_30/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_30/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_30/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_29/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_29/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_30/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_30/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_29/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_29/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_29/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_29/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_29/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_29/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_29/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_29/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_29/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_29/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_29/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_29/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_16/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_16/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_16/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_16/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_16/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_16/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_16/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_16/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_16/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_16/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_16/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_16/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_13/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_13/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_16/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_16/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_13/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_13/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_13/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_13/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_13/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_13/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_13/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_13/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_13/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_13/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_10/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_10/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_13/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_13/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_10/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_10/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_10/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_10/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_10/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_10/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_10/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_10/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_10/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_10/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_19/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_19/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_10/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_10/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_19/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_19/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_19/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_19/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_19/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_19/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_19/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_19/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_19/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_19/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_6/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_6/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_19/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_19/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_6/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_6/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_6/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_6/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_6/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_6/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_6/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_6/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_6/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_6/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_11/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_11/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_6/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_6/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_11/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_11/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_11/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_11/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_11/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_11/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_11/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_11/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_11/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_11/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_20/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_20/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_11/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_11/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_20/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_20/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_20/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_20/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_20/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_20/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_20/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_20/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_20/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_20/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_23/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_23/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_20/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_20/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_23/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_23/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_23/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_23/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_23/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_23/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_23/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_23/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_23/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_23/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_35/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_35/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_23/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_23/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_35/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_35/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_35/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_35/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_35/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_35/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_35/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_35/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_35/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_35/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_38/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_38/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_35/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_35/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_38/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_38/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_38/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_38/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_38/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_38/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_38/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_38/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_38/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_38/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_17/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_17/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_38/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_38/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_17/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_17/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_17/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_17/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_17/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_17/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_17/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_17/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_17/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_17/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_9/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_9/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_17/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_17/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_9/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_9/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_9/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_9/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_9/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_9/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_9/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_9/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_9/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_9/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_22/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_22/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_9/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_9/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_22/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_22/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_22/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_22/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_22/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_22/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_22/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_22/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_22/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_22/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_24/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_24/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_22/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_22/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_24/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_24/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_24/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_24/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_24/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_24/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_24/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_24/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_24/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_24/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_24/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-13b-hf/layer_24/mlp.down_proj/compressed.pt'} 27%|‚ñà‚ñà‚ñã       | 505/1848 [6:02:18<25:17:36, 67.80s/it] 27%|‚ñà‚ñà‚ñã       | 506/1848 [6:03:59<28:52:38, 77.47s/it] 27%|‚ñà‚ñà‚ñã       | 507/1848 [6:07:14<41:59:32, 112.73s/it] 27%|‚ñà‚ñà‚ñã       | 508/1848 [6:08:09<35:30:55, 95.41s/it]  28%|‚ñà‚ñà‚ñä       | 509/1848 [6:13:14<58:52:41, 158.30s/it] 28%|‚ñà‚ñà‚ñä       | 510/1848 [6:13:29<42:51:25, 115.31s/it]
/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/checkpoints.yaml
perplexity_inference_command:
 python -u perplexity_eval.py --base_model meta-llama/Llama-2-13b-hf --seqlen 4096 --checkpoint_list_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/checkpoints.yaml --log_wandb --wandb_project compression_no_finetune --wandb_id uck2ffxu
meta-llama/Llama-2-70b-hf/layer_3/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_3/self_attn.k_proj/compressed.log
best_loss 5.9295454025268555 running bpv: 2.008078
COMMANDS_FINISHED 506 n_commands 1850
running: nohup python -u perplexity_eval.py --base_model meta-llama/Llama-2-13b-hf --seqlen 4096 --checkpoint_list_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/checkpoints.yaml --log_wandb --wandb_project compression_no_finetune --wandb_id uck2ffxu --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/scarlet-fire-56/ppl_eval.log 2>&1 &
meta-llama/Llama-2-70b-hf
eval is done
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/hessians_new/pajama/128/layer_3/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_3/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_3/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf/layer_3/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_3/self_attn.q_proj/compressed.log
best_loss 17.716445922851562 running bpv: 2.00807
COMMANDS_FINISHED 508 n_commands 1850
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/hessians_new/pajama/128/layer_3/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_3/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_3/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf/layer_3/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_3/self_attn.o_proj/compressed.log
best_loss 0.4180109202861786 running bpv: 2.008062
COMMANDS_FINISHED 509 n_commands 1850
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/hessians_new/pajama/128/layer_3/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_3/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_3/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf/layer_3/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_3/self_attn.v_proj/compressed.log
best_loss 0.8532019853591919 running bpv: 2.008071
COMMANDS_FINISHED 510 n_commands 1850
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/hessians_new/pajama/128/layer_5/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_5/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_5/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf/layer_3/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_3/mlp.gate_proj/compressed.log
best_loss 31.573925018310547 running bpv: 2.008017
COMMANDS_FINISHED 511 n_commands 1850
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/hessians_new/pajama/128/layer_5/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_5/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_5/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf/layer_3/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_3/mlp.up_proj/compressed.log
best_loss 28.708293914794922 running bpv: 2.007965
COMMANDS_FINISHED 512 n_commands 1850
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/hessians_new/pajama/128/layer_5/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_5/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_5/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
 28%|‚ñà‚ñà‚ñä       | 511/1848 [6:18:04<60:37:10, 163.22s/it] 28%|‚ñà‚ñà‚ñä       | 512/1848 [6:18:59<48:31:34, 130.76s/it] 28%|‚ñà‚ñà‚ñä       | 513/1848 [6:20:44<45:37:31, 123.03s/it] 28%|‚ñà‚ñà‚ñä       | 514/1848 [6:23:39<51:22:11, 138.63s/it] 28%|‚ñà‚ñà‚ñä       | 515/1848 [6:23:54<37:35:55, 101.54s/it] 28%|‚ñà‚ñà‚ñä       | 516/1848 [6:24:39<31:17:43, 84.58s/it]  28%|‚ñà‚ñà‚ñä       | 517/1848 [6:28:54<50:10:36, 135.71s/it]meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf/layer_5/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_5/self_attn.q_proj/compressed.log
best_loss 44.72357177734375 running bpv: 2.007957
COMMANDS_FINISHED 513 n_commands 1850
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/hessians_new/pajama/128/layer_5/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_5/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_5/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf/layer_5/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_5/self_attn.k_proj/compressed.log
best_loss 12.763484001159668 running bpv: 2.007966
COMMANDS_FINISHED 514 n_commands 1850
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/hessians_new/pajama/128/layer_5/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_5/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_5/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf/layer_3/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_3/mlp.down_proj/compressed.log
best_loss 0.5575462579727173 running bpv: 2.007909
COMMANDS_FINISHED 515 n_commands 1850
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/hessians_new/pajama/128/layer_5/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_5/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_5/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf/layer_5/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_5/self_attn.o_proj/compressed.log
best_loss 0.5574174523353577 running bpv: 2.007902
COMMANDS_FINISHED 516 n_commands 1850
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/hessians_new/pajama/128/layer_5/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_5/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_5/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf/layer_5/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_5/mlp.gate_proj/compressed.log
best_loss 55.12204360961914 running bpv: 2.007852
COMMANDS_FINISHED 517 n_commands 1850
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/hessians_new/pajama/128/layer_8/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_8/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_8/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf/layer_5/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_5/self_attn.v_proj/compressed.log
best_loss 2.2521779537200928 running bpv: 2.007861
COMMANDS_FINISHED 518 n_commands 1850
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/hessians_new/pajama/128/layer_8/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_8/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_8/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf/layer_5/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_5/mlp.up_proj/compressed.log
best_loss 49.8851432800293 running bpv: 2.007812
COMMANDS_FINISHED 519 n_commands 1850
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/hessians_new/pajama/128/layer_8/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_8/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_8/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
 28%|‚ñà‚ñà‚ñä       | 518/1848 [6:33:29<65:34:45, 177.51s/it] 28%|‚ñà‚ñà‚ñä       | 519/1848 [6:34:24<51:57:47, 140.76s/it] 28%|‚ñà‚ñà‚ñä       | 520/1848 [6:37:29<56:49:20, 154.04s/it] 28%|‚ñà‚ñà‚ñä       | 521/1848 [6:39:04<50:15:08, 136.33s/it] 28%|‚ñà‚ñà‚ñä       | 522/1848 [6:39:29<37:54:48, 102.93s/it] 28%|‚ñà‚ñà‚ñä       | 523/1848 [6:40:04<30:23:05, 82.56s/it]  28%|‚ñà‚ñà‚ñä       | 524/1848 [6:40:19<22:54:32, 62.29s/it]meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf/layer_8/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_8/self_attn.q_proj/compressed.log
best_loss 67.96951293945312 running bpv: 2.007806
COMMANDS_FINISHED 520 n_commands 1850
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/hessians_new/pajama/128/layer_8/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_8/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_8/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf/layer_8/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_8/self_attn.k_proj/compressed.log
best_loss 20.196636199951172 running bpv: 2.007814
COMMANDS_FINISHED 521 n_commands 1850
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/hessians_new/pajama/128/layer_8/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_8/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_8/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf/layer_5/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_5/mlp.down_proj/compressed.log
best_loss 1.1758480072021484 running bpv: 2.007761
COMMANDS_FINISHED 522 n_commands 1850
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/hessians_new/pajama/128/layer_8/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_8/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_8/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf/layer_8/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_8/self_attn.o_proj/compressed.log
best_loss 2.4677278995513916 running bpv: 2.007755
COMMANDS_FINISHED 523 n_commands 1850
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/hessians_new/pajama/128/layer_8/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_8/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_8/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf/layer_8/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_8/mlp.gate_proj/compressed.log
best_loss 115.2674331665039 running bpv: 2.007709
COMMANDS_FINISHED 524 n_commands 1850
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/hessians_new/pajama/128/layer_76/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_76/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_76/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf/layer_8/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_8/self_attn.v_proj/compressed.log
best_loss 3.3064985275268555 running bpv: 2.007717
COMMANDS_FINISHED 525 n_commands 1850
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/hessians_new/pajama/128/layer_76/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_76/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_76/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf/layer_8/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_8/mlp.up_proj/compressed.log
best_loss 100.71903228759766 running bpv: 2.007672
COMMANDS_FINISHED 526 n_commands 1850
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/hessians_new/pajama/128/layer_76/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_76/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_76/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
 28%|‚ñà‚ñà‚ñä       | 525/1848 [6:44:54<46:20:43, 126.11s/it] 28%|‚ñà‚ñà‚ñä       | 526/1848 [6:45:49<38:28:38, 104.78s/it] 29%|‚ñà‚ñà‚ñä       | 527/1848 [6:50:24<57:11:22, 155.85s/it] 29%|‚ñà‚ñà‚ñä       | 528/1848 [6:54:19<65:51:16, 179.60s/it] 29%|‚ñà‚ñà‚ñä       | 529/1848 [6:55:14<52:06:34, 142.22s/it] 29%|‚ñà‚ñà‚ñä       | 530/1848 [6:55:49<40:17:38, 110.06s/it] 29%|‚ñà‚ñà‚ñä       | 531/1848 [6:56:54<35:19:08, 96.54s/it] meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf/layer_76/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_76/self_attn.q_proj/compressed.log
best_loss 273.27520751953125 running bpv: 2.007666
COMMANDS_FINISHED 527 n_commands 1850
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/hessians_new/pajama/128/layer_76/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_76/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_76/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf/layer_76/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_76/self_attn.k_proj/compressed.log
best_loss 65.9119873046875 running bpv: 2.007674
COMMANDS_FINISHED 528 n_commands 1850
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/hessians_new/pajama/128/layer_76/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_76/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_76/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf/layer_76/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_76/self_attn.o_proj/compressed.log
best_loss 189.72825622558594 running bpv: 2.007668
COMMANDS_FINISHED 529 n_commands 1850
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/hessians_new/pajama/128/layer_76/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_76/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_76/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf/layer_8/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_8/mlp.down_proj/compressed.log
best_loss 781.05419921875 running bpv: 2.007619
COMMANDS_FINISHED 530 n_commands 1850
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/hessians_new/pajama/128/layer_76/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_76/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_76/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf/layer_76/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_76/mlp.gate_proj/compressed.log
best_loss 1796.485107421875 running bpv: 2.007576
COMMANDS_FINISHED 531 n_commands 1850
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/hessians_new/pajama/128/layer_66/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_66/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_66/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf/layer_76/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_76/mlp.up_proj/compressed.log
best_loss 1630.743896484375 running bpv: 2.007534
COMMANDS_FINISHED 532 n_commands 1850
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/hessians_new/pajama/128/layer_66/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_66/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_66/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf/layer_76/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_76/self_attn.v_proj/compressed.log
best_loss 60.01142120361328 running bpv: 2.007679
COMMANDS_FINISHED 533 n_commands 1850
 29%|‚ñà‚ñà‚ñâ       | 532/1848 [7:09:59<110:47:59, 303.10s/it] 29%|‚ñà‚ñà‚ñâ       | 533/1848 [7:13:04<97:46:32, 267.68s/it]  29%|‚ñà‚ñà‚ñâ       | 534/1848 [7:28:29<169:41:12, 464.90s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/hessians_new/pajama/128/layer_66/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_66/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_66/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf/layer_66/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_66/self_attn.q_proj/compressed.log
best_loss 73.40914154052734 running bpv: 2.00748
COMMANDS_FINISHED 534 n_commands 1850
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/hessians_new/pajama/128/layer_66/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_66/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_66/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf/layer_66/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_66/self_attn.k_proj/compressed.log
best_loss 20.36900520324707 running bpv: 2.007502
COMMANDS_FINISHED 535 n_commands 1850
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/hessians_new/pajama/128/layer_66/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_66/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_66/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf/layer_66/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_66/self_attn.o_proj/compressed.log
best_loss 19.491046905517578 running bpv: 2.007512
COMMANDS_FINISHED 536 n_commands 1850
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/hessians_new/pajama/128/layer_66/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_66/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/scarlet-fire-56/meta-llama/Llama-2-70b-hf/layer_66/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-70b-hf
 29%|‚ñà‚ñà‚ñâ       | 535/1848 [7:39:04<188:10:32, 515.94s/it] 29%|‚ñà‚ñà‚ñâ       | 536/1848 [7:39:49<136:32:36, 374.66s/it]