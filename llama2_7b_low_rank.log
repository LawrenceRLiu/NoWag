/home/lliu/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.37s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.16it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.06it/s]
Model loaded. LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
4096
Loading tokenizer for meta-llama/Llama-2-7b-hf
Starting...
39011 MiB free out of 48676 MiB total
attention_mask torch.Size([1, 1, 4096, 4096]) cuda:7 torch.float16
position_ids torch.Size([1, 4096]) cuda:7 torch.int64
position_ids torch.Size([1, 4096])
attention_mask torch.Size([1, 1, 4096, 4096])
Ready.
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
0 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([0.8849, 0.6529, 0.2560,  ..., 0.5371, 0.5559, 0.4512], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.6149, 1.0973, 1.2322,  ..., 1.2838, 1.3711, 0.9672], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 0.6543768048286438
Epoch 100 Loss 0.014374969527125359
Epoch 200 Loss 0.009946165606379509
Epoch 300 Loss 0.008288619108498096
Epoch 400 Loss 0.007363418582826853
Epoch 500 Loss 0.006958945654332638
Epoch 600 Loss 0.006689778994768858
Epoch 700 Loss 0.006451140157878399
Epoch 800 Loss 0.006239881739020348
Epoch 900 Loss 0.006052569020539522
last loss 0.0058875735849142075
0 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.1756, 0.8264, 0.3096,  ..., 0.5891, 0.6551, 0.5814], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.0278, 1.4363, 1.2937,  ..., 1.3905, 1.6662, 0.9913], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 0.5647948980331421
Epoch 100 Loss 0.01948622241616249
Epoch 200 Loss 0.012474386021494865
Epoch 300 Loss 0.009907836094498634
Epoch 400 Loss 0.008533116430044174
Epoch 500 Loss 0.007658475544303656
Epoch 600 Loss 0.007044813130050898
Epoch 700 Loss 0.006586934439837933
Epoch 800 Loss 0.00622925441712141
Epoch 900 Loss 0.005942048504948616
last loss 0.00570797361433506
0 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([0.7236, 0.7618, 0.5036,  ..., 0.7654, 0.7673, 0.6849], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.4949, 0.5030, 0.4779,  ..., 0.4897, 0.5060, 0.5031], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 0.5511637330055237
Epoch 100 Loss 0.09089741110801697
Epoch 200 Loss 0.08062075823545456
Epoch 300 Loss 0.07803510129451752
Epoch 400 Loss 0.07657445967197418
Epoch 500 Loss 0.07560421526432037
Epoch 600 Loss 0.07495704293251038
Epoch 700 Loss 0.07452075928449631
Epoch 800 Loss 0.07436943054199219
Epoch 900 Loss 0.07425293326377869
last loss 0.07414890825748444
0 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([0.2801, 0.2821, 0.2653,  ..., 0.2623, 0.2674, 0.2648], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.4009, 0.4159, 0.4015,  ..., 0.4328, 0.4183, 0.4196], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 0.05561743676662445
Epoch 100 Loss 0.0021230499260127544
Epoch 200 Loss 0.0012579972390085459
Epoch 300 Loss 0.00103358319029212
Epoch 400 Loss 0.000896136334631592
Epoch 500 Loss 0.0007891742279753089
Epoch 600 Loss 0.0007049960549920797
Epoch 700 Loss 0.0006376685341820121
Epoch 800 Loss 0.0005875038914382458
Epoch 900 Loss 0.0005638598231598735
last loss 0.0005417225765995681
0 mlp.gate_proj
Pruning ...
0 mlp.up_proj
Pruning ...
0 mlp.down_proj
Pruning ...
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 153321744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 9.999107286606801e-08 val loss: 1.098023654222402e-07
7231 MiB free out of 48676 MiB total
epoch 1 loss: 9.972485348619387e-08 val loss: 1.0967274777229363e-07
7223 MiB free out of 48676 MiB total
epoch 2 loss: 9.949204632375697e-08 val loss: 1.0943946016084283e-07
7223 MiB free out of 48676 MiB total
epoch 3 loss: 9.931005012786542e-08 val loss: 1.0935689109814462e-07
7223 MiB free out of 48676 MiB total
epoch 4 loss: 9.914957754819653e-08 val loss: 1.0929463645226178e-07
7223 MiB free out of 48676 MiB total
39011 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
7223 MiB free out of 48676 MiB total
after cast to cpu
37799 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
1 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.9255, 1.9044, 1.8602,  ..., 1.6792, 1.9344, 1.6705], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.9452, 1.6639, 1.9007,  ..., 0.3805, 0.3928, 0.4772], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 13.91159439086914
Epoch 100 Loss 2.737574815750122
Epoch 200 Loss 2.537003755569458
Epoch 300 Loss 2.497976779937744
Epoch 400 Loss 2.483180046081543
Epoch 500 Loss 2.471735715866089
Epoch 600 Loss 2.463139533996582
Epoch 700 Loss 2.4567618370056152
Epoch 800 Loss 2.4520440101623535
Epoch 900 Loss 2.4485442638397217
last loss 2.445950508117676
1 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([2.0254, 1.9664, 2.0364,  ..., 1.6627, 1.8973, 1.8084], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.5419, 1.7741, 1.8750,  ..., 0.6519, 0.6390, 0.5201], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 15.821407318115234
Epoch 100 Loss 2.522373676300049
Epoch 200 Loss 2.360879421234131
Epoch 300 Loss 2.310659170150757
Epoch 400 Loss 2.2842893600463867
Epoch 500 Loss 2.2709407806396484
Epoch 600 Loss 2.2669196128845215
Epoch 700 Loss 2.264395236968994
Epoch 800 Loss 2.262129306793213
Epoch 900 Loss 2.2601277828216553
last loss 2.258394718170166
1 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([0.5438, 0.5367, 0.5507,  ..., 0.6505, 0.5752, 0.6442], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.8513, 0.8723, 0.8107,  ..., 0.3544, 0.3659, 0.3639], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 4.720095634460449
Epoch 100 Loss 1.4946134090423584
Epoch 200 Loss 1.4160000085830688
Epoch 300 Loss 1.4018912315368652
Epoch 400 Loss 1.3860702514648438
Epoch 500 Loss 1.3828872442245483
Epoch 600 Loss 1.3787957429885864
Epoch 700 Loss 1.3723680973052979
Epoch 800 Loss 1.3615440130233765
Epoch 900 Loss 1.3417062759399414
last loss 1.3020894527435303
1 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([0.8602, 0.8032, 0.6896,  ..., 0.2113, 0.2166, 0.2192], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.5290, 0.5332, 0.5174,  ..., 0.5289, 0.5185, 0.5111], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 0.6004645824432373
Epoch 100 Loss 0.1162581592798233
Epoch 200 Loss 0.08788960427045822
Epoch 300 Loss 0.08225394785404205
Epoch 400 Loss 0.08067543804645538
Epoch 500 Loss 0.08034627139568329
Epoch 600 Loss 0.08013966679573059
Epoch 700 Loss 0.07997114956378937
Epoch 800 Loss 0.07983481884002686
Epoch 900 Loss 0.07972471415996552
last loss 0.07963643968105316
1 mlp.gate_proj
Pruning ...
1 mlp.up_proj
Pruning ...
1 mlp.down_proj
Pruning ...
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 153321744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0005870968345078609 val loss: 0.00019375244210095843
9089 MiB free out of 48676 MiB total
epoch 1 loss: 0.00021611013846722926 val loss: 0.00019382708705961704
7033 MiB free out of 48676 MiB total
epoch 2 loss: 0.000205543707409106 val loss: 0.00019337181947776116
7033 MiB free out of 48676 MiB total
epoch 3 loss: 0.00028908009682027114 val loss: 0.0001922076589835342
7033 MiB free out of 48676 MiB total
epoch 4 loss: 0.00017756715978123339 val loss: 0.00019069237168878317
7033 MiB free out of 48676 MiB total
37799 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
7033 MiB free out of 48676 MiB total
after cast to cpu
37863 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
2 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.6843, 1.7193, 1.6812,  ..., 1.7487, 1.6237, 1.6486], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.0094, 1.2899, 1.5509,  ..., 2.1117, 0.7452, 2.0198], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 149.3779296875
Epoch 100 Loss 68.0631103515625
Epoch 200 Loss 66.00277709960938
Epoch 300 Loss 65.53609466552734
Epoch 400 Loss 65.42951202392578
Epoch 500 Loss 65.34295654296875
Epoch 600 Loss 65.27489471435547
Epoch 700 Loss 65.22218322753906
Epoch 800 Loss 65.1815185546875
Epoch 900 Loss 65.15013885498047
last loss 65.12606048583984
2 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.7840, 1.7515, 1.8243,  ..., 1.7953, 1.7517, 1.7821], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.7197, 1.3612, 1.5470,  ..., 2.5627, 0.6336, 2.3330], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 141.36773681640625
Epoch 100 Loss 73.47007751464844
Epoch 200 Loss 71.91185760498047
Epoch 300 Loss 71.55226135253906
Epoch 400 Loss 71.49169921875
Epoch 500 Loss 71.44564819335938
Epoch 600 Loss 71.41091918945312
Epoch 700 Loss 71.38488006591797
Epoch 800 Loss 71.36534881591797
Epoch 900 Loss 71.35063934326172
last loss 71.33961486816406
2 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([0.9290, 0.9331, 0.9195,  ..., 0.9050, 0.9574, 0.9601], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.9509, 0.9649, 0.9703,  ..., 0.8383, 0.9783, 0.9524], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 67.87667846679688
Epoch 100 Loss 34.00059509277344
Epoch 200 Loss 33.01615905761719
Epoch 300 Loss 32.815025329589844
Epoch 400 Loss 32.741554260253906
Epoch 500 Loss 32.69845199584961
Epoch 600 Loss 32.67286682128906
Epoch 700 Loss 32.65725326538086
Epoch 800 Loss 32.64750671386719
Epoch 900 Loss 32.64385986328125
last loss 32.64110565185547
2 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([0.9219, 0.9821, 0.9457,  ..., 0.9478, 0.9625, 0.9246], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.8827, 0.8924, 0.8919,  ..., 0.8741, 0.8675, 0.8812], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 1.0900474786758423
Epoch 100 Loss 0.5384474992752075
Epoch 200 Loss 0.5232478380203247
Epoch 300 Loss 0.5207728147506714
Epoch 400 Loss 0.5203179121017456
Epoch 500 Loss 0.5201229453086853
Epoch 600 Loss 0.5199725031852722
Epoch 700 Loss 0.5198583006858826
Epoch 800 Loss 0.519771933555603
Epoch 900 Loss 0.5197068452835083
last loss 0.5196579694747925
2 mlp.gate_proj
Pruning ...
2 mlp.up_proj
Pruning ...
2 mlp.down_proj
Pruning ...
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 153321744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.00013216850646813327 val loss: 0.001859251431596931
9153 MiB free out of 48676 MiB total
epoch 1 loss: 0.00012076490281742736 val loss: 0.0018355327192693949
7097 MiB free out of 48676 MiB total
epoch 2 loss: 0.00011826594436570304 val loss: 0.001816132920794189
7097 MiB free out of 48676 MiB total
epoch 3 loss: 0.00011677246135377573 val loss: 0.0018015796522377059
7097 MiB free out of 48676 MiB total
epoch 4 loss: 0.00011564087327542438 val loss: 0.0017905328204506077
7097 MiB free out of 48676 MiB total
37863 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
7097 MiB free out of 48676 MiB total
after cast to cpu
37863 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
3 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.5567, 1.6199, 1.6101,  ..., 1.6199, 1.6003, 1.6185], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.1710, 1.3686, 1.3253,  ..., 2.6906, 2.8910, 3.0339], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 447.6449279785156
Epoch 100 Loss 254.51812744140625
Epoch 200 Loss 249.91668701171875
Epoch 300 Loss 248.92759704589844
Epoch 400 Loss 248.77392578125
Epoch 500 Loss 248.66932678222656
Epoch 600 Loss 248.58778381347656
Epoch 700 Loss 248.52537536621094
Epoch 800 Loss 248.4778594970703
Epoch 900 Loss 248.44155883789062
last loss 248.41383361816406
3 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.6463, 1.6737, 1.6654,  ..., 1.6477, 1.6560, 1.6845], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.1294, 1.3448, 1.3066,  ..., 2.7185, 2.9419, 3.0465], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 482.92230224609375
Epoch 100 Loss 289.1914978027344
Epoch 200 Loss 284.9725646972656
Epoch 300 Loss 284.09130859375
Epoch 400 Loss 283.9449768066406
Epoch 500 Loss 283.8771057128906
Epoch 600 Loss 283.8215026855469
Epoch 700 Loss 283.7769775390625
Epoch 800 Loss 283.74163818359375
Epoch 900 Loss 283.7138671875
last loss 283.69219970703125
3 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([0.8999, 0.8761, 0.8859,  ..., 0.8799, 0.8948, 0.8878], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.8146, 0.8202, 0.8095,  ..., 0.4050, 0.4349, 0.3835], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 179.06280517578125
Epoch 100 Loss 100.83909606933594
Epoch 200 Loss 98.7032470703125
Epoch 300 Loss 98.25106811523438
Epoch 400 Loss 98.17079162597656
Epoch 500 Loss 98.12214660644531
Epoch 600 Loss 98.08780670166016
Epoch 700 Loss 98.063720703125
Epoch 800 Loss 98.04681396484375
Epoch 900 Loss 98.03483581542969
last loss 98.02635192871094
3 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([0.8149, 0.8113, 0.7834,  ..., 0.3370, 0.3588, 0.3251], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.8344, 0.8513, 0.8366,  ..., 0.8515, 0.8315, 0.8545], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 1.5744708776474
Epoch 100 Loss 0.645066499710083
Epoch 200 Loss 0.6121747493743896
Epoch 300 Loss 0.6044317483901978
Epoch 400 Loss 0.6023956537246704
Epoch 500 Loss 0.6018948554992676
Epoch 600 Loss 0.6014649868011475
Epoch 700 Loss 0.6011053323745728
Epoch 800 Loss 0.6008090376853943
Epoch 900 Loss 0.6005672216415405
last loss 0.6003724336624146
3 mlp.gate_proj
Pruning ...
3 mlp.up_proj
Pruning ...
3 mlp.down_proj
Pruning ...
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 153321744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.00019505165107602807 val loss: 0.005957109067821875
9153 MiB free out of 48676 MiB total
epoch 1 loss: 0.0001542314269045164 val loss: 0.005931871681241319
7097 MiB free out of 48676 MiB total
epoch 2 loss: 0.00014804168108639715 val loss: 0.005900852440390736
7097 MiB free out of 48676 MiB total
epoch 3 loss: 0.00014480737149824563 val loss: 0.005875609320355579
7097 MiB free out of 48676 MiB total
epoch 4 loss: 0.0001425297510877499 val loss: 0.005851967143826187
7097 MiB free out of 48676 MiB total
37863 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
7097 MiB free out of 48676 MiB total
after cast to cpu
37863 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
4 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.6591, 1.6807, 1.6291,  ..., 1.6611, 1.7065, 1.7137], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.9198, 1.3477, 1.4289,  ..., 2.3017, 2.2600, 2.2052], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 388.62139892578125
Epoch 100 Loss 217.44383239746094
Epoch 200 Loss 213.88455200195312
Epoch 300 Loss 213.3495635986328
Epoch 400 Loss 213.2365264892578
Epoch 500 Loss 213.1424560546875
Epoch 600 Loss 213.06744384765625
Epoch 700 Loss 213.00897216796875
Epoch 800 Loss 212.96388244628906
Epoch 900 Loss 212.92926025390625
last loss 212.90281677246094
4 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.6928, 1.7265, 1.7171,  ..., 1.6686, 1.7117, 1.7124], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.9987, 1.3206, 1.3897,  ..., 2.4271, 2.2609, 2.4617], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 400.2565002441406
Epoch 100 Loss 244.70236206054688
Epoch 200 Loss 241.3746795654297
Epoch 300 Loss 240.753173828125
Epoch 400 Loss 240.61920166015625
Epoch 500 Loss 240.55010986328125
Epoch 600 Loss 240.50619506835938
Epoch 700 Loss 240.47727966308594
Epoch 800 Loss 240.4574432373047
Epoch 900 Loss 240.44467163085938
last loss 240.4390869140625
4 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([0.9338, 0.9049, 0.9457,  ..., 0.9385, 0.9042, 0.9126], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.8676, 0.8579, 0.8606,  ..., 0.9248, 0.9169, 0.9350], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 170.95005798339844
Epoch 100 Loss 97.1019058227539
Epoch 200 Loss 95.247802734375
Epoch 300 Loss 94.91078186035156
Epoch 400 Loss 94.86840057373047
Epoch 500 Loss 94.83294677734375
Epoch 600 Loss 94.80393981933594
Epoch 700 Loss 94.78067016601562
Epoch 800 Loss 94.76219177246094
Epoch 900 Loss 94.74759674072266
last loss 94.73619079589844
4 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([0.8375, 0.8274, 0.8368,  ..., 0.9084, 0.9081, 0.9244], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.8678, 0.8990, 0.9099,  ..., 0.8872, 0.8625, 0.8959], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 3.422255039215088
Epoch 100 Loss 1.6797428131103516
Epoch 200 Loss 1.5881229639053345
Epoch 300 Loss 1.5665475130081177
Epoch 400 Loss 1.5593595504760742
Epoch 500 Loss 1.5563814640045166
Epoch 600 Loss 1.5553563833236694
Epoch 700 Loss 1.554806113243103
Epoch 800 Loss 1.5543792247772217
Epoch 900 Loss 1.5540474653244019
last loss 1.553790807723999
4 mlp.gate_proj
Pruning ...
4 mlp.up_proj
Pruning ...
4 mlp.down_proj
Pruning ...
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 153321744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0003555256237177673 val loss: 0.0064662142540328205
7105 MiB free out of 48676 MiB total
epoch 1 loss: 0.00031883528458820365 val loss: 0.006406850821804255
7097 MiB free out of 48676 MiB total
epoch 2 loss: 0.00030996476084510505 val loss: 0.0063816596230026335
7097 MiB free out of 48676 MiB total
epoch 3 loss: 0.00030420266671171703 val loss: 0.006367488735122606
7097 MiB free out of 48676 MiB total
epoch 4 loss: 0.00029972808943057316 val loss: 0.006361575913615525
7097 MiB free out of 48676 MiB total
37863 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
7097 MiB free out of 48676 MiB total
after cast to cpu
37863 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
5 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.6849, 1.6923, 1.6421,  ..., 1.6521, 1.6664, 1.7371], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.0021, 1.0970, 1.1329,  ..., 1.7344, 1.9421, 1.6516], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 492.1131591796875
Epoch 100 Loss 253.239501953125
Epoch 200 Loss 247.459228515625
Epoch 300 Loss 246.66705322265625
Epoch 400 Loss 246.29092407226562
Epoch 500 Loss 246.02313232421875
Epoch 600 Loss 245.83709716796875
Epoch 700 Loss 245.70864868164062
Epoch 800 Loss 245.61978149414062
Epoch 900 Loss 245.55792236328125
last loss 245.51486206054688
5 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.7228, 1.7929, 1.7542,  ..., 1.6983, 1.7302, 1.7280], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.9647, 1.0825, 1.1445,  ..., 1.4308, 2.6816, 2.3936], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 450.1612548828125
Epoch 100 Loss 280.65679931640625
Epoch 200 Loss 276.7850036621094
Epoch 300 Loss 276.11041259765625
Epoch 400 Loss 275.9505920410156
Epoch 500 Loss 275.8593444824219
Epoch 600 Loss 275.8062744140625
Epoch 700 Loss 275.7745666503906
Epoch 800 Loss 275.75665283203125
Epoch 900 Loss 275.74896240234375
last loss 275.74273681640625
5 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([0.9815, 0.9241, 0.9573,  ..., 0.9756, 0.9689, 0.9269], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.9928, 0.9724, 0.9818,  ..., 0.8969, 0.9049, 0.9102], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 203.50299072265625
Epoch 100 Loss 114.94915771484375
Epoch 200 Loss 112.07539367675781
Epoch 300 Loss 111.46683502197266
Epoch 400 Loss 111.35148620605469
Epoch 500 Loss 111.29237365722656
Epoch 600 Loss 111.24858856201172
Epoch 700 Loss 111.21673583984375
Epoch 800 Loss 111.19365692138672
Epoch 900 Loss 111.17667388916016
last loss 111.1637191772461
5 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([0.9817, 0.9510, 0.9640,  ..., 0.8447, 0.8656, 0.8768], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.9241, 0.9280, 0.9083,  ..., 0.9133, 0.8996, 0.9203], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 6.009734153747559
Epoch 100 Loss 2.249314546585083
Epoch 200 Loss 2.1385042667388916
Epoch 300 Loss 2.115243434906006
Epoch 400 Loss 2.10958194732666
Epoch 500 Loss 2.106842517852783
Epoch 600 Loss 2.105203866958618
Epoch 700 Loss 2.104193687438965
Epoch 800 Loss 2.1035735607147217
Epoch 900 Loss 2.1033363342285156
last loss 2.1031365394592285
5 mlp.gate_proj
Pruning ...
5 mlp.up_proj
Pruning ...
5 mlp.down_proj
Pruning ...
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 153321744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.00045405083255900536 val loss: 0.007677920220885426
9153 MiB free out of 48676 MiB total
epoch 1 loss: 0.00040320111224900756 val loss: 0.007640680967597291
7097 MiB free out of 48676 MiB total
epoch 2 loss: 0.0003914666260698141 val loss: 0.007626502629136667
7097 MiB free out of 48676 MiB total
epoch 3 loss: 0.0003827739992630086 val loss: 0.007600129581987858
7097 MiB free out of 48676 MiB total
epoch 4 loss: 0.00037637922719113703 val loss: 0.00759615478455089
7097 MiB free out of 48676 MiB total
37863 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
7097 MiB free out of 48676 MiB total
after cast to cpu
37863 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
6 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.5404, 1.5738, 1.6013,  ..., 1.5337, 1.5906, 1.6066], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.3378, 1.3558, 1.3186,  ..., 2.1034, 2.0254, 2.1746], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 668.0381469726562
Epoch 100 Loss 345.9208679199219
Epoch 200 Loss 336.5672302246094
Epoch 300 Loss 334.9283447265625
Epoch 400 Loss 334.44390869140625
Epoch 500 Loss 334.06854248046875
Epoch 600 Loss 333.7825927734375
Epoch 700 Loss 333.55841064453125
Epoch 800 Loss 333.3607177734375
Epoch 900 Loss 333.2021484375
last loss 333.10491943359375
6 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.6115, 1.6701, 1.6337,  ..., 1.6253, 1.6097, 1.6225], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.2987, 1.3126, 1.3289,  ..., 2.0024, 2.0803, 2.1513], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 597.620849609375
Epoch 100 Loss 364.6671142578125
Epoch 200 Loss 360.0400390625
Epoch 300 Loss 359.2786560058594
Epoch 400 Loss 359.0865783691406
Epoch 500 Loss 358.9752502441406
Epoch 600 Loss 358.9105224609375
Epoch 700 Loss 358.8720703125
Epoch 800 Loss 358.84930419921875
Epoch 900 Loss 358.8407287597656
last loss 358.83380126953125
6 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([0.9260, 0.8385, 0.8766,  ..., 0.9176, 0.8962, 0.8614], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.9808, 0.9636, 0.9873,  ..., 0.9508, 0.9518, 0.9481], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 279.34844970703125
Epoch 100 Loss 153.55169677734375
Epoch 200 Loss 149.3375244140625
Epoch 300 Loss 148.42422485351562
Epoch 400 Loss 148.20849609375
Epoch 500 Loss 148.11538696289062
Epoch 600 Loss 148.04959106445312
Epoch 700 Loss 148.01010131835938
Epoch 800 Loss 147.99765014648438
Epoch 900 Loss 147.98252868652344
last loss 147.96290588378906
6 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([0.9574, 0.9658, 0.9658,  ..., 0.9132, 0.9029, 0.9161], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.8659, 0.8919, 0.8469,  ..., 0.8721, 0.8547, 0.8473], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 7.97557258605957
Epoch 100 Loss 3.546178102493286
Epoch 200 Loss 3.3729429244995117
Epoch 300 Loss 3.3310654163360596
Epoch 400 Loss 3.3166372776031494
Epoch 500 Loss 3.3105931282043457
Epoch 600 Loss 3.307976722717285
Epoch 700 Loss 3.3064513206481934
Epoch 800 Loss 3.306030035018921
Epoch 900 Loss 3.305666446685791
last loss 3.305342197418213
6 mlp.gate_proj
Pruning ...
6 mlp.up_proj
Pruning ...
6 mlp.down_proj
Pruning ...
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 153321744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0006702843702441896 val loss: 0.008146461303113028
7105 MiB free out of 48676 MiB total
epoch 1 loss: 0.000579248747726524 val loss: 0.00809335321537219
7097 MiB free out of 48676 MiB total
epoch 2 loss: 0.0005573923783686041 val loss: 0.008071066171396524
7097 MiB free out of 48676 MiB total
epoch 3 loss: 0.0005430396167867002 val loss: 0.008049351745285094
7097 MiB free out of 48676 MiB total
epoch 4 loss: 0.0005326988627984974 val loss: 0.008045962051255628
7097 MiB free out of 48676 MiB total
37863 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
7097 MiB free out of 48676 MiB total
after cast to cpu
37863 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
7 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.5591, 1.5747, 1.5777,  ..., 1.5886, 1.5985, 1.6013], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.7524, 0.7359, 0.7657,  ..., 2.0297, 2.7848, 2.7748], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 740.22802734375
Epoch 100 Loss 391.2969055175781
Epoch 200 Loss 381.71099853515625
Epoch 300 Loss 379.84228515625
Epoch 400 Loss 379.310302734375
Epoch 500 Loss 378.9288635253906
Epoch 600 Loss 378.6610107421875
Epoch 700 Loss 378.4705505371094
Epoch 800 Loss 378.3277587890625
Epoch 900 Loss 378.2071533203125
last loss 378.1351318359375
7 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.5728, 1.6454, 1.5849,  ..., 1.5643, 1.5614, 1.5966], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.7458, 0.7266, 0.7555,  ..., 1.9018, 2.3899, 2.3772], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 643.4000854492188
Epoch 100 Loss 399.757568359375
Epoch 200 Loss 395.0060119628906
Epoch 300 Loss 394.26177978515625
Epoch 400 Loss 394.06683349609375
Epoch 500 Loss 393.9599304199219
Epoch 600 Loss 393.8985595703125
Epoch 700 Loss 393.86187744140625
Epoch 800 Loss 393.8437194824219
Epoch 900 Loss 393.83502197265625
last loss 393.827880859375
7 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([0.9560, 0.8397, 0.8979,  ..., 0.9439, 0.9081, 0.8830], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.9850, 0.9593, 0.9277,  ..., 0.8599, 0.8627, 0.8792], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 302.001220703125
Epoch 100 Loss 167.67514038085938
Epoch 200 Loss 163.4886474609375
Epoch 300 Loss 162.61024475097656
Epoch 400 Loss 162.40847778320312
Epoch 500 Loss 162.31570434570312
Epoch 600 Loss 162.2578887939453
Epoch 700 Loss 162.22084045410156
Epoch 800 Loss 162.19581604003906
Epoch 900 Loss 162.18270874023438
last loss 162.1761474609375
7 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.0083, 0.9239, 0.9096,  ..., 0.8438, 0.8473, 0.8590], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.8733, 0.9017, 0.8406,  ..., 0.8633, 0.8822, 0.8546], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 12.049712181091309
Epoch 100 Loss 5.494339942932129
Epoch 200 Loss 5.256038188934326
Epoch 300 Loss 5.200875282287598
Epoch 400 Loss 5.18527364730835
Epoch 500 Loss 5.179624080657959
Epoch 600 Loss 5.175937175750732
Epoch 700 Loss 5.173520088195801
Epoch 800 Loss 5.171919822692871
Epoch 900 Loss 5.170846939086914
last loss 5.1701741218566895
7 mlp.gate_proj
Pruning ...
7 mlp.up_proj
Pruning ...
7 mlp.down_proj
Pruning ...
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 153321744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0009198965326504549 val loss: 0.011978446273133159
9153 MiB free out of 48676 MiB total
epoch 1 loss: 0.0008302164224005537 val loss: 0.011983331176452339
7097 MiB free out of 48676 MiB total
epoch 2 loss: 0.0008023505665732955 val loss: 0.011948199535254389
7097 MiB free out of 48676 MiB total
epoch 3 loss: 0.0007825974903425958 val loss: 0.012017675559036434
7097 MiB free out of 48676 MiB total
epoch 4 loss: 0.0007669724463994498 val loss: 0.011984739918261766
7097 MiB free out of 48676 MiB total
37863 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
7097 MiB free out of 48676 MiB total
after cast to cpu
37863 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
8 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.5808, 1.6306, 1.5969,  ..., 1.5672, 1.5837, 1.5975], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.8247, 1.0033, 0.9929,  ..., 2.3955, 3.1897, 3.1726], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 772.3042602539062
Epoch 100 Loss 399.8487243652344
Epoch 200 Loss 391.3849182128906
Epoch 300 Loss 390.1575927734375
Epoch 400 Loss 389.68963623046875
Epoch 500 Loss 389.3313903808594
Epoch 600 Loss 389.05853271484375
Epoch 700 Loss 388.83831787109375
Epoch 800 Loss 388.62451171875
Epoch 900 Loss 388.5009460449219
last loss 388.382568359375
8 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.6222, 1.6256, 1.6392,  ..., 1.5824, 1.5826, 1.6133], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.8323, 1.0179, 0.9911,  ..., 1.9226, 2.3946, 2.4478], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 684.898193359375
Epoch 100 Loss 427.5388488769531
Epoch 200 Loss 423.9070129394531
Epoch 300 Loss 423.3671875
Epoch 400 Loss 423.232666015625
Epoch 500 Loss 423.1629943847656
Epoch 600 Loss 423.1246337890625
Epoch 700 Loss 423.1048583984375
Epoch 800 Loss 423.09765625
Epoch 900 Loss 423.09173583984375
last loss 423.0897216796875
8 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([0.9597, 0.8698, 0.8955,  ..., 0.9706, 0.9312, 0.9159], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.0011, 1.0007, 0.9877,  ..., 0.8776, 0.8515, 0.8656], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 290.42266845703125
Epoch 100 Loss 163.51382446289062
Epoch 200 Loss 160.49404907226562
Epoch 300 Loss 160.0263214111328
Epoch 400 Loss 159.9424285888672
Epoch 500 Loss 159.87705993652344
Epoch 600 Loss 159.82778930664062
Epoch 700 Loss 159.79116821289062
Epoch 800 Loss 159.76385498046875
Epoch 900 Loss 159.74325561523438
last loss 159.72756958007812
8 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([0.9737, 0.9628, 0.9414,  ..., 0.8193, 0.8024, 0.8064], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.9048, 0.9030, 0.8637,  ..., 0.8852, 0.8910, 0.8782], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 19.05988121032715
Epoch 100 Loss 8.674209594726562
Epoch 200 Loss 8.287174224853516
Epoch 300 Loss 8.201534271240234
Epoch 400 Loss 8.176032066345215
Epoch 500 Loss 8.165321350097656
Epoch 600 Loss 8.160738945007324
Epoch 700 Loss 8.1583833694458
Epoch 800 Loss 8.156670570373535
Epoch 900 Loss 8.15542221069336
last loss 8.154509544372559
8 mlp.gate_proj
Pruning ...
8 mlp.up_proj
Pruning ...
8 mlp.down_proj
Pruning ...
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 153321744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.001333590396825457 val loss: 0.012344682880211622
9153 MiB free out of 48676 MiB total
epoch 1 loss: 0.0012391906439006561 val loss: 0.012264141405466944
7097 MiB free out of 48676 MiB total
epoch 2 loss: 0.0012003340088995174 val loss: 0.012223998666740954
7097 MiB free out of 48676 MiB total
epoch 3 loss: 0.001171910813809518 val loss: 0.012208325846586376
7097 MiB free out of 48676 MiB total
epoch 4 loss: 0.001148476095750084 val loss: 0.012195865565445274
7097 MiB free out of 48676 MiB total
37863 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
7097 MiB free out of 48676 MiB total
after cast to cpu
37863 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
9 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.5445, 1.6070, 1.5710,  ..., 1.6273, 1.5779, 1.6374], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.0707, 1.1604, 1.1292,  ..., 1.9872, 2.0199, 2.1190], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 867.5814208984375
Epoch 100 Loss 458.5539855957031
Epoch 200 Loss 449.6170654296875
Epoch 300 Loss 448.65850830078125
Epoch 400 Loss 448.0998229980469
Epoch 500 Loss 447.6639709472656
Epoch 600 Loss 447.23724365234375
Epoch 700 Loss 446.8407897949219
Epoch 800 Loss 446.70538330078125
Epoch 900 Loss 446.5205383300781
last loss 446.25750732421875
9 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.6808, 1.6457, 1.6952,  ..., 1.6183, 1.6024, 1.6369], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.9793, 1.1443, 1.1281,  ..., 2.1393, 2.2283, 2.2846], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 767.2825927734375
Epoch 100 Loss 484.7952880859375
Epoch 200 Loss 480.41204833984375
Epoch 300 Loss 479.9338073730469
Epoch 400 Loss 479.7470397949219
Epoch 500 Loss 479.6380920410156
Epoch 600 Loss 479.57452392578125
Epoch 700 Loss 479.53704833984375
Epoch 800 Loss 479.514404296875
Epoch 900 Loss 479.50042724609375
last loss 479.49774169921875
9 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([0.9727, 0.9112, 0.8904,  ..., 0.9682, 0.9803, 0.9293], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.0864, 1.0820, 1.0906,  ..., 0.8114, 0.8140, 0.7966], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 316.6172180175781
Epoch 100 Loss 185.43353271484375
Epoch 200 Loss 181.9378662109375
Epoch 300 Loss 181.32003784179688
Epoch 400 Loss 181.19786071777344
Epoch 500 Loss 181.12399291992188
Epoch 600 Loss 181.07879638671875
Epoch 700 Loss 181.05050659179688
Epoch 800 Loss 181.03199768066406
Epoch 900 Loss 181.0190887451172
last loss 181.01348876953125
9 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.0343, 1.0450, 1.0397,  ..., 0.8190, 0.8186, 0.7914], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.9360, 0.9315, 0.8659,  ..., 0.9257, 0.9022, 0.9244], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 25.473297119140625
Epoch 100 Loss 12.527459144592285
Epoch 200 Loss 12.0568265914917
Epoch 300 Loss 11.963448524475098
Epoch 400 Loss 11.935432434082031
Epoch 500 Loss 11.927412986755371
Epoch 600 Loss 11.922350883483887
Epoch 700 Loss 11.919029235839844
Epoch 800 Loss 11.916802406311035
Epoch 900 Loss 11.91531753540039
last loss 11.914728164672852
9 mlp.gate_proj
Pruning ...
9 mlp.up_proj
Pruning ...
9 mlp.down_proj
Pruning ...
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 153321744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0018849916950784973 val loss: 0.01599210436688736
9153 MiB free out of 48676 MiB total
epoch 1 loss: 0.0017523935221106512 val loss: 0.01586340315407142
7097 MiB free out of 48676 MiB total
epoch 2 loss: 0.0016961984401859809 val loss: 0.01581044989870861
7097 MiB free out of 48676 MiB total
epoch 3 loss: 0.0016544732134207152 val loss: 0.015723980264738202
7097 MiB free out of 48676 MiB total
epoch 4 loss: 0.0016204420371650485 val loss: 0.015735960158053786
7097 MiB free out of 48676 MiB total
37863 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
7097 MiB free out of 48676 MiB total
after cast to cpu
37863 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
10 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.5843, 1.6006, 1.5543,  ..., 1.6136, 1.6117, 1.6147], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.8123, 0.8635, 0.8176,  ..., 2.0155, 2.2036, 2.1027], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 794.039794921875
Epoch 100 Loss 431.3115234375
Epoch 200 Loss 424.5303955078125
Epoch 300 Loss 423.45849609375
Epoch 400 Loss 422.94091796875
Epoch 500 Loss 422.43145751953125
Epoch 600 Loss 421.813720703125
Epoch 700 Loss 421.64739990234375
Epoch 800 Loss 421.419677734375
Epoch 900 Loss 421.0914306640625
last loss 420.5960388183594
10 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.6543, 1.6721, 1.6871,  ..., 1.5862, 1.6025, 1.6724], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.7476, 0.8385, 0.8215,  ..., 2.1863, 2.1931, 2.1422], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 729.1478271484375
Epoch 100 Loss 478.6065673828125
Epoch 200 Loss 475.5468444824219
Epoch 300 Loss 475.1932373046875
Epoch 400 Loss 475.09246826171875
Epoch 500 Loss 475.0320129394531
Epoch 600 Loss 474.9939880371094
Epoch 700 Loss 474.96856689453125
Epoch 800 Loss 474.95068359375
Epoch 900 Loss 474.93792724609375
last loss 474.93414306640625
10 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([0.9680, 0.9046, 0.8915,  ..., 0.9570, 0.9430, 0.9284], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.1302, 1.1081, 1.1169,  ..., 0.9286, 0.9147, 0.9270], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 310.07952880859375
Epoch 100 Loss 186.60765075683594
Epoch 200 Loss 184.19723510742188
Epoch 300 Loss 183.8638153076172
Epoch 400 Loss 183.78515625
Epoch 500 Loss 183.72781372070312
Epoch 600 Loss 183.6871795654297
Epoch 700 Loss 183.65867614746094
Epoch 800 Loss 183.63858032226562
Epoch 900 Loss 183.62423706054688
last loss 183.61386108398438
10 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.0892, 1.0734, 1.0835,  ..., 0.8955, 0.8985, 0.8911], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.9230, 0.9006, 0.8636,  ..., 0.9054, 0.9069, 0.8883], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 32.04829788208008
Epoch 100 Loss 16.892658233642578
Epoch 200 Loss 16.25640296936035
Epoch 300 Loss 16.119190216064453
Epoch 400 Loss 16.080181121826172
Epoch 500 Loss 16.06835174560547
Epoch 600 Loss 16.06069564819336
Epoch 700 Loss 16.05569839477539
Epoch 800 Loss 16.052371978759766
Epoch 900 Loss 16.050106048583984
last loss 16.048648834228516
10 mlp.gate_proj
Pruning ...
10 mlp.up_proj
Pruning ...
10 mlp.down_proj
Pruning ...
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 153321744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.002564897620686679 val loss: 0.02370139176491648
7105 MiB free out of 48676 MiB total
epoch 1 loss: 0.002381182952376548 val loss: 0.023686314467340708
7097 MiB free out of 48676 MiB total
epoch 2 loss: 0.002296350146025361 val loss: 0.02369439508765936
7097 MiB free out of 48676 MiB total
epoch 3 loss: 0.0022328334971462027 val loss: 0.023660099366679788
7097 MiB free out of 48676 MiB total
epoch 4 loss: 0.002181498977734009 val loss: 0.02369359042495489
7097 MiB free out of 48676 MiB total
37863 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
7097 MiB free out of 48676 MiB total
after cast to cpu
37863 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
11 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.5355, 1.5699, 1.5003,  ..., 1.5027, 1.5507, 1.5416], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.9395, 0.8253, 0.8040,  ..., 2.0374, 1.9353, 2.0648], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 792.7459106445312
Epoch 100 Loss 439.56634521484375
Epoch 200 Loss 431.9155578613281
Epoch 300 Loss 431.14715576171875
Epoch 400 Loss 430.77215576171875
Epoch 500 Loss 430.4842834472656
Epoch 600 Loss 430.2707214355469
Epoch 700 Loss 430.114013671875
Epoch 800 Loss 429.9986267089844
Epoch 900 Loss 429.91229248046875
last loss 429.84625244140625
11 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.4919, 1.5258, 1.5711,  ..., 1.5032, 1.4571, 1.5529], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.9077, 0.8226, 0.7986,  ..., 1.9173, 1.8759, 1.8916], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 713.4847412109375
Epoch 100 Loss 461.7909240722656
Epoch 200 Loss 458.07769775390625
Epoch 300 Loss 457.5134582519531
Epoch 400 Loss 457.39544677734375
Epoch 500 Loss 457.3260498046875
Epoch 600 Loss 457.2824401855469
Epoch 700 Loss 457.25390625
Epoch 800 Loss 457.2344055175781
Epoch 900 Loss 457.22674560546875
last loss 457.2265625
11 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([0.9840, 0.9265, 0.9024,  ..., 1.0096, 0.9917, 0.9475], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.9041, 0.9214, 0.9175,  ..., 1.0224, 1.0506, 1.0277], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 409.77032470703125
Epoch 100 Loss 243.56365966796875
Epoch 200 Loss 239.63446044921875
Epoch 300 Loss 238.97389221191406
Epoch 400 Loss 238.84237670898438
Epoch 500 Loss 238.76895141601562
Epoch 600 Loss 238.72686767578125
Epoch 700 Loss 238.70167541503906
Epoch 800 Loss 238.68548583984375
Epoch 900 Loss 238.67831420898438
last loss 238.67282104492188
11 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([0.8975, 0.9156, 0.8993,  ..., 0.9782, 0.9988, 0.9837], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.9541, 0.9431, 0.8961,  ..., 0.9448, 0.9398, 0.9368], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 32.977294921875
Epoch 100 Loss 16.91934585571289
Epoch 200 Loss 15.930119514465332
Epoch 300 Loss 15.646547317504883
Epoch 400 Loss 15.565999031066895
Epoch 500 Loss 15.525810241699219
Epoch 600 Loss 15.498917579650879
Epoch 700 Loss 15.480788230895996
Epoch 800 Loss 15.468412399291992
Epoch 900 Loss 15.45983600616455
last loss 15.455781936645508
11 mlp.gate_proj
Pruning ...
11 mlp.up_proj
Pruning ...
11 mlp.down_proj
Pruning ...
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 153321744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0023373658796117525 val loss: 0.01634328649379313
9151 MiB free out of 48676 MiB total
epoch 1 loss: 0.0021859866828890517 val loss: 0.016291698848363012
7095 MiB free out of 48676 MiB total
epoch 2 loss: 0.0021121078161741025 val loss: 0.016220193647313863
7095 MiB free out of 48676 MiB total
epoch 3 loss: 0.0020562212312142947 val loss: 0.016215834475588053
7095 MiB free out of 48676 MiB total
epoch 4 loss: 0.002010307985074178 val loss: 0.016207801643759012
7095 MiB free out of 48676 MiB total
37863 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
7095 MiB free out of 48676 MiB total
after cast to cpu
37863 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
12 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.5269, 1.5773, 1.5420,  ..., 1.5242, 1.5672, 1.5658], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.8878, 1.0026, 1.0124,  ..., 1.6680, 1.7091, 1.5400], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 959.3917236328125
Epoch 100 Loss 550.5843505859375
Epoch 200 Loss 540.8734130859375
Epoch 300 Loss 539.2206420898438
Epoch 400 Loss 538.8392944335938
Epoch 500 Loss 538.5597534179688
Epoch 600 Loss 538.36181640625
Epoch 700 Loss 538.2235107421875
Epoch 800 Loss 538.1267700195312
Epoch 900 Loss 538.0584716796875
last loss 538.0098266601562
12 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.6344, 1.6304, 1.6164,  ..., 1.5886, 1.5710, 1.6159], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.8726, 0.9940, 1.0412,  ..., 1.7685, 1.8328, 1.6122], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 873.418701171875
Epoch 100 Loss 570.4041137695312
Epoch 200 Loss 565.6793212890625
Epoch 300 Loss 565.0626220703125
Epoch 400 Loss 564.9158935546875
Epoch 500 Loss 564.8320922851562
Epoch 600 Loss 564.781982421875
Epoch 700 Loss 564.7513427734375
Epoch 800 Loss 564.734130859375
Epoch 900 Loss 564.7332153320312
early stopping at epoch 943
last loss 564.7332153320312
12 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([0.9805, 0.8820, 0.9145,  ..., 0.9917, 0.9768, 0.9477], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.9254, 0.9355, 0.9365,  ..., 0.7872, 0.8072, 0.7791], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 424.3338623046875
Epoch 100 Loss 259.5023498535156
Epoch 200 Loss 255.22219848632812
Epoch 300 Loss 254.4688262939453
Epoch 400 Loss 254.28985595703125
Epoch 500 Loss 254.21388244628906
Epoch 600 Loss 254.1666717529297
Epoch 700 Loss 254.13604736328125
Epoch 800 Loss 254.1163330078125
Epoch 900 Loss 254.10867309570312
last loss 254.102294921875
12 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([0.9019, 0.9160, 0.9263,  ..., 0.8095, 0.8280, 0.8232], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.9529, 0.9486, 0.9037,  ..., 0.9460, 0.9320, 0.9379], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 40.38670349121094
Epoch 100 Loss 21.56343650817871
Epoch 200 Loss 20.51950454711914
Epoch 300 Loss 20.26462173461914
Epoch 400 Loss 20.188940048217773
Epoch 500 Loss 20.170528411865234
Epoch 600 Loss 20.15608024597168
Epoch 700 Loss 20.14496612548828
Epoch 800 Loss 20.13650894165039
Epoch 900 Loss 20.130107879638672
last loss 20.125307083129883
12 mlp.gate_proj
Pruning ...
12 mlp.up_proj
Pruning ...
12 mlp.down_proj
Pruning ...
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 153321744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0029928057429060573 val loss: 0.01997954270336777
9153 MiB free out of 48676 MiB total
epoch 1 loss: 0.0028038407072017435 val loss: 0.019835506682284176
7097 MiB free out of 48676 MiB total
epoch 2 loss: 0.0027147805485583376 val loss: 0.019659415702335536
7097 MiB free out of 48676 MiB total
epoch 3 loss: 0.0026482439461688045 val loss: 0.019608112168498337
7097 MiB free out of 48676 MiB total
epoch 4 loss: 0.0025941696076188236 val loss: 0.019496256951242685
7097 MiB free out of 48676 MiB total
37863 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
7097 MiB free out of 48676 MiB total
after cast to cpu
37863 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
13 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.5021, 1.5756, 1.5207,  ..., 1.5473, 1.5423, 1.5433], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.0422, 1.0638, 1.0815,  ..., 1.8074, 1.7660, 1.7255], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 969.3510131835938
Epoch 100 Loss 558.8338623046875
Epoch 200 Loss 548.77490234375
Epoch 300 Loss 546.956787109375
Epoch 400 Loss 546.7088623046875
Epoch 500 Loss 546.5120239257812
Epoch 600 Loss 546.3587646484375
Epoch 700 Loss 546.2421875
Epoch 800 Loss 546.15478515625
Epoch 900 Loss 546.0897216796875
last loss 546.0418701171875
13 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.6126, 1.5780, 1.5905,  ..., 1.5597, 1.5394, 1.5684], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.8244, 1.0660, 1.0900,  ..., 1.8910, 1.9190, 1.7981], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 943.3302001953125
Epoch 100 Loss 609.3557739257812
Epoch 200 Loss 603.6008911132812
Epoch 300 Loss 602.7172241210938
Epoch 400 Loss 602.5428466796875
Epoch 500 Loss 602.4639282226562
Epoch 600 Loss 602.418701171875
Epoch 700 Loss 602.3897705078125
Epoch 800 Loss 602.3766479492188
Epoch 900 Loss 602.376220703125
early stopping at epoch 930
last loss 602.376220703125
13 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.0103, 0.9506, 0.9665,  ..., 0.9982, 1.0047, 0.9755], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.9152, 0.9285, 0.9353,  ..., 0.9549, 0.9411, 0.9663], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 509.48095703125
Epoch 100 Loss 306.76220703125
Epoch 200 Loss 300.40313720703125
Epoch 300 Loss 299.25982666015625
Epoch 400 Loss 299.00390625
Epoch 500 Loss 298.87646484375
Epoch 600 Loss 298.8044738769531
Epoch 700 Loss 298.77117919921875
Epoch 800 Loss 298.75286865234375
Epoch 900 Loss 298.7392883300781
last loss 298.729248046875
13 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([0.8974, 0.9448, 0.9435,  ..., 0.9521, 0.9254, 0.9538], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.9631, 0.9864, 0.9225,  ..., 0.9686, 0.9673, 0.9634], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 43.42497253417969
Epoch 100 Loss 22.437511444091797
Epoch 200 Loss 20.925830841064453
Epoch 300 Loss 20.498638153076172
Epoch 400 Loss 20.339778900146484
Epoch 500 Loss 20.271137237548828
Epoch 600 Loss 20.255250930786133
Epoch 700 Loss 20.242034912109375
Epoch 800 Loss 20.230995178222656
Epoch 900 Loss 20.221853256225586
last loss 20.214380264282227
13 mlp.gate_proj
Pruning ...
13 mlp.up_proj
Pruning ...
13 mlp.down_proj
Pruning ...
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 153321744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0031313805102399783 val loss: 0.024728955584578216
9153 MiB free out of 48676 MiB total
epoch 1 loss: 0.002904769375163596 val loss: 0.024469251511618495
7097 MiB free out of 48676 MiB total
epoch 2 loss: 0.0028027806783939013 val loss: 0.024380607879720628
7097 MiB free out of 48676 MiB total
epoch 3 loss: 0.0027262019975751173 val loss: 0.024241723935119808
7097 MiB free out of 48676 MiB total
epoch 4 loss: 0.0026631934706529137 val loss: 0.024232996278442442
7097 MiB free out of 48676 MiB total
37863 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
7097 MiB free out of 48676 MiB total
after cast to cpu
37863 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
14 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.5147, 1.5503, 1.5152,  ..., 1.5625, 1.5397, 1.5450], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.1204, 1.4476, 1.4390,  ..., 1.4891, 1.5953, 1.5113], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 946.6053466796875
Epoch 100 Loss 521.6009521484375
Epoch 200 Loss 511.1978454589844
Epoch 300 Loss 509.301025390625
Epoch 400 Loss 508.87396240234375
Epoch 500 Loss 508.55633544921875
Epoch 600 Loss 508.3257141113281
Epoch 700 Loss 508.1593933105469
Epoch 800 Loss 508.0392761230469
Epoch 900 Loss 507.9512634277344
last loss 507.8848876953125
14 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.6134, 1.5749, 1.5880,  ..., 1.5348, 1.5272, 1.5489], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.0522, 1.4499, 1.4432,  ..., 1.5537, 1.5905, 1.4958], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 844.6654052734375
Epoch 100 Loss 541.5147705078125
Epoch 200 Loss 535.3660888671875
Epoch 300 Loss 534.3292846679688
Epoch 400 Loss 534.1417846679688
Epoch 500 Loss 534.0294189453125
Epoch 600 Loss 533.960205078125
Epoch 700 Loss 533.9159545898438
Epoch 800 Loss 533.8864135742188
Epoch 900 Loss 533.86572265625
last loss 533.853759765625
14 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([0.9699, 0.9163, 0.9656,  ..., 0.9915, 0.9925, 0.9866], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.9107, 0.9021, 0.9065,  ..., 1.0683, 1.0652, 1.0858], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 506.42071533203125
Epoch 100 Loss 302.94403076171875
Epoch 200 Loss 296.17132568359375
Epoch 300 Loss 294.90533447265625
Epoch 400 Loss 294.688232421875
Epoch 500 Loss 294.568115234375
Epoch 600 Loss 294.4859619140625
Epoch 700 Loss 294.4298095703125
Epoch 800 Loss 294.39105224609375
Epoch 900 Loss 294.3638000488281
last loss 294.34442138671875
14 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([0.8922, 0.8743, 0.8909,  ..., 1.0444, 1.0262, 1.0497], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.9329, 0.9440, 0.9126,  ..., 0.9653, 0.9494, 0.9803], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 52.16472625732422
Epoch 100 Loss 27.21918487548828
Epoch 200 Loss 25.886573791503906
Epoch 300 Loss 25.547771453857422
Epoch 400 Loss 25.442729949951172
Epoch 500 Loss 25.405933380126953
Epoch 600 Loss 25.380901336669922
Epoch 700 Loss 25.363723754882812
Epoch 800 Loss 25.351787567138672
Epoch 900 Loss 25.343402862548828
last loss 25.337854385375977
14 mlp.gate_proj
Pruning ...
14 mlp.up_proj
Pruning ...
14 mlp.down_proj
Pruning ...
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 153321744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.004022235523734707 val loss: 0.025611389777623117
9151 MiB free out of 48676 MiB total
epoch 1 loss: 0.0037438140443555312 val loss: 0.025606527109630406
7095 MiB free out of 48676 MiB total
epoch 2 loss: 0.0036014638826600276 val loss: 0.025661880034022033
7095 MiB free out of 48676 MiB total
epoch 3 loss: 0.0034940641144203255 val loss: 0.025573779363185167
7095 MiB free out of 48676 MiB total
epoch 4 loss: 0.0034064811352436664 val loss: 0.025605434202589095
7095 MiB free out of 48676 MiB total
37863 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
7095 MiB free out of 48676 MiB total
after cast to cpu
37863 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
15 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.4933, 1.5343, 1.4894,  ..., 1.5105, 1.5305, 1.5341], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.8819, 0.9525, 0.9162,  ..., 1.9384, 1.9062, 1.9309], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 965.1611328125
Epoch 100 Loss 540.427001953125
Epoch 200 Loss 529.546875
Epoch 300 Loss 527.462646484375
Epoch 400 Loss 527.0911865234375
Epoch 500 Loss 526.82568359375
Epoch 600 Loss 526.63916015625
Epoch 700 Loss 526.507080078125
Epoch 800 Loss 526.4107666015625
Epoch 900 Loss 526.3363037109375
last loss 526.2733154296875
15 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.6233, 1.5785, 1.5584,  ..., 1.5217, 1.5787, 1.5373], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.8892, 0.9599, 0.9163,  ..., 1.9989, 2.0357, 2.1068], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 888.74853515625
Epoch 100 Loss 570.6278686523438
Epoch 200 Loss 564.0640258789062
Epoch 300 Loss 563.0856323242188
Epoch 400 Loss 562.8345336914062
Epoch 500 Loss 562.695556640625
Epoch 600 Loss 562.615234375
Epoch 700 Loss 562.5675048828125
Epoch 800 Loss 562.54541015625
Epoch 900 Loss 562.5427856445312
early stopping at epoch 961
last loss 562.5427856445312
15 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.0155, 0.9771, 0.9901,  ..., 1.0429, 1.0140, 1.0064], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.0221, 1.0299, 1.0216,  ..., 0.9816, 0.9779, 0.9640], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 538.625732421875
Epoch 100 Loss 319.5244140625
Epoch 200 Loss 312.05572509765625
Epoch 300 Loss 310.6766357421875
Epoch 400 Loss 310.3439636230469
Epoch 500 Loss 310.222412109375
Epoch 600 Loss 310.1459045410156
Epoch 700 Loss 310.0954895019531
Epoch 800 Loss 310.06512451171875
Epoch 900 Loss 310.05438232421875
last loss 310.0432434082031
15 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([0.9999, 1.0097, 1.0048,  ..., 0.9665, 0.9695, 0.9545], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.9702, 0.9879, 0.9669,  ..., 0.9994, 0.9682, 1.0115], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 60.708438873291016
Epoch 100 Loss 31.22650146484375
Epoch 200 Loss 29.833452224731445
Epoch 300 Loss 29.535018920898438
Epoch 400 Loss 29.46253204345703
Epoch 500 Loss 29.43191146850586
Epoch 600 Loss 29.412948608398438
Epoch 700 Loss 29.400999069213867
Epoch 800 Loss 29.39325714111328
Epoch 900 Loss 29.38809585571289
last loss 29.38523292541504
15 mlp.gate_proj
Pruning ...
15 mlp.up_proj
Pruning ...
15 mlp.down_proj
Pruning ...
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 153321744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.004624066896212753 val loss: 0.034431357868015766
9153 MiB free out of 48676 MiB total
epoch 1 loss: 0.004251384918461554 val loss: 0.033955184277147055
7097 MiB free out of 48676 MiB total
epoch 2 loss: 0.004084774154762272 val loss: 0.03374074096791446
7097 MiB free out of 48676 MiB total
epoch 3 loss: 0.003961041546062916 val loss: 0.033510686131194234
7097 MiB free out of 48676 MiB total
epoch 4 loss: 0.003860591810735059 val loss: 0.03338400740176439
7097 MiB free out of 48676 MiB total
37863 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
7097 MiB free out of 48676 MiB total
after cast to cpu
37863 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
16 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.4894, 1.4845, 1.4596,  ..., 1.4817, 1.5148, 1.4989], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.1535, 1.2194, 1.2913,  ..., 1.6515, 1.6507, 1.6518], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 984.6822509765625
Epoch 100 Loss 543.9228515625
Epoch 200 Loss 533.1818237304688
Epoch 300 Loss 531.2272338867188
Epoch 400 Loss 530.809326171875
Epoch 500 Loss 530.5084228515625
Epoch 600 Loss 530.2966918945312
Epoch 700 Loss 530.1490478515625
Epoch 800 Loss 530.0460205078125
Epoch 900 Loss 529.9736328125
last loss 529.922607421875
16 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.5899, 1.5800, 1.5417,  ..., 1.5215, 1.5134, 1.5515], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.1845, 1.2345, 1.2825,  ..., 1.7402, 1.7219, 1.7930], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 905.01416015625
Epoch 100 Loss 573.1109619140625
Epoch 200 Loss 566.063232421875
Epoch 300 Loss 564.9229125976562
Epoch 400 Loss 564.7384033203125
Epoch 500 Loss 564.6126708984375
Epoch 600 Loss 564.5247802734375
Epoch 700 Loss 564.4644165039062
Epoch 800 Loss 564.423095703125
Epoch 900 Loss 564.394287109375
last loss 564.3734130859375
16 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.0617, 1.0144, 1.0695,  ..., 1.0903, 1.0521, 1.0318], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.2892, 1.2590, 1.2708,  ..., 1.0814, 1.0957, 1.0983], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 644.3563232421875
Epoch 100 Loss 376.89776611328125
Epoch 200 Loss 367.35333251953125
Epoch 300 Loss 365.57574462890625
Epoch 400 Loss 365.1659851074219
Epoch 500 Loss 365.06280517578125
Epoch 600 Loss 364.98583984375
Epoch 700 Loss 364.92926025390625
Epoch 800 Loss 364.88775634765625
Epoch 900 Loss 364.85711669921875
last loss 364.83428955078125
16 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.2393, 1.2085, 1.2276,  ..., 1.0664, 1.0781, 1.0688], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.0223, 1.0249, 1.0036,  ..., 1.0444, 1.0177, 1.0147], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 88.10388946533203
Epoch 100 Loss 43.571712493896484
Epoch 200 Loss 41.63158416748047
Epoch 300 Loss 41.18601989746094
Epoch 400 Loss 41.04094314575195
Epoch 500 Loss 40.99247360229492
Epoch 600 Loss 40.971832275390625
Epoch 700 Loss 40.957305908203125
Epoch 800 Loss 40.946922302246094
Epoch 900 Loss 40.93936538696289
last loss 40.93383026123047
16 mlp.gate_proj
Pruning ...
16 mlp.up_proj
Pruning ...
16 mlp.down_proj
Pruning ...
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 153321744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.006753589324944187 val loss: 0.03691688133403659
7105 MiB free out of 48676 MiB total
epoch 1 loss: 0.00618297119581257 val loss: 0.036929459776729345
7097 MiB free out of 48676 MiB total
epoch 2 loss: 0.005912538443226367 val loss: 0.03692667931318283
7097 MiB free out of 48676 MiB total
epoch 3 loss: 0.005713079932320397 val loss: 0.03697354975156486
7097 MiB free out of 48676 MiB total
epoch 4 loss: 0.0055529160890728235 val loss: 0.037047047168016434
7097 MiB free out of 48676 MiB total
37863 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
7097 MiB free out of 48676 MiB total
after cast to cpu
37863 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
17 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.4791, 1.4705, 1.4639,  ..., 1.5067, 1.5329, 1.4977], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.7025, 0.6941, 0.6939,  ..., 2.0938, 2.2398, 2.0421], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 1143.139892578125
Epoch 100 Loss 628.8955078125
Epoch 200 Loss 612.1514892578125
Epoch 300 Loss 608.6044311523438
Epoch 400 Loss 608.0195922851562
Epoch 500 Loss 607.58740234375
Epoch 600 Loss 607.2677001953125
Epoch 700 Loss 607.0363159179688
Epoch 800 Loss 606.8702392578125
Epoch 900 Loss 606.750732421875
last loss 606.664794921875
17 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.5578, 1.5840, 1.5218,  ..., 1.4811, 1.4996, 1.5344], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.6770, 0.7048, 0.7073,  ..., 2.2019, 1.6335, 2.1880], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 1055.240478515625
Epoch 100 Loss 642.1058959960938
Epoch 200 Loss 630.282958984375
Epoch 300 Loss 628.1627807617188
Epoch 400 Loss 627.6162109375
Epoch 500 Loss 627.3223266601562
Epoch 600 Loss 627.1593017578125
Epoch 700 Loss 627.068603515625
Epoch 800 Loss 627.0307006835938
Epoch 900 Loss 627.0015869140625
last loss 626.9793090820312
17 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.0450, 1.0147, 1.0800,  ..., 1.0646, 1.0705, 1.0441], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.9612, 0.9027, 0.9841,  ..., 1.0400, 1.0164, 1.0498], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 724.2437744140625
Epoch 100 Loss 410.9584655761719
Epoch 200 Loss 396.636474609375
Epoch 300 Loss 393.4544982910156
Epoch 400 Loss 392.4779357910156
Epoch 500 Loss 392.12701416015625
Epoch 600 Loss 392.01690673828125
Epoch 700 Loss 391.93914794921875
Epoch 800 Loss 391.88385009765625
Epoch 900 Loss 391.84454345703125
last loss 391.81683349609375
17 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.0241, 1.0220, 0.9856,  ..., 1.0233, 0.9882, 1.0276], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.0365, 1.0348, 1.0110,  ..., 1.0520, 1.0362, 1.0467], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 67.23343658447266
Epoch 100 Loss 32.73589324951172
Epoch 200 Loss 30.94619369506836
Epoch 300 Loss 30.488996505737305
Epoch 400 Loss 30.333635330200195
Epoch 500 Loss 30.269744873046875
Epoch 600 Loss 30.250410079956055
Epoch 700 Loss 30.24085235595703
Epoch 800 Loss 30.232866287231445
Epoch 900 Loss 30.226259231567383
last loss 30.220870971679688
17 mlp.gate_proj
Pruning ...
17 mlp.up_proj
Pruning ...
17 mlp.down_proj
Pruning ...
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 153321744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.00519955641721026 val loss: 0.05474586901254952
9151 MiB free out of 48676 MiB total
epoch 1 loss: 0.0047750546800671145 val loss: 0.054191949078813195
7095 MiB free out of 48676 MiB total
epoch 2 loss: 0.004555523419185192 val loss: 0.05387020530179143
7095 MiB free out of 48676 MiB total
epoch 3 loss: 0.004394723468067241 val loss: 0.05370609648525715
7095 MiB free out of 48676 MiB total
epoch 4 loss: 0.004276163499525865 val loss: 0.053543015383183956
7095 MiB free out of 48676 MiB total
37863 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
7095 MiB free out of 48676 MiB total
after cast to cpu
37863 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
18 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.4467, 1.4891, 1.4806,  ..., 1.4545, 1.4652, 1.4646], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.0691, 1.1817, 1.1950,  ..., 1.9673, 2.0273, 2.0164], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 1276.1085205078125
Epoch 100 Loss 738.2332153320312
Epoch 200 Loss 716.8807373046875
Epoch 300 Loss 712.2850341796875
Epoch 400 Loss 711.6766967773438
Epoch 500 Loss 711.1826171875
Epoch 600 Loss 710.7930297851562
Epoch 700 Loss 710.4926147460938
Epoch 800 Loss 710.26318359375
Epoch 900 Loss 710.0880126953125
last loss 709.9546508789062
18 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.5252, 1.5307, 1.4716,  ..., 1.4937, 1.4664, 1.4463], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.0710, 1.1842, 1.2079,  ..., 2.1710, 2.3075, 2.3298], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 1256.476806640625
Epoch 100 Loss 766.760009765625
Epoch 200 Loss 749.0768432617188
Epoch 300 Loss 745.9109497070312
Epoch 400 Loss 745.02685546875
Epoch 500 Loss 744.5382080078125
Epoch 600 Loss 744.2615966796875
Epoch 700 Loss 744.097900390625
Epoch 800 Loss 744.0005493164062
Epoch 900 Loss 743.9749145507812
last loss 743.9524536132812
18 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.0789, 1.0596, 1.0864,  ..., 1.1258, 1.1193, 1.1207], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.2060, 1.2129, 1.2298,  ..., 0.9829, 0.9974, 1.0103], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 871.5144653320312
Epoch 100 Loss 506.08990478515625
Epoch 200 Loss 488.27447509765625
Epoch 300 Loss 484.2202453613281
Epoch 400 Loss 483.0018615722656
Epoch 500 Loss 482.67156982421875
Epoch 600 Loss 482.4587097167969
Epoch 700 Loss 482.3210144042969
Epoch 800 Loss 482.23077392578125
Epoch 900 Loss 482.1704406738281
last loss 482.1295471191406
18 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.1691, 1.1865, 1.1922,  ..., 0.9777, 0.9909, 0.9968], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.0622, 1.0683, 1.0524,  ..., 1.0709, 1.0725, 1.0642], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 68.74453735351562
Epoch 100 Loss 33.44776153564453
Epoch 200 Loss 31.90633201599121
Epoch 300 Loss 31.63328742980957
Epoch 400 Loss 31.554386138916016
Epoch 500 Loss 31.53033447265625
Epoch 600 Loss 31.518775939941406
Epoch 700 Loss 31.510942459106445
Epoch 800 Loss 31.505630493164062
Epoch 900 Loss 31.501998901367188
last loss 31.499488830566406
18 mlp.gate_proj
Pruning ...
18 mlp.up_proj
Pruning ...
18 mlp.down_proj
Pruning ...
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 153321744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0055853708909126 val loss: 0.05117421131581068
7105 MiB free out of 48676 MiB total
epoch 1 loss: 0.005110734928166494 val loss: 0.05125924712046981
7097 MiB free out of 48676 MiB total
epoch 2 loss: 0.004884451922407607 val loss: 0.05130523955449462
7097 MiB free out of 48676 MiB total
epoch 3 loss: 0.004714658593002241 val loss: 0.05138903693296015
7097 MiB free out of 48676 MiB total
epoch 4 loss: 0.004582568288242328 val loss: 0.05142281646840274
7097 MiB free out of 48676 MiB total
37863 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
7097 MiB free out of 48676 MiB total
after cast to cpu
37863 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
19 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.4172, 1.4455, 1.4177,  ..., 1.4245, 1.4529, 1.4584], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.9218, 1.1143, 1.1046,  ..., 1.9034, 1.8568, 1.8653], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 1170.9879150390625
Epoch 100 Loss 679.3031005859375
Epoch 200 Loss 661.6156005859375
Epoch 300 Loss 659.0099487304688
Epoch 400 Loss 658.0731201171875
Epoch 500 Loss 657.4097290039062
Epoch 600 Loss 656.9547119140625
Epoch 700 Loss 656.6455078125
Epoch 800 Loss 656.434814453125
Epoch 900 Loss 656.2897338867188
last loss 656.1892700195312
19 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.4944, 1.5200, 1.4792,  ..., 1.4335, 1.4468, 1.4712], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.8642, 1.1287, 1.1200,  ..., 2.0235, 2.0642, 2.0212], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 1123.6214599609375
Epoch 100 Loss 691.5738525390625
Epoch 200 Loss 677.1243286132812
Epoch 300 Loss 674.4060668945312
Epoch 400 Loss 673.7713623046875
Epoch 500 Loss 673.40087890625
Epoch 600 Loss 673.1797485351562
Epoch 700 Loss 673.04150390625
Epoch 800 Loss 672.9507446289062
Epoch 900 Loss 672.908935546875
last loss 672.8804931640625
19 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.0984, 1.0858, 1.1186,  ..., 1.1277, 1.1063, 1.0958], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.1414, 1.1464, 1.1453,  ..., 1.1046, 1.0892, 1.0853], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 858.5330200195312
Epoch 100 Loss 506.92376708984375
Epoch 200 Loss 491.76495361328125
Epoch 300 Loss 488.4614562988281
Epoch 400 Loss 487.5396728515625
Epoch 500 Loss 487.23046875
Epoch 600 Loss 487.0390625
Epoch 700 Loss 486.9173583984375
Epoch 800 Loss 486.83770751953125
Epoch 900 Loss 486.79229736328125
last loss 486.7713928222656
19 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.1254, 1.1186, 1.1293,  ..., 1.0896, 1.0747, 1.0727], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.0850, 1.0817, 1.0711,  ..., 1.0988, 1.0905, 1.0791], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 67.81217193603516
Epoch 100 Loss 35.812843322753906
Epoch 200 Loss 34.60116958618164
Epoch 300 Loss 34.366539001464844
Epoch 400 Loss 34.29951477050781
Epoch 500 Loss 34.28356170654297
Epoch 600 Loss 34.2733268737793
Epoch 700 Loss 34.26609802246094
Epoch 800 Loss 34.26091003417969
Epoch 900 Loss 34.257137298583984
last loss 34.254356384277344
19 mlp.gate_proj
Pruning ...
19 mlp.up_proj
Pruning ...
19 mlp.down_proj
Pruning ...
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 153321744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.006116523563832743 val loss: 0.05932893790304661
9153 MiB free out of 48676 MiB total
epoch 1 loss: 0.005628568142128643 val loss: 0.059415541123598814
7097 MiB free out of 48676 MiB total
epoch 2 loss: 0.0053808505908818915 val loss: 0.059463050216436386
7097 MiB free out of 48676 MiB total
epoch 3 loss: 0.005201706135267159 val loss: 0.059321295004338026
7097 MiB free out of 48676 MiB total
epoch 4 loss: 0.005058666380136856 val loss: 0.05930256354622543
7097 MiB free out of 48676 MiB total
37863 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
7097 MiB free out of 48676 MiB total
after cast to cpu
37863 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
20 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.4390, 1.4916, 1.4176,  ..., 1.4356, 1.4500, 1.4538], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.5440, 0.5997, 0.6395,  ..., 1.7327, 1.8238, 1.5132], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 1175.81494140625
Epoch 100 Loss 688.1517944335938
Epoch 200 Loss 671.4242553710938
Epoch 300 Loss 668.4869995117188
Epoch 400 Loss 667.95361328125
Epoch 500 Loss 667.5244750976562
Epoch 600 Loss 667.19189453125
Epoch 700 Loss 666.9395751953125
Epoch 800 Loss 666.7501220703125
Epoch 900 Loss 666.6082763671875
last loss 666.5025634765625
20 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.5040, 1.5277, 1.4980,  ..., 1.4534, 1.4546, 1.4831], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.5292, 0.5662, 0.5834,  ..., 1.8763, 2.0239, 2.0330], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 1155.326904296875
Epoch 100 Loss 707.164306640625
Epoch 200 Loss 692.9031372070312
Epoch 300 Loss 690.1199951171875
Epoch 400 Loss 689.4849853515625
Epoch 500 Loss 689.1929931640625
Epoch 600 Loss 689.0263671875
Epoch 700 Loss 688.9254150390625
Epoch 800 Loss 688.8671875
Epoch 900 Loss 688.84423828125
last loss 688.8240966796875
20 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.1192, 1.0834, 1.1373,  ..., 1.1337, 1.1346, 1.1265], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.0799, 1.1011, 1.0996,  ..., 1.0353, 1.0458, 1.0728], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 911.2036743164062
Epoch 100 Loss 533.9617309570312
Epoch 200 Loss 516.6710205078125
Epoch 300 Loss 513.0863037109375
Epoch 400 Loss 512.0772094726562
Epoch 500 Loss 511.7958984375
Epoch 600 Loss 511.6390686035156
Epoch 700 Loss 511.53411865234375
Epoch 800 Loss 511.4630432128906
Epoch 900 Loss 511.4141845703125
last loss 511.3805236816406
20 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.0672, 1.0759, 1.0829,  ..., 1.0529, 1.0531, 1.0652], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.1243, 1.1123, 1.0981,  ..., 1.0983, 1.1009, 1.1329], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 96.97268676757812
Epoch 100 Loss 43.35111999511719
Epoch 200 Loss 41.22974395751953
Epoch 300 Loss 40.746646881103516
Epoch 400 Loss 40.59116744995117
Epoch 500 Loss 40.54311752319336
Epoch 600 Loss 40.51848602294922
Epoch 700 Loss 40.501380920410156
Epoch 800 Loss 40.489437103271484
Epoch 900 Loss 40.481040954589844
last loss 40.47513198852539
20 mlp.gate_proj
Pruning ...
20 mlp.up_proj
Pruning ...
20 mlp.down_proj
Pruning ...
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 153321744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.007993436105607543 val loss: 0.07177667459473014
7105 MiB free out of 48676 MiB total
epoch 1 loss: 0.007248776168125914 val loss: 0.07149067334830761
7097 MiB free out of 48676 MiB total
epoch 2 loss: 0.006877356798213441 val loss: 0.0716130887158215
7097 MiB free out of 48676 MiB total
epoch 3 loss: 0.006611597036680905 val loss: 0.07153453258797526
7097 MiB free out of 48676 MiB total
epoch 4 loss: 0.006404410443792585 val loss: 0.07182651804760098
7097 MiB free out of 48676 MiB total
37863 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
7097 MiB free out of 48676 MiB total
after cast to cpu
37863 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
21 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.4144, 1.4360, 1.4147,  ..., 1.3882, 1.4228, 1.3867], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.8719, 0.9523, 0.9489,  ..., 1.8164, 1.4733, 1.9335], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 1323.812744140625
Epoch 100 Loss 746.9321899414062
Epoch 200 Loss 725.4384155273438
Epoch 300 Loss 721.8665161132812
Epoch 400 Loss 720.7235107421875
Epoch 500 Loss 719.9353637695312
Epoch 600 Loss 719.4056396484375
Epoch 700 Loss 719.05078125
Epoch 800 Loss 718.8104248046875
Epoch 900 Loss 718.6444091796875
last loss 718.5281982421875
21 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.4550, 1.4713, 1.4318,  ..., 1.4118, 1.4379, 1.4315], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.8120, 0.9468, 0.9507,  ..., 2.0480, 1.7255, 1.9224], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 1303.2476806640625
Epoch 100 Loss 766.61669921875
Epoch 200 Loss 748.417724609375
Epoch 300 Loss 744.7489624023438
Epoch 400 Loss 743.8872680664062
Epoch 500 Loss 743.5324096679688
Epoch 600 Loss 743.3082275390625
Epoch 700 Loss 743.1618041992188
Epoch 800 Loss 743.0623168945312
Epoch 900 Loss 742.99267578125
last loss 742.961181640625
21 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.1393, 1.1365, 1.1569,  ..., 1.1675, 1.1572, 1.1763], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.0815, 1.0853, 1.1115,  ..., 1.1556, 1.1620, 1.1422], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 1038.076904296875
Epoch 100 Loss 612.9854736328125
Epoch 200 Loss 590.9559936523438
Epoch 300 Loss 585.9727172851562
Epoch 400 Loss 584.4092407226562
Epoch 500 Loss 583.8041381835938
Epoch 600 Loss 583.5700073242188
Epoch 700 Loss 583.489990234375
Epoch 800 Loss 583.4273681640625
Epoch 900 Loss 583.3782958984375
last loss 583.3400268554688
21 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.0900, 1.0793, 1.1069,  ..., 1.1355, 1.1423, 1.1257], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.1539, 1.1705, 1.1258,  ..., 1.1432, 1.1037, 1.1304], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 80.16386413574219
Epoch 100 Loss 40.270809173583984
Epoch 200 Loss 38.45265197753906
Epoch 300 Loss 38.070289611816406
Epoch 400 Loss 37.951087951660156
Epoch 500 Loss 37.91246032714844
Epoch 600 Loss 37.89616394042969
Epoch 700 Loss 37.88518524169922
Epoch 800 Loss 37.87776565551758
Epoch 900 Loss 37.87267303466797
last loss 37.869144439697266
21 mlp.gate_proj
Pruning ...
21 mlp.up_proj
Pruning ...
21 mlp.down_proj
Pruning ...
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 153321744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.006981307909882162 val loss: 0.08138453820720315
9153 MiB free out of 48676 MiB total
epoch 1 loss: 0.006351390031340998 val loss: 0.0815092483535409
7097 MiB free out of 48676 MiB total
epoch 2 loss: 0.006045840538718039 val loss: 0.08139946032315493
7097 MiB free out of 48676 MiB total
epoch 3 loss: 0.005828943740198156 val loss: 0.08159521920606494
7097 MiB free out of 48676 MiB total
epoch 4 loss: 0.005662294253852451 val loss: 0.08141442341729999
7097 MiB free out of 48676 MiB total
37863 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
7097 MiB free out of 48676 MiB total
after cast to cpu
37863 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
22 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.4339, 1.4647, 1.4494,  ..., 1.4377, 1.4301, 1.4364], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.4712, 1.4866, 1.5007,  ..., 1.9845, 1.9318, 2.1921], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 1439.6011962890625
Epoch 100 Loss 833.4765014648438
Epoch 200 Loss 811.7423095703125
Epoch 300 Loss 808.7297973632812
Epoch 400 Loss 807.7077026367188
Epoch 500 Loss 806.95849609375
Epoch 600 Loss 806.42724609375
Epoch 700 Loss 806.0545043945312
Epoch 800 Loss 805.7923583984375
Epoch 900 Loss 805.6061401367188
last loss 805.4729614257812
22 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.4745, 1.4774, 1.4902,  ..., 1.4655, 1.4788, 1.4825], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.4680, 1.4894, 1.5025,  ..., 1.2110, 1.9322, 1.9817], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 1463.374755859375
Epoch 100 Loss 860.3583984375
Epoch 200 Loss 839.5894775390625
Epoch 300 Loss 835.3857421875
Epoch 400 Loss 834.5065307617188
Epoch 500 Loss 834.0761108398438
Epoch 600 Loss 833.7988891601562
Epoch 700 Loss 833.616943359375
Epoch 800 Loss 833.4935302734375
Epoch 900 Loss 833.4071044921875
last loss 833.3455810546875
22 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.1536, 1.1395, 1.1547,  ..., 1.1419, 1.1616, 1.1692], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.2635, 1.2563, 1.2672,  ..., 1.2306, 1.2221, 1.2212], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 1164.6534423828125
Epoch 100 Loss 662.915771484375
Epoch 200 Loss 640.7484741210938
Epoch 300 Loss 635.923828125
Epoch 400 Loss 634.4544677734375
Epoch 500 Loss 634.0774536132812
Epoch 600 Loss 633.8703002929688
Epoch 700 Loss 633.7258911132812
Epoch 800 Loss 633.6238403320312
Epoch 900 Loss 633.550048828125
last loss 633.49609375
22 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.2228, 1.2182, 1.2207,  ..., 1.1587, 1.1975, 1.1741], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.1274, 1.1286, 1.1356,  ..., 1.1232, 1.1369, 1.1320], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 113.727783203125
Epoch 100 Loss 54.587459564208984
Epoch 200 Loss 51.409542083740234
Epoch 300 Loss 50.68694305419922
Epoch 400 Loss 50.44688415527344
Epoch 500 Loss 50.34538269042969
Epoch 600 Loss 50.296871185302734
Epoch 700 Loss 50.28348159790039
Epoch 800 Loss 50.2730712890625
Epoch 900 Loss 50.264434814453125
last loss 50.257381439208984
22 mlp.gate_proj
Pruning ...
22 mlp.up_proj
Pruning ...
22 mlp.down_proj
Pruning ...
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 153321744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.01009518964929157 val loss: 0.07305412227287889
7105 MiB free out of 48676 MiB total
epoch 1 loss: 0.008890405366400955 val loss: 0.0728057473897934
7097 MiB free out of 48676 MiB total
epoch 2 loss: 0.008432141545199556 val loss: 0.07207800634205341
7097 MiB free out of 48676 MiB total
epoch 3 loss: 0.008120819828036474 val loss: 0.07176661305129528
7097 MiB free out of 48676 MiB total
epoch 4 loss: 0.007875433097069617 val loss: 0.07106176158413291
7097 MiB free out of 48676 MiB total
37863 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
7097 MiB free out of 48676 MiB total
after cast to cpu
37863 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
23 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.4343, 1.4455, 1.4322,  ..., 1.4610, 1.4012, 1.4335], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.7342, 0.7344, 0.7733,  ..., 1.6962, 1.9142, 1.6451], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 1738.045166015625
Epoch 100 Loss 998.6417846679688
Epoch 200 Loss 970.6951904296875
Epoch 300 Loss 965.6430053710938
Epoch 400 Loss 964.0486450195312
Epoch 500 Loss 963.2081298828125
Epoch 600 Loss 962.7399291992188
Epoch 700 Loss 962.4741821289062
Epoch 800 Loss 962.3780517578125
Epoch 900 Loss 962.2984619140625
last loss 962.2327270507812
23 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.4515, 1.4864, 1.4677,  ..., 1.4692, 1.4491, 1.4700], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.7387, 0.7335, 0.7738,  ..., 1.5745, 1.3933, 1.6235], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 1702.1748046875
Epoch 100 Loss 1005.901611328125
Epoch 200 Loss 982.0115966796875
Epoch 300 Loss 977.29833984375
Epoch 400 Loss 976.032958984375
Epoch 500 Loss 975.6830444335938
Epoch 600 Loss 975.4456176757812
Epoch 700 Loss 975.2839965820312
Epoch 800 Loss 975.1729736328125
Epoch 900 Loss 975.0958862304688
last loss 975.0419921875
23 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.2002, 1.1744, 1.2148,  ..., 1.1841, 1.2298, 1.2296], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.1003, 1.0871, 1.1005,  ..., 1.1547, 1.1473, 1.1477], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 1405.033935546875
Epoch 100 Loss 812.0845947265625
Epoch 200 Loss 782.369140625
Epoch 300 Loss 775.6235961914062
Epoch 400 Loss 773.47900390625
Epoch 500 Loss 772.6475219726562
Epoch 600 Loss 772.33935546875
Epoch 700 Loss 772.176513671875
Epoch 800 Loss 772.0599365234375
Epoch 900 Loss 771.9752197265625
last loss 771.9140014648438
23 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.0890, 1.0794, 1.0946,  ..., 1.1393, 1.1247, 1.1352], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.1980, 1.2021, 1.1762,  ..., 1.1868, 1.1935, 1.1843], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 94.47564697265625
Epoch 100 Loss 49.47426223754883
Epoch 200 Loss 47.51860427856445
Epoch 300 Loss 47.11393356323242
Epoch 400 Loss 47.012027740478516
Epoch 500 Loss 46.976688385009766
Epoch 600 Loss 46.953269958496094
Epoch 700 Loss 46.93767166137695
Epoch 800 Loss 46.92707443237305
Epoch 900 Loss 46.91972732543945
last loss 46.91458511352539
23 mlp.gate_proj
Pruning ...
23 mlp.up_proj
Pruning ...
23 mlp.down_proj
Pruning ...
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 153321744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.008923247591155814 val loss: 0.10049224318936467
9153 MiB free out of 48676 MiB total
epoch 1 loss: 0.008168766784365289 val loss: 0.10039372462779284
7097 MiB free out of 48676 MiB total
epoch 2 loss: 0.007777236161928158 val loss: 0.10036504222080112
7097 MiB free out of 48676 MiB total
epoch 3 loss: 0.0074979341952712275 val loss: 0.1002119891345501
7097 MiB free out of 48676 MiB total
epoch 4 loss: 0.007278436049091397 val loss: 0.10022330796346068
7097 MiB free out of 48676 MiB total
37863 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
7097 MiB free out of 48676 MiB total
after cast to cpu
37863 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
24 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.3881, 1.3955, 1.3956,  ..., 1.4213, 1.4150, 1.4089], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.3741, 1.3726, 1.4193,  ..., 1.6439, 1.7088, 1.6957], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 1392.5029296875
Epoch 100 Loss 809.7389526367188
Epoch 200 Loss 787.6206665039062
Epoch 300 Loss 784.2392578125
Epoch 400 Loss 783.2807006835938
Epoch 500 Loss 782.56982421875
Epoch 600 Loss 782.05810546875
Epoch 700 Loss 781.6924438476562
Epoch 800 Loss 781.4292602539062
Epoch 900 Loss 781.237060546875
last loss 781.0953369140625
24 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.4107, 1.4404, 1.4210,  ..., 1.3930, 1.3714, 1.3905], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.3660, 1.3716, 1.4155,  ..., 1.6762, 1.7466, 1.7519], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 1382.5068359375
Epoch 100 Loss 825.4251098632812
Epoch 200 Loss 805.7399291992188
Epoch 300 Loss 801.8099365234375
Epoch 400 Loss 801.0174560546875
Epoch 500 Loss 800.568115234375
Epoch 600 Loss 800.303955078125
Epoch 700 Loss 800.1423950195312
Epoch 800 Loss 800.0384521484375
Epoch 900 Loss 799.98974609375
last loss 799.957275390625
24 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.2120, 1.1798, 1.1826,  ..., 1.2159, 1.1909, 1.2084], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.2397, 1.2433, 1.2546,  ..., 1.1785, 1.1836, 1.1816], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 1295.3255615234375
Epoch 100 Loss 761.385009765625
Epoch 200 Loss 734.0264282226562
Epoch 300 Loss 727.8435668945312
Epoch 400 Loss 725.9547729492188
Epoch 500 Loss 725.3180541992188
Epoch 600 Loss 725.0806884765625
Epoch 700 Loss 724.9093017578125
Epoch 800 Loss 724.77490234375
Epoch 900 Loss 724.6906127929688
last loss 724.6165771484375
24 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.2159, 1.2214, 1.2327,  ..., 1.1572, 1.1644, 1.1607], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.1998, 1.1973, 1.1921,  ..., 1.1950, 1.1950, 1.1825], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 129.8848419189453
Epoch 100 Loss 60.644386291503906
Epoch 200 Loss 56.69718551635742
Epoch 300 Loss 55.706695556640625
Epoch 400 Loss 55.368568420410156
Epoch 500 Loss 55.22649002075195
Epoch 600 Loss 55.166053771972656
Epoch 700 Loss 55.140113830566406
Epoch 800 Loss 55.12045669555664
Epoch 900 Loss 55.10554504394531
last loss 55.09429168701172
24 mlp.gate_proj
Pruning ...
24 mlp.up_proj
Pruning ...
24 mlp.down_proj
Pruning ...
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 153321744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.010617257321428042 val loss: 0.10982629656791687
7105 MiB free out of 48676 MiB total
epoch 1 loss: 0.009596575939212926 val loss: 0.1107480232603848
7097 MiB free out of 48676 MiB total
epoch 2 loss: 0.009098740931221982 val loss: 0.11117976391687989
7097 MiB free out of 48676 MiB total
epoch 3 loss: 0.008749358024942921 val loss: 0.1119879400357604
7097 MiB free out of 48676 MiB total
epoch 4 loss: 0.00847744913698989 val loss: 0.11210737843066454
7097 MiB free out of 48676 MiB total
37863 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
7097 MiB free out of 48676 MiB total
after cast to cpu
37863 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
25 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.4169, 1.3969, 1.4229,  ..., 1.4106, 1.3910, 1.4140], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.5655, 0.5461, 0.5966,  ..., 1.6563, 1.7658, 1.6648], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 1807.757568359375
Epoch 100 Loss 1065.3642578125
Epoch 200 Loss 1034.48828125
Epoch 300 Loss 1028.867431640625
Epoch 400 Loss 1027.381591796875
Epoch 500 Loss 1026.3314208984375
Epoch 600 Loss 1025.6068115234375
Epoch 700 Loss 1025.1092529296875
Epoch 800 Loss 1024.7655029296875
Epoch 900 Loss 1024.525146484375
last loss 1024.355712890625
25 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.4192, 1.4690, 1.4127,  ..., 1.4213, 1.3962, 1.4222], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.5660, 0.5315, 0.5776,  ..., 1.6343, 1.7744, 1.7085], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 1821.62890625
Epoch 100 Loss 1097.33447265625
Epoch 200 Loss 1069.2010498046875
Epoch 300 Loss 1063.3133544921875
Epoch 400 Loss 1061.666748046875
Epoch 500 Loss 1061.3533935546875
Epoch 600 Loss 1061.11083984375
Epoch 700 Loss 1060.92529296875
Epoch 800 Loss 1060.7838134765625
Epoch 900 Loss 1060.6751708984375
last loss 1060.592041015625
25 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.2550, 1.2244, 1.2299,  ..., 1.2444, 1.2465, 1.2497], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.3124, 1.3050, 1.3245,  ..., 1.2849, 1.3018, 1.2839], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 1626.326904296875
Epoch 100 Loss 969.735595703125
Epoch 200 Loss 938.124755859375
Epoch 300 Loss 931.3487548828125
Epoch 400 Loss 929.3662109375
Epoch 500 Loss 928.9759521484375
Epoch 600 Loss 928.6672973632812
Epoch 700 Loss 928.4260864257812
Epoch 800 Loss 928.2389526367188
Epoch 900 Loss 928.09423828125
last loss 927.9835205078125
25 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.2653, 1.2502, 1.2811,  ..., 1.2720, 1.2811, 1.2801], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.2426, 1.2505, 1.2296,  ..., 1.2162, 1.2488, 1.2485], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 134.11685180664062
Epoch 100 Loss 50.40972137451172
Epoch 200 Loss 45.38092803955078
Epoch 300 Loss 43.73051452636719
Epoch 400 Loss 43.40742492675781
Epoch 500 Loss 43.19319534301758
Epoch 600 Loss 43.01167297363281
Epoch 700 Loss 42.86125564575195
Epoch 800 Loss 42.73809051513672
Epoch 900 Loss 42.63779067993164
last loss 42.55696487426758
25 mlp.gate_proj
Pruning ...
25 mlp.up_proj
Pruning ...
25 mlp.down_proj
Pruning ...
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 153321744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.009200879452691879 val loss: 0.14374625775963068
9153 MiB free out of 48676 MiB total
epoch 1 loss: 0.00823713661156944 val loss: 0.14320030808448792
7097 MiB free out of 48676 MiB total
epoch 2 loss: 0.007806082227034494 val loss: 0.14279054943472147
7097 MiB free out of 48676 MiB total
epoch 3 loss: 0.0075226492663205136 val loss: 0.14280049968510866
7097 MiB free out of 48676 MiB total
epoch 4 loss: 0.007243411590025062 val loss: 0.14251812547445297
7097 MiB free out of 48676 MiB total
37863 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
7097 MiB free out of 48676 MiB total
after cast to cpu
37863 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
26 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.3872, 1.3984, 1.4091,  ..., 1.3887, 1.3689, 1.4181], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.1334, 1.1177, 1.1413,  ..., 0.8847, 1.7802, 1.8081], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 1573.1568603515625
Epoch 100 Loss 924.9750366210938
Epoch 200 Loss 892.0614013671875
Epoch 300 Loss 885.8193359375
Epoch 400 Loss 883.9078369140625
Epoch 500 Loss 882.4067993164062
Epoch 600 Loss 881.2686767578125
Epoch 700 Loss 880.4202270507812
Epoch 800 Loss 879.791259765625
Epoch 900 Loss 879.324462890625
last loss 878.978759765625
26 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.3885, 1.4287, 1.3924,  ..., 1.4006, 1.3950, 1.3818], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.1274, 1.1214, 1.1409,  ..., 1.8750, 1.8001, 1.7344], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 1600.342041015625
Epoch 100 Loss 969.0362548828125
Epoch 200 Loss 934.03466796875
Epoch 300 Loss 924.7606201171875
Epoch 400 Loss 921.5703125
Epoch 500 Loss 920.6709594726562
Epoch 600 Loss 920.2332763671875
Epoch 700 Loss 919.9093017578125
Epoch 800 Loss 919.6707153320312
Epoch 900 Loss 919.494384765625
last loss 919.36376953125
26 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.2695, 1.2609, 1.2552,  ..., 1.2922, 1.2361, 1.2380], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.3299, 1.3174, 1.3327,  ..., 1.2755, 1.2536, 1.2610], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 1607.705810546875
Epoch 100 Loss 958.6185302734375
Epoch 200 Loss 913.7223510742188
Epoch 300 Loss 901.509521484375
Epoch 400 Loss 897.1288452148438
Epoch 500 Loss 895.3172607421875
Epoch 600 Loss 894.758544921875
Epoch 700 Loss 894.367431640625
Epoch 800 Loss 894.0844116210938
Epoch 900 Loss 893.8781127929688
last loss 893.7274169921875
26 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.3113, 1.3014, 1.3114,  ..., 1.2673, 1.2453, 1.2514], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.2578, 1.2535, 1.2387,  ..., 1.2614, 1.2616, 1.2322], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 161.55142211914062
Epoch 100 Loss 72.18566131591797
Epoch 200 Loss 66.12965393066406
Epoch 300 Loss 64.53096008300781
Epoch 400 Loss 63.96918487548828
Epoch 500 Loss 63.732608795166016
Epoch 600 Loss 63.629051208496094
Epoch 700 Loss 63.59469985961914
Epoch 800 Loss 63.5668830871582
Epoch 900 Loss 63.54448699951172
last loss 63.52666473388672
26 mlp.gate_proj
Pruning ...
26 mlp.up_proj
Pruning ...
26 mlp.down_proj
Pruning ...
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 153321744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.013572329851740506 val loss: 0.16245154663920403
9153 MiB free out of 48676 MiB total
epoch 1 loss: 0.012401520332787186 val loss: 0.16212532948702574
7097 MiB free out of 48676 MiB total
epoch 2 loss: 0.011799002044426743 val loss: 0.16230659186840057
7097 MiB free out of 48676 MiB total
epoch 3 loss: 0.011355428388924338 val loss: 0.16203638538718224
7097 MiB free out of 48676 MiB total
epoch 4 loss: 0.011009927569830325 val loss: 0.16238510701805353
7097 MiB free out of 48676 MiB total
37863 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
7097 MiB free out of 48676 MiB total
after cast to cpu
37863 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
27 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.4110, 1.4780, 1.4380,  ..., 1.4506, 1.4507, 1.4779], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.3049, 1.3313, 1.3302,  ..., 1.5344, 1.4997, 1.0326], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 1967.7696533203125
Epoch 100 Loss 1167.784912109375
Epoch 200 Loss 1119.1068115234375
Epoch 300 Loss 1105.162353515625
Epoch 400 Loss 1099.960205078125
Epoch 500 Loss 1098.91845703125
Epoch 600 Loss 1098.255615234375
Epoch 700 Loss 1097.6966552734375
Epoch 800 Loss 1097.2333984375
Epoch 900 Loss 1096.8536376953125
last loss 1096.54736328125
27 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.4432, 1.4596, 1.4588,  ..., 1.4564, 1.4578, 1.4376], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.2920, 1.3262, 1.3285,  ..., 1.5665, 1.5050, 1.6066], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 2008.105712890625
Epoch 100 Loss 1210.972900390625
Epoch 200 Loss 1164.9615478515625
Epoch 300 Loss 1152.698974609375
Epoch 400 Loss 1150.5654296875
Epoch 500 Loss 1149.185302734375
Epoch 600 Loss 1148.120361328125
Epoch 700 Loss 1147.3165283203125
Epoch 800 Loss 1146.716552734375
Epoch 900 Loss 1146.2705078125
last loss 1145.94189453125
27 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.2719, 1.2507, 1.2797,  ..., 1.2662, 1.2425, 1.2610], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.2851, 1.2833, 1.2784,  ..., 1.4586, 1.4483, 1.4834], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 1775.583740234375
Epoch 100 Loss 1040.399169921875
Epoch 200 Loss 988.3402099609375
Epoch 300 Loss 973.5140380859375
Epoch 400 Loss 968.5559692382812
Epoch 500 Loss 967.0801391601562
Epoch 600 Loss 966.31689453125
Epoch 700 Loss 965.6951293945312
Epoch 800 Loss 965.197265625
Epoch 900 Loss 964.8027954101562
last loss 964.4946899414062
27 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.2766, 1.2844, 1.2753,  ..., 1.5237, 1.4909, 1.5521], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.2326, 1.2740, 1.2605,  ..., 1.2930, 1.2936, 1.2707], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 129.8939666748047
Epoch 100 Loss 60.6351318359375
Epoch 200 Loss 55.392948150634766
Epoch 300 Loss 53.79066467285156
Epoch 400 Loss 53.1552619934082
Epoch 500 Loss 52.86344909667969
Epoch 600 Loss 52.716224670410156
Epoch 700 Loss 52.6690673828125
Epoch 800 Loss 52.63248062133789
Epoch 900 Loss 52.60350036621094
last loss 52.580814361572266
27 mlp.gate_proj
Pruning ...
27 mlp.up_proj
Pruning ...
27 mlp.down_proj
Pruning ...
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 153321744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.011148987425258383 val loss: 0.23416630364954472
7105 MiB free out of 48676 MiB total
epoch 1 loss: 0.010025788622442633 val loss: 0.23311497550457716
7097 MiB free out of 48676 MiB total
epoch 2 loss: 0.00951982117476291 val loss: 0.23298404738307
7097 MiB free out of 48676 MiB total
epoch 3 loss: 0.00916229332506191 val loss: 0.23277225252240896
7097 MiB free out of 48676 MiB total
epoch 4 loss: 0.008883040249202168 val loss: 0.23396718502044678
7097 MiB free out of 48676 MiB total
37863 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
7097 MiB free out of 48676 MiB total
after cast to cpu
37863 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
28 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.3899, 1.4050, 1.4191,  ..., 1.3927, 1.3823, 1.4056], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.6978, 0.7669, 0.7800,  ..., 1.5473, 1.6398, 1.7314], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 1804.2958984375
Epoch 100 Loss 1099.001953125
Epoch 200 Loss 1054.90576171875
Epoch 300 Loss 1042.47119140625
Epoch 400 Loss 1037.9593505859375
Epoch 500 Loss 1036.520751953125
Epoch 600 Loss 1035.84130859375
Epoch 700 Loss 1035.349365234375
Epoch 800 Loss 1034.9940185546875
Epoch 900 Loss 1034.735107421875
last loss 1034.5452880859375
28 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.4238, 1.4306, 1.4203,  ..., 1.4233, 1.4031, 1.4396], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.7822, 0.7447, 0.7523,  ..., 1.7620, 1.7353, 1.7418], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 1892.5465087890625
Epoch 100 Loss 1161.5267333984375
Epoch 200 Loss 1116.096435546875
Epoch 300 Loss 1103.4293212890625
Epoch 400 Loss 1098.8189697265625
Epoch 500 Loss 1097.6923828125
Epoch 600 Loss 1097.111328125
Epoch 700 Loss 1096.632080078125
Epoch 800 Loss 1096.2427978515625
Epoch 900 Loss 1095.929443359375
last loss 1095.6800537109375
28 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.2854, 1.2936, 1.3251,  ..., 1.2990, 1.3177, 1.3131], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.4762, 1.4871, 1.5179,  ..., 1.4363, 1.4516, 1.4401], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 1939.5723876953125
Epoch 100 Loss 1142.61328125
Epoch 200 Loss 1083.02734375
Epoch 300 Loss 1065.611328125
Epoch 400 Loss 1059.109375
Epoch 500 Loss 1056.9573974609375
Epoch 600 Loss 1056.3284912109375
Epoch 700 Loss 1055.7855224609375
Epoch 800 Loss 1055.325927734375
Epoch 900 Loss 1054.9417724609375
last loss 1054.6260986328125
28 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.5029, 1.4821, 1.5049,  ..., 1.4681, 1.4612, 1.4610], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.2865, 1.2987, 1.3002,  ..., 1.3400, 1.3499, 1.3088], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 185.3586883544922
Epoch 100 Loss 69.47574615478516
Epoch 200 Loss 61.30010986328125
Epoch 300 Loss 58.91940689086914
Epoch 400 Loss 58.00608825683594
Epoch 500 Loss 57.78901672363281
Epoch 600 Loss 57.648807525634766
Epoch 700 Loss 57.5350341796875
Epoch 800 Loss 57.444000244140625
Epoch 900 Loss 57.37162780761719
last loss 57.314674377441406
28 mlp.gate_proj
Pruning ...
28 mlp.up_proj
Pruning ...
28 mlp.down_proj
Pruning ...
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 153321744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.015708310529589653 val loss: 0.3191746920347214
7105 MiB free out of 48676 MiB total
epoch 1 loss: 0.013937221985543147 val loss: 0.31915872544050217
7097 MiB free out of 48676 MiB total
epoch 2 loss: 0.013137504938640632 val loss: 0.32004451006650925
7097 MiB free out of 48676 MiB total
epoch 3 loss: 0.012562373602122534 val loss: 0.32122428342700005
7097 MiB free out of 48676 MiB total
epoch 4 loss: 0.01210853907832643 val loss: 0.32228295505046844
7097 MiB free out of 48676 MiB total
37863 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
7097 MiB free out of 48676 MiB total
after cast to cpu
37863 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
29 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.3790, 1.3933, 1.3722,  ..., 1.3567, 1.3229, 1.3394], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.8758, 0.9288, 0.9866,  ..., 1.5234, 1.7083, 1.3679], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 1478.586181640625
Epoch 100 Loss 873.875
Epoch 200 Loss 835.8226318359375
Epoch 300 Loss 826.281494140625
Epoch 400 Loss 823.62255859375
Epoch 500 Loss 821.5831909179688
Epoch 600 Loss 820.0656127929688
Epoch 700 Loss 818.95068359375
Epoch 800 Loss 818.1339721679688
Epoch 900 Loss 817.5339965820312
last loss 817.0947265625
29 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.3682, 1.4166, 1.3816,  ..., 1.4090, 1.3446, 1.3857], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.9166, 0.9375, 0.9786,  ..., 1.5665, 1.7960, 1.9091], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 1511.8740234375
Epoch 100 Loss 914.2203369140625
Epoch 200 Loss 877.7487182617188
Epoch 300 Loss 866.9056396484375
Epoch 400 Loss 863.5556640625
Epoch 500 Loss 862.580322265625
Epoch 600 Loss 861.7777099609375
Epoch 700 Loss 861.133544921875
Epoch 800 Loss 860.6236572265625
Epoch 900 Loss 860.2227172851562
last loss 859.9107666015625
29 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.3328, 1.2889, 1.3329,  ..., 1.2950, 1.3074, 1.3143], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.2784, 1.2410, 1.2657,  ..., 1.2642, 1.2732, 1.2683], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 1765.4881591796875
Epoch 100 Loss 1054.538818359375
Epoch 200 Loss 999.0988159179688
Epoch 300 Loss 982.2347412109375
Epoch 400 Loss 975.7321166992188
Epoch 500 Loss 973.4736328125
Epoch 600 Loss 972.375244140625
Epoch 700 Loss 971.5941162109375
Epoch 800 Loss 971.0391845703125
Epoch 900 Loss 970.6433715820312
last loss 970.3614501953125
29 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.2741, 1.2564, 1.2702,  ..., 1.2577, 1.2643, 1.2598], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.3297, 1.2958, 1.2935,  ..., 1.3316, 1.3571, 1.3266], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 211.1722412109375
Epoch 100 Loss 87.37496948242188
Epoch 200 Loss 76.49366760253906
Epoch 300 Loss 72.70741271972656
Epoch 400 Loss 71.06885528564453
Epoch 500 Loss 70.48965454101562
Epoch 600 Loss 70.29777526855469
Epoch 700 Loss 70.12769317626953
Epoch 800 Loss 69.97975158691406
Epoch 900 Loss 69.85267639160156
last loss 69.74535369873047
29 mlp.gate_proj
Pruning ...
29 mlp.up_proj
Pruning ...
29 mlp.down_proj
Pruning ...
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 153321744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.015407189639518037 val loss: 1.0635968372225761
9153 MiB free out of 48676 MiB total
epoch 1 loss: 0.01413935306482017 val loss: 1.0656139217317104
7097 MiB free out of 48676 MiB total
epoch 2 loss: 0.013465264346450567 val loss: 1.0853481367230415
7097 MiB free out of 48676 MiB total
epoch 3 loss: 0.012960886204382405 val loss: 1.094372745603323
7097 MiB free out of 48676 MiB total
epoch 4 loss: 0.01256177478353493 val loss: 1.1257178373634815
7097 MiB free out of 48676 MiB total
37863 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
7097 MiB free out of 48676 MiB total
after cast to cpu
37863 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
30 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.3731, 1.4261, 1.3994,  ..., 1.3547, 1.3360, 1.3441], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.0392, 1.0339, 1.0422,  ..., 1.4349, 1.4045, 1.4240], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 1709.7437744140625
Epoch 100 Loss 1014.6383666992188
Epoch 200 Loss 962.203125
Epoch 300 Loss 944.9736328125
Epoch 400 Loss 938.6891479492188
Epoch 500 Loss 937.4039306640625
Epoch 600 Loss 936.2637939453125
Epoch 700 Loss 935.2766723632812
Epoch 800 Loss 934.4391479492188
Epoch 900 Loss 933.738037109375
last loss 933.161865234375
30 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.4019, 1.4178, 1.3976,  ..., 1.3864, 1.3602, 1.3972], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.0182, 1.0110, 1.0278,  ..., 1.4493, 1.4746, 1.4525], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 1802.073486328125
Epoch 100 Loss 1077.85791015625
Epoch 200 Loss 1022.571044921875
Epoch 300 Loss 1004.4288330078125
Epoch 400 Loss 996.8795166015625
Epoch 500 Loss 995.2118530273438
Epoch 600 Loss 994.0654296875
Epoch 700 Loss 993.108154296875
Epoch 800 Loss 992.3236694335938
Epoch 900 Loss 991.6881103515625
last loss 991.1808471679688
30 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.3517, 1.3246, 1.3371,  ..., 1.3470, 1.3623, 1.3652], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.3536, 1.3972, 1.3478,  ..., 1.4740, 1.4633, 1.5104], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 2025.42236328125
Epoch 100 Loss 1187.0372314453125
Epoch 200 Loss 1113.505859375
Epoch 300 Loss 1088.9095458984375
Epoch 400 Loss 1083.3460693359375
Epoch 500 Loss 1079.8270263671875
Epoch 600 Loss 1077.04248046875
Epoch 700 Loss 1074.8856201171875
Epoch 800 Loss 1073.2333984375
Epoch 900 Loss 1071.9737548828125
last loss 1071.0223388671875
30 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.3750, 1.4141, 1.3648,  ..., 1.4676, 1.4893, 1.4816], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.3430, 1.3294, 1.3135,  ..., 1.3429, 1.3585, 1.3574], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 190.18600463867188
Epoch 100 Loss 74.911376953125
Epoch 200 Loss 66.6757583618164
Epoch 300 Loss 63.981544494628906
Epoch 400 Loss 63.073028564453125
Epoch 500 Loss 62.85032653808594
Epoch 600 Loss 62.65219497680664
Epoch 700 Loss 62.48088836669922
Epoch 800 Loss 62.335670471191406
Epoch 900 Loss 62.21418380737305
last loss 62.11427688598633
30 mlp.gate_proj
Pruning ...
30 mlp.up_proj
Pruning ...
30 mlp.down_proj
Pruning ...
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 153321744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.014907092088833451 val loss: 68.85916630923748
7105 MiB free out of 48676 MiB total
epoch 1 loss: 0.013091994696878828 val loss: 67.17985831201077
7097 MiB free out of 48676 MiB total
epoch 2 loss: 0.01238267057487974 val loss: 66.73407812416553
7097 MiB free out of 48676 MiB total
epoch 3 loss: 0.011885084961249959 val loss: 66.3874471783638
7097 MiB free out of 48676 MiB total
epoch 4 loss: 0.011501132030389272 val loss: 66.21113426983356
7097 MiB free out of 48676 MiB total
37863 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
7097 MiB free out of 48676 MiB total
after cast to cpu
37863 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
31 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.3578, 1.4003, 1.4142,  ..., 1.4286, 1.3539, 1.3484], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.9185, 0.9745, 1.0542,  ..., 1.5913, 1.5933, 1.6090], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 945.2294311523438
Epoch 100 Loss 567.0609741210938
Epoch 200 Loss 537.711669921875
Epoch 300 Loss 528.028076171875
Epoch 400 Loss 526.1124267578125
Epoch 500 Loss 524.5115966796875
Epoch 600 Loss 523.2078247070312
Epoch 700 Loss 522.16845703125
Epoch 800 Loss 521.3497314453125
Epoch 900 Loss 520.7085571289062
last loss 520.2117309570312
31 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.4408, 1.4562, 1.4368,  ..., 1.4340, 1.4106, 1.4039], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.9789, 0.9707, 1.0469,  ..., 1.6500, 1.6098, 1.6455], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 1076.574951171875
Epoch 100 Loss 635.2109375
Epoch 200 Loss 603.31494140625
Epoch 300 Loss 594.8405151367188
Epoch 400 Loss 592.4000244140625
Epoch 500 Loss 590.4132080078125
Epoch 600 Loss 588.8477172851562
Epoch 700 Loss 587.6337890625
Epoch 800 Loss 586.6990966796875
Epoch 900 Loss 585.9801635742188
last loss 585.43115234375
31 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.1832, 1.2165, 1.2963,  ..., 1.1931, 1.2608, 1.2214], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.1387, 1.1403, 1.1530,  ..., 1.1925, 1.2135, 1.2031], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 1134.2642822265625
Epoch 100 Loss 644.1063842773438
Epoch 200 Loss 602.86572265625
Epoch 300 Loss 594.5123901367188
Epoch 400 Loss 588.7313842773438
Epoch 500 Loss 584.744873046875
Epoch 600 Loss 582.0162353515625
Epoch 700 Loss 580.139892578125
Epoch 800 Loss 578.8360595703125
Epoch 900 Loss 577.9183349609375
last loss 577.4550170898438
31 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.2308, 1.2274, 1.2312,  ..., 1.2517, 1.2610, 1.2420], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.1774, 1.2117, 1.2020,  ..., 1.2123, 1.2554, 1.2226], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 183.34120178222656
Epoch 100 Loss 64.36955261230469
Epoch 200 Loss 53.248504638671875
Epoch 300 Loss 49.53417205810547
Epoch 400 Loss 48.40638732910156
Epoch 500 Loss 47.43947982788086
Epoch 600 Loss 46.62947082519531
Epoch 700 Loss 45.957862854003906
Epoch 800 Loss 45.40291976928711
Epoch 900 Loss 44.94444274902344
last loss 44.56854248046875
31 mlp.gate_proj
Pruning ...
31 mlp.up_proj
Pruning ...
31 mlp.down_proj
Pruning ...
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 153321744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.020367077268019784 val loss: 11.592989921569824
9151 MiB free out of 48676 MiB total
epoch 1 loss: 0.016244311365881003 val loss: 11.63081169128418
7095 MiB free out of 48676 MiB total
epoch 2 loss: 0.015340474201366305 val loss: 11.532284021377563
7095 MiB free out of 48676 MiB total
epoch 3 loss: 0.014815634429396596 val loss: 11.441424190998077
7095 MiB free out of 48676 MiB total
epoch 4 loss: 0.01414606438629562 val loss: 12.314963638782501
7095 MiB free out of 48676 MiB total
37863 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
7095 MiB free out of 48676 MiB total
after cast to cpu
37863 MiB free out of 48676 MiB total
Total bits: tensor(16184856576, device='cuda:7') Total params: 3758096384
average bits per value: tensor(4.3067, device='cuda:7')
total time taken: 8660.580061912537
Loading tokenizer for meta-llama/Llama-2-7b-hf
/home/lliu/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Dataset: wikitext2
Evaluating ...
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
Perplexity: 8.249492
