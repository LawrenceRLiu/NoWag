/home/lliu/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.33s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.19it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.10it/s]
Model loaded. LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
4096
Loading tokenizer for meta-llama/Llama-2-7b-hf
Starting...
39011 MiB free out of 48676 MiB total
attention_mask torch.Size([1, 1, 4096, 4096]) cuda:4 torch.float16
position_ids torch.Size([1, 4096]) cuda:4 torch.int64
position_ids torch.Size([1, 4096])
attention_mask torch.Size([1, 1, 4096, 4096])
Ready.
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
0 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.1756, 0.8264, 0.3096,  ..., 0.5891, 0.6551, 0.5814], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.0278, 1.4363, 1.2937,  ..., 1.3905, 1.6662, 0.9913], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 0.5647948980331421
Epoch 100 Loss 0.01948622241616249
Epoch 200 Loss 0.012474386021494865
Epoch 300 Loss 0.009907836094498634
Epoch 400 Loss 0.008533116430044174
Epoch 500 Loss 0.007658475544303656
Epoch 600 Loss 0.007044813130050898
Epoch 700 Loss 0.006586934439837933
Epoch 800 Loss 0.00622925441712141
Epoch 900 Loss 0.005942048504948616
last loss 0.00570797361433506
0 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([0.7236, 0.7618, 0.5036,  ..., 0.7654, 0.7673, 0.6849], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.4949, 0.5030, 0.4779,  ..., 0.4897, 0.5060, 0.5031], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 0.5511637330055237
Epoch 100 Loss 0.09089741110801697
Epoch 200 Loss 0.08062075823545456
Epoch 300 Loss 0.07803510129451752
Epoch 400 Loss 0.07657445967197418
Epoch 500 Loss 0.07560421526432037
Epoch 600 Loss 0.07495704293251038
Epoch 700 Loss 0.07452075928449631
Epoch 800 Loss 0.07436943054199219
Epoch 900 Loss 0.07425293326377869
last loss 0.07414890825748444
0 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([0.8849, 0.6529, 0.2560,  ..., 0.5371, 0.5559, 0.4512], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.6149, 1.0973, 1.2322,  ..., 1.2838, 1.3711, 0.9672], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 0.6543768048286438
Epoch 100 Loss 0.014374969527125359
Epoch 200 Loss 0.009946165606379509
Epoch 300 Loss 0.008288619108498096
Epoch 400 Loss 0.007363418582826853
Epoch 500 Loss 0.006958945654332638
Epoch 600 Loss 0.006689778994768858
Epoch 700 Loss 0.006451140157878399
Epoch 800 Loss 0.006239881739020348
Epoch 900 Loss 0.006052569020539522
last loss 0.0058875735849142075
0 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([0.2801, 0.2821, 0.2653,  ..., 0.2623, 0.2674, 0.2648], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.4009, 0.4159, 0.4015,  ..., 0.4328, 0.4183, 0.4196], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 0.05561743676662445
Epoch 100 Loss 0.0021230499260127544
Epoch 200 Loss 0.0012579972390085459
Epoch 300 Loss 0.00103358319029212
Epoch 400 Loss 0.000896136334631592
Epoch 500 Loss 0.0007891742279753089
Epoch 600 Loss 0.0007049960549920797
Epoch 700 Loss 0.0006376685341820121
Epoch 800 Loss 0.0005875038914382458
Epoch 900 Loss 0.0005638598231598735
last loss 0.0005417225765995681
0 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 85737744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 5.614155122657394e-05 val loss: 6.728437119818409e-05
9021 MiB free out of 48676 MiB total
epoch 1 loss: 3.310613311668931e-05 val loss: 8.579643917983049e-05
9013 MiB free out of 48676 MiB total
epoch 2 loss: 2.375760685424666e-05 val loss: 9.747366721057915e-05
9013 MiB free out of 48676 MiB total
epoch 3 loss: 1.7546029489778903e-05 val loss: 0.00010669672019503196
9013 MiB free out of 48676 MiB total
epoch 4 loss: 1.3174780988833845e-05 val loss: 0.00011411855439291685
9013 MiB free out of 48676 MiB total
epoch 5 loss: 1.0130946655806383e-05 val loss: 0.00012003016718153958
9013 MiB free out of 48676 MiB total
early stopping
39011 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
9013 MiB free out of 48676 MiB total
after cast to cpu
37541 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
1 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([2.0254, 1.9664, 2.0364,  ..., 1.6627, 1.8973, 1.8084], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.5419, 1.7741, 1.8750,  ..., 0.6519, 0.6390, 0.5201], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 14.037137985229492
Epoch 100 Loss 2.3694636821746826
Epoch 200 Loss 2.190207004547119
Epoch 300 Loss 2.1634585857391357
Epoch 400 Loss 2.143167495727539
Epoch 500 Loss 2.1282832622528076
Epoch 600 Loss 2.117537260055542
Epoch 700 Loss 2.109776020050049
Epoch 800 Loss 2.1041343212127686
Epoch 900 Loss 2.1000022888183594
last loss 2.0969815254211426
1 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([0.5438, 0.5367, 0.5507,  ..., 0.6505, 0.5752, 0.6442], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.8513, 0.8723, 0.8107,  ..., 0.3544, 0.3659, 0.3639], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 4.2599406242370605
Epoch 100 Loss 1.4272732734680176
Epoch 200 Loss 1.3342201709747314
Epoch 300 Loss 1.3242137432098389
Epoch 400 Loss 1.3129879236221313
Epoch 500 Loss 1.277026653289795
Epoch 600 Loss 0.9909795522689819
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
Epoch 700 Loss -2.569504976272583
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
Epoch 800 Loss -28.738384246826172
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
Epoch 900 Loss -117.93058776855469
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
last loss -306.66571044921875
1 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.9255, 1.9044, 1.8602,  ..., 1.6792, 1.9344, 1.6705], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.9452, 1.6639, 1.9007,  ..., 0.3805, 0.3928, 0.4772], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 12.097909927368164
Epoch 100 Loss 2.5260825157165527
Epoch 200 Loss 2.351112127304077
Epoch 300 Loss 2.314056396484375
Epoch 400 Loss 2.29805064201355
Epoch 500 Loss 2.287790536880493
Epoch 600 Loss 2.2811460494995117
Epoch 700 Loss 2.2767186164855957
Epoch 800 Loss 2.2738752365112305
Epoch 900 Loss 2.2727017402648926
last loss 2.2716872692108154
1 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([0.8602, 0.8032, 0.6896,  ..., 0.2113, 0.2166, 0.2192], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.5290, 0.5332, 0.5174,  ..., 0.5289, 0.5185, 0.5111], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 0.42770031094551086
Epoch 100 Loss 0.1295493245124817
Epoch 200 Loss 0.08593136072158813
Epoch 300 Loss 0.06725218147039413
Epoch 400 Loss 0.06341059505939484
Epoch 500 Loss 0.060253314673900604
Epoch 600 Loss 0.05745624005794525
Epoch 700 Loss 0.055046673864126205
Epoch 800 Loss 0.05301205441355705
Epoch 900 Loss 0.05131884664297104
last loss 0.04993690550327301
1 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 85737744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0017800539807240057 val loss: 0.03596139186993241
8831 MiB free out of 48676 MiB total
epoch 1 loss: 0.00035983815405415953 val loss: 0.025414334959350526
8823 MiB free out of 48676 MiB total
epoch 2 loss: 0.000177992945339156 val loss: 0.013435131986625493
8823 MiB free out of 48676 MiB total
epoch 3 loss: 0.0001160322575515238 val loss: 0.007534101925557479
8823 MiB free out of 48676 MiB total
epoch 4 loss: 8.807543932221051e-05 val loss: 0.005041771044488996
8823 MiB free out of 48676 MiB total
epoch 5 loss: 7.183127695498115e-05 val loss: 0.0036890276678605005
8823 MiB free out of 48676 MiB total
epoch 6 loss: 6.17739901827008e-05 val loss: 0.0030471443315036595
8823 MiB free out of 48676 MiB total
epoch 7 loss: 5.519210765214666e-05 val loss: 0.0027017734246328473
8823 MiB free out of 48676 MiB total
epoch 8 loss: 5.0006347407816065e-05 val loss: 0.0024999433517223224
8823 MiB free out of 48676 MiB total
epoch 9 loss: 4.601729582986991e-05 val loss: 0.002411610577837564
8823 MiB free out of 48676 MiB total
37541 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8823 MiB free out of 48676 MiB total
after cast to cpu
37863 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
2 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.7840, 1.7515, 1.8243,  ..., 1.7953, 1.7517, 1.7821], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.7197, 1.3612, 1.5470,  ..., 2.5627, 0.6336, 2.3330], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 125.56075286865234
Epoch 100 Loss 49.499263763427734
Epoch 200 Loss 46.02207946777344
Epoch 300 Loss 44.664527893066406
Epoch 400 Loss 44.07539367675781
Epoch 500 Loss 43.898277282714844
Epoch 600 Loss 43.743995666503906
Epoch 700 Loss 43.61162567138672
Epoch 800 Loss 43.499237060546875
Epoch 900 Loss 43.40434265136719
last loss 43.3251953125
2 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([0.9290, 0.9331, 0.9195,  ..., 0.9050, 0.9574, 0.9601], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.9509, 0.9649, 0.9703,  ..., 0.8383, 0.9783, 0.9524], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 58.02021408081055
Epoch 100 Loss 25.02686882019043
Epoch 200 Loss 22.661523818969727
Epoch 300 Loss 21.700340270996094
Epoch 400 Loss 21.21651268005371
Epoch 500 Loss 21.016765594482422
Epoch 600 Loss 20.927412033081055
Epoch 700 Loss 20.850461959838867
Epoch 800 Loss 20.7847957611084
Epoch 900 Loss 20.728466033935547
last loss 20.67877960205078
2 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.6843, 1.7193, 1.6812,  ..., 1.7487, 1.6237, 1.6486], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.0094, 1.2899, 1.5509,  ..., 2.1117, 0.7452, 2.0198], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 144.22056579589844
Epoch 100 Loss 46.31389617919922
Epoch 200 Loss 42.832088470458984
Epoch 300 Loss 41.45235824584961
Epoch 400 Loss 40.827796936035156
Epoch 500 Loss 40.67822265625
Epoch 600 Loss 40.536865234375
Epoch 700 Loss 40.406673431396484
Epoch 800 Loss 40.288902282714844
Epoch 900 Loss 40.183555603027344
last loss 40.09074401855469
2 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([0.9219, 0.9821, 0.9457,  ..., 0.9478, 0.9625, 0.9246], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.8827, 0.8924, 0.8919,  ..., 0.8741, 0.8675, 0.8812], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 0.9672638773918152
Epoch 100 Loss 0.16753889620304108
Epoch 200 Loss 0.12909825146198273
Epoch 300 Loss 0.11150329560041428
Epoch 400 Loss 0.10132545232772827
Epoch 500 Loss 0.09462974965572357
Epoch 600 Loss 0.09205656498670578
Epoch 700 Loss 0.09000968188047409
Epoch 800 Loss 0.08814035356044769
Epoch 900 Loss 0.08644480258226395
last loss 0.0849277526140213
2 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 85737744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0001705435118992682 val loss: 0.0018756462741293944
8961 MiB free out of 48676 MiB total
epoch 1 loss: 9.312866836808098e-05 val loss: 0.0018660172645468265
8953 MiB free out of 48676 MiB total
epoch 2 loss: 6.709466146048726e-05 val loss: 0.0018604399156174622
8953 MiB free out of 48676 MiB total
epoch 3 loss: 5.3252270163284265e-05 val loss: 0.0018636649620020762
8953 MiB free out of 48676 MiB total
epoch 4 loss: 4.5467476269323015e-05 val loss: 0.0018736162892309949
8953 MiB free out of 48676 MiB total
epoch 5 loss: 4.070458507499097e-05 val loss: 0.0018885490353568457
8953 MiB free out of 48676 MiB total
epoch 6 loss: 3.7432600407782957e-05 val loss: 0.001907736026623752
8953 MiB free out of 48676 MiB total
epoch 7 loss: 3.499180789390266e-05 val loss: 0.001930565464135725
8953 MiB free out of 48676 MiB total
early stopping
37863 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8953 MiB free out of 48676 MiB total
after cast to cpu
37863 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
3 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.6463, 1.6737, 1.6654,  ..., 1.6477, 1.6560, 1.6845], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.1294, 1.3448, 1.3066,  ..., 2.7185, 2.9419, 3.0465], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 388.7323303222656
Epoch 100 Loss 145.9695587158203
Epoch 200 Loss 132.38885498046875
Epoch 300 Loss 126.48954772949219
Epoch 400 Loss 123.260986328125
Epoch 500 Loss 121.28398132324219
Epoch 600 Loss 120.55952453613281
Epoch 700 Loss 120.01358032226562
Epoch 800 Loss 119.53248596191406
Epoch 900 Loss 119.1120376586914
last loss 118.7498779296875
3 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([0.8999, 0.8761, 0.8859,  ..., 0.8799, 0.8948, 0.8878], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.8146, 0.8202, 0.8095,  ..., 0.4050, 0.4349, 0.3835], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 140.8114013671875
Epoch 100 Loss 55.35660934448242
Epoch 200 Loss 50.590396881103516
Epoch 300 Loss 48.943763732910156
Epoch 400 Loss 47.587154388427734
Epoch 500 Loss 46.49015808105469
Epoch 600 Loss 45.59513854980469
Epoch 700 Loss 44.82707977294922
Epoch 800 Loss 44.233909606933594
Epoch 900 Loss 44.0881462097168
last loss 43.8942985534668
3 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.5567, 1.6199, 1.6101,  ..., 1.6199, 1.6003, 1.6185], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.1710, 1.3686, 1.3253,  ..., 2.6906, 2.8910, 3.0339], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 413.0696716308594
Epoch 100 Loss 127.99111938476562
Epoch 200 Loss 116.47325134277344
Epoch 300 Loss 111.79174041748047
Epoch 400 Loss 110.79617309570312
Epoch 500 Loss 109.87895965576172
Epoch 600 Loss 109.04824829101562
Epoch 700 Loss 108.30836486816406
Epoch 800 Loss 107.65547943115234
Epoch 900 Loss 107.08238220214844
last loss 106.58544921875
3 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([0.8149, 0.8113, 0.7834,  ..., 0.3370, 0.3588, 0.3251], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.8344, 0.8513, 0.8366,  ..., 0.8515, 0.8315, 0.8545], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 33.328758239746094
Epoch 100 Loss 2.38397216796875
Epoch 200 Loss 1.7040807008743286
Epoch 300 Loss 1.5675489902496338
Epoch 400 Loss 1.4412076473236084
Epoch 500 Loss 1.328734278678894
Epoch 600 Loss 1.2300742864608765
Epoch 700 Loss 1.14383065700531
Epoch 800 Loss 1.068373441696167
Epoch 900 Loss 1.0021915435791016
last loss 0.9445126056671143
3 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 85737744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0007990018352757033 val loss: 0.005230928858509287
8961 MiB free out of 48676 MiB total
epoch 1 loss: 0.00041070103100082633 val loss: 0.005187546863453463
8953 MiB free out of 48676 MiB total
epoch 2 loss: 0.0003313900601824571 val loss: 0.00516115443315357
8953 MiB free out of 48676 MiB total
epoch 3 loss: 0.00029644190453836927 val loss: 0.00514221319463104
8953 MiB free out of 48676 MiB total
epoch 4 loss: 0.0002779841867095456 val loss: 0.005127284704940394
8953 MiB free out of 48676 MiB total
epoch 5 loss: 0.00027212567704282264 val loss: 0.0051074510847684
8953 MiB free out of 48676 MiB total
epoch 6 loss: 0.0002752257126985569 val loss: 0.005101036542328075
8953 MiB free out of 48676 MiB total
epoch 7 loss: 0.000244793565798318 val loss: 0.00508999775047414
8953 MiB free out of 48676 MiB total
epoch 8 loss: 0.00023827941322451807 val loss: 0.005081079376395792
8953 MiB free out of 48676 MiB total
epoch 9 loss: 0.00022690503442390764 val loss: 0.005072761356132105
8953 MiB free out of 48676 MiB total
37863 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8953 MiB free out of 48676 MiB total
after cast to cpu
37863 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
4 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.6928, 1.7265, 1.7171,  ..., 1.6686, 1.7117, 1.7124], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.9987, 1.3206, 1.3897,  ..., 2.4271, 2.2609, 2.4617], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 517.09619140625
Epoch 100 Loss 46.20406723022461
Epoch 200 Loss 35.26581573486328
Epoch 300 Loss 30.501941680908203
Epoch 400 Loss 27.879016876220703
Epoch 500 Loss 27.10617446899414
Epoch 600 Loss 26.405515670776367
Epoch 700 Loss 25.756690979003906
Epoch 800 Loss 25.16033172607422
Epoch 900 Loss 24.6137752532959
last loss 24.1177978515625
4 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([0.9338, 0.9049, 0.9457,  ..., 0.9385, 0.9042, 0.9126], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.8676, 0.8579, 0.8606,  ..., 0.9248, 0.9169, 0.9350], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 213.1102294921875
Epoch 100 Loss 21.47220802307129
Epoch 200 Loss 16.443559646606445
Epoch 300 Loss 14.104730606079102
Epoch 400 Loss 13.670876502990723
Epoch 500 Loss 13.271551132202148
Epoch 600 Loss 12.889342308044434
Epoch 700 Loss 12.530069351196289
Epoch 800 Loss 12.195793151855469
Epoch 900 Loss 11.886478424072266
last loss 11.60367202758789
4 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.6591, 1.6807, 1.6291,  ..., 1.6611, 1.7065, 1.7137], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.9198, 1.3477, 1.4289,  ..., 2.3017, 2.2600, 2.2052], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 457.6978759765625
Epoch 100 Loss 38.96047592163086
Epoch 200 Loss 30.31876564025879
Epoch 300 Loss 26.492473602294922
Epoch 400 Loss 24.248653411865234
Epoch 500 Loss 22.807456970214844
Epoch 600 Loss 22.247337341308594
Epoch 700 Loss 21.80489730834961
Epoch 800 Loss 21.389995574951172
Epoch 900 Loss 21.003170013427734
last loss 20.646976470947266
4 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([0.8375, 0.8274, 0.8368,  ..., 0.9084, 0.9081, 0.9244], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.8678, 0.8990, 0.9099,  ..., 0.8872, 0.8625, 0.8959], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 155.72286987304688
Epoch 100 Loss 9.12868595123291
Epoch 200 Loss 5.555816650390625
Epoch 300 Loss 4.096755027770996
Epoch 400 Loss 3.492933988571167
Epoch 500 Loss 3.101210117340088
Epoch 600 Loss 2.7799835205078125
Epoch 700 Loss 2.5151259899139404
Epoch 800 Loss 2.2944319248199463
Epoch 900 Loss 2.108344078063965
last loss 1.9854562282562256
4 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 85737744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0025130519088634173 val loss: 0.0056150874006561935
8897 MiB free out of 48676 MiB total
epoch 1 loss: 0.001040715887938859 val loss: 0.0055834743834566325
8889 MiB free out of 48676 MiB total
epoch 2 loss: 0.0007843379371479386 val loss: 0.005568398133618757
8889 MiB free out of 48676 MiB total
epoch 3 loss: 0.00065907259386222 val loss: 0.0055504026531707495
8889 MiB free out of 48676 MiB total
epoch 4 loss: 0.0006402249966868112 val loss: 0.00553892130847089
8889 MiB free out of 48676 MiB total
epoch 5 loss: 0.0006518958768992889 val loss: 0.005546742409933358
8889 MiB free out of 48676 MiB total
epoch 6 loss: 0.0006100713324030949 val loss: 0.005527893343241885
8889 MiB free out of 48676 MiB total
epoch 7 loss: 0.0005127022275246418 val loss: 0.005518353747902438
8889 MiB free out of 48676 MiB total
epoch 8 loss: 0.0005125526022311533 val loss: 0.0055074868723750114
8889 MiB free out of 48676 MiB total
epoch 9 loss: 0.000518548272566477 val loss: 0.005504803324583918
8889 MiB free out of 48676 MiB total
37863 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8889 MiB free out of 48676 MiB total
after cast to cpu
37863 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
5 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.7228, 1.7929, 1.7542,  ..., 1.6983, 1.7302, 1.7280], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.9647, 1.0825, 1.1445,  ..., 1.4308, 2.6816, 2.3936], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 573.490234375
Epoch 100 Loss 25.537437438964844
Epoch 200 Loss 16.836841583251953
Epoch 300 Loss 13.900397300720215
Epoch 400 Loss 13.327991485595703
Epoch 500 Loss 12.783525466918945
Epoch 600 Loss 12.275392532348633
Epoch 700 Loss 11.808406829833984
Epoch 800 Loss 11.382450103759766
Epoch 900 Loss 10.994897842407227
last loss 10.645548820495605
5 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([0.9815, 0.9241, 0.9573,  ..., 0.9756, 0.9689, 0.9269], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.9928, 0.9724, 0.9818,  ..., 0.8969, 0.9049, 0.9102], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 245.3741455078125
Epoch 100 Loss 14.7692232131958
Epoch 200 Loss 9.50767993927002
Epoch 300 Loss 7.402218818664551
Epoch 400 Loss 6.477677822113037
Epoch 500 Loss 6.14415979385376
Epoch 600 Loss 5.840096473693848
Epoch 700 Loss 5.566459655761719
Epoch 800 Loss 5.321582794189453
Epoch 900 Loss 5.102665901184082
last loss 4.90848445892334
5 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.6849, 1.6923, 1.6421,  ..., 1.6521, 1.6664, 1.7371], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.0021, 1.0970, 1.1329,  ..., 1.7344, 1.9421, 1.6516], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 544.5665283203125
Epoch 100 Loss 21.625568389892578
Epoch 200 Loss 13.942520141601562
Epoch 300 Loss 9.929603576660156
Epoch 400 Loss 3.356407403945923
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
Epoch 500 Loss -12.152423858642578
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
Epoch 600 Loss -16.191221237182617
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
Epoch 700 Loss -21.65811538696289
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
Epoch 800 Loss -29.243099212646484
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
Epoch 900 Loss -40.046016693115234
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
last loss -55.67957305908203
5 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([0.9817, 0.9510, 0.9640,  ..., 0.8447, 0.8656, 0.8768], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.9241, 0.9280, 0.9083,  ..., 0.9133, 0.8996, 0.9203], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 226.81375122070312
Epoch 100 Loss 13.13734245300293
Epoch 200 Loss 9.054152488708496
Epoch 300 Loss 7.8795623779296875
Epoch 400 Loss 6.876474857330322
Epoch 500 Loss 6.05230712890625
Epoch 600 Loss 5.381219387054443
Epoch 700 Loss 4.832540988922119
Epoch 800 Loss 4.379687309265137
Epoch 900 Loss 4.001663684844971
last loss 3.6852619647979736
5 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 85737744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.004905781755041971 val loss: 0.006999989156611264
8897 MiB free out of 48676 MiB total
epoch 1 loss: 0.0018079097199006355 val loss: 0.006919204548466951
8889 MiB free out of 48676 MiB total
epoch 2 loss: 0.0013015070521760208 val loss: 0.006877477397210896
8889 MiB free out of 48676 MiB total
epoch 3 loss: 0.001089088388653181 val loss: 0.006848369026556611
8889 MiB free out of 48676 MiB total
epoch 4 loss: 0.0009649735552557104 val loss: 0.0068323801388032734
8889 MiB free out of 48676 MiB total
epoch 5 loss: 0.0009034994754983927 val loss: 0.006815846107201651
8889 MiB free out of 48676 MiB total
epoch 6 loss: 0.0009075581992874504 val loss: 0.006811109546106309
8889 MiB free out of 48676 MiB total
epoch 7 loss: 0.0008595567410338845 val loss: 0.006800426286645234
8889 MiB free out of 48676 MiB total
epoch 8 loss: 0.0008052915536609362 val loss: 0.006795755703933537
8889 MiB free out of 48676 MiB total
epoch 9 loss: 0.0007640069420631335 val loss: 0.006791005755076185
8889 MiB free out of 48676 MiB total
37863 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8889 MiB free out of 48676 MiB total
after cast to cpu
37863 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
6 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.6115, 1.6701, 1.6337,  ..., 1.6253, 1.6097, 1.6225], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.2987, 1.3126, 1.3289,  ..., 2.0024, 2.0803, 2.1513], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 847.2520141601562
Epoch 100 Loss 37.84933090209961
Epoch 200 Loss 23.31406021118164
Epoch 300 Loss 17.760051727294922
Epoch 400 Loss 16.15460777282715
Epoch 500 Loss 15.30606460571289
Epoch 600 Loss 14.521066665649414
Epoch 700 Loss 13.805191040039062
Epoch 800 Loss 13.156563758850098
Epoch 900 Loss 12.570037841796875
last loss 12.044370651245117
6 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([0.9260, 0.8385, 0.8766,  ..., 0.9176, 0.8962, 0.8614], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.9808, 0.9636, 0.9873,  ..., 0.9508, 0.9518, 0.9481], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 315.4212341308594
Epoch 100 Loss 19.41764259338379
Epoch 200 Loss 11.874700546264648
Epoch 300 Loss 9.536548614501953
Epoch 400 Loss 8.737969398498535
Epoch 500 Loss 8.031497955322266
Epoch 600 Loss 7.4209699630737305
Epoch 700 Loss 6.897995471954346
Epoch 800 Loss 6.450231552124023
Epoch 900 Loss 6.065251350402832
last loss 5.735119819641113
6 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.5404, 1.5738, 1.6013,  ..., 1.5337, 1.5906, 1.6066], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.3378, 1.3558, 1.3186,  ..., 2.1034, 2.0254, 2.1746], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 758.3523559570312
Epoch 100 Loss 29.858766555786133
Epoch 200 Loss 18.561182022094727
Epoch 300 Loss 14.135319709777832
Epoch 400 Loss 12.436817169189453
Epoch 500 Loss 11.674520492553711
Epoch 600 Loss 10.896293640136719
Epoch 700 Loss 10.022059440612793
Epoch 800 Loss 8.777026176452637
Epoch 900 Loss 5.7599077224731445
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
breaking because the loss is negative
last loss -1.900724172592163
6 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([0.9574, 0.9658, 0.9658,  ..., 0.9132, 0.9029, 0.9161], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.8659, 0.8919, 0.8469,  ..., 0.8721, 0.8547, 0.8473], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 274.71478271484375
Epoch 100 Loss 16.104202270507812
Epoch 200 Loss 11.58250617980957
Epoch 300 Loss 9.800577163696289
Epoch 400 Loss 8.367209434509277
Epoch 500 Loss 7.237372398376465
Epoch 600 Loss 6.343107223510742
Epoch 700 Loss 5.627588272094727
Epoch 800 Loss 5.0480122566223145
Epoch 900 Loss 4.572486877441406
last loss 4.180777549743652
6 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 85737744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.009986215869503212 val loss: 0.006993532762862742
8961 MiB free out of 48676 MiB total
epoch 1 loss: 0.003522818217788881 val loss: 0.006985824904404581
8953 MiB free out of 48676 MiB total
epoch 2 loss: 0.002278702761941531 val loss: 0.006979203957598656
8953 MiB free out of 48676 MiB total
epoch 3 loss: 0.0018237085578221013 val loss: 0.006969080190174282
8953 MiB free out of 48676 MiB total
epoch 4 loss: 0.0015375212910839764 val loss: 0.006961846316698939
8953 MiB free out of 48676 MiB total
epoch 5 loss: 0.0013465539946082572 val loss: 0.006956523982807994
8953 MiB free out of 48676 MiB total
epoch 6 loss: 0.0013605625154013978 val loss: 0.0069532645284198225
8953 MiB free out of 48676 MiB total
epoch 7 loss: 0.0012136101913711173 val loss: 0.006944072403712198
8953 MiB free out of 48676 MiB total
epoch 8 loss: 0.001191634051338042 val loss: 0.006944262742763385
8953 MiB free out of 48676 MiB total
epoch 9 loss: 0.0011489269677440461 val loss: 0.006947281479369849
8953 MiB free out of 48676 MiB total
37863 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8953 MiB free out of 48676 MiB total
after cast to cpu
37863 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
7 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.5728, 1.6454, 1.5849,  ..., 1.5643, 1.5614, 1.5966], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.7458, 0.7266, 0.7555,  ..., 1.9018, 2.3899, 2.3772], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 939.2325439453125
Epoch 100 Loss 40.605712890625
Epoch 200 Loss 23.994508743286133
Epoch 300 Loss 17.582178115844727
Epoch 400 Loss 16.065147399902344
Epoch 500 Loss 14.924811363220215
Epoch 600 Loss 13.91189956665039
Epoch 700 Loss 13.022067070007324
Epoch 800 Loss 12.242708206176758
Epoch 900 Loss 11.559030532836914
last loss 10.96258544921875
7 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([0.9560, 0.8397, 0.8979,  ..., 0.9439, 0.9081, 0.8830], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.9850, 0.9593, 0.9277,  ..., 0.8599, 0.8627, 0.8792], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 363.1707763671875
Epoch 100 Loss 17.122211456298828
Epoch 200 Loss 11.06714153289795
Epoch 300 Loss 9.753664016723633
Epoch 400 Loss 8.635283470153809
Epoch 500 Loss 7.7163496017456055
Epoch 600 Loss 6.969370365142822
Epoch 700 Loss 6.36125373840332
Epoch 800 Loss 5.862305641174316
Epoch 900 Loss 5.448288917541504
last loss 5.103515148162842
7 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.5591, 1.5747, 1.5777,  ..., 1.5886, 1.5985, 1.6013], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.7524, 0.7359, 0.7657,  ..., 2.0297, 2.7848, 2.7748], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 884.146240234375
Epoch 100 Loss 35.40643310546875
Epoch 200 Loss 20.965740203857422
Epoch 300 Loss 15.312278747558594
Epoch 400 Loss 12.830029487609863
Epoch 500 Loss 11.981945037841797
Epoch 600 Loss 11.18989086151123
Epoch 700 Loss 10.43098258972168
Epoch 800 Loss 9.640531539916992
Epoch 900 Loss 8.610784530639648
last loss 6.286087989807129
7 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.0083, 0.9239, 0.9096,  ..., 0.8438, 0.8473, 0.8590], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.8733, 0.9017, 0.8406,  ..., 0.8633, 0.8822, 0.8546], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 306.57403564453125
Epoch 100 Loss 18.963363647460938
Epoch 200 Loss 14.407286643981934
Epoch 300 Loss 11.398590087890625
Epoch 400 Loss 9.355655670166016
Epoch 500 Loss 7.900725841522217
Epoch 600 Loss 6.820611476898193
Epoch 700 Loss 6.023953914642334
Epoch 800 Loss 5.752258777618408
Epoch 900 Loss 5.509218692779541
last loss 5.277210712432861
7 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 85737744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.009167654186967411 val loss: 0.010558704612776637
8961 MiB free out of 48676 MiB total
epoch 1 loss: 0.0036750035023942473 val loss: 0.010527390462812036
8953 MiB free out of 48676 MiB total
epoch 2 loss: 0.0024359421395274694 val loss: 0.010507794911973178
8953 MiB free out of 48676 MiB total
epoch 3 loss: 0.001919248617014091 val loss: 0.010491538152564317
8953 MiB free out of 48676 MiB total
epoch 4 loss: 0.0016671405423949182 val loss: 0.010478048759978265
8953 MiB free out of 48676 MiB total
epoch 5 loss: 0.0014852827944196179 val loss: 0.010437537159305066
8953 MiB free out of 48676 MiB total
epoch 6 loss: 0.0013255946596473223 val loss: 0.010409795388113707
8953 MiB free out of 48676 MiB total
epoch 7 loss: 0.001241931377990113 val loss: 0.010365483409259468
8953 MiB free out of 48676 MiB total
epoch 8 loss: 0.0012010096970698214 val loss: 0.010331569879781455
8953 MiB free out of 48676 MiB total
epoch 9 loss: 0.0012419422337188735 val loss: 0.010325139330234379
8953 MiB free out of 48676 MiB total
37863 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8953 MiB free out of 48676 MiB total
after cast to cpu
37863 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
8 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.6222, 1.6256, 1.6392,  ..., 1.5824, 1.5826, 1.6133], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.8323, 1.0179, 0.9911,  ..., 1.9226, 2.3946, 2.4478], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 929.30908203125
Epoch 100 Loss 40.59419631958008
Epoch 200 Loss 23.653587341308594
Epoch 300 Loss 17.060436248779297
Epoch 400 Loss 15.083292007446289
Epoch 500 Loss 14.039731979370117
Epoch 600 Loss 13.106414794921875
Epoch 700 Loss 12.282459259033203
Epoch 800 Loss 11.558103561401367
Epoch 900 Loss 10.9207181930542
last loss 10.363018035888672
8 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([0.9597, 0.8698, 0.8955,  ..., 0.9706, 0.9312, 0.9159], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.0011, 1.0007, 0.9877,  ..., 0.8776, 0.8515, 0.8656], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 338.5971374511719
Epoch 100 Loss 17.32434844970703
Epoch 200 Loss 10.612236976623535
Epoch 300 Loss 9.450640678405762
Epoch 400 Loss 8.419832229614258
Epoch 500 Loss 7.546665191650391
Epoch 600 Loss 6.821387767791748
Epoch 700 Loss 6.22197151184082
Epoch 800 Loss 5.725038528442383
Epoch 900 Loss 5.309875965118408
last loss 4.962644577026367
8 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.5808, 1.6306, 1.5969,  ..., 1.5672, 1.5837, 1.5975], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.8247, 1.0033, 0.9929,  ..., 2.3955, 3.1897, 3.1726], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 890.2847290039062
Epoch 100 Loss 35.20329666137695
Epoch 200 Loss 20.30518341064453
Epoch 300 Loss 14.595974922180176
Epoch 400 Loss 13.202362060546875
Epoch 500 Loss 12.393115043640137
Epoch 600 Loss 11.638301849365234
Epoch 700 Loss 10.941255569458008
Epoch 800 Loss 10.294448852539062
Epoch 900 Loss 9.680258750915527
last loss 9.070656776428223
8 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([0.9737, 0.9628, 0.9414,  ..., 0.8193, 0.8024, 0.8064], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.9048, 0.9030, 0.8637,  ..., 0.8852, 0.8910, 0.8782], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 283.95843505859375
Epoch 100 Loss 18.741195678710938
Epoch 200 Loss 14.263213157653809
Epoch 300 Loss 11.141093254089355
Epoch 400 Loss 9.007813453674316
Epoch 500 Loss 7.515983581542969
Epoch 600 Loss 6.428624153137207
Epoch 700 Loss 5.605928421020508
Epoch 800 Loss 5.272027969360352
Epoch 900 Loss 5.020391464233398
last loss 4.783420562744141
8 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 85737744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.011737084019841859 val loss: 0.011056805145926774
8961 MiB free out of 48676 MiB total
epoch 1 loss: 0.004413184033182915 val loss: 0.01105417450889945
8953 MiB free out of 48676 MiB total
epoch 2 loss: 0.0028208302273924346 val loss: 0.011046972998883575
8953 MiB free out of 48676 MiB total
epoch 3 loss: 0.0021425160239232355 val loss: 0.011035610630642623
8953 MiB free out of 48676 MiB total
epoch 4 loss: 0.001750932937284233 val loss: 0.011015151860192418
8953 MiB free out of 48676 MiB total
epoch 5 loss: 0.001531848594368057 val loss: 0.010986589710228145
8953 MiB free out of 48676 MiB total
epoch 6 loss: 0.001394071126014751 val loss: 0.010958711383864284
8953 MiB free out of 48676 MiB total
epoch 7 loss: 0.0012453083172658808 val loss: 0.010935551312286407
8953 MiB free out of 48676 MiB total
epoch 8 loss: 0.0011789158406827482 val loss: 0.010921982291620225
8953 MiB free out of 48676 MiB total
epoch 9 loss: 0.0011611114123297739 val loss: 0.010908153490163386
8953 MiB free out of 48676 MiB total
37863 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8953 MiB free out of 48676 MiB total
after cast to cpu
37863 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
9 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.6808, 1.6457, 1.6952,  ..., 1.6183, 1.6024, 1.6369], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.9793, 1.1443, 1.1281,  ..., 2.1393, 2.2283, 2.2846], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 1031.2510986328125
Epoch 100 Loss 42.935951232910156
Epoch 200 Loss 24.954830169677734
Epoch 300 Loss 19.904842376708984
Epoch 400 Loss 18.188533782958984
Epoch 500 Loss 16.663419723510742
Epoch 600 Loss 15.340374946594238
Epoch 700 Loss 14.20306396484375
Epoch 800 Loss 13.226055145263672
Epoch 900 Loss 12.38343334197998
last loss 11.65885066986084
9 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([0.9727, 0.9112, 0.8904,  ..., 0.9682, 0.9803, 0.9293], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.0864, 1.0820, 1.0906,  ..., 0.8114, 0.8140, 0.7966], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 390.145263671875
Epoch 100 Loss 19.60538673400879
Epoch 200 Loss 11.669930458068848
Epoch 300 Loss 9.59669303894043
Epoch 400 Loss 8.090252876281738
Epoch 500 Loss 6.988135814666748
Epoch 600 Loss 6.167036056518555
Epoch 700 Loss 5.642276763916016
Epoch 800 Loss 5.441935062408447
Epoch 900 Loss 5.248040199279785
last loss 5.063618183135986
9 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.5445, 1.6070, 1.5710,  ..., 1.6273, 1.5779, 1.6374], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.0707, 1.1604, 1.1292,  ..., 1.9872, 2.0199, 2.1190], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 963.120849609375
Epoch 100 Loss 36.67732238769531
Epoch 200 Loss 20.96185302734375
Epoch 300 Loss 15.102745056152344
Epoch 400 Loss 13.18661880493164
Epoch 500 Loss 12.39352035522461
Epoch 600 Loss 11.662158966064453
Epoch 700 Loss 10.997198104858398
Epoch 800 Loss 10.395751953125
Epoch 900 Loss 9.851351737976074
last loss 9.360773086547852
9 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.0343, 1.0450, 1.0397,  ..., 0.8190, 0.8186, 0.7914], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.9360, 0.9315, 0.8659,  ..., 0.9257, 0.9022, 0.9244], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 352.3910827636719
Epoch 100 Loss 21.981550216674805
Epoch 200 Loss 17.06279754638672
Epoch 300 Loss 14.071117401123047
Epoch 400 Loss 11.770227432250977
Epoch 500 Loss 10.029186248779297
Epoch 600 Loss 8.699965476989746
Epoch 700 Loss 7.66453742980957
Epoch 800 Loss 6.839366436004639
Epoch 900 Loss 6.167802810668945
last loss 5.700428009033203
9 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 85737744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.013507862779079005 val loss: 0.01398348924703896
8961 MiB free out of 48676 MiB total
epoch 1 loss: 0.005215669454628369 val loss: 0.013933407026343048
8953 MiB free out of 48676 MiB total
epoch 2 loss: 0.003372071963894996 val loss: 0.01389496453339234
8953 MiB free out of 48676 MiB total
epoch 3 loss: 0.002606414337606111 val loss: 0.013872449984773993
8953 MiB free out of 48676 MiB total
epoch 4 loss: 0.0021860977130927495 val loss: 0.013856715871952474
8953 MiB free out of 48676 MiB total
epoch 5 loss: 0.0020123304193475633 val loss: 0.013834258890710771
8953 MiB free out of 48676 MiB total
epoch 6 loss: 0.0017532859174025361 val loss: 0.013809765805490315
8953 MiB free out of 48676 MiB total
epoch 7 loss: 0.0015244782593981654 val loss: 0.013787799631245434
8953 MiB free out of 48676 MiB total
epoch 8 loss: 0.0014651543051513727 val loss: 0.01377272920217365
8953 MiB free out of 48676 MiB total
epoch 9 loss: 0.0013873005641471536 val loss: 0.01375608512898907
8953 MiB free out of 48676 MiB total
37863 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8953 MiB free out of 48676 MiB total
after cast to cpu
37863 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
10 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.6543, 1.6721, 1.6871,  ..., 1.5862, 1.6025, 1.6724], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.7476, 0.8385, 0.8215,  ..., 2.1863, 2.1931, 2.1422], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 952.2833251953125
Epoch 100 Loss 37.544090270996094
Epoch 200 Loss 21.572078704833984
Epoch 300 Loss 17.610506057739258
Epoch 400 Loss 16.02587890625
Epoch 500 Loss 14.632268905639648
Epoch 600 Loss 13.433432579040527
Epoch 700 Loss 12.40956974029541
Epoch 800 Loss 11.534377098083496
Epoch 900 Loss 10.782215118408203
last loss 10.136947631835938
10 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([0.9680, 0.9046, 0.8915,  ..., 0.9570, 0.9430, 0.9284], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.1302, 1.1081, 1.1169,  ..., 0.9286, 0.9147, 0.9270], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 387.8202819824219
Epoch 100 Loss 17.218502044677734
Epoch 200 Loss 9.765140533447266
Epoch 300 Loss 8.386459350585938
Epoch 400 Loss 7.659152030944824
Epoch 500 Loss 7.003789901733398
Epoch 600 Loss 6.428866386413574
Epoch 700 Loss 5.930392265319824
Epoch 800 Loss 5.499653339385986
Epoch 900 Loss 5.1269049644470215
last loss 4.805986404418945
10 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.5843, 1.6006, 1.5543,  ..., 1.6136, 1.6117, 1.6147], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.8123, 0.8635, 0.8176,  ..., 2.0155, 2.2036, 2.1027], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 938.7304077148438
Epoch 100 Loss 29.870655059814453
Epoch 200 Loss 16.797765731811523
Epoch 300 Loss 13.866615295410156
Epoch 400 Loss 12.511395454406738
Epoch 500 Loss 11.36838150024414
Epoch 600 Loss 10.415607452392578
Epoch 700 Loss 9.619165420532227
Epoch 800 Loss 8.946996688842773
Epoch 900 Loss 8.372621536254883
last loss 7.879809379577637
10 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.0892, 1.0734, 1.0835,  ..., 0.8955, 0.8985, 0.8911], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.9230, 0.9006, 0.8636,  ..., 0.9054, 0.9069, 0.8883], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 304.6064147949219
Epoch 100 Loss 19.452293395996094
Epoch 200 Loss 14.498186111450195
Epoch 300 Loss 11.981290817260742
Epoch 400 Loss 10.075217247009277
Epoch 500 Loss 8.634037017822266
Epoch 600 Loss 7.529534339904785
Epoch 700 Loss 6.6656813621521
Epoch 800 Loss 5.9743266105651855
Epoch 900 Loss 5.408908367156982
last loss 5.043446063995361
10 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 85737744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.024462056331685744 val loss: 0.019922255887649953
8961 MiB free out of 48676 MiB total
epoch 1 loss: 0.008371420413823216 val loss: 0.019911224022507668
8953 MiB free out of 48676 MiB total
epoch 2 loss: 0.0048895184645516565 val loss: 0.019942794926464558
8953 MiB free out of 48676 MiB total
epoch 3 loss: 0.0034025249578917283 val loss: 0.019985731109045446
8953 MiB free out of 48676 MiB total
epoch 4 loss: 0.0026176078708886052 val loss: 0.020036592963151634
8953 MiB free out of 48676 MiB total
epoch 5 loss: 0.0021499595777640934 val loss: 0.020073691266588867
8953 MiB free out of 48676 MiB total
epoch 6 loss: 0.001857628774814657 val loss: 0.02013573225121945
8953 MiB free out of 48676 MiB total
early stopping
37863 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8953 MiB free out of 48676 MiB total
after cast to cpu
37863 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
11 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.4919, 1.5258, 1.5711,  ..., 1.5032, 1.4571, 1.5529], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.9077, 0.8226, 0.7986,  ..., 1.9173, 1.8759, 1.8916], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 970.803955078125
Epoch 100 Loss 37.23495864868164
Epoch 200 Loss 21.670387268066406
Epoch 300 Loss 16.227970123291016
Epoch 400 Loss 15.232805252075195
Epoch 500 Loss 14.285374641418457
Epoch 600 Loss 13.403295516967773
Epoch 700 Loss 12.595232009887695
Epoch 800 Loss 11.861126899719238
Epoch 900 Loss 11.19662857055664
last loss 10.60130500793457
11 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([0.9840, 0.9265, 0.9024,  ..., 1.0096, 0.9917, 0.9475], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.9041, 0.9214, 0.9175,  ..., 1.0224, 1.0506, 1.0277], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 475.9872131347656
Epoch 100 Loss 23.02002716064453
Epoch 200 Loss 13.363482475280762
Epoch 300 Loss 10.022624969482422
Epoch 400 Loss 8.74360466003418
Epoch 500 Loss 7.732694149017334
Epoch 600 Loss 6.930874824523926
Epoch 700 Loss 6.285484790802002
Epoch 800 Loss 5.756739616394043
Epoch 900 Loss 5.376376152038574
last loss 5.182307720184326
11 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.5355, 1.5699, 1.5003,  ..., 1.5027, 1.5507, 1.5416], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.9395, 0.8253, 0.8040,  ..., 2.0374, 1.9353, 2.0648], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 909.5743408203125
Epoch 100 Loss 28.613677978515625
Epoch 200 Loss 16.49417495727539
Epoch 300 Loss 12.823966026306152
Epoch 400 Loss 11.867307662963867
Epoch 500 Loss 10.999408721923828
Epoch 600 Loss 10.228987693786621
Epoch 700 Loss 9.55013370513916
Epoch 800 Loss 8.951692581176758
Epoch 900 Loss 8.421709060668945
last loss 7.953738212585449
11 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([0.8975, 0.9156, 0.8993,  ..., 0.9782, 0.9988, 0.9837], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.9541, 0.9431, 0.8961,  ..., 0.9448, 0.9398, 0.9368], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 464.640625
Epoch 100 Loss 28.15264129638672
Epoch 200 Loss 20.236595153808594
Epoch 300 Loss 15.67912483215332
Epoch 400 Loss 12.706380844116211
Epoch 500 Loss 11.237296104431152
Epoch 600 Loss 10.640634536743164
Epoch 700 Loss 10.068758010864258
Epoch 800 Loss 9.52758502960205
Epoch 900 Loss 9.019542694091797
last loss 8.54944133758545
11 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 85737744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.03339452530781273 val loss: 0.018441287567839026
8961 MiB free out of 48676 MiB total
epoch 1 loss: 0.011156613156344974 val loss: 0.018277467926964164
8953 MiB free out of 48676 MiB total
epoch 2 loss: 0.006557769849678152 val loss: 0.018170950701460242
8953 MiB free out of 48676 MiB total
epoch 3 loss: 0.004650225147997844 val loss: 0.018078487017191947
8953 MiB free out of 48676 MiB total
epoch 4 loss: 0.003640959726908477 val loss: 0.018000429030507803
8953 MiB free out of 48676 MiB total
epoch 5 loss: 0.0031883220617601182 val loss: 0.01799937547184527
8953 MiB free out of 48676 MiB total
epoch 6 loss: 0.002732222166741849 val loss: 0.018035559100098908
8953 MiB free out of 48676 MiB total
epoch 7 loss: 0.0024102733168547275 val loss: 0.01799381012097001
8953 MiB free out of 48676 MiB total
epoch 8 loss: 0.0022088584628363606 val loss: 0.017955152201466262
8953 MiB free out of 48676 MiB total
epoch 9 loss: 0.002120610921338084 val loss: 0.01793176180217415
8953 MiB free out of 48676 MiB total
37863 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8953 MiB free out of 48676 MiB total
after cast to cpu
37863 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
12 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.6344, 1.6304, 1.6164,  ..., 1.5886, 1.5710, 1.6159], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.8726, 0.9940, 1.0412,  ..., 1.7685, 1.8328, 1.6122], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 1187.565185546875
Epoch 100 Loss 39.84153747558594
Epoch 200 Loss 23.028169631958008
Epoch 300 Loss 17.853004455566406
Epoch 400 Loss 16.298381805419922
Epoch 500 Loss 14.921709060668945
Epoch 600 Loss 13.725716590881348
Epoch 700 Loss 12.692063331604004
Epoch 800 Loss 11.796993255615234
Epoch 900 Loss 11.017706871032715
last loss 10.340802192687988
12 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([0.9805, 0.8820, 0.9145,  ..., 0.9917, 0.9768, 0.9477], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.9254, 0.9355, 0.9365,  ..., 0.7872, 0.8072, 0.7791], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 486.1211242675781
Epoch 100 Loss 20.18247413635254
Epoch 200 Loss 11.535590171813965
Epoch 300 Loss 9.071006774902344
Epoch 400 Loss 8.282365798950195
Epoch 500 Loss 7.577243328094482
Epoch 600 Loss 6.960602283477783
Epoch 700 Loss 6.425441741943359
Epoch 800 Loss 5.960976600646973
Epoch 900 Loss 5.556303024291992
last loss 5.205011367797852
12 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.5269, 1.5773, 1.5420,  ..., 1.5242, 1.5672, 1.5658], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.8878, 1.0026, 1.0124,  ..., 1.6680, 1.7091, 1.5400], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 1191.0721435546875
Epoch 100 Loss 35.5323486328125
Epoch 200 Loss 20.011518478393555
Epoch 300 Loss 17.1588077545166
Epoch 400 Loss 15.332086563110352
Epoch 500 Loss 13.806634902954102
Epoch 600 Loss 12.543994903564453
Epoch 700 Loss 11.49477481842041
Epoch 800 Loss 10.614408493041992
Epoch 900 Loss 9.866775512695312
last loss 9.22986125946045
12 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([0.9019, 0.9160, 0.9263,  ..., 0.8095, 0.8280, 0.8232], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.9529, 0.9486, 0.9037,  ..., 0.9460, 0.9320, 0.9379], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 423.5518493652344
Epoch 100 Loss 30.57101058959961
Epoch 200 Loss 20.03817367553711
Epoch 300 Loss 17.108661651611328
Epoch 400 Loss 14.69753646850586
Epoch 500 Loss 12.770612716674805
Epoch 600 Loss 11.236608505249023
Epoch 700 Loss 10.004722595214844
Epoch 800 Loss 9.000816345214844
Epoch 900 Loss 8.16939926147461
last loss 7.476789474487305
12 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 85737744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.027616313924227143 val loss: 0.01727479451801628
8961 MiB free out of 48676 MiB total
epoch 1 loss: 0.009772650693776086 val loss: 0.01715713646262884
8953 MiB free out of 48676 MiB total
epoch 2 loss: 0.005771429436208564 val loss: 0.01708353601861745
8953 MiB free out of 48676 MiB total
epoch 3 loss: 0.004086710629962909 val loss: 0.01706354704219848
8953 MiB free out of 48676 MiB total
epoch 4 loss: 0.003206498033250682 val loss: 0.017038733814843
8953 MiB free out of 48676 MiB total
epoch 5 loss: 0.002662826688720088 val loss: 0.01702018640935421
8953 MiB free out of 48676 MiB total
epoch 6 loss: 0.002365643149460084 val loss: 0.017009929986670613
8953 MiB free out of 48676 MiB total
epoch 7 loss: 0.0022010331458659493 val loss: 0.017015829565934837
8953 MiB free out of 48676 MiB total
epoch 8 loss: 0.0018894844733949867 val loss: 0.017004678957164288
8953 MiB free out of 48676 MiB total
epoch 9 loss: 0.0018195013844888308 val loss: 0.01700376544613391
8953 MiB free out of 48676 MiB total
37863 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8953 MiB free out of 48676 MiB total
after cast to cpu
37863 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
13 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.6126, 1.5780, 1.5905,  ..., 1.5597, 1.5394, 1.5684], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.8244, 1.0660, 1.0900,  ..., 1.8910, 1.9190, 1.7981], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 1158.325439453125
Epoch 100 Loss 41.71892547607422
Epoch 200 Loss 24.215572357177734
Epoch 300 Loss 19.614604949951172
Epoch 400 Loss 17.805023193359375
Epoch 500 Loss 16.224262237548828
Epoch 600 Loss 14.865312576293945
Epoch 700 Loss 13.700613021850586
Epoch 800 Loss 12.699021339416504
Epoch 900 Loss 11.831992149353027
last loss 11.082284927368164
13 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.0103, 0.9506, 0.9665,  ..., 0.9982, 1.0047, 0.9755], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.9152, 0.9285, 0.9353,  ..., 0.9549, 0.9411, 0.9663], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 577.5516357421875
Epoch 100 Loss 23.32761001586914
Epoch 200 Loss 13.55941390991211
Epoch 300 Loss 9.966493606567383
Epoch 400 Loss 9.293699264526367
Epoch 500 Loss 8.710655212402344
Epoch 600 Loss 8.164186477661133
Epoch 700 Loss 7.661099910736084
Epoch 800 Loss 7.202391147613525
Epoch 900 Loss 6.786072731018066
last loss 6.4122819900512695
13 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.5021, 1.5756, 1.5207,  ..., 1.5473, 1.5423, 1.5433], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.0422, 1.0638, 1.0815,  ..., 1.8074, 1.7660, 1.7255], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 1127.54833984375
Epoch 100 Loss 36.24799346923828
Epoch 200 Loss 20.791728973388672
Epoch 300 Loss 17.920761108398438
Epoch 400 Loss 16.219940185546875
Epoch 500 Loss 14.74887466430664
Epoch 600 Loss 13.49377155303955
Epoch 700 Loss 12.423846244812012
Epoch 800 Loss 11.507086753845215
Epoch 900 Loss 10.715468406677246
last loss 10.032291412353516
13 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([0.8974, 0.9448, 0.9435,  ..., 0.9521, 0.9254, 0.9538], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.9631, 0.9864, 0.9225,  ..., 0.9686, 0.9673, 0.9634], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 534.8353881835938
Epoch 100 Loss 38.03810501098633
Epoch 200 Loss 24.64859390258789
Epoch 300 Loss 20.87224769592285
Epoch 400 Loss 17.861391067504883
Epoch 500 Loss 15.507854461669922
Epoch 600 Loss 13.653861045837402
Epoch 700 Loss 12.169126510620117
Epoch 800 Loss 10.95731258392334
Epoch 900 Loss 9.950238227844238
last loss 9.342053413391113
13 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 85737744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.022822282215201994 val loss: 0.01967017340939492
8961 MiB free out of 48676 MiB total
epoch 1 loss: 0.008833961775962962 val loss: 0.01945004309527576
8953 MiB free out of 48676 MiB total
epoch 2 loss: 0.005561794550885679 val loss: 0.01930009387433529
8953 MiB free out of 48676 MiB total
epoch 3 loss: 0.004111291542358231 val loss: 0.01917529117781669
8953 MiB free out of 48676 MiB total
epoch 4 loss: 0.0033183191408170387 val loss: 0.01907656481489539
8953 MiB free out of 48676 MiB total
epoch 5 loss: 0.0028421592733138823 val loss: 0.01899681834038347
8953 MiB free out of 48676 MiB total
epoch 6 loss: 0.002527853870560648 val loss: 0.018910384620539844
8953 MiB free out of 48676 MiB total
epoch 7 loss: 0.0023482343030991615 val loss: 0.018857415881939232
8953 MiB free out of 48676 MiB total
epoch 8 loss: 0.002170405740798742 val loss: 0.018789658090099692
8953 MiB free out of 48676 MiB total
epoch 9 loss: 0.001997706051952264 val loss: 0.01873237092513591
8953 MiB free out of 48676 MiB total
37863 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8953 MiB free out of 48676 MiB total
after cast to cpu
37863 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
14 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.6134, 1.5749, 1.5880,  ..., 1.5348, 1.5272, 1.5489], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.0522, 1.4499, 1.4432,  ..., 1.5537, 1.5905, 1.4958], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 1095.8953857421875
Epoch 100 Loss 38.092132568359375
Epoch 200 Loss 22.003042221069336
Epoch 300 Loss 16.803279876708984
Epoch 400 Loss 15.41867446899414
Epoch 500 Loss 14.177900314331055
Epoch 600 Loss 13.083419799804688
Epoch 700 Loss 12.124119758605957
Epoch 800 Loss 11.28303050994873
Epoch 900 Loss 10.542879104614258
last loss 9.894146919250488
14 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([0.9699, 0.9163, 0.9656,  ..., 0.9915, 0.9925, 0.9866], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.9107, 0.9021, 0.9065,  ..., 1.0683, 1.0652, 1.0858], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 573.2227783203125
Epoch 100 Loss 26.909250259399414
Epoch 200 Loss 17.496540069580078
Epoch 300 Loss 15.080453872680664
Epoch 400 Loss 13.08146858215332
Epoch 500 Loss 11.484167098999023
Epoch 600 Loss 10.214317321777344
Epoch 700 Loss 9.195608139038086
Epoch 800 Loss 8.365750312805176
Epoch 900 Loss 7.678085803985596
last loss 7.104115009307861
14 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.5147, 1.5503, 1.5152,  ..., 1.5625, 1.5397, 1.5450], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.1204, 1.4476, 1.4390,  ..., 1.4891, 1.5953, 1.5113], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 1102.81298828125
Epoch 100 Loss 34.431846618652344
Epoch 200 Loss 20.625606536865234
Epoch 300 Loss 17.119476318359375
Epoch 400 Loss 15.347319602966309
Epoch 500 Loss 13.855951309204102
Epoch 600 Loss 12.614094734191895
Epoch 700 Loss 11.575798034667969
Epoch 800 Loss 10.698575973510742
Epoch 900 Loss 9.948124885559082
last loss 9.304094314575195
14 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([0.8922, 0.8743, 0.8909,  ..., 1.0444, 1.0262, 1.0497], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.9329, 0.9440, 0.9126,  ..., 0.9653, 0.9494, 0.9803], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 434.7296447753906
Epoch 100 Loss 33.657588958740234
Epoch 200 Loss 20.289745330810547
Epoch 300 Loss 17.334503173828125
Epoch 400 Loss 14.895841598510742
Epoch 500 Loss 12.948949813842773
Epoch 600 Loss 11.402922630310059
Epoch 700 Loss 10.163863182067871
Epoch 800 Loss 9.154150009155273
Epoch 900 Loss 8.31549072265625
last loss 7.613086700439453
14 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 85737744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.02140395914466353 val loss: 0.021653906092979014
8961 MiB free out of 48676 MiB total
epoch 1 loss: 0.008317160562000936 val loss: 0.021547243581153452
8953 MiB free out of 48676 MiB total
epoch 2 loss: 0.005196349295147229 val loss: 0.021462319418787956
8953 MiB free out of 48676 MiB total
epoch 3 loss: 0.003802896300840075 val loss: 0.02137796115130186
8953 MiB free out of 48676 MiB total
epoch 4 loss: 0.003043946728212177 val loss: 0.021304282126948237
8953 MiB free out of 48676 MiB total
epoch 5 loss: 0.002579063215307542 val loss: 0.021252439124509692
8953 MiB free out of 48676 MiB total
epoch 6 loss: 0.0022842327180114808 val loss: 0.021192425163462758
8953 MiB free out of 48676 MiB total
epoch 7 loss: 0.0021183553735681926 val loss: 0.021137885167263448
8953 MiB free out of 48676 MiB total
epoch 8 loss: 0.002042821877694223 val loss: 0.021077528363093734
8953 MiB free out of 48676 MiB total
epoch 9 loss: 0.0018281018965353724 val loss: 0.021018459112383425
8953 MiB free out of 48676 MiB total
37863 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8953 MiB free out of 48676 MiB total
after cast to cpu
37863 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
15 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.6233, 1.5785, 1.5584,  ..., 1.5217, 1.5787, 1.5373], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.8892, 0.9599, 0.9163,  ..., 1.9989, 2.0357, 2.1068], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 1129.9033203125
Epoch 100 Loss 41.812843322753906
Epoch 200 Loss 23.967082977294922
Epoch 300 Loss 17.292362213134766
Epoch 400 Loss 15.878230094909668
Epoch 500 Loss 14.871274948120117
Epoch 600 Loss 13.935171127319336
Epoch 700 Loss 13.07863712310791
Epoch 800 Loss 12.300907135009766
Epoch 900 Loss 11.5966215133667
last loss 10.964620590209961
15 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.0155, 0.9771, 0.9901,  ..., 1.0429, 1.0140, 1.0064], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.0221, 1.0299, 1.0216,  ..., 0.9816, 0.9779, 0.9640], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 589.84814453125
Epoch 100 Loss 24.29156494140625
Epoch 200 Loss 15.810834884643555
Epoch 300 Loss 14.000086784362793
Epoch 400 Loss 12.412375450134277
Epoch 500 Loss 11.079880714416504
Epoch 600 Loss 9.976715087890625
Epoch 700 Loss 9.062545776367188
Epoch 800 Loss 8.299362182617188
Epoch 900 Loss 7.655854225158691
last loss 7.112421035766602
15 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.4933, 1.5343, 1.4894,  ..., 1.5105, 1.5305, 1.5341], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.8819, 0.9525, 0.9162,  ..., 1.9384, 1.9062, 1.9309], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 1128.9573974609375
Epoch 100 Loss 38.972496032714844
Epoch 200 Loss 22.075767517089844
Epoch 300 Loss 15.841617584228516
Epoch 400 Loss 14.472107887268066
Epoch 500 Loss 13.33749008178711
Epoch 600 Loss 12.328923225402832
Epoch 700 Loss 11.44101333618164
Epoch 800 Loss 10.660745620727539
Epoch 900 Loss 9.973348617553711
last loss 9.37063217163086
15 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([0.9999, 1.0097, 1.0048,  ..., 0.9665, 0.9695, 0.9545], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.9702, 0.9879, 0.9669,  ..., 0.9994, 0.9682, 1.0115], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 515.1544189453125
Epoch 100 Loss 36.4754638671875
Epoch 200 Loss 22.89514923095703
Epoch 300 Loss 19.50335693359375
Epoch 400 Loss 16.76837921142578
Epoch 500 Loss 14.60006332397461
Epoch 600 Loss 12.87437915802002
Epoch 700 Loss 11.483084678649902
Epoch 800 Loss 10.341915130615234
Epoch 900 Loss 9.389949798583984
last loss 8.656508445739746
15 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 85737744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.027999914746033028 val loss: 0.028212006320245564
8961 MiB free out of 48676 MiB total
epoch 1 loss: 0.009827728870732244 val loss: 0.028926356812007725
8953 MiB free out of 48676 MiB total
epoch 2 loss: 0.005945610912021948 val loss: 0.02932857582345605
8953 MiB free out of 48676 MiB total
epoch 3 loss: 0.004314006275308202 val loss: 0.029549534083344042
8953 MiB free out of 48676 MiB total
epoch 4 loss: 0.0034359272567598964 val loss: 0.02974623383488506
8953 MiB free out of 48676 MiB total
epoch 5 loss: 0.0029343042078835424 val loss: 0.02984309068415314
8953 MiB free out of 48676 MiB total
early stopping
37863 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8953 MiB free out of 48676 MiB total
after cast to cpu
37863 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
16 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.5899, 1.5800, 1.5417,  ..., 1.5215, 1.5134, 1.5515], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.1845, 1.2345, 1.2825,  ..., 1.7402, 1.7219, 1.7930], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 1218.283935546875
Epoch 100 Loss 48.6117057800293
Epoch 200 Loss 28.375774383544922
Epoch 300 Loss 21.34677505493164
Epoch 400 Loss 19.700870513916016
Epoch 500 Loss 18.186817169189453
Epoch 600 Loss 16.818470001220703
Epoch 700 Loss 15.596102714538574
Epoch 800 Loss 14.508684158325195
Epoch 900 Loss 13.541311264038086
last loss 12.6868314743042
16 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.0617, 1.0144, 1.0695,  ..., 1.0903, 1.0521, 1.0318], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.2892, 1.2590, 1.2708,  ..., 1.0814, 1.0957, 1.0983], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 743.6751098632812
Epoch 100 Loss 33.71797561645508
Epoch 200 Loss 22.38280487060547
Epoch 300 Loss 19.811012268066406
Epoch 400 Loss 17.544252395629883
Epoch 500 Loss 15.636116981506348
Epoch 600 Loss 14.054827690124512
Epoch 700 Loss 12.744607925415039
Epoch 800 Loss 11.650732040405273
Epoch 900 Loss 10.72732925415039
last loss 9.945624351501465
16 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.4894, 1.4845, 1.4596,  ..., 1.4817, 1.5148, 1.4989], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.1535, 1.2194, 1.2913,  ..., 1.6515, 1.6507, 1.6518], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 1105.62646484375
Epoch 100 Loss 41.6094970703125
Epoch 200 Loss 23.8761043548584
Epoch 300 Loss 17.199546813964844
Epoch 400 Loss 15.071671485900879
Epoch 500 Loss 14.174755096435547
Epoch 600 Loss 13.324638366699219
Epoch 700 Loss 12.532820701599121
Epoch 800 Loss 11.802324295043945
Epoch 900 Loss 11.131620407104492
last loss 10.522738456726074
16 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.2393, 1.2085, 1.2276,  ..., 1.0664, 1.0781, 1.0688], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.0223, 1.0249, 1.0036,  ..., 1.0444, 1.0177, 1.0147], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 679.94091796875
Epoch 100 Loss 44.177398681640625
Epoch 200 Loss 32.37785720825195
Epoch 300 Loss 24.90386962890625
Epoch 400 Loss 20.074161529541016
Epoch 500 Loss 16.77768325805664
Epoch 600 Loss 14.399238586425781
Epoch 700 Loss 13.165464401245117
Epoch 800 Loss 12.427942276000977
Epoch 900 Loss 11.736560821533203
last loss 11.098498344421387
16 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 85737744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0302453841213719 val loss: 0.03807802754454315
8961 MiB free out of 48676 MiB total
epoch 1 loss: 0.0113782781700138 val loss: 0.03824811917729676
8953 MiB free out of 48676 MiB total
epoch 2 loss: 0.007093891030308441 val loss: 0.03836462087929249
8953 MiB free out of 48676 MiB total
epoch 3 loss: 0.005322415061527863 val loss: 0.03839486441574991
8953 MiB free out of 48676 MiB total
epoch 4 loss: 0.004371452947452781 val loss: 0.03837429475970566
8953 MiB free out of 48676 MiB total
epoch 5 loss: 0.003793638141360134 val loss: 0.0383392043877393
8953 MiB free out of 48676 MiB total
early stopping
37863 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8953 MiB free out of 48676 MiB total
after cast to cpu
37863 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
17 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.5578, 1.5840, 1.5218,  ..., 1.4811, 1.4996, 1.5344], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.6770, 0.7048, 0.7073,  ..., 2.2019, 1.6335, 2.1880], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 1402.2923583984375
Epoch 100 Loss 55.771148681640625
Epoch 200 Loss 31.890094757080078
Epoch 300 Loss 25.417739868164062
Epoch 400 Loss 23.102758407592773
Epoch 500 Loss 21.033552169799805
Epoch 600 Loss 19.222850799560547
Epoch 700 Loss 17.648988723754883
Epoch 800 Loss 16.279815673828125
Epoch 900 Loss 15.083216667175293
last loss 14.040847778320312
17 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.0450, 1.0147, 1.0800,  ..., 1.0646, 1.0705, 1.0441], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.9612, 0.9027, 0.9841,  ..., 1.0400, 1.0164, 1.0498], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 809.668701171875
Epoch 100 Loss 50.906707763671875
Epoch 200 Loss 37.465023040771484
Epoch 300 Loss 30.33026695251465
Epoch 400 Loss 25.203460693359375
Epoch 500 Loss 21.46933364868164
Epoch 600 Loss 18.673992156982422
Epoch 700 Loss 16.511640548706055
Epoch 800 Loss 14.985864639282227
Epoch 900 Loss 14.237859725952148
last loss 13.538317680358887
17 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.4791, 1.4705, 1.4639,  ..., 1.5067, 1.5329, 1.4977], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.7025, 0.6941, 0.6939,  ..., 2.0938, 2.2398, 2.0421], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 1305.90478515625
Epoch 100 Loss 53.661224365234375
Epoch 200 Loss 30.131114959716797
Epoch 300 Loss 21.959426879882812
Epoch 400 Loss 20.007568359375
Epoch 500 Loss 18.255502700805664
Epoch 600 Loss 16.712982177734375
Epoch 700 Loss 15.365697860717773
Epoch 800 Loss 14.189654350280762
Epoch 900 Loss 13.159789085388184
last loss 12.26200008392334
17 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.0241, 1.0220, 0.9856,  ..., 1.0233, 0.9882, 1.0276], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.0365, 1.0348, 1.0110,  ..., 1.0520, 1.0362, 1.0467], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 649.1895141601562
Epoch 100 Loss 44.674198150634766
Epoch 200 Loss 34.458152770996094
Epoch 300 Loss 28.364545822143555
Epoch 400 Loss 23.79552459716797
Epoch 500 Loss 20.371826171875
Epoch 600 Loss 17.746742248535156
Epoch 700 Loss 15.682578086853027
Epoch 800 Loss 14.021517753601074
Epoch 900 Loss 12.657386779785156
last loss 11.567766189575195
17 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 85737744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.028053596062818542 val loss: 0.06540936301462352
8961 MiB free out of 48676 MiB total
epoch 1 loss: 0.010727343258622568 val loss: 0.06515366025269032
8953 MiB free out of 48676 MiB total
epoch 2 loss: 0.006902605844516074 val loss: 0.06475608609616756
8953 MiB free out of 48676 MiB total
epoch 3 loss: 0.005232407982475706 val loss: 0.06435042666271329
8953 MiB free out of 48676 MiB total
epoch 4 loss: 0.00434260018846544 val loss: 0.06399153475649655
8953 MiB free out of 48676 MiB total
epoch 5 loss: 0.003892741848176229 val loss: 0.06376489950343966
8953 MiB free out of 48676 MiB total
epoch 6 loss: 0.003758485421712976 val loss: 0.06345991976559162
8953 MiB free out of 48676 MiB total
epoch 7 loss: 0.003796556061388401 val loss: 0.06322757946327329
8953 MiB free out of 48676 MiB total
epoch 8 loss: 0.0031811552798899356 val loss: 0.06298587238416076
8953 MiB free out of 48676 MiB total
epoch 9 loss: 0.00294164800834551 val loss: 0.06285362713970244
8953 MiB free out of 48676 MiB total
37863 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8953 MiB free out of 48676 MiB total
after cast to cpu
37863 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
18 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.5252, 1.5307, 1.4716,  ..., 1.4937, 1.4664, 1.4463], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.0710, 1.1842, 1.2079,  ..., 2.1710, 2.3075, 2.3298], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 1575.019775390625
Epoch 100 Loss 64.74666595458984
Epoch 200 Loss 39.35349655151367
Epoch 300 Loss 34.68698501586914
Epoch 400 Loss 30.677684783935547
Epoch 500 Loss 27.324886322021484
Epoch 600 Loss 24.537792205810547
Epoch 700 Loss 22.2099552154541
Epoch 800 Loss 20.24941062927246
Epoch 900 Loss 18.582551956176758
last loss 17.165061950683594
18 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.0789, 1.0596, 1.0864,  ..., 1.1258, 1.1193, 1.1207], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.2060, 1.2129, 1.2298,  ..., 0.9829, 0.9974, 1.0103], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 995.3736572265625
Epoch 100 Loss 58.12321853637695
Epoch 200 Loss 43.51260757446289
Epoch 300 Loss 34.010833740234375
Epoch 400 Loss 27.71677017211914
Epoch 500 Loss 23.330591201782227
Epoch 600 Loss 20.129716873168945
Epoch 700 Loss 18.73666763305664
Epoch 800 Loss 17.885202407836914
Epoch 900 Loss 17.06652069091797
last loss 16.293075561523438
18 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.4467, 1.4891, 1.4806,  ..., 1.4545, 1.4652, 1.4646], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.0691, 1.1817, 1.1950,  ..., 1.9673, 2.0273, 2.0164], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 1490.6114501953125
Epoch 100 Loss 61.82524490356445
Epoch 200 Loss 34.989418029785156
Epoch 300 Loss 28.054332733154297
Epoch 400 Loss 25.470172882080078
Epoch 500 Loss 23.16545867919922
Epoch 600 Loss 21.149906158447266
Epoch 700 Loss 19.397546768188477
Epoch 800 Loss 17.872451782226562
Epoch 900 Loss 16.539403915405273
last loss 15.378653526306152
18 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.1691, 1.1865, 1.1922,  ..., 0.9777, 0.9909, 0.9968], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.0622, 1.0683, 1.0524,  ..., 1.0709, 1.0725, 1.0642], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 1025.810546875
Epoch 100 Loss 64.13825225830078
Epoch 200 Loss 47.2737922668457
Epoch 300 Loss 36.46608352661133
Epoch 400 Loss 29.40249252319336
Epoch 500 Loss 24.542461395263672
Epoch 600 Loss 21.016090393066406
Epoch 700 Loss 19.257295608520508
Epoch 800 Loss 18.253149032592773
Epoch 900 Loss 17.298521041870117
last loss 16.406211853027344
18 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 85737744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.02603418996295659 val loss: 0.04328214447014034
8961 MiB free out of 48676 MiB total
epoch 1 loss: 0.010904669848969206 val loss: 0.04320836765691638
8953 MiB free out of 48676 MiB total
epoch 2 loss: 0.007499920924601611 val loss: 0.04318871139548719
8953 MiB free out of 48676 MiB total
epoch 3 loss: 0.006039999985659961 val loss: 0.04320726706646383
8953 MiB free out of 48676 MiB total
epoch 4 loss: 0.005276191968732746 val loss: 0.04310385277494788
8953 MiB free out of 48676 MiB total
epoch 5 loss: 0.004717142965091625 val loss: 0.04312912793830037
8953 MiB free out of 48676 MiB total
epoch 6 loss: 0.004292162258934695 val loss: 0.043111055390909314
8953 MiB free out of 48676 MiB total
epoch 7 loss: 0.004177316110144602 val loss: 0.04299641842953861
8953 MiB free out of 48676 MiB total
epoch 8 loss: 0.0037213184496067697 val loss: 0.043053670320659876
8953 MiB free out of 48676 MiB total
epoch 9 loss: 0.0034594532553455792 val loss: 0.04309836635366082
8953 MiB free out of 48676 MiB total
37863 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8953 MiB free out of 48676 MiB total
after cast to cpu
37863 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
19 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.4944, 1.5200, 1.4792,  ..., 1.4335, 1.4468, 1.4712], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.8642, 1.1287, 1.1200,  ..., 2.0235, 2.0642, 2.0212], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 1394.080322265625
Epoch 100 Loss 61.21990966796875
Epoch 200 Loss 44.20536804199219
Epoch 300 Loss 36.42452621459961
Epoch 400 Loss 30.65595817565918
Epoch 500 Loss 26.393482208251953
Epoch 600 Loss 23.16822052001953
Epoch 700 Loss 20.653770446777344
Epoch 800 Loss 18.636695861816406
Epoch 900 Loss 17.413320541381836
last loss 16.663503646850586
19 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.0984, 1.0858, 1.1186,  ..., 1.1277, 1.1063, 1.0958], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.1414, 1.1464, 1.1453,  ..., 1.1046, 1.0892, 1.0853], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 939.9210205078125
Epoch 100 Loss 60.983524322509766
Epoch 200 Loss 45.92456817626953
Epoch 300 Loss 35.92676544189453
Epoch 400 Loss 29.17458724975586
Epoch 500 Loss 24.4027042388916
Epoch 600 Loss 20.90886116027832
Epoch 700 Loss 18.551525115966797
Epoch 800 Loss 17.552959442138672
Epoch 900 Loss 16.613842010498047
last loss 15.744488716125488
19 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.4172, 1.4455, 1.4177,  ..., 1.4245, 1.4529, 1.4584], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.9218, 1.1143, 1.1046,  ..., 1.9034, 1.8568, 1.8653], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 1336.53759765625
Epoch 100 Loss 56.82742691040039
Epoch 200 Loss 31.86206817626953
Epoch 300 Loss 26.92805290222168
Epoch 400 Loss 23.92812728881836
Epoch 500 Loss 21.406536102294922
Epoch 600 Loss 19.305763244628906
Epoch 700 Loss 17.548282623291016
Epoch 800 Loss 16.064037322998047
Epoch 900 Loss 14.796507835388184
last loss 13.712093353271484
19 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.1254, 1.1186, 1.1293,  ..., 1.0896, 1.0747, 1.0727], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.0850, 1.0817, 1.0711,  ..., 1.0988, 1.0905, 1.0791], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 912.5211791992188
Epoch 100 Loss 55.31743621826172
Epoch 200 Loss 43.14397430419922
Epoch 300 Loss 34.570396423339844
Epoch 400 Loss 28.500457763671875
Epoch 500 Loss 24.121746063232422
Epoch 600 Loss 20.85881233215332
Epoch 700 Loss 18.34639549255371
Epoch 800 Loss 16.353158950805664
Epoch 900 Loss 14.880999565124512
last loss 14.183262825012207
19 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 85737744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.02678298741375329 val loss: 0.046139633283019066
8961 MiB free out of 48676 MiB total
epoch 1 loss: 0.010787127182993572 val loss: 0.046823375625535846
8953 MiB free out of 48676 MiB total
epoch 2 loss: 0.007180039661761839 val loss: 0.047034726943820715
8953 MiB free out of 48676 MiB total
epoch 3 loss: 0.005603663235888234 val loss: 0.04710908490233123
8953 MiB free out of 48676 MiB total
epoch 4 loss: 0.004980131565389456 val loss: 0.04707949189469218
8953 MiB free out of 48676 MiB total
epoch 5 loss: 0.004357561283541145 val loss: 0.04717952944338322
8953 MiB free out of 48676 MiB total
early stopping
37863 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8953 MiB free out of 48676 MiB total
after cast to cpu
37863 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
20 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.5040, 1.5277, 1.4980,  ..., 1.4534, 1.4546, 1.4831], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.5292, 0.5662, 0.5834,  ..., 1.8763, 2.0239, 2.0330], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 1442.166259765625
Epoch 100 Loss 68.94847106933594
Epoch 200 Loss 39.32468795776367
Epoch 300 Loss 31.33405876159668
Epoch 400 Loss 28.76285171508789
Epoch 500 Loss 26.40907859802246
Epoch 600 Loss 24.30453109741211
Epoch 700 Loss 22.439868927001953
Epoch 800 Loss 20.790374755859375
Epoch 900 Loss 19.328020095825195
last loss 18.03863525390625
20 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.1192, 1.0834, 1.1373,  ..., 1.1337, 1.1346, 1.1265], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.0799, 1.1011, 1.0996,  ..., 1.0353, 1.0458, 1.0728], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 1065.95458984375
Epoch 100 Loss 66.58346557617188
Epoch 200 Loss 51.09768295288086
Epoch 300 Loss 40.397945404052734
Epoch 400 Loss 32.98710632324219
Epoch 500 Loss 27.76696014404297
Epoch 600 Loss 23.956974029541016
Epoch 700 Loss 21.064701080322266
Epoch 800 Loss 18.78879737854004
Epoch 900 Loss 17.71068000793457
last loss 17.070201873779297
20 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.4390, 1.4916, 1.4176,  ..., 1.4356, 1.4500, 1.4538], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.5440, 0.5997, 0.6395,  ..., 1.7327, 1.8238, 1.5132], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 1324.0411376953125
Epoch 100 Loss 58.54778289794922
Epoch 200 Loss 32.94343566894531
Epoch 300 Loss 28.290081024169922
Epoch 400 Loss 25.041706085205078
Epoch 500 Loss 22.340782165527344
Epoch 600 Loss 20.104358673095703
Epoch 700 Loss 18.238046646118164
Epoch 800 Loss 16.66236686706543
Epoch 900 Loss 15.315794944763184
last loss 14.162771224975586
20 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.0672, 1.0759, 1.0829,  ..., 1.0529, 1.0531, 1.0652], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.1243, 1.1123, 1.0981,  ..., 1.0983, 1.1009, 1.1329], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 1121.610595703125
Epoch 100 Loss 63.704925537109375
Epoch 200 Loss 47.8104133605957
Epoch 300 Loss 37.10584259033203
Epoch 400 Loss 30.13011360168457
Epoch 500 Loss 25.35190200805664
Epoch 600 Loss 21.863569259643555
Epoch 700 Loss 19.18579864501953
Epoch 800 Loss 18.030872344970703
Epoch 900 Loss 17.275104522705078
last loss 16.547260284423828
20 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 85737744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0331003002502257 val loss: 0.060141525929793715
8961 MiB free out of 48676 MiB total
epoch 1 loss: 0.01296051168901613 val loss: 0.05958126322366297
8953 MiB free out of 48676 MiB total
epoch 2 loss: 0.008567390344978776 val loss: 0.05932226940058172
8953 MiB free out of 48676 MiB total
epoch 3 loss: 0.006738073847373016 val loss: 0.05922611989080906
8953 MiB free out of 48676 MiB total
epoch 4 loss: 0.005854391460161423 val loss: 0.059185916325077415
8953 MiB free out of 48676 MiB total
epoch 5 loss: 0.005463570949359564 val loss: 0.05877413600683212
8953 MiB free out of 48676 MiB total
epoch 6 loss: 0.0051948742311651586 val loss: 0.05873578041791916
8953 MiB free out of 48676 MiB total
epoch 7 loss: 0.004472265109143336 val loss: 0.058480999898165464
8953 MiB free out of 48676 MiB total
epoch 8 loss: 0.004192037618850009 val loss: 0.058385576820001006
8953 MiB free out of 48676 MiB total
epoch 9 loss: 0.003904967747075716 val loss: 0.05827453196980059
8953 MiB free out of 48676 MiB total
37863 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8953 MiB free out of 48676 MiB total
after cast to cpu
37863 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
21 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.4550, 1.4713, 1.4318,  ..., 1.4118, 1.4379, 1.4315], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.8120, 0.9468, 0.9507,  ..., 2.0480, 1.7255, 1.9224], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 1662.11962890625
Epoch 100 Loss 67.13426971435547
Epoch 200 Loss 46.35829544067383
Epoch 300 Loss 38.393775939941406
Epoch 400 Loss 32.49571228027344
Epoch 500 Loss 28.118457794189453
Epoch 600 Loss 24.769811630249023
Epoch 700 Loss 22.121253967285156
Epoch 800 Loss 19.967058181762695
Epoch 900 Loss 18.438400268554688
last loss 17.64942741394043
21 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.1393, 1.1365, 1.1569,  ..., 1.1675, 1.1572, 1.1763], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.0815, 1.0853, 1.1115,  ..., 1.1556, 1.1620, 1.1422], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 1204.917724609375
Epoch 100 Loss 79.31214904785156
Epoch 200 Loss 60.71739959716797
Epoch 300 Loss 48.57604217529297
Epoch 400 Loss 39.88336181640625
Epoch 500 Loss 33.61771774291992
Epoch 600 Loss 29.01197624206543
Epoch 700 Loss 25.523151397705078
Epoch 800 Loss 22.783260345458984
Epoch 900 Loss 21.40392303466797
last loss 20.36872100830078
21 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.4144, 1.4360, 1.4147,  ..., 1.3882, 1.4228, 1.3867], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.8719, 0.9523, 0.9489,  ..., 1.8164, 1.4733, 1.9335], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 1564.716552734375
Epoch 100 Loss 73.25927734375
Epoch 200 Loss 40.41651916503906
Epoch 300 Loss 32.298309326171875
Epoch 400 Loss 29.677570343017578
Epoch 500 Loss 27.26788330078125
Epoch 600 Loss 25.109416961669922
Epoch 700 Loss 23.197193145751953
Epoch 800 Loss 21.50760269165039
Epoch 900 Loss 20.01200294494629
last loss 18.695199966430664
21 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.0900, 1.0793, 1.1069,  ..., 1.1355, 1.1423, 1.1257], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.1539, 1.1705, 1.1258,  ..., 1.1432, 1.1037, 1.1304], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 1366.0869140625
Epoch 100 Loss 79.82769775390625
Epoch 200 Loss 60.87882995605469
Epoch 300 Loss 48.64527893066406
Epoch 400 Loss 40.054847717285156
Epoch 500 Loss 33.93895721435547
Epoch 600 Loss 29.42502212524414
Epoch 700 Loss 25.958894729614258
Epoch 800 Loss 23.20351791381836
Epoch 900 Loss 20.952035903930664
last loss 19.820144653320312
21 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 85737744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.03330396304227179 val loss: 0.08230075612664223
8961 MiB free out of 48676 MiB total
epoch 1 loss: 0.013546455968025839 val loss: 0.08199510630220175
8953 MiB free out of 48676 MiB total
epoch 2 loss: 0.009277448316424852 val loss: 0.08220389252528548
8953 MiB free out of 48676 MiB total
epoch 3 loss: 0.007468685911590001 val loss: 0.08178623579442501
8953 MiB free out of 48676 MiB total
epoch 4 loss: 0.0063153476894513005 val loss: 0.08230715431272984
8953 MiB free out of 48676 MiB total
epoch 5 loss: 0.005870382376087946 val loss: 0.0820980560965836
8953 MiB free out of 48676 MiB total
epoch 6 loss: 0.0053509138197114225 val loss: 0.08263379195705056
8953 MiB free out of 48676 MiB total
epoch 7 loss: 0.004938284590025432 val loss: 0.08240290824323893
8953 MiB free out of 48676 MiB total
epoch 8 loss: 0.004617056123606744 val loss: 0.0825395374558866
8953 MiB free out of 48676 MiB total
early stopping
37863 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8953 MiB free out of 48676 MiB total
after cast to cpu
37863 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
22 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.4745, 1.4774, 1.4902,  ..., 1.4655, 1.4788, 1.4825], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.4680, 1.4894, 1.5025,  ..., 1.2110, 1.9322, 1.9817], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 1816.6038818359375
Epoch 100 Loss 84.17884826660156
Epoch 200 Loss 63.881141662597656
Epoch 300 Loss 52.50981140136719
Epoch 400 Loss 44.07334518432617
Epoch 500 Loss 37.84345626831055
Epoch 600 Loss 33.13494110107422
Epoch 700 Loss 29.464000701904297
Epoch 800 Loss 26.515050888061523
Epoch 900 Loss 24.085683822631836
last loss 22.704992294311523
22 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.1536, 1.1395, 1.1547,  ..., 1.1419, 1.1616, 1.1692], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.2635, 1.2563, 1.2672,  ..., 1.2306, 1.2221, 1.2212], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 1349.4169921875
Epoch 100 Loss 82.92311096191406
Epoch 200 Loss 62.886512756347656
Epoch 300 Loss 49.28227996826172
Epoch 400 Loss 40.01771926879883
Epoch 500 Loss 33.56034851074219
Epoch 600 Loss 28.90355682373047
Epoch 700 Loss 25.401905059814453
Epoch 800 Loss 23.291595458984375
Epoch 900 Loss 22.075244903564453
last loss 20.945947647094727
22 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.4339, 1.4647, 1.4494,  ..., 1.4377, 1.4301, 1.4364], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.4712, 1.4866, 1.5007,  ..., 1.9845, 1.9318, 2.1921], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 1749.6597900390625
Epoch 100 Loss 83.6919937133789
Epoch 200 Loss 59.83744812011719
Epoch 300 Loss 49.9062614440918
Epoch 400 Loss 42.23063278198242
Epoch 500 Loss 36.407264709472656
Epoch 600 Loss 31.933557510375977
Epoch 700 Loss 28.41477394104004
Epoch 800 Loss 25.577484130859375
Epoch 900 Loss 23.238506317138672
last loss 21.29248809814453
22 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.2228, 1.2182, 1.2207,  ..., 1.1587, 1.1975, 1.1741], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.1274, 1.1286, 1.1356,  ..., 1.1232, 1.1369, 1.1320], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 1581.83447265625
Epoch 100 Loss 90.79048156738281
Epoch 200 Loss 69.70781707763672
Epoch 300 Loss 59.359619140625
Epoch 400 Loss 50.96495056152344
Epoch 500 Loss 44.395362854003906
Epoch 600 Loss 39.25474166870117
Epoch 700 Loss 35.159423828125
Epoch 800 Loss 31.814128875732422
Epoch 900 Loss 29.014366149902344
last loss 26.6470947265625
22 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 85737744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.04056480979488697 val loss: 0.06966111995279789
8961 MiB free out of 48676 MiB total
epoch 1 loss: 0.01633771842898568 val loss: 0.06949022505432367
8953 MiB free out of 48676 MiB total
epoch 2 loss: 0.011241115113080014 val loss: 0.06938838586211205
8953 MiB free out of 48676 MiB total
epoch 3 loss: 0.009331398192443885 val loss: 0.069144525565207
8953 MiB free out of 48676 MiB total
epoch 4 loss: 0.008086318670393666 val loss: 0.06880114786326885
8953 MiB free out of 48676 MiB total
epoch 5 loss: 0.007683995196202886 val loss: 0.06894680485129356
8953 MiB free out of 48676 MiB total
epoch 6 loss: 0.007218090851893066 val loss: 0.06882813246920705
8953 MiB free out of 48676 MiB total
epoch 7 loss: 0.0068037460623600055 val loss: 0.0688336524181068
8953 MiB free out of 48676 MiB total
epoch 8 loss: 0.005992533288008417 val loss: 0.06909950915724039
8953 MiB free out of 48676 MiB total
epoch 9 loss: 0.005879180474948953 val loss: 0.06912408908829093
8953 MiB free out of 48676 MiB total
early stopping
37863 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8953 MiB free out of 48676 MiB total
after cast to cpu
37863 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
23 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.4515, 1.4864, 1.4677,  ..., 1.4692, 1.4491, 1.4700], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.7387, 0.7335, 0.7738,  ..., 1.5745, 1.3933, 1.6235], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 2110.04638671875
Epoch 100 Loss 98.25286102294922
Epoch 200 Loss 62.86933898925781
Epoch 300 Loss 55.409523010253906
Epoch 400 Loss 48.97627258300781
Epoch 500 Loss 43.622337341308594
Epoch 600 Loss 39.19319152832031
Epoch 700 Loss 35.507652282714844
Epoch 800 Loss 32.40959930419922
Epoch 900 Loss 29.775205612182617
last loss 27.53009796142578
23 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.2002, 1.1744, 1.2148,  ..., 1.1841, 1.2298, 1.2296], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.1003, 1.0871, 1.1005,  ..., 1.1547, 1.1473, 1.1477], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 1651.5609130859375
Epoch 100 Loss 99.68588256835938
Epoch 200 Loss 73.25826263427734
Epoch 300 Loss 56.17401885986328
Epoch 400 Loss 45.06175994873047
Epoch 500 Loss 37.56584930419922
Epoch 600 Loss 32.27546691894531
Epoch 700 Loss 29.348424911499023
Epoch 800 Loss 27.933856964111328
Epoch 900 Loss 26.59096908569336
last loss 25.335636138916016
23 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.4343, 1.4455, 1.4322,  ..., 1.4610, 1.4012, 1.4335], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.7342, 0.7344, 0.7733,  ..., 1.6962, 1.9142, 1.6451], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 2128.36865234375
Epoch 100 Loss 90.18280029296875
Epoch 200 Loss 59.70878601074219
Epoch 300 Loss 50.47935485839844
Epoch 400 Loss 43.248741149902344
Epoch 500 Loss 37.69731903076172
Epoch 600 Loss 33.37928771972656
Epoch 700 Loss 29.93925666809082
Epoch 800 Loss 27.12942886352539
Epoch 900 Loss 24.783344268798828
last loss 22.80722427368164
23 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.0890, 1.0794, 1.0946,  ..., 1.1393, 1.1247, 1.1352], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.1980, 1.2021, 1.1762,  ..., 1.1868, 1.1935, 1.1843], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 2206.6025390625
Epoch 100 Loss 122.02203369140625
Epoch 200 Loss 90.64473724365234
Epoch 300 Loss 70.19578552246094
Epoch 400 Loss 56.997135162353516
Epoch 500 Loss 47.99085998535156
Epoch 600 Loss 41.47658920288086
Epoch 700 Loss 36.52396011352539
Epoch 800 Loss 33.680030822753906
Epoch 900 Loss 31.907604217529297
last loss 30.25613784790039
23 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 85737744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0386870984439156 val loss: 0.08676510630175471
8961 MiB free out of 48676 MiB total
epoch 1 loss: 0.016607616635155864 val loss: 0.08643236244097352
8953 MiB free out of 48676 MiB total
epoch 2 loss: 0.012091513472114457 val loss: 0.08623561961576343
8953 MiB free out of 48676 MiB total
epoch 3 loss: 0.010119173566636164 val loss: 0.08618504228070378
8953 MiB free out of 48676 MiB total
epoch 4 loss: 0.008436751766566886 val loss: 0.08609319850802422
8953 MiB free out of 48676 MiB total
epoch 5 loss: 0.007749547778075794 val loss: 0.08602838264778256
8953 MiB free out of 48676 MiB total
epoch 6 loss: 0.007225780911539914 val loss: 0.08599012019112706
8953 MiB free out of 48676 MiB total
epoch 7 loss: 0.006920672394699068 val loss: 0.08593808114528656
8953 MiB free out of 48676 MiB total
epoch 8 loss: 0.006336561191346846 val loss: 0.08587750233709812
8953 MiB free out of 48676 MiB total
epoch 9 loss: 0.006080096904042875 val loss: 0.08581509813666344
8953 MiB free out of 48676 MiB total
37863 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8953 MiB free out of 48676 MiB total
after cast to cpu
37863 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
24 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.4107, 1.4404, 1.4210,  ..., 1.3930, 1.3714, 1.3905], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.3660, 1.3716, 1.4155,  ..., 1.6762, 1.7466, 1.7519], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 1714.24072265625
Epoch 100 Loss 78.51985168457031
Epoch 200 Loss 54.95735549926758
Epoch 300 Loss 41.24477005004883
Epoch 400 Loss 33.04314041137695
Epoch 500 Loss 28.62579917907715
Epoch 600 Loss 27.1437931060791
Epoch 700 Loss 25.725509643554688
Epoch 800 Loss 24.383346557617188
Epoch 900 Loss 23.12262535095215
last loss 21.95467185974121
24 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.2120, 1.1798, 1.1826,  ..., 1.2159, 1.1909, 1.2084], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.2397, 1.2433, 1.2546,  ..., 1.1785, 1.1836, 1.1816], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 1560.0302734375
Epoch 100 Loss 93.79820251464844
Epoch 200 Loss 71.46527099609375
Epoch 300 Loss 58.1836051940918
Epoch 400 Loss 48.365142822265625
Epoch 500 Loss 41.18433380126953
Epoch 600 Loss 35.82014846801758
Epoch 700 Loss 31.682300567626953
Epoch 800 Loss 28.393856048583984
Epoch 900 Loss 25.717273712158203
last loss 24.113399505615234
24 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.3881, 1.3955, 1.3956,  ..., 1.4213, 1.4150, 1.4089], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.3741, 1.3726, 1.4193,  ..., 1.6439, 1.7088, 1.6957], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 1698.4840087890625
Epoch 100 Loss 73.53238677978516
Epoch 200 Loss 53.311790466308594
Epoch 300 Loss 44.11656188964844
Epoch 400 Loss 37.22494888305664
Epoch 500 Loss 32.081153869628906
Epoch 600 Loss 28.150163650512695
Epoch 700 Loss 25.059328079223633
Epoch 800 Loss 22.56468963623047
Epoch 900 Loss 20.50547981262207
last loss 18.888233184814453
24 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.2159, 1.2214, 1.2327,  ..., 1.1572, 1.1644, 1.1607], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.1998, 1.1973, 1.1921,  ..., 1.1950, 1.1950, 1.1825], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 1860.1971435546875
Epoch 100 Loss 108.36726379394531
Epoch 200 Loss 84.65288543701172
Epoch 300 Loss 71.49896240234375
Epoch 400 Loss 61.03805160522461
Epoch 500 Loss 52.92184829711914
Epoch 600 Loss 46.56522750854492
Epoch 700 Loss 41.481544494628906
Epoch 800 Loss 37.32843780517578
Epoch 900 Loss 33.874271392822266
last loss 30.985797882080078
24 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 85737744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.04021979893150274 val loss: 0.10100972838699818
8961 MiB free out of 48676 MiB total
epoch 1 loss: 0.01680660069541773 val loss: 0.1010081134736538
8953 MiB free out of 48676 MiB total
epoch 2 loss: 0.011816745864052791 val loss: 0.1010041912086308
8953 MiB free out of 48676 MiB total
epoch 3 loss: 0.009514765821222682 val loss: 0.10085343103855848
8953 MiB free out of 48676 MiB total
epoch 4 loss: 0.008461959765554639 val loss: 0.100670016836375
8953 MiB free out of 48676 MiB total
epoch 5 loss: 0.007489239524147706 val loss: 0.10047333035618067
8953 MiB free out of 48676 MiB total
epoch 6 loss: 0.007101179435267113 val loss: 0.10034182807430625
8953 MiB free out of 48676 MiB total
epoch 7 loss: 0.006695367621432524 val loss: 0.10023016948252916
8953 MiB free out of 48676 MiB total
epoch 8 loss: 0.006448383879614994 val loss: 0.10024112090468407
8953 MiB free out of 48676 MiB total
epoch 9 loss: 0.0061217043239594204 val loss: 0.10007718903943896
8953 MiB free out of 48676 MiB total
37863 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8953 MiB free out of 48676 MiB total
after cast to cpu
37863 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
25 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.4192, 1.4690, 1.4127,  ..., 1.4213, 1.3962, 1.4222], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.5660, 0.5315, 0.5776,  ..., 1.6343, 1.7744, 1.7085], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 2381.46240234375
Epoch 100 Loss 104.60760498046875
Epoch 200 Loss 68.72189331054688
Epoch 300 Loss 57.31975555419922
Epoch 400 Loss 48.703590393066406
Epoch 500 Loss 42.24647903442383
Epoch 600 Loss 37.29912567138672
Epoch 700 Loss 33.393638610839844
Epoch 800 Loss 30.221071243286133
Epoch 900 Loss 27.58053970336914
last loss 26.12615203857422
25 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.2550, 1.2244, 1.2299,  ..., 1.2444, 1.2465, 1.2497], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.3124, 1.3050, 1.3245,  ..., 1.2849, 1.3018, 1.2839], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 2023.777587890625
Epoch 100 Loss 123.7855224609375
Epoch 200 Loss 90.78892517089844
Epoch 300 Loss 69.43648529052734
Epoch 400 Loss 55.68174362182617
Epoch 500 Loss 46.40920639038086
Epoch 600 Loss 39.81540298461914
Epoch 700 Loss 36.760223388671875
Epoch 800 Loss 35.227630615234375
Epoch 900 Loss 33.738075256347656
last loss 32.31572723388672
25 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.4169, 1.3969, 1.4229,  ..., 1.4106, 1.3910, 1.4140], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.5655, 0.5461, 0.5966,  ..., 1.6563, 1.7658, 1.6648], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 2303.2744140625
Epoch 100 Loss 97.94853973388672
Epoch 200 Loss 69.00421142578125
Epoch 300 Loss 57.36841583251953
Epoch 400 Loss 48.59728240966797
Epoch 500 Loss 42.05022430419922
Epoch 600 Loss 37.046485900878906
Epoch 700 Loss 33.09961700439453
Epoch 800 Loss 29.89458656311035
Epoch 900 Loss 27.228178024291992
last loss 25.07823371887207
25 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.2653, 1.2502, 1.2811,  ..., 1.2720, 1.2811, 1.2801], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.2426, 1.2505, 1.2296,  ..., 1.2162, 1.2488, 1.2485], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 2618.48974609375
Epoch 100 Loss 151.83509826660156
Epoch 200 Loss 106.4224853515625
Epoch 300 Loss 80.29225158691406
Epoch 400 Loss 64.42223358154297
Epoch 500 Loss 53.89680099487305
Epoch 600 Loss 47.960941314697266
Epoch 700 Loss 45.565216064453125
Epoch 800 Loss 43.27107238769531
Epoch 900 Loss 41.09407043457031
last loss 39.06060791015625
25 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 85737744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.04048526473343372 val loss: 0.1245552976615727
8961 MiB free out of 48676 MiB total
epoch 1 loss: 0.018530845220084302 val loss: 0.124056211207062
8953 MiB free out of 48676 MiB total
epoch 2 loss: 0.013976926937175449 val loss: 0.12371894624084234
8953 MiB free out of 48676 MiB total
epoch 3 loss: 0.011785469545429805 val loss: 0.12356720492243767
8953 MiB free out of 48676 MiB total
epoch 4 loss: 0.01043722891699872 val loss: 0.12346991384401917
8953 MiB free out of 48676 MiB total
epoch 5 loss: 0.009600449975550873 val loss: 0.12337192939594388
8953 MiB free out of 48676 MiB total
epoch 6 loss: 0.008720924550289055 val loss: 0.12331411894410849
8953 MiB free out of 48676 MiB total
epoch 7 loss: 0.008279620411485666 val loss: 0.123309928458184
8953 MiB free out of 48676 MiB total
epoch 8 loss: 0.008020333170861704 val loss: 0.12325072102248669
8953 MiB free out of 48676 MiB total
epoch 9 loss: 0.007832000745111145 val loss: 0.1231989786028862
8953 MiB free out of 48676 MiB total
37863 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8953 MiB free out of 48676 MiB total
after cast to cpu
37863 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
26 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.3885, 1.4287, 1.3924,  ..., 1.4006, 1.3950, 1.3818], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.1274, 1.1214, 1.1409,  ..., 1.8750, 1.8001, 1.7344], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 2033.12841796875
Epoch 100 Loss 98.41625213623047
Epoch 200 Loss 66.06559753417969
Epoch 300 Loss 58.05943298339844
Epoch 400 Loss 51.127689361572266
Epoch 500 Loss 45.3902587890625
Epoch 600 Loss 40.69432830810547
Epoch 700 Loss 36.830909729003906
Epoch 800 Loss 33.61255645751953
Epoch 900 Loss 30.89185905456543
last loss 28.580381393432617
26 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.2695, 1.2609, 1.2552,  ..., 1.2922, 1.2361, 1.2380], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.3299, 1.3174, 1.3327,  ..., 1.2755, 1.2536, 1.2610], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 1895.35400390625
Epoch 100 Loss 119.9036636352539
Epoch 200 Loss 91.89073181152344
Epoch 300 Loss 72.98685455322266
Epoch 400 Loss 59.66551208496094
Epoch 500 Loss 50.2421760559082
Epoch 600 Loss 43.39811706542969
Epoch 700 Loss 38.24644470214844
Epoch 800 Loss 34.223270416259766
Epoch 900 Loss 30.97614860534668
last loss 29.178970336914062
26 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.3872, 1.3984, 1.4091,  ..., 1.3887, 1.3689, 1.4181], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.1334, 1.1177, 1.1413,  ..., 0.8847, 1.7802, 1.8081], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 1893.67919921875
Epoch 100 Loss 85.80896759033203
Epoch 200 Loss 57.29185485839844
Epoch 300 Loss 48.28926086425781
Epoch 400 Loss 41.2601318359375
Epoch 500 Loss 35.87815475463867
Epoch 600 Loss 31.700366973876953
Epoch 700 Loss 28.377391815185547
Epoch 800 Loss 25.666948318481445
Epoch 900 Loss 23.406171798706055
last loss 21.50240707397461
26 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.3113, 1.3014, 1.3114,  ..., 1.2673, 1.2453, 1.2514], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.2578, 1.2535, 1.2387,  ..., 1.2614, 1.2616, 1.2322], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 2545.549560546875
Epoch 100 Loss 145.20799255371094
Epoch 200 Loss 119.67877960205078
Epoch 300 Loss 100.16644287109375
Epoch 400 Loss 84.93803405761719
Epoch 500 Loss 73.35216522216797
Epoch 600 Loss 64.43362426757812
Epoch 700 Loss 57.407562255859375
Epoch 800 Loss 51.744422912597656
Epoch 900 Loss 47.08544921875
last loss 43.21625900268555
26 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 85737744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.04510168402339332 val loss: 0.1336840409785509
8961 MiB free out of 48676 MiB total
epoch 1 loss: 0.0202663998643402 val loss: 0.13292749784886837
8953 MiB free out of 48676 MiB total
epoch 2 loss: 0.015430286362970946 val loss: 0.13242736086249352
8953 MiB free out of 48676 MiB total
epoch 3 loss: 0.013194519087846857 val loss: 0.13217362854629755
8953 MiB free out of 48676 MiB total
epoch 4 loss: 0.011014271753083449 val loss: 0.131828882265836
8953 MiB free out of 48676 MiB total
epoch 5 loss: 0.010082101445732405 val loss: 0.1316321287304163
8953 MiB free out of 48676 MiB total
epoch 6 loss: 0.010103916963998927 val loss: 0.13140036864206195
8953 MiB free out of 48676 MiB total
epoch 7 loss: 0.009132247931120219 val loss: 0.13124636746942997
8953 MiB free out of 48676 MiB total
epoch 8 loss: 0.008536719742551213 val loss: 0.13117927126586437
8953 MiB free out of 48676 MiB total
epoch 9 loss: 0.008450104163785 val loss: 0.13116375915706158
8953 MiB free out of 48676 MiB total
37863 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8953 MiB free out of 48676 MiB total
after cast to cpu
37863 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
27 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.4432, 1.4596, 1.4588,  ..., 1.4564, 1.4578, 1.4376], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.2920, 1.3262, 1.3285,  ..., 1.5665, 1.5050, 1.6066], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 2623.205078125
Epoch 100 Loss 122.50031280517578
Epoch 200 Loss 90.26156616210938
Epoch 300 Loss 73.54216766357422
Epoch 400 Loss 61.4395751953125
Epoch 500 Loss 52.63715362548828
Epoch 600 Loss 46.036277770996094
Epoch 700 Loss 40.92245101928711
Epoch 800 Loss 36.842987060546875
Epoch 900 Loss 33.504241943359375
last loss 31.366046905517578
27 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.2719, 1.2507, 1.2797,  ..., 1.2662, 1.2425, 1.2610], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.2851, 1.2833, 1.2784,  ..., 1.4586, 1.4483, 1.4834], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 2208.2685546875
Epoch 100 Loss 116.86502838134766
Epoch 200 Loss 89.4281234741211
Epoch 300 Loss 70.4874496459961
Epoch 400 Loss 57.68446731567383
Epoch 500 Loss 48.7657470703125
Epoch 600 Loss 42.25598907470703
Epoch 700 Loss 37.290557861328125
Epoch 800 Loss 33.36880111694336
Epoch 900 Loss 30.694631576538086
last loss 29.30764389038086
27 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.4110, 1.4780, 1.4380,  ..., 1.4506, 1.4507, 1.4779], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.3049, 1.3313, 1.3302,  ..., 1.5344, 1.4997, 1.0326], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 2482.611083984375
Epoch 100 Loss 109.18505096435547
Epoch 200 Loss 80.56591796875
Epoch 300 Loss 68.27349853515625
Epoch 400 Loss 58.54756164550781
Epoch 500 Loss 51.03622817993164
Epoch 600 Loss 45.18474197387695
Epoch 700 Loss 40.53166198730469
Epoch 800 Loss 36.74602508544922
Epoch 900 Loss 33.60002899169922
last loss 30.96128273010254
27 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.2766, 1.2844, 1.2753,  ..., 1.5237, 1.4909, 1.5521], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.2326, 1.2740, 1.2605,  ..., 1.2930, 1.2936, 1.2707], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 3313.98046875
Epoch 100 Loss 171.56092834472656
Epoch 200 Loss 138.79270935058594
Epoch 300 Loss 113.53562927246094
Epoch 400 Loss 94.84649658203125
Epoch 500 Loss 81.11299896240234
Epoch 600 Loss 70.82725524902344
Epoch 700 Loss 62.8911247253418
Epoch 800 Loss 56.571964263916016
Epoch 900 Loss 51.40006637573242
last loss 47.112159729003906
27 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 85737744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.04776895436225459 val loss: 0.19698114693164825
8961 MiB free out of 48676 MiB total
epoch 1 loss: 0.02103128229646245 val loss: 0.1959359822794795
8953 MiB free out of 48676 MiB total
epoch 2 loss: 0.015909204048512038 val loss: 0.19498018641024828
8953 MiB free out of 48676 MiB total
epoch 3 loss: 0.013681838718184736 val loss: 0.19415179826319218
8953 MiB free out of 48676 MiB total
epoch 4 loss: 0.011592612139793346 val loss: 0.19364914018660784
8953 MiB free out of 48676 MiB total
epoch 5 loss: 0.011060577413445571 val loss: 0.1933932388201356
8953 MiB free out of 48676 MiB total
epoch 6 loss: 0.010317242325982079 val loss: 0.1932337051257491
8953 MiB free out of 48676 MiB total
epoch 7 loss: 0.009862003495072713 val loss: 0.19302472658455372
8953 MiB free out of 48676 MiB total
epoch 8 loss: 0.009088531274755951 val loss: 0.19314490910619497
8953 MiB free out of 48676 MiB total
epoch 9 loss: 0.008719876430404838 val loss: 0.19310752395540476
8953 MiB free out of 48676 MiB total
37863 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8953 MiB free out of 48676 MiB total
after cast to cpu
37863 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
28 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.4238, 1.4306, 1.4203,  ..., 1.4233, 1.4031, 1.4396], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.7822, 0.7447, 0.7523,  ..., 1.7620, 1.7353, 1.7418], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 2462.6435546875
Epoch 100 Loss 116.3267822265625
Epoch 200 Loss 92.05433654785156
Epoch 300 Loss 74.26184844970703
Epoch 400 Loss 61.62945556640625
Epoch 500 Loss 52.606788635253906
Epoch 600 Loss 45.945716857910156
Epoch 700 Loss 40.83600997924805
Epoch 800 Loss 36.77665328979492
Epoch 900 Loss 33.457523345947266
last loss 30.89012908935547
28 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.2854, 1.2936, 1.3251,  ..., 1.2990, 1.3177, 1.3131], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.4762, 1.4871, 1.5179,  ..., 1.4363, 1.4516, 1.4401], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 2435.873046875
Epoch 100 Loss 137.15921020507812
Epoch 200 Loss 98.44021606445312
Epoch 300 Loss 74.59748840332031
Epoch 400 Loss 59.687164306640625
Epoch 500 Loss 49.83184051513672
Epoch 600 Loss 42.89617156982422
Epoch 700 Loss 38.99641799926758
Epoch 800 Loss 36.94270324707031
Epoch 900 Loss 35.01695251464844
last loss 33.23626708984375
28 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.3899, 1.4050, 1.4191,  ..., 1.3927, 1.3823, 1.4056], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.6978, 0.7669, 0.7800,  ..., 1.5473, 1.6398, 1.7314], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 2332.48486328125
Epoch 100 Loss 97.62886810302734
Epoch 200 Loss 65.70579528808594
Epoch 300 Loss 55.19466018676758
Epoch 400 Loss 47.08214569091797
Epoch 500 Loss 40.9306526184082
Epoch 600 Loss 36.186279296875
Epoch 700 Loss 32.425010681152344
Epoch 800 Loss 29.36359214782715
Epoch 900 Loss 26.817981719970703
last loss 24.685161590576172
28 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.5029, 1.4821, 1.5049,  ..., 1.4681, 1.4612, 1.4610], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.2865, 1.2987, 1.3002,  ..., 1.3400, 1.3499, 1.3088], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 3692.88330078125
Epoch 100 Loss 214.56605529785156
Epoch 200 Loss 154.82566833496094
Epoch 300 Loss 122.61150360107422
Epoch 400 Loss 100.18675231933594
Epoch 500 Loss 84.50519561767578
Epoch 600 Loss 73.1863021850586
Epoch 700 Loss 64.64871215820312
Epoch 800 Loss 57.946617126464844
Epoch 900 Loss 54.90785217285156
last loss 52.327354431152344
28 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 85737744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.05787070434598718 val loss: 0.2576732784509659
8961 MiB free out of 48676 MiB total
epoch 1 loss: 0.024734207589062862 val loss: 0.2581317853182554
8953 MiB free out of 48676 MiB total
epoch 2 loss: 0.01780632810550742 val loss: 0.2575407652184367
8953 MiB free out of 48676 MiB total
epoch 3 loss: 0.01503839799988782 val loss: 0.256711826659739
8953 MiB free out of 48676 MiB total
epoch 4 loss: 0.013814963214827003 val loss: 0.2560531571507454
8953 MiB free out of 48676 MiB total
epoch 5 loss: 0.013136167468474014 val loss: 0.25549477245658636
8953 MiB free out of 48676 MiB total
epoch 6 loss: 0.011757957479858305 val loss: 0.2550612594932318
8953 MiB free out of 48676 MiB total
epoch 7 loss: 0.01145018634997541 val loss: 0.25479397270828485
8953 MiB free out of 48676 MiB total
epoch 8 loss: 0.010674696113710525 val loss: 0.2545485245063901
8953 MiB free out of 48676 MiB total
epoch 9 loss: 0.010062720793939661 val loss: 0.25406128354370594
8953 MiB free out of 48676 MiB total
37863 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8953 MiB free out of 48676 MiB total
after cast to cpu
37863 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
29 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.3682, 1.4166, 1.3816,  ..., 1.4090, 1.3446, 1.3857], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.9166, 0.9375, 0.9786,  ..., 1.5665, 1.7960, 1.9091], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 1967.330322265625
Epoch 100 Loss 95.303955078125
Epoch 200 Loss 65.66050720214844
Epoch 300 Loss 54.681495666503906
Epoch 400 Loss 46.296958923339844
Epoch 500 Loss 39.99259948730469
Epoch 600 Loss 35.180416107177734
Epoch 700 Loss 31.409421920776367
Epoch 800 Loss 28.37156867980957
Epoch 900 Loss 25.8638973236084
last loss 23.772464752197266
29 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.3328, 1.2889, 1.3329,  ..., 1.2950, 1.3074, 1.3143], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.2784, 1.2410, 1.2657,  ..., 1.2642, 1.2732, 1.2683], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 2195.0625
Epoch 100 Loss 131.94894409179688
Epoch 200 Loss 91.60887145996094
Epoch 300 Loss 68.4421615600586
Epoch 400 Loss 54.264198303222656
Epoch 500 Loss 46.92742919921875
Epoch 600 Loss 44.55583190917969
Epoch 700 Loss 42.26583480834961
Epoch 800 Loss 40.08378601074219
Epoch 900 Loss 38.02424621582031
last loss 36.11111068725586
29 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.3790, 1.3933, 1.3722,  ..., 1.3567, 1.3229, 1.3394], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.8758, 0.9288, 0.9866,  ..., 1.5234, 1.7083, 1.3679], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 1877.5440673828125
Epoch 100 Loss 92.78694915771484
Epoch 200 Loss 49.932373046875
Epoch 300 Loss 40.413856506347656
Epoch 400 Loss 36.370513916015625
Epoch 500 Loss 32.85877990722656
Epoch 600 Loss 29.85622787475586
Epoch 700 Loss 27.291532516479492
Epoch 800 Loss 25.088314056396484
Epoch 900 Loss 23.180320739746094
last loss 21.529294967651367
29 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.2741, 1.2564, 1.2702,  ..., 1.2577, 1.2643, 1.2598], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.3297, 1.2958, 1.2935,  ..., 1.3316, 1.3571, 1.3266], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 3207.956298828125
Epoch 100 Loss 175.91278076171875
Epoch 200 Loss 138.09584045410156
Epoch 300 Loss 116.3527603149414
Epoch 400 Loss 99.08647155761719
Epoch 500 Loss 85.75193786621094
Epoch 600 Loss 75.39918518066406
Epoch 700 Loss 67.2225341796875
Epoch 800 Loss 60.62936782836914
Epoch 900 Loss 55.19940185546875
last loss 50.6801643371582
29 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 85737744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.06810959399444982 val loss: 0.9886996112763882
8961 MiB free out of 48676 MiB total
epoch 1 loss: 0.02734922965464648 val loss: 1.122215285897255
8953 MiB free out of 48676 MiB total
epoch 2 loss: 0.019064830463321414 val loss: 1.0684568621218204
8953 MiB free out of 48676 MiB total
epoch 3 loss: 0.01624441814783495 val loss: 1.1044192351400852
8953 MiB free out of 48676 MiB total
epoch 4 loss: 0.014018746267538518 val loss: 1.0078802518546581
8953 MiB free out of 48676 MiB total
epoch 5 loss: 0.012563979580590967 val loss: 0.929672010242939
8953 MiB free out of 48676 MiB total
epoch 6 loss: 0.012109701427107211 val loss: 0.9052033759653568
8953 MiB free out of 48676 MiB total
epoch 7 loss: 0.010994287447829265 val loss: 0.8588615581393242
8953 MiB free out of 48676 MiB total
epoch 8 loss: 0.010623786718497286 val loss: 0.8466367907822132
8953 MiB free out of 48676 MiB total
epoch 9 loss: 0.010518644736293936 val loss: 0.8110033515840769
8953 MiB free out of 48676 MiB total
37863 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8953 MiB free out of 48676 MiB total
after cast to cpu
37863 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
30 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.4019, 1.4178, 1.3976,  ..., 1.3864, 1.3602, 1.3972], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.0182, 1.0110, 1.0278,  ..., 1.4493, 1.4746, 1.4525], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 2359.32421875
Epoch 100 Loss 120.70774841308594
Epoch 200 Loss 64.78179168701172
Epoch 300 Loss 56.619384765625
Epoch 400 Loss 50.05071258544922
Epoch 500 Loss 44.62440490722656
Epoch 600 Loss 40.174930572509766
Epoch 700 Loss 36.493675231933594
Epoch 800 Loss 33.40245056152344
Epoch 900 Loss 30.7664794921875
last loss 28.508934020996094
30 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.3517, 1.3246, 1.3371,  ..., 1.3470, 1.3623, 1.3652], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.3536, 1.3972, 1.3478,  ..., 1.4740, 1.4633, 1.5104], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 2676.01904296875
Epoch 100 Loss 149.85244750976562
Epoch 200 Loss 96.366943359375
Epoch 300 Loss 69.71794128417969
Epoch 400 Loss 61.153053283691406
Epoch 500 Loss 56.47595977783203
Epoch 600 Loss 52.24861526489258
Epoch 700 Loss 48.48101043701172
Epoch 800 Loss 45.136558532714844
Epoch 900 Loss 42.16236877441406
last loss 39.53028869628906
30 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.3731, 1.4261, 1.3994,  ..., 1.3547, 1.3360, 1.3441], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.0392, 1.0339, 1.0422,  ..., 1.4349, 1.4045, 1.4240], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 2240.01513671875
Epoch 100 Loss 97.62179565429688
Epoch 200 Loss 66.67642211914062
Epoch 300 Loss 57.396934509277344
Epoch 400 Loss 49.75947570800781
Epoch 500 Loss 43.706851959228516
Epoch 600 Loss 38.91827392578125
Epoch 700 Loss 35.07288360595703
Epoch 800 Loss 31.919940948486328
Epoch 900 Loss 29.280181884765625
last loss 27.049917221069336
30 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.3750, 1.4141, 1.3648,  ..., 1.4676, 1.4893, 1.4816], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.3430, 1.3294, 1.3135,  ..., 1.3429, 1.3585, 1.3574], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 4417.7666015625
Epoch 100 Loss 257.2987365722656
Epoch 200 Loss 184.9617462158203
Epoch 300 Loss 139.94967651367188
Epoch 400 Loss 112.03594970703125
Epoch 500 Loss 93.54507446289062
Epoch 600 Loss 80.50843048095703
Epoch 700 Loss 70.8221206665039
Epoch 800 Loss 66.43585205078125
Epoch 900 Loss 64.05448913574219
last loss 61.74283981323242
30 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 85737744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 4.611585418082541 val loss: 54.2182032763958
8961 MiB free out of 48676 MiB total
epoch 1 loss: 1.493514113331912 val loss: 55.694605976343155
8953 MiB free out of 48676 MiB total
epoch 2 loss: 1.4374710978590883 val loss: 43.575013510882854
8953 MiB free out of 48676 MiB total
epoch 3 loss: 0.538420429060352 val loss: 51.45890881866217
8953 MiB free out of 48676 MiB total
epoch 4 loss: 0.025886218470986933 val loss: 51.261840641498566
8953 MiB free out of 48676 MiB total
epoch 5 loss: 0.03242605480045313 val loss: 37.62755957990885
8953 MiB free out of 48676 MiB total
epoch 6 loss: 0.022665231423161458 val loss: 38.25873391330242
8953 MiB free out of 48676 MiB total
epoch 7 loss: 0.02087406811915571 val loss: 39.28094080090523
8953 MiB free out of 48676 MiB total
epoch 8 loss: 0.018971782563312445 val loss: 38.71972352266312
8953 MiB free out of 48676 MiB total
epoch 9 loss: 0.027232758071477292 val loss: 37.34913443773985
8953 MiB free out of 48676 MiB total
37863 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8953 MiB free out of 48676 MiB total
after cast to cpu
37863 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
31 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.4408, 1.4562, 1.4368,  ..., 1.4340, 1.4106, 1.4039], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.9789, 0.9707, 1.0469,  ..., 1.6500, 1.6098, 1.6455], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 1457.9219970703125
Epoch 100 Loss 67.58380889892578
Epoch 200 Loss 40.00696563720703
Epoch 300 Loss 36.079795837402344
Epoch 400 Loss 32.47797393798828
Epoch 500 Loss 29.324800491333008
Epoch 600 Loss 26.62565803527832
Epoch 700 Loss 24.328725814819336
Epoch 800 Loss 22.368478775024414
Epoch 900 Loss 20.683334350585938
last loss 19.2349910736084
31 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.1832, 1.2165, 1.2963,  ..., 1.1931, 1.2608, 1.2214], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.1387, 1.1403, 1.1530,  ..., 1.1925, 1.2135, 1.2031], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 1463.3475341796875
Epoch 100 Loss 86.27842712402344
Epoch 200 Loss 63.85832214355469
Epoch 300 Loss 49.3387451171875
Epoch 400 Loss 39.75875473022461
Epoch 500 Loss 33.241172790527344
Epoch 600 Loss 28.62424659729004
Epoch 700 Loss 25.200729370117188
Epoch 800 Loss 23.163898468017578
Epoch 900 Loss 22.29027557373047
last loss 21.4488582611084
31 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.3578, 1.4003, 1.4142,  ..., 1.4286, 1.3539, 1.3484], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.9185, 0.9745, 1.0542,  ..., 1.5913, 1.5933, 1.6090], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 1278.8646240234375
Epoch 100 Loss 56.70404052734375
Epoch 200 Loss 31.25128173828125
Epoch 300 Loss 27.356136322021484
Epoch 400 Loss 24.110544204711914
Epoch 500 Loss 21.456188201904297
Epoch 600 Loss 19.287412643432617
Epoch 700 Loss 17.492538452148438
Epoch 800 Loss 15.983163833618164
Epoch 900 Loss 14.694795608520508
last loss 13.591453552246094
31 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.2308, 1.2274, 1.2312,  ..., 1.2517, 1.2610, 1.2420], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.1774, 1.2117, 1.2020,  ..., 1.2123, 1.2554, 1.2226], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
A
B
Epoch 0 Loss 1855.9541015625
Epoch 100 Loss 103.844482421875
Epoch 200 Loss 71.96871948242188
Epoch 300 Loss 58.99843215942383
Epoch 400 Loss 49.30815887451172
Epoch 500 Loss 42.16465759277344
Epoch 600 Loss 36.818214416503906
Epoch 700 Loss 32.714561462402344
Epoch 800 Loss 29.469707489013672
Epoch 900 Loss 26.82599639892578
last loss 24.634418487548828
31 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 85737744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.14809563616290689 val loss: 13.16122442483902
8961 MiB free out of 48676 MiB total
epoch 1 loss: 0.031716121462523006 val loss: 12.849093973636627
8953 MiB free out of 48676 MiB total
epoch 2 loss: 0.018380398563749623 val loss: 12.432263731956482
8953 MiB free out of 48676 MiB total
epoch 3 loss: 0.014140371527901152 val loss: 12.485431551933289
8953 MiB free out of 48676 MiB total
epoch 4 loss: 0.011563239022507332 val loss: 12.243227362632751
8953 MiB free out of 48676 MiB total
epoch 5 loss: 0.009887982429063413 val loss: 12.50023102760315
8953 MiB free out of 48676 MiB total
epoch 6 loss: 0.00907730668768636 val loss: 12.497290909290314
8953 MiB free out of 48676 MiB total
epoch 7 loss: 0.008436828284175135 val loss: 12.516032755374908
8953 MiB free out of 48676 MiB total
epoch 8 loss: 0.007749074051389471 val loss: 12.344188451766968
8953 MiB free out of 48676 MiB total
epoch 9 loss: 0.007118717359844595 val loss: 12.310677647590637
8953 MiB free out of 48676 MiB total
early stopping
37863 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8953 MiB free out of 48676 MiB total
after cast to cpu
37863 MiB free out of 48676 MiB total
Total bits: tensor(43901829120, device='cuda:4') Total params: 6476005376
average bits per value: tensor(6.7792, device='cuda:4')
total time taken: 11642.321622371674
Loading tokenizer for meta-llama/Llama-2-7b-hf
/home/lliu/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Dataset: wikitext2
Evaluating ...
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
Perplexity: 93303.968750
