/home/lliu/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.01s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.54it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.42it/s]
Model loaded. LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
4096
Loading tokenizer for meta-llama/Llama-2-7b-hf
Starting...
38748 MiB free out of 48676 MiB total
attention_mask torch.Size([1, 1, 4096, 4096]) cuda:6 torch.float16
position_ids torch.Size([1, 4096]) cuda:6 torch.int64
position_ids torch.Size([1, 4096])
attention_mask torch.Size([1, 1, 4096, 4096])
Ready.
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
0 self_attn.q_proj
Pruning ...
256
iter 0, train loss 1.539240837097168, val loss 1.5551683902740479
iter 10, train loss 1.3275091648101807, val loss 1.3391706943511963
iter 20, train loss 1.3247530460357666, val loss 1.334544062614441
iter 30, train loss 1.0055323839187622, val loss 1.0133731365203857
iter 40, train loss 0.8627423048019409, val loss 0.8695060014724731
iter 50, train loss 0.9098653197288513, val loss 0.9162530899047852
iter 60, train loss 0.7742913961410522, val loss 0.7799912691116333
iter 70, train loss 0.7714749574661255, val loss 0.7776618599891663
iter 80, train loss 0.772995114326477, val loss 0.7791616916656494
iter 90, train loss 0.7368890047073364, val loss 0.7423539757728577
best loss 0.7024013996124268
not here
quantized in 34.020304679870605 seconds
36490 MiB free out of 48676 MiB total
0 self_attn.k_proj
Pruning ...
256
iter 0, train loss 1.238694429397583, val loss 1.249128818511963
iter 10, train loss 0.9339669942855835, val loss 0.9410219192504883
iter 20, train loss 0.8166754245758057, val loss 0.8223265409469604
iter 30, train loss 0.7943630814552307, val loss 0.7983620762825012
iter 40, train loss 0.687296450138092, val loss 0.6897246837615967
iter 50, train loss 0.6478574275970459, val loss 0.6498303413391113
iter 60, train loss 0.6012144684791565, val loss 0.6028896570205688
iter 70, train loss 0.577594518661499, val loss 0.5793419480323792
iter 80, train loss 0.5741013288497925, val loss 0.5755456686019897
iter 90, train loss 0.5419108867645264, val loss 0.5433346033096313
best loss 0.5275446176528931
not here
quantized in 32.639254570007324 seconds
36458 MiB free out of 48676 MiB total
0 self_attn.v_proj
Pruning ...
256
iter 0, train loss 0.0800636038184166, val loss 0.07972458750009537
iter 10, train loss 0.07562747597694397, val loss 0.07512325048446655
iter 20, train loss 0.07391969859600067, val loss 0.07341375946998596
iter 30, train loss 0.07260031253099442, val loss 0.07206330448389053
iter 40, train loss 0.07057321071624756, val loss 0.07006508111953735
iter 50, train loss 0.06959658861160278, val loss 0.06908179819583893
iter 60, train loss 0.06845349073410034, val loss 0.06795002520084381
iter 70, train loss 0.06807161867618561, val loss 0.0675438642501831
iter 80, train loss 0.06756667792797089, val loss 0.06710143387317657
iter 90, train loss 0.06704296171665192, val loss 0.06659063696861267
best loss 0.06644072383642197
not here
quantized in 32.82712697982788 seconds
36458 MiB free out of 48676 MiB total
0 self_attn.o_proj
Pruning ...
256
iter 0, train loss 0.010799665004014969, val loss 0.010869271121919155
iter 10, train loss 0.010423609986901283, val loss 0.010483304969966412
iter 20, train loss 0.009595566429197788, val loss 0.009591903537511826
iter 30, train loss 0.010089416988193989, val loss 0.010201715864241123
iter 40, train loss 0.008834700100123882, val loss 0.008835170418024063
iter 50, train loss 0.008693552576005459, val loss 0.008686389774084091
iter 60, train loss 0.00831171777099371, val loss 0.00830650795251131
iter 70, train loss 0.008381731808185577, val loss 0.00838275533169508
iter 80, train loss 0.008335040882229805, val loss 0.00832438562065363
iter 90, train loss 0.008234450593590736, val loss 0.008271959610283375
best loss 0.00810930784791708
not here
quantized in 31.966005325317383 seconds
36426 MiB free out of 48676 MiB total
0 mlp.gate_proj
Pruning ...
256
iter 0, train loss 3.9480443000793457, val loss 3.9192986488342285
iter 10, train loss 4.103921413421631, val loss 4.071527481079102
iter 20, train loss 4.0965776443481445, val loss 4.062873840332031
iter 30, train loss 4.034256458282471, val loss 4.00203275680542
iter 40, train loss 3.991262197494507, val loss 3.9600884914398193
iter 50, train loss 3.9763901233673096, val loss 3.9438300132751465
iter 60, train loss 3.9398303031921387, val loss 3.9056570529937744
iter 70, train loss 3.9244437217712402, val loss 3.891392230987549
iter 80, train loss 3.9134325981140137, val loss 3.881011724472046
iter 90, train loss 3.9058918952941895, val loss 3.8738362789154053
best loss 3.746044158935547
not here
quantized in 88.75297904014587 seconds
36124 MiB free out of 48676 MiB total
0 mlp.up_proj
Pruning ...
256
iter 0, train loss 3.5419487953186035, val loss 3.5142924785614014
iter 10, train loss 3.7418277263641357, val loss 3.709853172302246
iter 20, train loss 3.5561232566833496, val loss 3.5260727405548096
iter 30, train loss 3.5499653816223145, val loss 3.5196423530578613
iter 40, train loss 3.5339508056640625, val loss 3.5031657218933105
iter 50, train loss 3.5250306129455566, val loss 3.4944939613342285
iter 60, train loss 3.517113208770752, val loss 3.4856178760528564
iter 70, train loss 3.5172061920166016, val loss 3.4852499961853027
iter 80, train loss 3.5197296142578125, val loss 3.4877350330352783
iter 90, train loss 3.5140538215637207, val loss 3.4823131561279297
best loss 3.362034797668457
not here
quantized in 87.31434345245361 seconds
35930 MiB free out of 48676 MiB total
0 mlp.down_proj
Pruning ...
256
iter 0, train loss 0.010733775794506073, val loss 0.010662795975804329
iter 10, train loss 0.011761096306145191, val loss 0.011644423007965088
iter 20, train loss 0.011560875922441483, val loss 0.011430333368480206
iter 30, train loss 0.0112678911536932, val loss 0.01114411186426878
iter 40, train loss 0.011172942817211151, val loss 0.011053776368498802
iter 50, train loss 0.011102825403213501, val loss 0.01098722405731678
iter 60, train loss 0.01100496668368578, val loss 0.01088614109903574
iter 70, train loss 0.010963489301502705, val loss 0.010848484933376312
iter 80, train loss 0.010969397611916065, val loss 0.010853652842342854
iter 90, train loss 0.010969669558107853, val loss 0.010854902677237988
best loss 0.010006040334701538
not here
quantized in 97.35635089874268 seconds
35736 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35736 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31640 MiB free out of 48676 MiB total
after cast to cpu
37852 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
1 self_attn.q_proj
Pruning ...
256
iter 0, train loss 18.671218872070312, val loss 17.848604202270508
iter 10, train loss 16.12361717224121, val loss 18.106510162353516
iter 20, train loss 16.340694427490234, val loss 20.108675003051758
iter 30, train loss 14.779134750366211, val loss 19.859039306640625
iter 40, train loss 14.145978927612305, val loss 20.225536346435547
iter 50, train loss 13.86923599243164, val loss 20.55295753479004
iter 60, train loss 13.827993392944336, val loss 20.616910934448242
iter 70, train loss 13.868093490600586, val loss 20.61281967163086
iter 80, train loss 13.749003410339355, val loss 20.80767822265625
iter 90, train loss 13.892658233642578, val loss 20.793357849121094
best loss 15.920761108398438
not here
quantized in 36.9700870513916 seconds
36424 MiB free out of 48676 MiB total
1 self_attn.k_proj
Pruning ...
256
iter 0, train loss 17.621417999267578, val loss 17.92568588256836
iter 10, train loss 14.475915908813477, val loss 17.897357940673828
iter 20, train loss 13.898500442504883, val loss 19.463455200195312
iter 30, train loss 13.133769035339355, val loss 19.39789581298828
iter 40, train loss 13.117043495178223, val loss 19.85641098022461
iter 50, train loss 12.953868865966797, val loss 19.7622013092041
iter 60, train loss 12.7927885055542, val loss 19.6282958984375
iter 70, train loss 12.707578659057617, val loss 19.599964141845703
iter 80, train loss 12.589343070983887, val loss 19.58454704284668
iter 90, train loss 12.472760200500488, val loss 19.620101928710938
best loss 15.86257553100586
not here
quantized in 35.59103465080261 seconds
36414 MiB free out of 48676 MiB total
1 self_attn.v_proj
Pruning ...
256
iter 0, train loss 0.8272805213928223, val loss 1.4665054082870483
iter 10, train loss 0.8590883016586304, val loss 1.5860319137573242
iter 20, train loss 0.8043610453605652, val loss 1.4919757843017578
iter 30, train loss 0.7906813621520996, val loss 1.469703197479248
iter 40, train loss 0.7863103151321411, val loss 1.4550304412841797
iter 50, train loss 0.779343843460083, val loss 1.441532850265503
iter 60, train loss 0.7739792466163635, val loss 1.4307425022125244
iter 70, train loss 0.7718281745910645, val loss 1.4292614459991455
iter 80, train loss 0.7697812914848328, val loss 1.428100824356079
iter 90, train loss 0.7713295221328735, val loss 1.4318159818649292
best loss 1.4100278615951538
not here
quantized in 32.3725905418396 seconds
36404 MiB free out of 48676 MiB total
1 self_attn.o_proj
Pruning ...
256
iter 0, train loss 0.08269337564706802, val loss 0.06473755836486816
iter 10, train loss 0.07924914360046387, val loss 0.06825236976146698
iter 20, train loss 0.07754939794540405, val loss 0.0645204409956932
iter 30, train loss 0.07697094976902008, val loss 0.06806758046150208
iter 40, train loss 0.07603880763053894, val loss 0.06959192454814911
iter 50, train loss 0.07526423037052155, val loss 0.06789495050907135
iter 60, train loss 0.07507342100143433, val loss 0.06658965349197388
iter 70, train loss 0.0748424232006073, val loss 0.06847071647644043
iter 80, train loss 0.07479964941740036, val loss 0.06508271396160126
iter 90, train loss 0.07476814091205597, val loss 0.06562875211238861
best loss 0.06342057883739471
not here
quantized in 31.60967469215393 seconds
36372 MiB free out of 48676 MiB total
1 mlp.gate_proj
Pruning ...
256
iter 0, train loss 16.127309799194336, val loss 31.54191017150879
iter 10, train loss 17.086830139160156, val loss 32.6414794921875
iter 20, train loss 16.502248764038086, val loss 31.73851776123047
iter 30, train loss 16.870603561401367, val loss 32.36335372924805
iter 40, train loss 16.968101501464844, val loss 32.46953201293945
iter 50, train loss 16.94450569152832, val loss 32.250144958496094
iter 60, train loss 16.822772979736328, val loss 32.209449768066406
iter 70, train loss 16.746509552001953, val loss 32.084434509277344
iter 80, train loss 16.705236434936523, val loss 32.060081481933594
iter 90, train loss 16.69354248046875, val loss 32.04340362548828
best loss 28.557395935058594
not here
quantized in 88.76836061477661 seconds
36070 MiB free out of 48676 MiB total
1 mlp.up_proj
Pruning ...
256
iter 0, train loss 13.301376342773438, val loss 26.25379180908203
iter 10, train loss 13.800500869750977, val loss 26.627986907958984
iter 20, train loss 13.50333309173584, val loss 26.28631019592285
iter 30, train loss 13.569232940673828, val loss 26.412416458129883
iter 40, train loss 13.48745346069336, val loss 26.179365158081055
iter 50, train loss 13.49337387084961, val loss 26.282785415649414
iter 60, train loss 13.491352081298828, val loss 26.130414962768555
iter 70, train loss 13.454194068908691, val loss 25.91924285888672
iter 80, train loss 13.436654090881348, val loss 25.887638092041016
iter 90, train loss 13.43154525756836, val loss 25.98065185546875
best loss 25.13817596435547
not here
quantized in 86.77945446968079 seconds
35876 MiB free out of 48676 MiB total
1 mlp.down_proj
Pruning ...
256
iter 0, train loss 0.08624444156885147, val loss 0.20525453984737396
iter 10, train loss 0.22511263191699982, val loss 0.2613660395145416
iter 20, train loss 0.16619721055030823, val loss 0.2642354965209961
iter 30, train loss 0.1877800077199936, val loss 0.25758782029151917
iter 40, train loss 0.22354483604431152, val loss 0.25200629234313965
iter 50, train loss 0.13941197097301483, val loss 0.24740737676620483
iter 60, train loss 0.12249491363763809, val loss 0.24065527319908142
iter 70, train loss 0.11449682712554932, val loss 0.2363249659538269
iter 80, train loss 0.1116037368774414, val loss 0.2328844964504242
iter 90, train loss 0.10838258266448975, val loss 0.23127509653568268
best loss 0.20525453984737396
not here
quantized in 96.10140061378479 seconds
35682 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35682 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31586 MiB free out of 48676 MiB total
after cast to cpu
37852 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
2 self_attn.q_proj
Pruning ...
256
iter 0, train loss 63.987396240234375, val loss 79.60752868652344
iter 10, train loss 61.62053680419922, val loss 89.58552551269531
iter 20, train loss 64.15089416503906, val loss 95.25325775146484
iter 30, train loss 61.803627014160156, val loss 92.16419982910156
iter 40, train loss 61.52804183959961, val loss 92.0999755859375
iter 50, train loss 60.915321350097656, val loss 91.15255737304688
iter 60, train loss 60.624839782714844, val loss 91.23285675048828
iter 70, train loss 60.35027313232422, val loss 90.95426940917969
iter 80, train loss 59.970672607421875, val loss 90.45539855957031
iter 90, train loss 59.66295623779297, val loss 90.26271057128906
best loss 76.5041732788086
not here
quantized in 34.90015196800232 seconds
36392 MiB free out of 48676 MiB total
2 self_attn.k_proj
Pruning ...
256
iter 0, train loss 75.14726257324219, val loss 90.54283142089844
iter 10, train loss 69.81607055664062, val loss 100.60801696777344
iter 20, train loss 73.62834167480469, val loss 109.44375610351562
iter 30, train loss 71.59263610839844, val loss 106.96971130371094
iter 40, train loss 70.9399185180664, val loss 106.1327896118164
iter 50, train loss 70.15093231201172, val loss 104.80935668945312
iter 60, train loss 70.04449462890625, val loss 105.03338623046875
iter 70, train loss 69.515625, val loss 104.79920196533203
iter 80, train loss 69.11457824707031, val loss 104.51057434082031
iter 90, train loss 68.9740219116211, val loss 104.36274719238281
best loss 86.87268829345703
not here
quantized in 34.51438903808594 seconds
36382 MiB free out of 48676 MiB total
2 self_attn.v_proj
Pruning ...
256
iter 0, train loss 13.63044261932373, val loss 20.909812927246094
iter 10, train loss 14.254129409790039, val loss 22.314895629882812
iter 20, train loss 13.781143188476562, val loss 21.609710693359375
iter 30, train loss 13.843742370605469, val loss 21.68967628479004
iter 40, train loss 13.785085678100586, val loss 21.667007446289062
iter 50, train loss 13.781538963317871, val loss 21.629247665405273
iter 60, train loss 13.7640962600708, val loss 21.54539680480957
iter 70, train loss 13.711881637573242, val loss 21.508983612060547
iter 80, train loss 13.718884468078613, val loss 21.513458251953125
iter 90, train loss 13.698062896728516, val loss 21.48192596435547
best loss 20.76874542236328
not here
quantized in 32.25138568878174 seconds
36372 MiB free out of 48676 MiB total
2 self_attn.o_proj
Pruning ...
256
iter 0, train loss 0.1595909148454666, val loss 0.956961989402771
iter 10, train loss 0.1601901650428772, val loss 0.9565649032592773
iter 20, train loss 0.1554831713438034, val loss 0.9466530084609985
iter 30, train loss 0.15589098632335663, val loss 0.934089183807373
iter 40, train loss 0.1546006202697754, val loss 0.9425135254859924
iter 50, train loss 0.1542656272649765, val loss 0.9493730664253235
iter 60, train loss 0.15418659150600433, val loss 0.9619113802909851
iter 70, train loss 0.15392683446407318, val loss 0.9862805008888245
iter 80, train loss 0.15380102396011353, val loss 0.9891238212585449
iter 90, train loss 0.1535012423992157, val loss 0.980093777179718
best loss 0.9154627323150635
not here
quantized in 32.41331624984741 seconds
36372 MiB free out of 48676 MiB total
2 mlp.gate_proj
Pruning ...
256
iter 0, train loss 34.28915786743164, val loss 56.25537872314453
iter 10, train loss 35.611183166503906, val loss 59.78286361694336
iter 20, train loss 34.32270050048828, val loss 59.11388397216797
iter 30, train loss 34.515846252441406, val loss 59.82176971435547
iter 40, train loss 34.499359130859375, val loss 59.5004768371582
iter 50, train loss 34.47407150268555, val loss 59.468257904052734
iter 60, train loss 34.477020263671875, val loss 59.98912048339844
iter 70, train loss 34.41581726074219, val loss 60.561492919921875
iter 80, train loss 34.411224365234375, val loss 59.94340896606445
iter 90, train loss 34.396018981933594, val loss 59.44024658203125
best loss 54.402862548828125
not here
quantized in 87.84032726287842 seconds
36070 MiB free out of 48676 MiB total
2 mlp.up_proj
Pruning ...
256
iter 0, train loss 28.089208602905273, val loss 46.089576721191406
iter 10, train loss 28.289688110351562, val loss 47.31450271606445
iter 20, train loss 28.25176239013672, val loss 47.337581634521484
iter 30, train loss 28.197216033935547, val loss 47.71237564086914
iter 40, train loss 28.180389404296875, val loss 47.557228088378906
iter 50, train loss 28.18915557861328, val loss 47.921791076660156
iter 60, train loss 28.189136505126953, val loss 47.860374450683594
iter 70, train loss 28.183074951171875, val loss 47.871337890625
iter 80, train loss 28.179533004760742, val loss 47.66236877441406
iter 90, train loss 28.165489196777344, val loss 47.40408706665039
best loss 46.089576721191406
not here
quantized in 84.74531865119934 seconds
35876 MiB free out of 48676 MiB total
2 mlp.down_proj
Pruning ...
256
iter 0, train loss 0.166778102517128, val loss 0.3791835308074951
iter 10, train loss 0.16629551351070404, val loss 0.38006317615509033
iter 20, train loss 0.16507010161876678, val loss 0.3764325976371765
iter 30, train loss 0.16416674852371216, val loss 0.37754401564598083
iter 40, train loss 0.1638510525226593, val loss 0.37588486075401306
iter 50, train loss 0.16368699073791504, val loss 0.3738150894641876
iter 60, train loss 0.16327086091041565, val loss 0.3683358132839203
iter 70, train loss 0.16304455697536469, val loss 0.36682504415512085
iter 80, train loss 0.1630251407623291, val loss 0.36446109414100647
iter 90, train loss 0.16292919218540192, val loss 0.36148202419281006
best loss 0.360772043466568
not here
quantized in 93.64939951896667 seconds
35682 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35682 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31586 MiB free out of 48676 MiB total
after cast to cpu
37852 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
3 self_attn.q_proj
Pruning ...
256
iter 0, train loss 147.91818237304688, val loss 173.0102996826172
iter 10, train loss 156.4375762939453, val loss 203.86322021484375
iter 20, train loss 151.64804077148438, val loss 199.24595642089844
iter 30, train loss 146.19903564453125, val loss 192.90000915527344
iter 40, train loss 145.9839324951172, val loss 193.71926879882812
iter 50, train loss 145.82791137695312, val loss 193.81936645507812
iter 60, train loss 145.10159301757812, val loss 193.5154571533203
iter 70, train loss 145.06520080566406, val loss 193.66883850097656
iter 80, train loss 144.630859375, val loss 193.57948303222656
iter 90, train loss 144.14212036132812, val loss 192.76837158203125
best loss 172.44882202148438
not here
quantized in 33.96526288986206 seconds
36424 MiB free out of 48676 MiB total
3 self_attn.k_proj
Pruning ...
256
iter 0, train loss 172.34201049804688, val loss 193.6302490234375
iter 10, train loss 172.0762481689453, val loss 224.06692504882812
iter 20, train loss 174.74107360839844, val loss 230.2344207763672
iter 30, train loss 169.73167419433594, val loss 225.353759765625
iter 40, train loss 167.50624084472656, val loss 222.87063598632812
iter 50, train loss 165.63681030273438, val loss 220.975341796875
iter 60, train loss 165.21688842773438, val loss 219.82034301757812
iter 70, train loss 165.03555297851562, val loss 220.19918823242188
iter 80, train loss 164.32455444335938, val loss 220.15762329101562
iter 90, train loss 164.054931640625, val loss 219.575927734375
best loss 191.8115234375
not here
quantized in 33.01393532752991 seconds
36414 MiB free out of 48676 MiB total
3 self_attn.v_proj
Pruning ...
256
iter 0, train loss 35.40785217285156, val loss 48.073036193847656
iter 10, train loss 35.98130416870117, val loss 49.49326705932617
iter 20, train loss 35.57289505004883, val loss 49.00314712524414
iter 30, train loss 35.52517318725586, val loss 48.96306228637695
iter 40, train loss 35.4998664855957, val loss 49.204742431640625
iter 50, train loss 35.49869155883789, val loss 49.06853485107422
iter 60, train loss 35.461055755615234, val loss 48.92181396484375
iter 70, train loss 35.473426818847656, val loss 49.036705017089844
iter 80, train loss 35.431114196777344, val loss 49.068145751953125
iter 90, train loss 35.37641906738281, val loss 49.00703430175781
best loss 48.05638122558594
not here
quantized in 31.58247423171997 seconds
36404 MiB free out of 48676 MiB total
3 self_attn.o_proj
Pruning ...
256
iter 0, train loss 0.26570045948028564, val loss 2.154358386993408
iter 10, train loss 0.25098225474357605, val loss 2.274106979370117
iter 20, train loss 0.24462094902992249, val loss 2.243098020553589
iter 30, train loss 0.24269597232341766, val loss 2.222282886505127
iter 40, train loss 0.24225321412086487, val loss 2.2481865882873535
iter 50, train loss 0.24297209084033966, val loss 2.2362380027770996
iter 60, train loss 0.24154232442378998, val loss 2.253019332885742
iter 70, train loss 0.2405606508255005, val loss 2.199317455291748
iter 80, train loss 0.2410825937986374, val loss 2.17111873626709
iter 90, train loss 0.2410721480846405, val loss 2.197052478790283
best loss 2.1390976905822754
not here
quantized in 31.65345859527588 seconds
36404 MiB free out of 48676 MiB total
3 mlp.gate_proj
Pruning ...
256
iter 0, train loss 54.239498138427734, val loss 98.04603576660156
iter 10, train loss 55.1960563659668, val loss 100.77301025390625
iter 20, train loss 54.71910858154297, val loss 100.27857208251953
iter 30, train loss 54.838069915771484, val loss 101.24630737304688
iter 40, train loss 54.85384750366211, val loss 100.5474624633789
iter 50, train loss 54.820587158203125, val loss 100.81965637207031
iter 60, train loss 54.80380630493164, val loss 101.24258422851562
iter 70, train loss 54.783363342285156, val loss 102.28025817871094
iter 80, train loss 54.77262878417969, val loss 101.75405883789062
iter 90, train loss 54.79409408569336, val loss 101.91152954101562
best loss 97.82926177978516
not here
quantized in 85.55353260040283 seconds
36102 MiB free out of 48676 MiB total
3 mlp.up_proj
Pruning ...
256
iter 0, train loss 44.59185791015625, val loss 80.59187316894531
iter 10, train loss 44.616111755371094, val loss 81.49632263183594
iter 20, train loss 44.7180290222168, val loss 82.23810577392578
iter 30, train loss 44.711753845214844, val loss 83.24516296386719
iter 40, train loss 44.69198226928711, val loss 81.83199310302734
iter 50, train loss 44.707088470458984, val loss 81.72879028320312
iter 60, train loss 44.6513671875, val loss 81.86589050292969
iter 70, train loss 44.63676452636719, val loss 81.339599609375
iter 80, train loss 44.667545318603516, val loss 81.67547607421875
iter 90, train loss 44.654624938964844, val loss 81.30354309082031
best loss 80.46849060058594
not here
quantized in 83.77821516990662 seconds
35908 MiB free out of 48676 MiB total
3 mlp.down_proj
Pruning ...
256
iter 0, train loss 0.30864429473876953, val loss 0.8659163117408752
iter 10, train loss 0.30818504095077515, val loss 0.8824958801269531
iter 20, train loss 0.3041616380214691, val loss 0.8768994808197021
iter 30, train loss 0.3026893734931946, val loss 0.8875999450683594
iter 40, train loss 0.300998717546463, val loss 0.8900531530380249
iter 50, train loss 0.30065709352493286, val loss 0.8980454206466675
iter 60, train loss 0.30022871494293213, val loss 0.8922141790390015
iter 70, train loss 0.29945966601371765, val loss 0.8971498012542725
iter 80, train loss 0.2989203929901123, val loss 0.8992263078689575
iter 90, train loss 0.298664391040802, val loss 0.8894035816192627
best loss 0.8659163117408752
not here
quantized in 93.06307744979858 seconds
35714 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35714 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31618 MiB free out of 48676 MiB total
after cast to cpu
37852 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
4 self_attn.q_proj
Pruning ...
256
iter 0, train loss 139.2643585205078, val loss 152.00277709960938
iter 10, train loss 147.57205200195312, val loss 177.2435302734375
iter 20, train loss 144.33554077148438, val loss 173.8692169189453
iter 30, train loss 140.90170288085938, val loss 170.26870727539062
iter 40, train loss 139.5921173095703, val loss 170.6007537841797
iter 50, train loss 138.67127990722656, val loss 169.13290405273438
iter 60, train loss 138.36447143554688, val loss 169.230224609375
iter 70, train loss 138.13311767578125, val loss 169.09600830078125
iter 80, train loss 138.4214324951172, val loss 169.61106872558594
iter 90, train loss 137.98324584960938, val loss 169.25717163085938
best loss 152.00277709960938
not here
quantized in 33.72956824302673 seconds
36424 MiB free out of 48676 MiB total
4 self_attn.k_proj
Pruning ...
256
iter 0, train loss 154.57882690429688, val loss 164.36341857910156
iter 10, train loss 158.7460479736328, val loss 187.34942626953125
iter 20, train loss 162.92459106445312, val loss 197.04611206054688
iter 30, train loss 156.59158325195312, val loss 190.12716674804688
iter 40, train loss 154.42733764648438, val loss 188.57955932617188
iter 50, train loss 154.19659423828125, val loss 188.09310913085938
iter 60, train loss 154.04437255859375, val loss 188.272705078125
iter 70, train loss 153.8665771484375, val loss 187.64328002929688
iter 80, train loss 153.47219848632812, val loss 187.5482635498047
iter 90, train loss 152.85028076171875, val loss 187.07894897460938
best loss 164.36341857910156
not here
quantized in 33.03145980834961 seconds
36414 MiB free out of 48676 MiB total
4 self_attn.v_proj
Pruning ...
256
iter 0, train loss 35.273704528808594, val loss 43.9741096496582
iter 10, train loss 35.67481231689453, val loss 45.29962921142578
iter 20, train loss 35.578529357910156, val loss 45.13359832763672
iter 30, train loss 35.544898986816406, val loss 45.022178649902344
iter 40, train loss 35.40007400512695, val loss 44.938053131103516
iter 50, train loss 35.37272644042969, val loss 44.9782600402832
iter 60, train loss 35.38911056518555, val loss 44.882781982421875
iter 70, train loss 35.36774444580078, val loss 44.961463928222656
iter 80, train loss 35.34759521484375, val loss 44.962310791015625
iter 90, train loss 35.35686492919922, val loss 44.90632629394531
best loss 43.9741096496582
not here
quantized in 31.558321237564087 seconds
36404 MiB free out of 48676 MiB total
4 self_attn.o_proj
Pruning ...
256
iter 0, train loss 0.511184573173523, val loss 1.9685389995574951
iter 10, train loss 0.4718323349952698, val loss 1.8352577686309814
iter 20, train loss 0.45611435174942017, val loss 1.764648199081421
iter 30, train loss 0.45094576478004456, val loss 1.7177057266235352
iter 40, train loss 0.44428950548171997, val loss 1.7829495668411255
iter 50, train loss 0.4434666335582733, val loss 1.7234078645706177
iter 60, train loss 0.4415220618247986, val loss 1.7228503227233887
iter 70, train loss 0.4398922026157379, val loss 1.6900070905685425
iter 80, train loss 0.43682169914245605, val loss 1.7211717367172241
iter 90, train loss 0.4379524886608124, val loss 1.7200270891189575
best loss 1.6784619092941284
not here
quantized in 31.55094838142395 seconds
36404 MiB free out of 48676 MiB total
4 mlp.gate_proj
Pruning ...
256
iter 0, train loss 81.05332946777344, val loss 118.32791137695312
iter 10, train loss 82.18074798583984, val loss 121.54232025146484
iter 20, train loss 81.85579681396484, val loss 122.94610595703125
iter 30, train loss 81.62277221679688, val loss 121.81285858154297
iter 40, train loss 81.66043853759766, val loss 122.06640625
iter 50, train loss 81.58353424072266, val loss 121.61107635498047
iter 60, train loss 81.48724365234375, val loss 122.67796325683594
iter 70, train loss 81.30924987792969, val loss 123.69738006591797
iter 80, train loss 81.31478881835938, val loss 122.5354232788086
iter 90, train loss 81.24177551269531, val loss 121.9009017944336
best loss 117.2022933959961
not here
quantized in 84.90065717697144 seconds
36102 MiB free out of 48676 MiB total
4 mlp.up_proj
Pruning ...
256
iter 0, train loss 63.67616271972656, val loss 94.43186950683594
iter 10, train loss 63.81866455078125, val loss 93.52558135986328
iter 20, train loss 63.78904342651367, val loss 96.17164611816406
iter 30, train loss 63.808387756347656, val loss 96.71044921875
iter 40, train loss 63.73508834838867, val loss 97.09807586669922
iter 50, train loss 63.80714416503906, val loss 95.67536926269531
iter 60, train loss 63.82245635986328, val loss 95.85163879394531
iter 70, train loss 63.752601623535156, val loss 95.86158752441406
iter 80, train loss 63.692413330078125, val loss 96.5886459350586
iter 90, train loss 63.71356201171875, val loss 95.92730712890625
best loss 93.41708374023438
not here
quantized in 83.61994791030884 seconds
35908 MiB free out of 48676 MiB total
4 mlp.down_proj
Pruning ...
256
iter 0, train loss 0.5886592864990234, val loss 1.1876440048217773
iter 10, train loss 0.5877653956413269, val loss 1.2039833068847656
iter 20, train loss 0.5843598246574402, val loss 1.18918776512146
iter 30, train loss 0.5823813676834106, val loss 1.1854321956634521
iter 40, train loss 0.58079993724823, val loss 1.1806585788726807
iter 50, train loss 0.5786037445068359, val loss 1.1960762739181519
iter 60, train loss 0.5783591270446777, val loss 1.1880404949188232
iter 70, train loss 0.5777370929718018, val loss 1.163740634918213
iter 80, train loss 0.5785661935806274, val loss 1.1696611642837524
iter 90, train loss 0.5812996625900269, val loss 1.1738710403442383
best loss 1.1607340574264526
not here
quantized in 92.18986082077026 seconds
35714 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35714 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31618 MiB free out of 48676 MiB total
after cast to cpu
37852 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
5 self_attn.q_proj
Pruning ...
256
iter 0, train loss 157.85153198242188, val loss 160.26637268066406
iter 10, train loss 165.44232177734375, val loss 188.61329650878906
iter 20, train loss 161.94561767578125, val loss 186.45982360839844
iter 30, train loss 157.01136779785156, val loss 181.079345703125
iter 40, train loss 155.8675079345703, val loss 179.21859741210938
iter 50, train loss 155.60250854492188, val loss 179.2791748046875
iter 60, train loss 155.57763671875, val loss 179.50753784179688
iter 70, train loss 155.16677856445312, val loss 179.31675720214844
iter 80, train loss 154.75083923339844, val loss 179.56448364257812
iter 90, train loss 154.54734802246094, val loss 179.07386779785156
best loss 160.26637268066406
not here
quantized in 33.85735845565796 seconds
36424 MiB free out of 48676 MiB total
5 self_attn.k_proj
Pruning ...
256
iter 0, train loss 184.072509765625, val loss 181.93495178222656
iter 10, train loss 187.18531799316406, val loss 211.23692321777344
iter 20, train loss 190.5560302734375, val loss 217.078369140625
iter 30, train loss 185.23849487304688, val loss 212.03573608398438
iter 40, train loss 183.783935546875, val loss 210.6236572265625
iter 50, train loss 182.21182250976562, val loss 209.13064575195312
iter 60, train loss 180.66494750976562, val loss 208.31509399414062
iter 70, train loss 180.19500732421875, val loss 208.07357788085938
iter 80, train loss 179.70408630371094, val loss 207.6040802001953
iter 90, train loss 179.3291473388672, val loss 208.05340576171875
best loss 181.3875732421875
not here
quantized in 32.86956834793091 seconds
36414 MiB free out of 48676 MiB total
5 self_attn.v_proj
Pruning ...
256
iter 0, train loss 40.92411804199219, val loss 48.15321350097656
iter 10, train loss 41.19207000732422, val loss 49.07282638549805
iter 20, train loss 41.08800506591797, val loss 48.952613830566406
iter 30, train loss 40.9903564453125, val loss 48.86438751220703
iter 40, train loss 40.947898864746094, val loss 48.742828369140625
iter 50, train loss 40.903297424316406, val loss 48.76820755004883
iter 60, train loss 40.875755310058594, val loss 48.75730895996094
iter 70, train loss 40.85224151611328, val loss 48.80818176269531
iter 80, train loss 40.81330871582031, val loss 48.783294677734375
iter 90, train loss 40.76416778564453, val loss 48.693965911865234
best loss 48.15321350097656
not here
quantized in 31.386101722717285 seconds
36404 MiB free out of 48676 MiB total
5 self_attn.o_proj
Pruning ...
256
iter 0, train loss 0.7004039287567139, val loss 2.432117462158203
iter 10, train loss 0.6918278932571411, val loss 2.5407073497772217
iter 20, train loss 0.6849741339683533, val loss 2.3512399196624756
iter 30, train loss 0.6833436489105225, val loss 2.3787899017333984
iter 40, train loss 0.6810189485549927, val loss 2.3257503509521484
iter 50, train loss 0.6788139343261719, val loss 2.325425386428833
iter 60, train loss 0.679437518119812, val loss 2.3444693088531494
iter 70, train loss 0.6773074865341187, val loss 2.3630974292755127
iter 80, train loss 0.6747061610221863, val loss 2.3425588607788086
iter 90, train loss 0.673994779586792, val loss 2.306598663330078
best loss 2.272038459777832
not here
quantized in 31.51154589653015 seconds
36372 MiB free out of 48676 MiB total
5 mlp.gate_proj
Pruning ...
256
iter 0, train loss 102.5947494506836, val loss 137.81602478027344
iter 10, train loss 103.67768096923828, val loss 140.57763671875
iter 20, train loss 103.29612731933594, val loss 137.61929321289062
iter 30, train loss 102.90327453613281, val loss 137.78262329101562
iter 40, train loss 102.68255615234375, val loss 136.63043212890625
iter 50, train loss 102.6453857421875, val loss 137.3600616455078
iter 60, train loss 102.59352111816406, val loss 135.71792602539062
iter 70, train loss 102.60892486572266, val loss 137.140869140625
iter 80, train loss 102.52533721923828, val loss 137.1822509765625
iter 90, train loss 102.52232360839844, val loss 137.25848388671875
best loss 135.71792602539062
not here
quantized in 85.46020722389221 seconds
36070 MiB free out of 48676 MiB total
5 mlp.up_proj
Pruning ...
256
iter 0, train loss 79.90278625488281, val loss 106.97647094726562
iter 10, train loss 80.15973663330078, val loss 106.45924377441406
iter 20, train loss 80.18671417236328, val loss 107.71699523925781
iter 30, train loss 80.32068634033203, val loss 107.75141906738281
iter 40, train loss 80.22531127929688, val loss 108.68927001953125
iter 50, train loss 80.27224731445312, val loss 108.12178802490234
iter 60, train loss 80.22876739501953, val loss 107.59782409667969
iter 70, train loss 80.25209045410156, val loss 107.3919677734375
iter 80, train loss 80.25186920166016, val loss 107.43350219726562
iter 90, train loss 80.28926849365234, val loss 107.30792999267578
best loss 105.87445068359375
not here
quantized in 83.96253800392151 seconds
35876 MiB free out of 48676 MiB total
5 mlp.down_proj
Pruning ...
256
iter 0, train loss 0.8160549998283386, val loss 1.2452996969223022
iter 10, train loss 0.8180303573608398, val loss 1.262333869934082
iter 20, train loss 0.8134564757347107, val loss 1.2692561149597168
iter 30, train loss 0.8120768070220947, val loss 1.2901713848114014
iter 40, train loss 0.8105566501617432, val loss 1.2803220748901367
iter 50, train loss 0.8099450469017029, val loss 1.2667392492294312
iter 60, train loss 0.810074508190155, val loss 1.258365511894226
iter 70, train loss 0.808506965637207, val loss 1.2728822231292725
iter 80, train loss 0.808883786201477, val loss 1.2686008214950562
iter 90, train loss 0.8083773851394653, val loss 1.2624139785766602
best loss 1.2452996969223022
not here
quantized in 96.42121696472168 seconds
35682 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35682 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31586 MiB free out of 48676 MiB total
after cast to cpu
37852 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
6 self_attn.q_proj
Pruning ...
256
iter 0, train loss 233.7772674560547, val loss 217.8859100341797
iter 10, train loss 246.10720825195312, val loss 268.1040344238281
iter 20, train loss 242.60675048828125, val loss 262.8247985839844
iter 30, train loss 237.3949737548828, val loss 258.0958251953125
iter 40, train loss 234.84799194335938, val loss 255.7420654296875
iter 50, train loss 233.69496154785156, val loss 254.9009552001953
iter 60, train loss 233.19363403320312, val loss 254.8943634033203
iter 70, train loss 233.0233154296875, val loss 254.729248046875
iter 80, train loss 231.69247436523438, val loss 253.81594848632812
iter 90, train loss 231.11907958984375, val loss 253.30172729492188
best loss 217.8859100341797
not here
quantized in 46.64634656906128 seconds
36424 MiB free out of 48676 MiB total
6 self_attn.k_proj
Pruning ...
256
iter 0, train loss 251.4046630859375, val loss 229.21771240234375
iter 10, train loss 258.7941589355469, val loss 278.29107666015625
iter 20, train loss 260.74298095703125, val loss 282.89276123046875
iter 30, train loss 256.23089599609375, val loss 279.1128234863281
iter 40, train loss 251.2188262939453, val loss 274.7980041503906
iter 50, train loss 249.36883544921875, val loss 273.50048828125
iter 60, train loss 248.384765625, val loss 272.3212585449219
iter 70, train loss 248.568115234375, val loss 273.11895751953125
iter 80, train loss 248.16226196289062, val loss 271.9459228515625
iter 90, train loss 246.97235107421875, val loss 271.3075256347656
best loss 229.21771240234375
not here
quantized in 43.48675990104675 seconds
36414 MiB free out of 48676 MiB total
6 self_attn.v_proj
Pruning ...
256
iter 0, train loss 58.36299133300781, val loss 63.78617477416992
iter 10, train loss 59.13544464111328, val loss 65.7570571899414
iter 20, train loss 58.843772888183594, val loss 65.79215240478516
iter 30, train loss 58.820350646972656, val loss 65.92475128173828
iter 40, train loss 58.56100845336914, val loss 65.583251953125
iter 50, train loss 58.425323486328125, val loss 65.76634216308594
iter 60, train loss 58.4442024230957, val loss 65.83242797851562
iter 70, train loss 58.47431564331055, val loss 65.87413024902344
iter 80, train loss 58.456459045410156, val loss 65.65972900390625
iter 90, train loss 58.48027801513672, val loss 65.63029479980469
best loss 63.78617477416992
not here
quantized in 31.656302452087402 seconds
36404 MiB free out of 48676 MiB total
6 self_attn.o_proj
Pruning ...
256
iter 0, train loss 1.3568613529205322, val loss 2.783114433288574
iter 10, train loss 1.3679721355438232, val loss 2.6126515865325928
iter 20, train loss 1.3554058074951172, val loss 2.726463794708252
iter 30, train loss 1.3222709894180298, val loss 2.582333564758301
iter 40, train loss 1.3127931356430054, val loss 2.7144384384155273
iter 50, train loss 1.3044300079345703, val loss 2.651592493057251
iter 60, train loss 1.2984566688537598, val loss 2.594877243041992
iter 70, train loss 1.2984373569488525, val loss 2.628807544708252
iter 80, train loss 1.293773889541626, val loss 2.5873517990112305
iter 90, train loss 1.2919344902038574, val loss 2.652583122253418
best loss 2.5437629222869873
not here
quantized in 31.5593900680542 seconds
36372 MiB free out of 48676 MiB total
6 mlp.gate_proj
Pruning ...
256
iter 0, train loss 126.35409545898438, val loss 152.7539520263672
iter 10, train loss 130.2059326171875, val loss 162.7705841064453
iter 20, train loss 128.47296142578125, val loss 156.802001953125
iter 30, train loss 127.98221588134766, val loss 157.82467651367188
iter 40, train loss 127.55747985839844, val loss 156.04119873046875
iter 50, train loss 127.32352447509766, val loss 155.17320251464844
iter 60, train loss 127.27496337890625, val loss 154.57901000976562
iter 70, train loss 127.15734100341797, val loss 156.3684539794922
iter 80, train loss 126.95022583007812, val loss 155.54624938964844
iter 90, train loss 126.92062377929688, val loss 155.92481994628906
best loss 151.21405029296875
not here
quantized in 85.42449283599854 seconds
36070 MiB free out of 48676 MiB total
6 mlp.up_proj
Pruning ...
256
iter 0, train loss 94.345947265625, val loss 113.43183135986328
iter 10, train loss 94.44148254394531, val loss 113.81492614746094
iter 20, train loss 94.56355285644531, val loss 114.25607299804688
iter 30, train loss 94.41480255126953, val loss 114.65544128417969
iter 40, train loss 94.46002960205078, val loss 114.72023010253906
iter 50, train loss 94.3781509399414, val loss 114.69374084472656
iter 60, train loss 94.24068450927734, val loss 117.01310729980469
iter 70, train loss 94.25272369384766, val loss 115.88031005859375
iter 80, train loss 94.18659210205078, val loss 115.71382141113281
iter 90, train loss 94.28148651123047, val loss 116.08643341064453
best loss 112.52229309082031
not here
quantized in 84.52023220062256 seconds
35876 MiB free out of 48676 MiB total
6 mlp.down_proj
Pruning ...
256
iter 0, train loss 1.243721604347229, val loss 1.4350666999816895
iter 10, train loss 1.2486991882324219, val loss 1.3842298984527588
iter 20, train loss 1.2348299026489258, val loss 1.4460840225219727
iter 30, train loss 1.2306504249572754, val loss 1.408031702041626
iter 40, train loss 1.2255077362060547, val loss 1.3949509859085083
iter 50, train loss 1.2242578268051147, val loss 1.4478577375411987
iter 60, train loss 1.2208335399627686, val loss 1.438492774963379
iter 70, train loss 1.2197043895721436, val loss 1.414820671081543
iter 80, train loss 1.2209386825561523, val loss 1.4314405918121338
iter 90, train loss 1.2207657098770142, val loss 1.4501314163208008
best loss 1.3725647926330566
not here
quantized in 92.58090043067932 seconds
35682 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35682 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31586 MiB free out of 48676 MiB total
after cast to cpu
37852 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
7 self_attn.q_proj
Pruning ...
256
iter 0, train loss 256.2555847167969, val loss 232.90142822265625
iter 10, train loss 271.2806396484375, val loss 289.74151611328125
iter 20, train loss 266.0591735839844, val loss 283.4100036621094
iter 30, train loss 261.84906005859375, val loss 280.24505615234375
iter 40, train loss 257.526611328125, val loss 274.7487487792969
iter 50, train loss 254.92535400390625, val loss 273.400390625
iter 60, train loss 254.60362243652344, val loss 273.57061767578125
iter 70, train loss 254.96499633789062, val loss 274.9416809082031
iter 80, train loss 254.61277770996094, val loss 275.25665283203125
iter 90, train loss 254.35894775390625, val loss 275.2143859863281
best loss 232.90142822265625
not here
quantized in 34.24146246910095 seconds
36424 MiB free out of 48676 MiB total
7 self_attn.k_proj
Pruning ...
256
iter 0, train loss 263.8680114746094, val loss 237.2728271484375
iter 10, train loss 272.2658996582031, val loss 286.905517578125
iter 20, train loss 277.0094299316406, val loss 295.0808410644531
iter 30, train loss 270.85394287109375, val loss 289.9187927246094
iter 40, train loss 264.0853271484375, val loss 282.04815673828125
iter 50, train loss 262.7351379394531, val loss 280.96337890625
iter 60, train loss 261.076416015625, val loss 280.4580993652344
iter 70, train loss 261.1556396484375, val loss 280.6571044921875
iter 80, train loss 261.2009582519531, val loss 280.2063293457031
iter 90, train loss 260.70257568359375, val loss 280.37786865234375
best loss 237.2728271484375
not here
quantized in 33.140864610672 seconds
36414 MiB free out of 48676 MiB total
7 self_attn.v_proj
Pruning ...
256
iter 0, train loss 64.98348999023438, val loss 69.5321044921875
iter 10, train loss 65.53313446044922, val loss 71.78669738769531
iter 20, train loss 65.24082946777344, val loss 71.70233154296875
iter 30, train loss 65.32264709472656, val loss 71.43135070800781
iter 40, train loss 65.11004638671875, val loss 71.59506225585938
iter 50, train loss 65.18285369873047, val loss 71.39685821533203
iter 60, train loss 65.05403900146484, val loss 71.46572875976562
iter 70, train loss 65.06128692626953, val loss 71.32272338867188
iter 80, train loss 65.04853820800781, val loss 71.2061767578125
iter 90, train loss 65.0020980834961, val loss 71.2773666381836
best loss 69.5321044921875
not here
quantized in 31.4714298248291 seconds
36404 MiB free out of 48676 MiB total
7 self_attn.o_proj
Pruning ...
256
iter 0, train loss 2.096320390701294, val loss 2.4709084033966064
iter 10, train loss 2.006154775619507, val loss 2.6010138988494873
iter 20, train loss 1.9776685237884521, val loss 2.4633986949920654
iter 30, train loss 1.9698413610458374, val loss 2.5440311431884766
iter 40, train loss 1.9635014533996582, val loss 2.5433804988861084
iter 50, train loss 1.9548938274383545, val loss 2.5483579635620117
iter 60, train loss 1.9420937299728394, val loss 2.5168228149414062
iter 70, train loss 1.9427382946014404, val loss 2.509154796600342
iter 80, train loss 1.9417691230773926, val loss 2.4962260723114014
iter 90, train loss 1.9383059740066528, val loss 2.5600132942199707
best loss 2.4633986949920654
not here
quantized in 31.23503303527832 seconds
36372 MiB free out of 48676 MiB total
7 mlp.gate_proj
Pruning ...
256
iter 0, train loss 144.24575805664062, val loss 182.46749877929688
iter 10, train loss 148.2552490234375, val loss 186.52903747558594
iter 20, train loss 146.0939483642578, val loss 185.44752502441406
iter 30, train loss 145.7181854248047, val loss 185.29656982421875
iter 40, train loss 145.20492553710938, val loss 188.3169708251953
iter 50, train loss 145.0539093017578, val loss 188.1304931640625
iter 60, train loss 144.59161376953125, val loss 188.16452026367188
iter 70, train loss 144.6264190673828, val loss 188.98898315429688
iter 80, train loss 144.63540649414062, val loss 188.4290008544922
iter 90, train loss 144.45474243164062, val loss 189.13929748535156
best loss 181.400146484375
not here
quantized in 84.9753258228302 seconds
36070 MiB free out of 48676 MiB total
7 mlp.up_proj
Pruning ...
256
iter 0, train loss 108.77853393554688, val loss 137.3999786376953
iter 10, train loss 108.92318725585938, val loss 140.85446166992188
iter 20, train loss 108.8130111694336, val loss 141.716552734375
iter 30, train loss 108.72791290283203, val loss 141.05111694335938
iter 40, train loss 108.72227478027344, val loss 140.4398193359375
iter 50, train loss 108.65336608886719, val loss 140.08949279785156
iter 60, train loss 108.68707275390625, val loss 139.75299072265625
iter 70, train loss 108.63365173339844, val loss 141.22178649902344
iter 80, train loss 108.48148345947266, val loss 139.73306274414062
iter 90, train loss 108.46488189697266, val loss 140.21200561523438
best loss 137.34783935546875
not here
quantized in 83.74904584884644 seconds
35876 MiB free out of 48676 MiB total
7 mlp.down_proj
Pruning ...
256
iter 0, train loss 1.640805959701538, val loss 1.8724751472473145
iter 10, train loss 1.6430482864379883, val loss 1.9481765031814575
iter 20, train loss 1.6355247497558594, val loss 1.9393830299377441
iter 30, train loss 1.630638599395752, val loss 1.9388980865478516
iter 40, train loss 1.6301721334457397, val loss 1.9407837390899658
iter 50, train loss 1.6278244256973267, val loss 1.9665451049804688
iter 60, train loss 1.6271884441375732, val loss 1.975417971611023
iter 70, train loss 1.6249005794525146, val loss 1.9631335735321045
iter 80, train loss 1.6240670680999756, val loss 1.9780157804489136
iter 90, train loss 1.622964859008789, val loss 1.9760715961456299
best loss 1.8724751472473145
not here
quantized in 91.72739243507385 seconds
35682 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35682 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31586 MiB free out of 48676 MiB total
after cast to cpu
37852 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
8 self_attn.q_proj
Pruning ...
256
iter 0, train loss 247.9855499267578, val loss 227.74609375
iter 10, train loss 260.1058349609375, val loss 274.7467041015625
iter 20, train loss 254.00851440429688, val loss 268.9825134277344
iter 30, train loss 251.1017608642578, val loss 266.11328125
iter 40, train loss 248.9498291015625, val loss 263.06036376953125
iter 50, train loss 248.52308654785156, val loss 263.81097412109375
iter 60, train loss 246.78720092773438, val loss 262.609130859375
iter 70, train loss 246.36474609375, val loss 261.9842834472656
iter 80, train loss 246.09716796875, val loss 262.3869323730469
iter 90, train loss 245.558837890625, val loss 261.8153991699219
best loss 227.74609375
not here
quantized in 34.01978778839111 seconds
36424 MiB free out of 48676 MiB total
8 self_attn.k_proj
Pruning ...
256
iter 0, train loss 257.082763671875, val loss 231.70941162109375
iter 10, train loss 265.3443603515625, val loss 278.27581787109375
iter 20, train loss 269.97027587890625, val loss 284.80035400390625
iter 30, train loss 263.9517822265625, val loss 279.38232421875
iter 40, train loss 260.62359619140625, val loss 276.1477966308594
iter 50, train loss 258.30548095703125, val loss 274.6131591796875
iter 60, train loss 255.67274475097656, val loss 272.83599853515625
iter 70, train loss 255.26138305664062, val loss 272.5345458984375
iter 80, train loss 254.35894775390625, val loss 271.3981628417969
iter 90, train loss 253.95547485351562, val loss 270.5397033691406
best loss 231.70941162109375
not here
quantized in 32.76592659950256 seconds
36414 MiB free out of 48676 MiB total
8 self_attn.v_proj
Pruning ...
256
iter 0, train loss 67.18895721435547, val loss 70.7658920288086
iter 10, train loss 68.30380249023438, val loss 73.965576171875
iter 20, train loss 67.50357055664062, val loss 73.21044158935547
iter 30, train loss 67.37091827392578, val loss 73.00418090820312
iter 40, train loss 67.21090698242188, val loss 73.22996520996094
iter 50, train loss 67.2499771118164, val loss 73.19157409667969
iter 60, train loss 67.16992950439453, val loss 73.08628845214844
iter 70, train loss 67.09658813476562, val loss 73.09109497070312
iter 80, train loss 67.0369644165039, val loss 73.29788970947266
iter 90, train loss 66.94709777832031, val loss 73.28738403320312
best loss 70.7658920288086
not here
quantized in 31.49207878112793 seconds
36404 MiB free out of 48676 MiB total
8 self_attn.o_proj
Pruning ...
256
iter 0, train loss 3.207969903945923, val loss 2.9166057109832764
iter 10, train loss 3.1039180755615234, val loss 3.0378448963165283
iter 20, train loss 3.073371410369873, val loss 3.1023597717285156
iter 30, train loss 3.0627636909484863, val loss 3.0760655403137207
iter 40, train loss 3.045635223388672, val loss 3.1148276329040527
iter 50, train loss 3.033414363861084, val loss 3.1063709259033203
iter 60, train loss 3.0232791900634766, val loss 3.0684866905212402
iter 70, train loss 3.0157511234283447, val loss 3.1045749187469482
iter 80, train loss 3.0179572105407715, val loss 3.142977714538574
iter 90, train loss 3.009174346923828, val loss 3.1739675998687744
best loss 2.9166057109832764
not here
quantized in 31.802950859069824 seconds
36372 MiB free out of 48676 MiB total
8 mlp.gate_proj
Pruning ...
256
iter 0, train loss 150.17080688476562, val loss 182.76988220214844
iter 10, train loss 154.59326171875, val loss 194.82272338867188
iter 20, train loss 151.3461151123047, val loss 192.4413299560547
iter 30, train loss 151.31060791015625, val loss 192.97518920898438
iter 40, train loss 150.83309936523438, val loss 192.026123046875
iter 50, train loss 150.9009246826172, val loss 190.0107421875
iter 60, train loss 150.3494873046875, val loss 190.4383544921875
iter 70, train loss 150.21017456054688, val loss 191.21743774414062
iter 80, train loss 150.0584259033203, val loss 191.6449737548828
iter 90, train loss 149.9138641357422, val loss 192.06771850585938
best loss 182.76988220214844
not here
quantized in 84.31107020378113 seconds
36070 MiB free out of 48676 MiB total
8 mlp.up_proj
Pruning ...
256
iter 0, train loss 120.08004760742188, val loss 150.48699951171875
iter 10, train loss 120.38785552978516, val loss 149.78887939453125
iter 20, train loss 120.55076599121094, val loss 152.4371337890625
iter 30, train loss 120.36973571777344, val loss 154.57662963867188
iter 40, train loss 120.24983978271484, val loss 153.62457275390625
iter 50, train loss 120.10287475585938, val loss 154.5941162109375
iter 60, train loss 120.06224060058594, val loss 155.0682373046875
iter 70, train loss 120.0465316772461, val loss 153.36854553222656
iter 80, train loss 120.0344467163086, val loss 152.7539520263672
iter 90, train loss 120.05560302734375, val loss 151.94126892089844
best loss 149.470947265625
not here
quantized in 84.39358520507812 seconds
35876 MiB free out of 48676 MiB total
8 mlp.down_proj
Pruning ...
256
iter 0, train loss 2.083888530731201, val loss 2.1504831314086914
iter 10, train loss 2.0964202880859375, val loss 2.122142791748047
iter 20, train loss 2.0853896141052246, val loss 2.0979628562927246
iter 30, train loss 2.0809738636016846, val loss 2.1275227069854736
iter 40, train loss 2.0764529705047607, val loss 2.149451732635498
iter 50, train loss 2.0724074840545654, val loss 2.1302647590637207
iter 60, train loss 2.0679502487182617, val loss 2.169790506362915
iter 70, train loss 2.067261219024658, val loss 2.1218721866607666
iter 80, train loss 2.066828489303589, val loss 2.1458280086517334
iter 90, train loss 2.0660133361816406, val loss 2.147333860397339
best loss 2.0861895084381104
not here
quantized in 92.11247086524963 seconds
35682 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35682 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31586 MiB free out of 48676 MiB total
after cast to cpu
37852 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
9 self_attn.q_proj
Pruning ...
256
iter 0, train loss 258.8455810546875, val loss 235.52671813964844
iter 10, train loss 275.4520263671875, val loss 285.03302001953125
iter 20, train loss 265.2693176269531, val loss 273.4305419921875
iter 30, train loss 258.97296142578125, val loss 268.40008544921875
iter 40, train loss 257.32037353515625, val loss 266.9788818359375
iter 50, train loss 256.3688049316406, val loss 267.9532470703125
iter 60, train loss 255.96597290039062, val loss 267.2609558105469
iter 70, train loss 255.76092529296875, val loss 266.4090270996094
iter 80, train loss 255.28851318359375, val loss 265.7394714355469
iter 90, train loss 255.17364501953125, val loss 265.5980529785156
best loss 235.52671813964844
not here
quantized in 33.77305364608765 seconds
36424 MiB free out of 48676 MiB total
9 self_attn.k_proj
Pruning ...
256
iter 0, train loss 279.940673828125, val loss 250.58973693847656
iter 10, train loss 295.367919921875, val loss 300.74951171875
iter 20, train loss 295.0369567871094, val loss 305.93280029296875
iter 30, train loss 284.0680847167969, val loss 294.5085144042969
iter 40, train loss 281.1654357910156, val loss 291.10443115234375
iter 50, train loss 278.69921875, val loss 289.754150390625
iter 60, train loss 278.8265380859375, val loss 290.8874206542969
iter 70, train loss 277.8682861328125, val loss 291.0334167480469
iter 80, train loss 277.70098876953125, val loss 290.24310302734375
iter 90, train loss 277.2313537597656, val loss 290.138916015625
best loss 250.58973693847656
not here
quantized in 32.94981551170349 seconds
36414 MiB free out of 48676 MiB total
9 self_attn.v_proj
Pruning ...
256
iter 0, train loss 73.72290802001953, val loss 75.82238006591797
iter 10, train loss 74.45964813232422, val loss 78.90265655517578
iter 20, train loss 74.075439453125, val loss 78.58485412597656
iter 30, train loss 73.85728454589844, val loss 78.4656982421875
iter 40, train loss 73.84979248046875, val loss 78.4486312866211
iter 50, train loss 73.69393920898438, val loss 78.3810806274414
iter 60, train loss 73.60858154296875, val loss 78.30064392089844
iter 70, train loss 73.60987854003906, val loss 78.43113708496094
iter 80, train loss 73.51728057861328, val loss 78.37165832519531
iter 90, train loss 73.48983764648438, val loss 78.29582214355469
best loss 75.82238006591797
not here
quantized in 31.450483798980713 seconds
36404 MiB free out of 48676 MiB total
9 self_attn.o_proj
Pruning ...
256
iter 0, train loss 5.906004428863525, val loss 2.7732253074645996
iter 10, train loss 5.411227226257324, val loss 3.146625280380249
iter 20, train loss 5.258636951446533, val loss 3.133272171020508
iter 30, train loss 5.165397644042969, val loss 3.0773773193359375
iter 40, train loss 5.12548828125, val loss 2.995359182357788
iter 50, train loss 5.084766387939453, val loss 3.0691919326782227
iter 60, train loss 5.066157817840576, val loss 3.0546607971191406
iter 70, train loss 5.062004089355469, val loss 3.079038143157959
iter 80, train loss 5.0521955490112305, val loss 3.0290379524230957
iter 90, train loss 5.041670322418213, val loss 3.0966901779174805
best loss 2.7732253074645996
not here
quantized in 31.083672046661377 seconds
36372 MiB free out of 48676 MiB total
9 mlp.gate_proj
Pruning ...
256
iter 0, train loss 156.5841522216797, val loss 191.87989807128906
iter 10, train loss 163.0862579345703, val loss 201.14942932128906
iter 20, train loss 158.6818389892578, val loss 202.38717651367188
iter 30, train loss 158.37393188476562, val loss 204.14144897460938
iter 40, train loss 157.9810791015625, val loss 200.5831756591797
iter 50, train loss 157.7767333984375, val loss 201.2465362548828
iter 60, train loss 157.527099609375, val loss 199.6682891845703
iter 70, train loss 156.99806213378906, val loss 201.22817993164062
iter 80, train loss 156.91180419921875, val loss 200.5748748779297
iter 90, train loss 156.81069946289062, val loss 199.87347412109375
best loss 190.38645935058594
not here
quantized in 84.80131435394287 seconds
36070 MiB free out of 48676 MiB total
9 mlp.up_proj
Pruning ...
256
iter 0, train loss 128.5756378173828, val loss 162.69671630859375
iter 10, train loss 129.28453063964844, val loss 162.71591186523438
iter 20, train loss 129.62646484375, val loss 164.15101623535156
iter 30, train loss 129.58377075195312, val loss 163.78921508789062
iter 40, train loss 129.29931640625, val loss 164.50779724121094
iter 50, train loss 129.26687622070312, val loss 164.63478088378906
iter 60, train loss 129.3077850341797, val loss 164.19290161132812
iter 70, train loss 129.23300170898438, val loss 165.05612182617188
iter 80, train loss 129.25436401367188, val loss 167.51858520507812
iter 90, train loss 129.16102600097656, val loss 165.99716186523438
best loss 161.82989501953125
not here
quantized in 83.53195190429688 seconds
35876 MiB free out of 48676 MiB total
9 mlp.down_proj
Pruning ...
256
iter 0, train loss 2.6521193981170654, val loss 2.756434202194214
iter 10, train loss 2.6426682472229004, val loss 2.7702889442443848
iter 20, train loss 2.6375842094421387, val loss 2.839240789413452
iter 30, train loss 2.6305527687072754, val loss 2.8565573692321777
iter 40, train loss 2.612607955932617, val loss 2.8469371795654297
iter 50, train loss 2.606440544128418, val loss 2.7802734375
iter 60, train loss 2.607924222946167, val loss 2.7256407737731934
iter 70, train loss 2.605311870574951, val loss 2.7839701175689697
iter 80, train loss 2.602203369140625, val loss 2.794297695159912
iter 90, train loss 2.600710391998291, val loss 2.798555850982666
best loss 2.700150489807129
not here
quantized in 116.60938501358032 seconds
35682 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35682 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31586 MiB free out of 48676 MiB total
after cast to cpu
37852 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
10 self_attn.q_proj
Pruning ...
256
iter 0, train loss 263.5037841796875, val loss 233.76158142089844
iter 10, train loss 277.0112609863281, val loss 282.3412780761719
iter 20, train loss 268.34912109375, val loss 272.59185791015625
iter 30, train loss 265.2351379394531, val loss 269.0758361816406
iter 40, train loss 264.7091064453125, val loss 269.2478942871094
iter 50, train loss 264.41510009765625, val loss 269.9342041015625
iter 60, train loss 264.22760009765625, val loss 270.51324462890625
iter 70, train loss 263.6124572753906, val loss 269.7481384277344
iter 80, train loss 262.8050842285156, val loss 268.4760437011719
iter 90, train loss 262.4942626953125, val loss 268.72027587890625
best loss 233.76158142089844
not here
quantized in 41.715412616729736 seconds
36424 MiB free out of 48676 MiB total
10 self_attn.k_proj
Pruning ...
256
iter 0, train loss 290.2720947265625, val loss 253.2430419921875
iter 10, train loss 304.57891845703125, val loss 306.31365966796875
iter 20, train loss 305.7372741699219, val loss 309.8415832519531
iter 30, train loss 296.1797790527344, val loss 300.8854675292969
iter 40, train loss 292.6023254394531, val loss 297.4250183105469
iter 50, train loss 291.3839111328125, val loss 297.64288330078125
iter 60, train loss 290.45263671875, val loss 296.7203369140625
iter 70, train loss 289.385009765625, val loss 295.66497802734375
iter 80, train loss 289.0708312988281, val loss 295.10577392578125
iter 90, train loss 288.4408264160156, val loss 294.7393493652344
best loss 253.2430419921875
not here
quantized in 36.05128073692322 seconds
36414 MiB free out of 48676 MiB total
10 self_attn.v_proj
Pruning ...
256
iter 0, train loss 74.0799560546875, val loss 75.06725311279297
iter 10, train loss 74.7052001953125, val loss 77.99492645263672
iter 20, train loss 74.42283630371094, val loss 78.09043884277344
iter 30, train loss 74.32827758789062, val loss 77.72142028808594
iter 40, train loss 74.35965728759766, val loss 77.72589111328125
iter 50, train loss 74.27717590332031, val loss 77.74421691894531
iter 60, train loss 74.29637145996094, val loss 77.69477844238281
iter 70, train loss 74.18141174316406, val loss 77.52153778076172
iter 80, train loss 74.12771606445312, val loss 77.42933654785156
iter 90, train loss 74.01148223876953, val loss 77.44908905029297
best loss 75.06725311279297
not here
quantized in 34.744903564453125 seconds
36404 MiB free out of 48676 MiB total
10 self_attn.o_proj
Pruning ...
256
iter 0, train loss 8.161653518676758, val loss 2.6926088333129883
iter 10, train loss 7.6839823722839355, val loss 2.796565055847168
iter 20, train loss 7.351701259613037, val loss 2.7136125564575195
iter 30, train loss 7.160488128662109, val loss 2.679699182510376
iter 40, train loss 7.089366912841797, val loss 2.588411331176758
iter 50, train loss 6.995172500610352, val loss 2.694277763366699
iter 60, train loss 6.955060958862305, val loss 2.6343750953674316
iter 70, train loss 6.904971599578857, val loss 2.659158229827881
iter 80, train loss 6.890529632568359, val loss 2.639181613922119
iter 90, train loss 6.87849760055542, val loss 2.630852222442627
best loss 2.5540294647216797
not here
quantized in 34.11283588409424 seconds
36372 MiB free out of 48676 MiB total
10 mlp.gate_proj
Pruning ...
256
iter 0, train loss 166.3470001220703, val loss 189.67398071289062
iter 10, train loss 174.15109252929688, val loss 207.55142211914062
iter 20, train loss 170.06082153320312, val loss 199.61561584472656
iter 30, train loss 169.9624786376953, val loss 197.99002075195312
iter 40, train loss 168.7967071533203, val loss 200.43783569335938
iter 50, train loss 168.42041015625, val loss 197.53558349609375
iter 60, train loss 168.02381896972656, val loss 200.67994689941406
iter 70, train loss 167.67823791503906, val loss 200.63534545898438
iter 80, train loss 167.39614868164062, val loss 199.9444580078125
iter 90, train loss 167.2069854736328, val loss 201.47665405273438
best loss 189.49085998535156
not here
quantized in 94.44888854026794 seconds
36070 MiB free out of 48676 MiB total
10 mlp.up_proj
Pruning ...
256
iter 0, train loss 140.26010131835938, val loss 163.88018798828125
iter 10, train loss 140.91055297851562, val loss 163.8638916015625
iter 20, train loss 141.04824829101562, val loss 166.841064453125
iter 30, train loss 140.7665252685547, val loss 170.9910125732422
iter 40, train loss 140.59715270996094, val loss 171.07241821289062
iter 50, train loss 140.57359313964844, val loss 168.46261596679688
iter 60, train loss 140.5140380859375, val loss 168.24713134765625
iter 70, train loss 140.4903564453125, val loss 167.7601318359375
iter 80, train loss 140.5244903564453, val loss 167.46543884277344
iter 90, train loss 140.4368438720703, val loss 169.5338134765625
best loss 161.75755310058594
not here
quantized in 94.8029899597168 seconds
35876 MiB free out of 48676 MiB total
10 mlp.down_proj
Pruning ...
256
iter 0, train loss 3.10855770111084, val loss 3.7152624130249023
iter 10, train loss 3.1047189235687256, val loss 3.8969173431396484
iter 20, train loss 3.087136745452881, val loss 3.8474185466766357
iter 30, train loss 3.0621178150177, val loss 3.965616226196289
iter 40, train loss 3.0517866611480713, val loss 3.9636125564575195
iter 50, train loss 3.042957067489624, val loss 3.965965747833252
iter 60, train loss 3.030754804611206, val loss 3.912569046020508
iter 70, train loss 3.0294628143310547, val loss 3.9121007919311523
iter 80, train loss 3.0271081924438477, val loss 3.900310516357422
iter 90, train loss 3.0262153148651123, val loss 3.923288583755493
best loss 3.7152624130249023
not here
quantized in 102.72875928878784 seconds
35682 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35682 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31586 MiB free out of 48676 MiB total
after cast to cpu
37852 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
11 self_attn.q_proj
Pruning ...
256
iter 0, train loss 300.026123046875, val loss 256.6651306152344
iter 10, train loss 320.392333984375, val loss 321.4367370605469
iter 20, train loss 311.1309814453125, val loss 313.1240539550781
iter 30, train loss 305.5039367675781, val loss 308.1143798828125
iter 40, train loss 302.6289978027344, val loss 305.3375244140625
iter 50, train loss 301.5487060546875, val loss 303.708251953125
iter 60, train loss 300.2306213378906, val loss 303.41790771484375
iter 70, train loss 298.8771667480469, val loss 302.5441589355469
iter 80, train loss 298.7986145019531, val loss 302.3105163574219
iter 90, train loss 298.5605773925781, val loss 301.9634704589844
best loss 256.6651306152344
not here
quantized in 37.651583433151245 seconds
36424 MiB free out of 48676 MiB total
11 self_attn.k_proj
Pruning ...
256
iter 0, train loss 301.6415710449219, val loss 259.40008544921875
iter 10, train loss 317.71600341796875, val loss 314.5277099609375
iter 20, train loss 315.3915100097656, val loss 314.9727783203125
iter 30, train loss 307.76739501953125, val loss 308.5072021484375
iter 40, train loss 303.31884765625, val loss 304.9549560546875
iter 50, train loss 302.4342956542969, val loss 304.73175048828125
iter 60, train loss 300.92218017578125, val loss 302.5317687988281
iter 70, train loss 300.5420227050781, val loss 302.03204345703125
iter 80, train loss 300.8153381347656, val loss 303.01275634765625
iter 90, train loss 300.48284912109375, val loss 302.59027099609375
best loss 259.40008544921875
not here
quantized in 36.69758939743042 seconds
36414 MiB free out of 48676 MiB total
11 self_attn.v_proj
Pruning ...
256
iter 0, train loss 99.248779296875, val loss 99.6772232055664
iter 10, train loss 100.50250244140625, val loss 103.32833862304688
iter 20, train loss 99.89798736572266, val loss 102.89376831054688
iter 30, train loss 99.48857879638672, val loss 102.62686920166016
iter 40, train loss 99.59809875488281, val loss 102.68548583984375
iter 50, train loss 99.37141418457031, val loss 102.86200714111328
iter 60, train loss 99.41749572753906, val loss 102.70801544189453
iter 70, train loss 99.31856536865234, val loss 102.64837646484375
iter 80, train loss 99.43406677246094, val loss 102.6274185180664
iter 90, train loss 99.31340026855469, val loss 102.57695770263672
best loss 99.6772232055664
not here
quantized in 35.005911111831665 seconds
36404 MiB free out of 48676 MiB total
11 self_attn.o_proj
Pruning ...
256
iter 0, train loss 9.420147895812988, val loss 4.231429100036621
iter 10, train loss 9.070587158203125, val loss 4.6785173416137695
iter 20, train loss 8.985222816467285, val loss 4.429972171783447
iter 30, train loss 8.925603866577148, val loss 4.484344482421875
iter 40, train loss 8.87154769897461, val loss 4.511706352233887
iter 50, train loss 8.795835494995117, val loss 4.492027282714844
iter 60, train loss 8.717366218566895, val loss 4.451305866241455
iter 70, train loss 8.67524528503418, val loss 4.37072229385376
iter 80, train loss 8.68509578704834, val loss 4.515844345092773
iter 90, train loss 8.687765121459961, val loss 4.583611965179443
best loss 4.231429100036621
not here
quantized in 34.33564591407776 seconds
36372 MiB free out of 48676 MiB total
11 mlp.gate_proj
Pruning ...
256
iter 0, train loss 175.633544921875, val loss 197.16961669921875
iter 10, train loss 183.69659423828125, val loss 210.64346313476562
iter 20, train loss 179.60806274414062, val loss 206.3756103515625
iter 30, train loss 178.12713623046875, val loss 205.5697021484375
iter 40, train loss 177.28106689453125, val loss 204.6510009765625
iter 50, train loss 177.01177978515625, val loss 203.76026916503906
iter 60, train loss 176.39129638671875, val loss 201.6298828125
iter 70, train loss 176.26895141601562, val loss 201.34112548828125
iter 80, train loss 176.38294982910156, val loss 203.27084350585938
iter 90, train loss 176.0858154296875, val loss 203.13389587402344
best loss 192.80145263671875
not here
quantized in 94.03583407402039 seconds
36070 MiB free out of 48676 MiB total
11 mlp.up_proj
Pruning ...
256
iter 0, train loss 152.82260131835938, val loss 172.56105041503906
iter 10, train loss 153.63214111328125, val loss 178.42523193359375
iter 20, train loss 153.79702758789062, val loss 178.8580322265625
iter 30, train loss 153.52011108398438, val loss 180.86624145507812
iter 40, train loss 153.3622283935547, val loss 179.97662353515625
iter 50, train loss 153.3923797607422, val loss 180.8038330078125
iter 60, train loss 153.25686645507812, val loss 178.58982849121094
iter 70, train loss 153.1555938720703, val loss 177.06800842285156
iter 80, train loss 153.19610595703125, val loss 177.53475952148438
iter 90, train loss 153.19944763183594, val loss 177.89254760742188
best loss 172.56105041503906
not here
quantized in 92.57344603538513 seconds
35876 MiB free out of 48676 MiB total
11 mlp.down_proj
Pruning ...
256
iter 0, train loss 3.3276219367980957, val loss 2.7273943424224854
iter 10, train loss 3.3112905025482178, val loss 2.86715030670166
iter 20, train loss 3.28035569190979, val loss 2.8526906967163086
iter 30, train loss 3.272441864013672, val loss 2.840477705001831
iter 40, train loss 3.261378765106201, val loss 2.834380626678467
iter 50, train loss 3.2669355869293213, val loss 2.8419103622436523
iter 60, train loss 3.2629599571228027, val loss 2.8012795448303223
iter 70, train loss 3.256190776824951, val loss 2.8500566482543945
iter 80, train loss 3.2496557235717773, val loss 2.838181972503662
iter 90, train loss 3.247497081756592, val loss 2.826533079147339
best loss 2.7273943424224854
not here
quantized in 101.4199492931366 seconds
35682 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35682 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31586 MiB free out of 48676 MiB total
after cast to cpu
37852 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
12 self_attn.q_proj
Pruning ...
256
iter 0, train loss 308.1722106933594, val loss 271.4943542480469
iter 10, train loss 326.37286376953125, val loss 327.8953857421875
iter 20, train loss 315.6108093261719, val loss 318.7089538574219
iter 30, train loss 310.49627685546875, val loss 314.6229553222656
iter 40, train loss 308.55596923828125, val loss 311.05523681640625
iter 50, train loss 308.2656555175781, val loss 312.17437744140625
iter 60, train loss 308.2459411621094, val loss 312.03375244140625
iter 70, train loss 306.4664001464844, val loss 309.7804260253906
iter 80, train loss 306.3189697265625, val loss 310.3994140625
iter 90, train loss 305.6125183105469, val loss 310.47149658203125
best loss 271.4943542480469
not here
quantized in 37.855220794677734 seconds
36424 MiB free out of 48676 MiB total
12 self_attn.k_proj
Pruning ...
256
iter 0, train loss 338.3414306640625, val loss 289.7902526855469
iter 10, train loss 358.3123779296875, val loss 356.3074951171875
iter 20, train loss 352.590087890625, val loss 355.78363037109375
iter 30, train loss 346.1679992675781, val loss 350.077392578125
iter 40, train loss 342.548828125, val loss 346.38970947265625
iter 50, train loss 339.88494873046875, val loss 344.341064453125
iter 60, train loss 339.0363464355469, val loss 343.1625671386719
iter 70, train loss 338.442626953125, val loss 342.8401794433594
iter 80, train loss 337.4970703125, val loss 342.30023193359375
iter 90, train loss 337.74896240234375, val loss 341.9190673828125
best loss 289.7902526855469
not here
quantized in 36.72189497947693 seconds
36414 MiB free out of 48676 MiB total
12 self_attn.v_proj
Pruning ...
256
iter 0, train loss 97.29463195800781, val loss 98.33429718017578
iter 10, train loss 98.08329010009766, val loss 101.78341674804688
iter 20, train loss 97.77156066894531, val loss 101.12115478515625
iter 30, train loss 97.58302307128906, val loss 101.18468475341797
iter 40, train loss 97.64237976074219, val loss 101.2838363647461
iter 50, train loss 97.48529052734375, val loss 101.20683288574219
iter 60, train loss 97.48690032958984, val loss 101.22142028808594
iter 70, train loss 97.54751586914062, val loss 101.20450592041016
iter 80, train loss 97.41557312011719, val loss 101.2413101196289
iter 90, train loss 97.34835052490234, val loss 100.9394302368164
best loss 98.33429718017578
not here
quantized in 35.0539972782135 seconds
36404 MiB free out of 48676 MiB total
12 self_attn.o_proj
Pruning ...
256
iter 0, train loss 10.249765396118164, val loss 4.223359107971191
iter 10, train loss 9.963157653808594, val loss 4.195199966430664
iter 20, train loss 9.924580574035645, val loss 4.2269086837768555
iter 30, train loss 9.885709762573242, val loss 4.391816139221191
iter 40, train loss 9.793943405151367, val loss 4.325863838195801
iter 50, train loss 9.7677640914917, val loss 4.296243667602539
iter 60, train loss 9.760608673095703, val loss 4.333883762359619
iter 70, train loss 9.732976913452148, val loss 4.301090240478516
iter 80, train loss 9.702693939208984, val loss 4.348604202270508
iter 90, train loss 9.697846412658691, val loss 4.1948981285095215
best loss 4.134922504425049
not here
quantized in 34.192654848098755 seconds
36404 MiB free out of 48676 MiB total
12 mlp.gate_proj
Pruning ...
256
iter 0, train loss 188.02427673339844, val loss 204.243408203125
iter 10, train loss 196.51812744140625, val loss 224.16311645507812
iter 20, train loss 192.53106689453125, val loss 221.95028686523438
iter 30, train loss 191.5280303955078, val loss 219.38534545898438
iter 40, train loss 190.69119262695312, val loss 222.89523315429688
iter 50, train loss 189.8790740966797, val loss 221.04391479492188
iter 60, train loss 189.75115966796875, val loss 220.15579223632812
iter 70, train loss 189.45809936523438, val loss 218.83001708984375
iter 80, train loss 189.18092346191406, val loss 216.9591064453125
iter 90, train loss 189.2290802001953, val loss 216.9503173828125
best loss 204.243408203125
not here
quantized in 86.32744526863098 seconds
36102 MiB free out of 48676 MiB total
12 mlp.up_proj
Pruning ...
256
iter 0, train loss 168.26856994628906, val loss 189.3079833984375
iter 10, train loss 169.12716674804688, val loss 197.21475219726562
iter 20, train loss 168.9280242919922, val loss 196.67991638183594
iter 30, train loss 168.23387145996094, val loss 199.0419921875
iter 40, train loss 168.4584503173828, val loss 199.4366912841797
iter 50, train loss 168.20648193359375, val loss 199.04537963867188
iter 60, train loss 168.1282196044922, val loss 197.88330078125
iter 70, train loss 168.03704833984375, val loss 197.59132385253906
iter 80, train loss 168.07388305664062, val loss 197.03053283691406
iter 90, train loss 168.0035400390625, val loss 196.69070434570312
best loss 189.3079833984375
not here
quantized in 84.69265866279602 seconds
35908 MiB free out of 48676 MiB total
12 mlp.down_proj
Pruning ...
256
iter 0, train loss 3.7992844581604004, val loss 2.7764945030212402
iter 10, train loss 3.8239147663116455, val loss 2.8582510948181152
iter 20, train loss 3.822676181793213, val loss 2.899721622467041
iter 30, train loss 3.819596767425537, val loss 2.8077993392944336
iter 40, train loss 3.813140392303467, val loss 2.828819751739502
iter 50, train loss 3.8076393604278564, val loss 2.8275933265686035
iter 60, train loss 3.809441089630127, val loss 2.811504364013672
iter 70, train loss 3.8082170486450195, val loss 2.8349156379699707
iter 80, train loss 3.8120651245117188, val loss 2.870490550994873
iter 90, train loss 3.8110451698303223, val loss 2.869091272354126
best loss 2.7764945030212402
not here
quantized in 92.3632583618164 seconds
35714 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35714 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31618 MiB free out of 48676 MiB total
after cast to cpu
37852 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
13 self_attn.q_proj
Pruning ...
256
iter 0, train loss 318.2452392578125, val loss 276.98699951171875
iter 10, train loss 333.8973083496094, val loss 335.583984375
iter 20, train loss 321.9116516113281, val loss 323.7947082519531
iter 30, train loss 317.19683837890625, val loss 320.5311279296875
iter 40, train loss 314.0433349609375, val loss 317.4237060546875
iter 50, train loss 312.85955810546875, val loss 316.17266845703125
iter 60, train loss 312.66412353515625, val loss 316.011962890625
iter 70, train loss 311.87115478515625, val loss 315.4960021972656
iter 80, train loss 311.1595153808594, val loss 315.227783203125
iter 90, train loss 310.49798583984375, val loss 315.329833984375
best loss 276.98699951171875
not here
quantized in 33.853068828582764 seconds
36424 MiB free out of 48676 MiB total
13 self_attn.k_proj
Pruning ...
256
iter 0, train loss 339.5174865722656, val loss 287.86981201171875
iter 10, train loss 354.4788818359375, val loss 353.917236328125
iter 20, train loss 349.46771240234375, val loss 351.31329345703125
iter 30, train loss 341.83648681640625, val loss 343.97528076171875
iter 40, train loss 338.93560791015625, val loss 340.875732421875
iter 50, train loss 335.2452697753906, val loss 338.31390380859375
iter 60, train loss 332.721923828125, val loss 336.1216735839844
iter 70, train loss 331.4263000488281, val loss 333.57171630859375
iter 80, train loss 330.424072265625, val loss 333.57598876953125
iter 90, train loss 330.28668212890625, val loss 333.7041931152344
best loss 287.86981201171875
not here
quantized in 33.05138063430786 seconds
36414 MiB free out of 48676 MiB total
13 self_attn.v_proj
Pruning ...
256
iter 0, train loss 107.73013305664062, val loss 109.49555969238281
iter 10, train loss 107.80113983154297, val loss 111.03536987304688
iter 20, train loss 107.68397521972656, val loss 111.35298919677734
iter 30, train loss 107.46829986572266, val loss 110.76414489746094
iter 40, train loss 107.1541519165039, val loss 110.81475830078125
iter 50, train loss 107.25054931640625, val loss 110.93617248535156
iter 60, train loss 107.28005981445312, val loss 110.86038208007812
iter 70, train loss 107.21331787109375, val loss 111.0356216430664
iter 80, train loss 107.25901794433594, val loss 111.0048828125
iter 90, train loss 107.11295318603516, val loss 110.9931869506836
best loss 109.49555969238281
not here
quantized in 31.82990312576294 seconds
36404 MiB free out of 48676 MiB total
13 self_attn.o_proj
Pruning ...
256
iter 0, train loss 9.584016799926758, val loss 4.359027862548828
iter 10, train loss 9.142130851745605, val loss 4.601310729980469
iter 20, train loss 8.993207931518555, val loss 4.6415863037109375
iter 30, train loss 8.880474090576172, val loss 4.802989959716797
iter 40, train loss 8.811607360839844, val loss 4.831844806671143
iter 50, train loss 8.777817726135254, val loss 4.978701591491699
iter 60, train loss 8.733304023742676, val loss 4.946413040161133
iter 70, train loss 8.70145034790039, val loss 4.790097236633301
iter 80, train loss 8.679197311401367, val loss 4.712821960449219
iter 90, train loss 8.658639907836914, val loss 4.722395896911621
best loss 4.359027862548828
not here
quantized in 31.64205312728882 seconds
36372 MiB free out of 48676 MiB total
13 mlp.gate_proj
Pruning ...
256
iter 0, train loss 196.80026245117188, val loss 217.02357482910156
iter 10, train loss 203.45504760742188, val loss 234.16079711914062
iter 20, train loss 200.12936401367188, val loss 229.8017578125
iter 30, train loss 200.2088623046875, val loss 228.11770629882812
iter 40, train loss 199.1297149658203, val loss 229.92796325683594
iter 50, train loss 199.05203247070312, val loss 229.75161743164062
iter 60, train loss 198.5411834716797, val loss 230.6929168701172
iter 70, train loss 197.9485321044922, val loss 231.26588439941406
iter 80, train loss 197.67691040039062, val loss 231.02182006835938
iter 90, train loss 197.71063232421875, val loss 231.28262329101562
best loss 217.02357482910156
not here
quantized in 84.63762593269348 seconds
36070 MiB free out of 48676 MiB total
13 mlp.up_proj
Pruning ...
256
iter 0, train loss 180.96099853515625, val loss 207.08590698242188
iter 10, train loss 181.21629333496094, val loss 211.99127197265625
iter 20, train loss 181.32467651367188, val loss 213.95803833007812
iter 30, train loss 181.01280212402344, val loss 213.99652099609375
iter 40, train loss 180.64395141601562, val loss 214.23187255859375
iter 50, train loss 180.55227661132812, val loss 215.97813415527344
iter 60, train loss 180.581298828125, val loss 214.1714324951172
iter 70, train loss 180.5227813720703, val loss 213.87005615234375
iter 80, train loss 180.48782348632812, val loss 212.06227111816406
iter 90, train loss 180.41012573242188, val loss 211.78097534179688
best loss 207.08590698242188
not here
quantized in 83.87127900123596 seconds
35876 MiB free out of 48676 MiB total
13 mlp.down_proj
Pruning ...
256
iter 0, train loss 4.400941848754883, val loss 3.535537004470825
iter 10, train loss 4.367311954498291, val loss 3.5845344066619873
iter 20, train loss 4.334034442901611, val loss 3.5444018840789795
iter 30, train loss 4.317083358764648, val loss 3.631913185119629
iter 40, train loss 4.303735733032227, val loss 3.557581663131714
iter 50, train loss 4.297098159790039, val loss 3.5708322525024414
iter 60, train loss 4.301183223724365, val loss 3.520400047302246
iter 70, train loss 4.294482707977295, val loss 3.411402702331543
iter 80, train loss 4.289963722229004, val loss 3.4786834716796875
iter 90, train loss 4.287813663482666, val loss 3.4630913734436035
best loss 3.405487537384033
not here
quantized in 91.75819993019104 seconds
35682 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35682 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31586 MiB free out of 48676 MiB total
after cast to cpu
37852 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
14 self_attn.q_proj
Pruning ...
256
iter 0, train loss 332.2781982421875, val loss 286.1700439453125
iter 10, train loss 355.70794677734375, val loss 356.830810546875
iter 20, train loss 343.6715393066406, val loss 346.64385986328125
iter 30, train loss 334.5047302246094, val loss 338.5865478515625
iter 40, train loss 332.76019287109375, val loss 334.51800537109375
iter 50, train loss 330.73602294921875, val loss 332.9745788574219
iter 60, train loss 328.8124084472656, val loss 331.817138671875
iter 70, train loss 328.7762145996094, val loss 331.6944885253906
iter 80, train loss 329.36883544921875, val loss 333.24169921875
iter 90, train loss 328.9540100097656, val loss 333.6204833984375
best loss 286.1700439453125
not here
quantized in 34.178826093673706 seconds
36424 MiB free out of 48676 MiB total
14 self_attn.k_proj
Pruning ...
256
iter 0, train loss 362.86956787109375, val loss 299.8583679199219
iter 10, train loss 380.1805725097656, val loss 381.073974609375
iter 20, train loss 375.84649658203125, val loss 376.01483154296875
iter 30, train loss 367.26141357421875, val loss 368.5267333984375
iter 40, train loss 361.4522705078125, val loss 361.9189147949219
iter 50, train loss 357.15460205078125, val loss 358.7597351074219
iter 60, train loss 356.08880615234375, val loss 358.7906494140625
iter 70, train loss 354.13702392578125, val loss 357.4811706542969
iter 80, train loss 354.1528015136719, val loss 357.3087463378906
iter 90, train loss 353.8614807128906, val loss 357.1563720703125
best loss 299.8583679199219
not here
quantized in 32.66886782646179 seconds
36414 MiB free out of 48676 MiB total
14 self_attn.v_proj
Pruning ...
256
iter 0, train loss 108.61404418945312, val loss 111.0289306640625
iter 10, train loss 108.7330093383789, val loss 112.32884979248047
iter 20, train loss 108.30157470703125, val loss 112.24322509765625
iter 30, train loss 108.08554077148438, val loss 112.3255615234375
iter 40, train loss 107.92550659179688, val loss 112.05004119873047
iter 50, train loss 107.78565979003906, val loss 111.99484252929688
iter 60, train loss 107.75852966308594, val loss 111.99482727050781
iter 70, train loss 107.72166442871094, val loss 112.10250091552734
iter 80, train loss 107.67254638671875, val loss 112.09909057617188
iter 90, train loss 107.69789123535156, val loss 112.08584594726562
best loss 111.0289306640625
not here
quantized in 31.261807203292847 seconds
36404 MiB free out of 48676 MiB total
14 self_attn.o_proj
Pruning ...
256
iter 0, train loss 12.342884063720703, val loss 4.104592800140381
iter 10, train loss 12.141767501831055, val loss 4.2178473472595215
iter 20, train loss 12.03812026977539, val loss 4.235886573791504
iter 30, train loss 11.965004920959473, val loss 4.416834354400635
iter 40, train loss 11.838813781738281, val loss 4.2350754737854
iter 50, train loss 11.811711311340332, val loss 4.248193740844727
iter 60, train loss 11.771219253540039, val loss 4.163129806518555
iter 70, train loss 11.733509063720703, val loss 4.222979545593262
iter 80, train loss 11.692192077636719, val loss 4.219102382659912
iter 90, train loss 11.689508438110352, val loss 4.329276084899902
best loss 4.104592800140381
not here
quantized in 30.81946897506714 seconds
36372 MiB free out of 48676 MiB total
14 mlp.gate_proj
Pruning ...
256
iter 0, train loss 215.30001831054688, val loss 235.77359008789062
iter 10, train loss 224.6353759765625, val loss 253.38653564453125
iter 20, train loss 218.6898651123047, val loss 253.72879028320312
iter 30, train loss 218.34634399414062, val loss 251.4696044921875
iter 40, train loss 216.7345428466797, val loss 249.2093505859375
iter 50, train loss 216.5933074951172, val loss 251.4172821044922
iter 60, train loss 216.06784057617188, val loss 250.97662353515625
iter 70, train loss 216.02720642089844, val loss 250.36077880859375
iter 80, train loss 215.95327758789062, val loss 251.30227661132812
iter 90, train loss 215.9178466796875, val loss 250.36325073242188
best loss 235.5486602783203
not here
quantized in 84.92884922027588 seconds
36070 MiB free out of 48676 MiB total
14 mlp.up_proj
Pruning ...
256
iter 0, train loss 198.54376220703125, val loss 223.4358673095703
iter 10, train loss 199.49928283691406, val loss 228.85476684570312
iter 20, train loss 199.43943786621094, val loss 230.59027099609375
iter 30, train loss 198.9557342529297, val loss 227.61810302734375
iter 40, train loss 198.8089599609375, val loss 225.57403564453125
iter 50, train loss 198.51190185546875, val loss 227.5973358154297
iter 60, train loss 198.5928192138672, val loss 230.75262451171875
iter 70, train loss 198.5572052001953, val loss 233.14715576171875
iter 80, train loss 198.47433471679688, val loss 233.0218963623047
iter 90, train loss 198.3543243408203, val loss 230.1947021484375
best loss 223.4358673095703
not here
quantized in 84.29214477539062 seconds
35876 MiB free out of 48676 MiB total
14 mlp.down_proj
Pruning ...
256
iter 0, train loss 5.010619163513184, val loss 3.99790620803833
iter 10, train loss 5.02672815322876, val loss 4.001569747924805
iter 20, train loss 5.0102458000183105, val loss 3.9634556770324707
iter 30, train loss 5.000134468078613, val loss 4.010196208953857
iter 40, train loss 4.990453720092773, val loss 4.005690097808838
iter 50, train loss 4.980472564697266, val loss 4.023840427398682
iter 60, train loss 4.980844974517822, val loss 3.9885973930358887
iter 70, train loss 4.975798606872559, val loss 4.015756607055664
iter 80, train loss 4.970088958740234, val loss 3.999926805496216
iter 90, train loss 4.972527027130127, val loss 4.015214443206787
best loss 3.912210464477539
not here
quantized in 92.88165163993835 seconds
35682 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35682 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31586 MiB free out of 48676 MiB total
after cast to cpu
37852 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
15 self_attn.q_proj
Pruning ...
256
iter 0, train loss 313.556640625, val loss 266.0202331542969
iter 10, train loss 330.423583984375, val loss 330.8349914550781
iter 20, train loss 314.90594482421875, val loss 314.2441711425781
iter 30, train loss 311.3182067871094, val loss 311.7816162109375
iter 40, train loss 306.9842224121094, val loss 306.8705749511719
iter 50, train loss 304.6968994140625, val loss 305.17974853515625
iter 60, train loss 304.8082275390625, val loss 306.4196472167969
iter 70, train loss 304.90869140625, val loss 305.60125732421875
iter 80, train loss 303.8261413574219, val loss 304.792724609375
iter 90, train loss 303.54742431640625, val loss 304.6791687011719
best loss 266.0202331542969
not here
quantized in 33.98763561248779 seconds
36424 MiB free out of 48676 MiB total
15 self_attn.k_proj
Pruning ...
256
iter 0, train loss 351.31903076171875, val loss 289.5208435058594
iter 10, train loss 366.7982482910156, val loss 364.3544616699219
iter 20, train loss 364.1717834472656, val loss 363.42755126953125
iter 30, train loss 356.6999816894531, val loss 355.7683410644531
iter 40, train loss 350.96868896484375, val loss 349.7068786621094
iter 50, train loss 348.3569641113281, val loss 347.68798828125
iter 60, train loss 346.1507568359375, val loss 346.39154052734375
iter 70, train loss 345.7107849121094, val loss 345.8855285644531
iter 80, train loss 345.8172912597656, val loss 346.272216796875
iter 90, train loss 345.079345703125, val loss 346.2646179199219
best loss 289.5208435058594
not here
quantized in 33.04268741607666 seconds
36414 MiB free out of 48676 MiB total
15 self_attn.v_proj
Pruning ...
256
iter 0, train loss 113.00819396972656, val loss 113.94242095947266
iter 10, train loss 113.49602508544922, val loss 116.1163101196289
iter 20, train loss 113.09026336669922, val loss 116.48609924316406
iter 30, train loss 112.8776626586914, val loss 115.97222900390625
iter 40, train loss 112.97500610351562, val loss 116.18675231933594
iter 50, train loss 112.77392578125, val loss 116.1849365234375
iter 60, train loss 112.78410339355469, val loss 116.10182189941406
iter 70, train loss 112.39871978759766, val loss 116.20510864257812
iter 80, train loss 112.26227569580078, val loss 116.11004638671875
iter 90, train loss 112.22061157226562, val loss 115.96476745605469
best loss 113.94242095947266
not here
quantized in 31.243075609207153 seconds
36404 MiB free out of 48676 MiB total
15 self_attn.o_proj
Pruning ...
256
iter 0, train loss 11.289308547973633, val loss 5.2669477462768555
iter 10, train loss 10.910320281982422, val loss 5.438406467437744
iter 20, train loss 10.836020469665527, val loss 5.5077009201049805
iter 30, train loss 10.707420349121094, val loss 5.446812629699707
iter 40, train loss 10.653728485107422, val loss 5.3806071281433105
iter 50, train loss 10.602180480957031, val loss 5.276676177978516
iter 60, train loss 10.530442237854004, val loss 5.166199207305908
iter 70, train loss 10.538599014282227, val loss 5.406245231628418
iter 80, train loss 10.510931015014648, val loss 5.4269819259643555
iter 90, train loss 10.482704162597656, val loss 5.3397417068481445
best loss 5.142561435699463
not here
quantized in 31.047139883041382 seconds
36404 MiB free out of 48676 MiB total
15 mlp.gate_proj
Pruning ...
256
iter 0, train loss 233.49432373046875, val loss 259.4780578613281
iter 10, train loss 242.62203979492188, val loss 277.5236511230469
iter 20, train loss 238.24468994140625, val loss 276.78314208984375
iter 30, train loss 238.2562713623047, val loss 274.11669921875
iter 40, train loss 237.562744140625, val loss 271.4338073730469
iter 50, train loss 237.89901733398438, val loss 271.9945068359375
iter 60, train loss 237.0224609375, val loss 273.83819580078125
iter 70, train loss 236.5489501953125, val loss 274.32501220703125
iter 80, train loss 236.16641235351562, val loss 274.30706787109375
iter 90, train loss 235.61398315429688, val loss 276.6292724609375
best loss 259.4780578613281
not here
quantized in 84.50316619873047 seconds
36102 MiB free out of 48676 MiB total
15 mlp.up_proj
Pruning ...
256
iter 0, train loss 216.19479370117188, val loss 247.31594848632812
iter 10, train loss 216.8712615966797, val loss 253.23968505859375
iter 20, train loss 217.249267578125, val loss 253.34869384765625
iter 30, train loss 217.0931396484375, val loss 251.2318878173828
iter 40, train loss 216.9193878173828, val loss 254.42642211914062
iter 50, train loss 216.76356506347656, val loss 255.9983367919922
iter 60, train loss 216.75970458984375, val loss 253.62625122070312
iter 70, train loss 216.55108642578125, val loss 253.52288818359375
iter 80, train loss 216.53115844726562, val loss 253.9644317626953
iter 90, train loss 216.373046875, val loss 252.85708618164062
best loss 247.31594848632812
not here
quantized in 83.7318069934845 seconds
35908 MiB free out of 48676 MiB total
15 mlp.down_proj
Pruning ...
256
iter 0, train loss 6.115209579467773, val loss 4.4203667640686035
iter 10, train loss 6.132540225982666, val loss 4.456151962280273
iter 20, train loss 6.114100456237793, val loss 4.444491863250732
iter 30, train loss 6.096043586730957, val loss 4.461199760437012
iter 40, train loss 6.092147350311279, val loss 4.502867221832275
iter 50, train loss 6.090126991271973, val loss 4.484142780303955
iter 60, train loss 6.078296661376953, val loss 4.529671669006348
iter 70, train loss 6.075179576873779, val loss 4.521559238433838
iter 80, train loss 6.076428413391113, val loss 4.471811771392822
iter 90, train loss 6.078139781951904, val loss 4.44214391708374
best loss 4.371192932128906
not here
quantized in 91.51461911201477 seconds
35714 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35714 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31618 MiB free out of 48676 MiB total
after cast to cpu
37852 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
16 self_attn.q_proj
Pruning ...
256
iter 0, train loss 328.0283508300781, val loss 280.47576904296875
iter 10, train loss 340.11834716796875, val loss 344.9108581542969
iter 20, train loss 333.31646728515625, val loss 338.5482482910156
iter 30, train loss 326.3199768066406, val loss 331.00848388671875
iter 40, train loss 322.4766540527344, val loss 327.67041015625
iter 50, train loss 319.4437561035156, val loss 324.10443115234375
iter 60, train loss 317.31683349609375, val loss 323.0848693847656
iter 70, train loss 316.10064697265625, val loss 322.6155090332031
iter 80, train loss 316.0437316894531, val loss 322.4517517089844
iter 90, train loss 315.2459716796875, val loss 322.98175048828125
best loss 280.47576904296875
not here
quantized in 33.93192958831787 seconds
36424 MiB free out of 48676 MiB total
16 self_attn.k_proj
Pruning ...
256
iter 0, train loss 360.58642578125, val loss 299.60528564453125
iter 10, train loss 375.9938659667969, val loss 376.9256591796875
iter 20, train loss 371.98309326171875, val loss 374.4726257324219
iter 30, train loss 364.618408203125, val loss 368.40655517578125
iter 40, train loss 359.1407470703125, val loss 362.3985595703125
iter 50, train loss 357.98712158203125, val loss 360.693359375
iter 60, train loss 355.7281494140625, val loss 358.2132568359375
iter 70, train loss 355.2117004394531, val loss 357.6515197753906
iter 80, train loss 352.91583251953125, val loss 357.05364990234375
iter 90, train loss 350.97357177734375, val loss 355.3358154296875
best loss 299.60528564453125
not here
quantized in 32.88361716270447 seconds
36414 MiB free out of 48676 MiB total
16 self_attn.v_proj
Pruning ...
256
iter 0, train loss 127.90254211425781, val loss 133.36383056640625
iter 10, train loss 127.54777526855469, val loss 134.3518524169922
iter 20, train loss 127.13873291015625, val loss 133.796630859375
iter 30, train loss 126.92402648925781, val loss 134.06312561035156
iter 40, train loss 127.12316131591797, val loss 133.77462768554688
iter 50, train loss 126.63687133789062, val loss 133.66395568847656
iter 60, train loss 126.62821960449219, val loss 133.6090087890625
iter 70, train loss 126.701904296875, val loss 133.56915283203125
iter 80, train loss 126.51483917236328, val loss 133.5590362548828
iter 90, train loss 126.45662689208984, val loss 133.697998046875
best loss 133.36383056640625
not here
quantized in 31.051863193511963 seconds
36404 MiB free out of 48676 MiB total
16 self_attn.o_proj
Pruning ...
256
iter 0, train loss 13.494001388549805, val loss 6.474688529968262
iter 10, train loss 13.431160926818848, val loss 6.808833122253418
iter 20, train loss 13.305398941040039, val loss 6.65709114074707
iter 30, train loss 13.250468254089355, val loss 6.780281066894531
iter 40, train loss 13.14161491394043, val loss 6.8781938552856445
iter 50, train loss 13.106077194213867, val loss 6.794820308685303
iter 60, train loss 13.065643310546875, val loss 6.644363880157471
iter 70, train loss 13.050601959228516, val loss 6.632297992706299
iter 80, train loss 13.003402709960938, val loss 6.666067123413086
iter 90, train loss 13.010672569274902, val loss 6.64777135848999
best loss 6.474688529968262
not here
quantized in 31.38392186164856 seconds
36372 MiB free out of 48676 MiB total
16 mlp.gate_proj
Pruning ...
256
iter 0, train loss 269.4271240234375, val loss 283.1923828125
iter 10, train loss 282.13775634765625, val loss 307.3479919433594
iter 20, train loss 276.1580810546875, val loss 304.7049560546875
iter 30, train loss 274.85797119140625, val loss 300.1775207519531
iter 40, train loss 274.4809265136719, val loss 297.134033203125
iter 50, train loss 273.1715393066406, val loss 294.62908935546875
iter 60, train loss 272.0419616699219, val loss 297.32745361328125
iter 70, train loss 271.5910339355469, val loss 302.36602783203125
iter 80, train loss 271.1919250488281, val loss 303.7905578613281
iter 90, train loss 271.1213684082031, val loss 305.4873962402344
best loss 283.1219177246094
not here
quantized in 85.18636012077332 seconds
36070 MiB free out of 48676 MiB total
16 mlp.up_proj
Pruning ...
256
iter 0, train loss 245.21511840820312, val loss 270.3349914550781
iter 10, train loss 246.3669891357422, val loss 273.0459899902344
iter 20, train loss 246.37969970703125, val loss 277.8822326660156
iter 30, train loss 245.726318359375, val loss 275.58966064453125
iter 40, train loss 245.80795288085938, val loss 275.3764343261719
iter 50, train loss 245.62916564941406, val loss 274.1266174316406
iter 60, train loss 245.37460327148438, val loss 276.05291748046875
iter 70, train loss 244.90382385253906, val loss 273.95098876953125
iter 80, train loss 244.88609313964844, val loss 277.08819580078125
iter 90, train loss 244.87069702148438, val loss 274.1976013183594
best loss 267.47882080078125
not here
quantized in 84.21906757354736 seconds
35876 MiB free out of 48676 MiB total
16 mlp.down_proj
Pruning ...
256
iter 0, train loss 8.117446899414062, val loss 6.815737724304199
iter 10, train loss 8.130172729492188, val loss 6.901623725891113
iter 20, train loss 8.090499877929688, val loss 6.857754230499268
iter 30, train loss 8.074644088745117, val loss 6.787540435791016
iter 40, train loss 8.068696975708008, val loss 6.950277328491211
iter 50, train loss 8.068873405456543, val loss 6.847438335418701
iter 60, train loss 8.064474105834961, val loss 6.840756893157959
iter 70, train loss 8.056556701660156, val loss 7.020125389099121
iter 80, train loss 8.05754566192627, val loss 6.91105318069458
iter 90, train loss 8.060506820678711, val loss 6.8271894454956055
best loss 6.714293003082275
not here
quantized in 91.64651775360107 seconds
35682 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35682 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31586 MiB free out of 48676 MiB total
after cast to cpu
37852 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
17 self_attn.q_proj
Pruning ...
256
iter 0, train loss 340.50738525390625, val loss 292.6359558105469
iter 10, train loss 359.4996337890625, val loss 363.5530700683594
iter 20, train loss 353.4706726074219, val loss 358.1092529296875
iter 30, train loss 340.1728515625, val loss 344.628662109375
iter 40, train loss 337.1640625, val loss 342.0469970703125
iter 50, train loss 333.88275146484375, val loss 339.5340576171875
iter 60, train loss 332.37451171875, val loss 338.6275634765625
iter 70, train loss 331.10821533203125, val loss 337.6405944824219
iter 80, train loss 330.1864318847656, val loss 336.0303955078125
iter 90, train loss 330.26495361328125, val loss 336.0633544921875
best loss 292.6359558105469
not here
quantized in 34.14928197860718 seconds
36424 MiB free out of 48676 MiB total
17 self_attn.k_proj
Pruning ...
256
iter 0, train loss 367.9024658203125, val loss 310.3815612792969
iter 10, train loss 389.0794677734375, val loss 390.0336608886719
iter 20, train loss 384.432861328125, val loss 389.38592529296875
iter 30, train loss 377.1012268066406, val loss 381.4015808105469
iter 40, train loss 372.64453125, val loss 377.46453857421875
iter 50, train loss 368.85260009765625, val loss 374.8008117675781
iter 60, train loss 365.7206726074219, val loss 371.4930419921875
iter 70, train loss 364.0799865722656, val loss 370.4261474609375
iter 80, train loss 361.9378662109375, val loss 370.6434631347656
iter 90, train loss 361.2440185546875, val loss 368.96142578125
best loss 310.3815612792969
not here
quantized in 33.289095878601074 seconds
36414 MiB free out of 48676 MiB total
17 self_attn.v_proj
Pruning ...
256
iter 0, train loss 136.03175354003906, val loss 140.769775390625
iter 10, train loss 136.64382934570312, val loss 143.23748779296875
iter 20, train loss 136.3634033203125, val loss 142.88522338867188
iter 30, train loss 135.7535400390625, val loss 142.4688720703125
iter 40, train loss 135.59381103515625, val loss 142.76828002929688
iter 50, train loss 135.37550354003906, val loss 142.70565795898438
iter 60, train loss 135.29608154296875, val loss 142.6475830078125
iter 70, train loss 135.46685791015625, val loss 142.45018005371094
iter 80, train loss 135.3948974609375, val loss 142.3655548095703
iter 90, train loss 135.32308959960938, val loss 141.97877502441406
best loss 140.769775390625
not here
quantized in 31.30444097518921 seconds
36404 MiB free out of 48676 MiB total
17 self_attn.o_proj
Pruning ...
256
iter 0, train loss 10.395223617553711, val loss 6.763936519622803
iter 10, train loss 10.19062328338623, val loss 7.067750930786133
iter 20, train loss 10.08426284790039, val loss 7.279656410217285
iter 30, train loss 9.982836723327637, val loss 7.097522735595703
iter 40, train loss 9.974743843078613, val loss 7.007699012756348
iter 50, train loss 9.904438018798828, val loss 7.026458740234375
iter 60, train loss 9.886415481567383, val loss 7.00634765625
iter 70, train loss 9.85225772857666, val loss 6.7212114334106445
iter 80, train loss 9.835951805114746, val loss 6.783609867095947
iter 90, train loss 9.825908660888672, val loss 6.725437641143799
best loss 6.676530838012695
not here
quantized in 31.274394750595093 seconds
36372 MiB free out of 48676 MiB total
17 mlp.gate_proj
Pruning ...
256
iter 0, train loss 308.05096435546875, val loss 325.7154541015625
iter 10, train loss 321.9986572265625, val loss 370.06170654296875
iter 20, train loss 317.31280517578125, val loss 368.4330749511719
iter 30, train loss 316.0736083984375, val loss 366.767578125
iter 40, train loss 314.8925476074219, val loss 363.9450378417969
iter 50, train loss 314.02154541015625, val loss 356.48443603515625
iter 60, train loss 313.41839599609375, val loss 355.255126953125
iter 70, train loss 313.13232421875, val loss 359.03350830078125
iter 80, train loss 312.85150146484375, val loss 355.9864807128906
iter 90, train loss 312.7532958984375, val loss 355.767822265625
best loss 325.7154541015625
not here
quantized in 84.96573829650879 seconds
36070 MiB free out of 48676 MiB total
17 mlp.up_proj
Pruning ...
256
iter 0, train loss 271.5715637207031, val loss 302.551025390625
iter 10, train loss 272.06207275390625, val loss 313.44793701171875
iter 20, train loss 272.674072265625, val loss 314.6890869140625
iter 30, train loss 272.1114807128906, val loss 314.9923095703125
iter 40, train loss 271.9434509277344, val loss 314.9277648925781
iter 50, train loss 272.09307861328125, val loss 310.4730529785156
iter 60, train loss 272.0021057128906, val loss 317.0074157714844
iter 70, train loss 272.18963623046875, val loss 313.70098876953125
iter 80, train loss 271.82550048828125, val loss 312.1986083984375
iter 90, train loss 271.84857177734375, val loss 314.51580810546875
best loss 302.551025390625
not here
quantized in 84.03523588180542 seconds
35876 MiB free out of 48676 MiB total
17 mlp.down_proj
Pruning ...
256
iter 0, train loss 9.104879379272461, val loss 8.282254219055176
iter 10, train loss 9.090813636779785, val loss 8.563820838928223
iter 20, train loss 9.045797348022461, val loss 8.283882141113281
iter 30, train loss 9.027941703796387, val loss 8.235685348510742
iter 40, train loss 9.005313873291016, val loss 8.28502082824707
iter 50, train loss 8.996641159057617, val loss 8.35594654083252
iter 60, train loss 8.988622665405273, val loss 8.264372825622559
iter 70, train loss 8.984289169311523, val loss 8.252229690551758
iter 80, train loss 8.977243423461914, val loss 8.253156661987305
iter 90, train loss 8.968048095703125, val loss 8.233833312988281
best loss 8.149993896484375
not here
quantized in 91.73864436149597 seconds
35682 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35682 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31586 MiB free out of 48676 MiB total
after cast to cpu
37852 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
18 self_attn.q_proj
Pruning ...
256
iter 0, train loss 375.9291076660156, val loss 312.92181396484375
iter 10, train loss 392.0730285644531, val loss 395.3861083984375
iter 20, train loss 377.6330871582031, val loss 381.58660888671875
iter 30, train loss 365.1797180175781, val loss 371.576171875
iter 40, train loss 362.9106140136719, val loss 368.8953857421875
iter 50, train loss 358.5311584472656, val loss 364.3374938964844
iter 60, train loss 356.25933837890625, val loss 361.7465515136719
iter 70, train loss 354.9024658203125, val loss 360.43951416015625
iter 80, train loss 354.2463073730469, val loss 360.3211669921875
iter 90, train loss 354.1988830566406, val loss 360.1563415527344
best loss 312.92181396484375
not here
quantized in 34.23506689071655 seconds
36424 MiB free out of 48676 MiB total
18 self_attn.k_proj
Pruning ...
256
iter 0, train loss 398.97088623046875, val loss 330.19793701171875
iter 10, train loss 419.67535400390625, val loss 421.79107666015625
iter 20, train loss 407.02313232421875, val loss 409.68951416015625
iter 30, train loss 396.45977783203125, val loss 403.62176513671875
iter 40, train loss 388.9552001953125, val loss 396.27239990234375
iter 50, train loss 387.17462158203125, val loss 392.6495056152344
iter 60, train loss 383.7486572265625, val loss 389.8082580566406
iter 70, train loss 382.2216796875, val loss 389.0306091308594
iter 80, train loss 379.935546875, val loss 386.14312744140625
iter 90, train loss 378.30810546875, val loss 384.76678466796875
best loss 330.19793701171875
not here
quantized in 33.15864109992981 seconds
36414 MiB free out of 48676 MiB total
18 self_attn.v_proj
Pruning ...
256
iter 0, train loss 165.21884155273438, val loss 170.43829345703125
iter 10, train loss 165.04531860351562, val loss 172.2263641357422
iter 20, train loss 165.18600463867188, val loss 172.19998168945312
iter 30, train loss 164.66183471679688, val loss 172.703125
iter 40, train loss 164.2904052734375, val loss 171.81756591796875
iter 50, train loss 164.04696655273438, val loss 172.68740844726562
iter 60, train loss 164.14999389648438, val loss 172.3902130126953
iter 70, train loss 163.90065002441406, val loss 172.73825073242188
iter 80, train loss 163.79550170898438, val loss 172.6875
iter 90, train loss 163.7233123779297, val loss 172.32135009765625
best loss 170.43829345703125
not here
quantized in 31.14298915863037 seconds
36404 MiB free out of 48676 MiB total
18 self_attn.o_proj
Pruning ...
256
iter 0, train loss 8.983325958251953, val loss 8.42425537109375
iter 10, train loss 8.547353744506836, val loss 9.092479705810547
iter 20, train loss 8.416394233703613, val loss 9.420490264892578
iter 30, train loss 8.328551292419434, val loss 9.3888578414917
iter 40, train loss 8.243410110473633, val loss 8.776782035827637
iter 50, train loss 8.184091567993164, val loss 8.88038444519043
iter 60, train loss 8.136383056640625, val loss 8.863643646240234
iter 70, train loss 8.115124702453613, val loss 9.104961395263672
iter 80, train loss 8.078723907470703, val loss 9.1444091796875
iter 90, train loss 8.070292472839355, val loss 9.12829303741455
best loss 8.28331184387207
not here
quantized in 31.670252561569214 seconds
36372 MiB free out of 48676 MiB total
18 mlp.gate_proj
Pruning ...
256
iter 0, train loss 347.45941162109375, val loss 370.1856689453125
iter 10, train loss 361.41094970703125, val loss 409.97528076171875
iter 20, train loss 358.1990661621094, val loss 407.923828125
iter 30, train loss 358.07293701171875, val loss 406.39398193359375
iter 40, train loss 355.800537109375, val loss 407.2412109375
iter 50, train loss 354.35491943359375, val loss 408.63134765625
iter 60, train loss 354.8083190917969, val loss 404.6039733886719
iter 70, train loss 354.6813049316406, val loss 402.96478271484375
iter 80, train loss 354.3061828613281, val loss 406.9530334472656
iter 90, train loss 354.158935546875, val loss 404.24652099609375
best loss 370.1856689453125
not here
quantized in 85.00010943412781 seconds
36070 MiB free out of 48676 MiB total
18 mlp.up_proj
Pruning ...
256
iter 0, train loss 298.0468444824219, val loss 332.8522033691406
iter 10, train loss 298.45770263671875, val loss 342.6565856933594
iter 20, train loss 298.7643127441406, val loss 339.593017578125
iter 30, train loss 298.2515563964844, val loss 342.3819274902344
iter 40, train loss 297.8717041015625, val loss 344.30224609375
iter 50, train loss 298.134521484375, val loss 347.5696105957031
iter 60, train loss 298.0224609375, val loss 349.3568115234375
iter 70, train loss 297.93182373046875, val loss 352.10003662109375
iter 80, train loss 297.8077392578125, val loss 351.51739501953125
iter 90, train loss 298.106689453125, val loss 350.8908996582031
best loss 332.8522033691406
not here
quantized in 83.7543249130249 seconds
35876 MiB free out of 48676 MiB total
18 mlp.down_proj
Pruning ...
256
iter 0, train loss 10.575376510620117, val loss 7.4674177169799805
iter 10, train loss 10.579177856445312, val loss 7.578288555145264
iter 20, train loss 10.532320976257324, val loss 7.80584192276001
iter 30, train loss 10.503924369812012, val loss 7.647371768951416
iter 40, train loss 10.480547904968262, val loss 7.566652297973633
iter 50, train loss 10.477052688598633, val loss 7.650956153869629
iter 60, train loss 10.455601692199707, val loss 7.654336929321289
iter 70, train loss 10.44859504699707, val loss 7.514199256896973
iter 80, train loss 10.444129943847656, val loss 7.57656192779541
iter 90, train loss 10.438704490661621, val loss 7.616636753082275
best loss 7.388096332550049
not here
quantized in 91.0105619430542 seconds
35682 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35682 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31586 MiB free out of 48676 MiB total
after cast to cpu
37852 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
19 self_attn.q_proj
Pruning ...
256
iter 0, train loss 356.88775634765625, val loss 301.9381103515625
iter 10, train loss 376.93377685546875, val loss 378.42388916015625
iter 20, train loss 360.3785400390625, val loss 363.87060546875
iter 30, train loss 354.53216552734375, val loss 358.2673034667969
iter 40, train loss 348.64068603515625, val loss 353.1684265136719
iter 50, train loss 347.5458679199219, val loss 351.19366455078125
iter 60, train loss 345.0190124511719, val loss 350.017822265625
iter 70, train loss 343.7622985839844, val loss 349.6820373535156
iter 80, train loss 344.07574462890625, val loss 350.4357604980469
iter 90, train loss 343.8533935546875, val loss 350.4316101074219
best loss 301.9381103515625
not here
quantized in 34.2123236656189 seconds
36424 MiB free out of 48676 MiB total
19 self_attn.k_proj
Pruning ...
256
iter 0, train loss 381.7904968261719, val loss 317.6841735839844
iter 10, train loss 399.864013671875, val loss 400.20159912109375
iter 20, train loss 394.13629150390625, val loss 399.61138916015625
iter 30, train loss 383.4724426269531, val loss 389.20660400390625
iter 40, train loss 378.08587646484375, val loss 381.9285888671875
iter 50, train loss 375.1792907714844, val loss 379.568115234375
iter 60, train loss 373.1734313964844, val loss 378.87127685546875
iter 70, train loss 371.40997314453125, val loss 376.888671875
iter 80, train loss 371.0242004394531, val loss 377.47479248046875
iter 90, train loss 371.044677734375, val loss 377.2026672363281
best loss 317.6841735839844
not here
quantized in 33.015517234802246 seconds
36414 MiB free out of 48676 MiB total
19 self_attn.v_proj
Pruning ...
256
iter 0, train loss 167.77926635742188, val loss 173.48312377929688
iter 10, train loss 168.23507690429688, val loss 175.4154052734375
iter 20, train loss 167.87973022460938, val loss 175.6802978515625
iter 30, train loss 168.0341796875, val loss 175.20193481445312
iter 40, train loss 167.56019592285156, val loss 175.0783233642578
iter 50, train loss 167.10446166992188, val loss 175.12527465820312
iter 60, train loss 167.2994384765625, val loss 174.99728393554688
iter 70, train loss 166.9879150390625, val loss 175.14529418945312
iter 80, train loss 166.84133911132812, val loss 174.67910766601562
iter 90, train loss 166.84251403808594, val loss 174.83070373535156
best loss 173.48312377929688
not here
quantized in 31.14857029914856 seconds
36404 MiB free out of 48676 MiB total
19 self_attn.o_proj
Pruning ...
256
iter 0, train loss 9.67906665802002, val loss 8.944915771484375
iter 10, train loss 8.951316833496094, val loss 9.91270923614502
iter 20, train loss 8.80890941619873, val loss 9.758955955505371
iter 30, train loss 8.734066009521484, val loss 9.589749336242676
iter 40, train loss 8.665007591247559, val loss 9.994994163513184
iter 50, train loss 8.603119850158691, val loss 9.741277694702148
iter 60, train loss 8.547990798950195, val loss 9.791462898254395
iter 70, train loss 8.477547645568848, val loss 9.918744087219238
iter 80, train loss 8.452750205993652, val loss 9.907695770263672
iter 90, train loss 8.42619514465332, val loss 9.819079399108887
best loss 8.944915771484375
not here
quantized in 31.58068609237671 seconds
36372 MiB free out of 48676 MiB total
19 mlp.gate_proj
Pruning ...
256
iter 0, train loss 371.08966064453125, val loss 393.8509521484375
iter 10, train loss 384.78594970703125, val loss 432.2920227050781
iter 20, train loss 383.04583740234375, val loss 436.9397277832031
iter 30, train loss 382.95233154296875, val loss 435.855224609375
iter 40, train loss 381.032958984375, val loss 433.26995849609375
iter 50, train loss 379.9627380371094, val loss 430.1533508300781
iter 60, train loss 379.54510498046875, val loss 425.95947265625
iter 70, train loss 379.09033203125, val loss 433.19390869140625
iter 80, train loss 378.78240966796875, val loss 435.05157470703125
iter 90, train loss 378.89154052734375, val loss 439.8068542480469
best loss 393.8509521484375
not here
quantized in 84.88891363143921 seconds
36070 MiB free out of 48676 MiB total
19 mlp.up_proj
Pruning ...
256
iter 0, train loss 320.15460205078125, val loss 349.59869384765625
iter 10, train loss 320.1457214355469, val loss 360.62420654296875
iter 20, train loss 319.9988098144531, val loss 359.2315979003906
iter 30, train loss 319.9659729003906, val loss 363.6759948730469
iter 40, train loss 320.2843322753906, val loss 368.0338439941406
iter 50, train loss 319.9686279296875, val loss 369.32598876953125
iter 60, train loss 319.80322265625, val loss 372.57373046875
iter 70, train loss 319.7652587890625, val loss 368.67132568359375
iter 80, train loss 319.7889404296875, val loss 371.90362548828125
iter 90, train loss 319.9268493652344, val loss 369.361328125
best loss 349.59869384765625
not here
quantized in 84.54127073287964 seconds
35876 MiB free out of 48676 MiB total
19 mlp.down_proj
Pruning ...
256
iter 0, train loss 11.461468696594238, val loss 7.850292682647705
iter 10, train loss 11.471158981323242, val loss 8.23729133605957
iter 20, train loss 11.439779281616211, val loss 8.102547645568848
iter 30, train loss 11.405755996704102, val loss 8.031142234802246
iter 40, train loss 11.410477638244629, val loss 8.119046211242676
iter 50, train loss 11.40660285949707, val loss 8.053047180175781
iter 60, train loss 11.401225090026855, val loss 8.037467956542969
iter 70, train loss 11.388189315795898, val loss 7.921730041503906
iter 80, train loss 11.382750511169434, val loss 7.9809250831604
iter 90, train loss 11.382658004760742, val loss 8.017278671264648
best loss 7.850292682647705
not here
quantized in 91.92396569252014 seconds
35682 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35682 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31586 MiB free out of 48676 MiB total
after cast to cpu
37852 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
20 self_attn.q_proj
Pruning ...
256
iter 0, train loss 368.457763671875, val loss 312.382568359375
iter 10, train loss 385.14105224609375, val loss 387.6656188964844
iter 20, train loss 374.2269287109375, val loss 377.5635986328125
iter 30, train loss 362.7207946777344, val loss 367.59423828125
iter 40, train loss 358.0857238769531, val loss 364.4271240234375
iter 50, train loss 356.8690185546875, val loss 364.08050537109375
iter 60, train loss 355.9173278808594, val loss 362.3531188964844
iter 70, train loss 354.8381652832031, val loss 361.18963623046875
iter 80, train loss 354.65765380859375, val loss 359.7642822265625
iter 90, train loss 353.89337158203125, val loss 360.47137451171875
best loss 312.382568359375
not here
quantized in 33.956634521484375 seconds
36424 MiB free out of 48676 MiB total
20 self_attn.k_proj
Pruning ...
256
iter 0, train loss 386.5956115722656, val loss 328.3379211425781
iter 10, train loss 409.1703186035156, val loss 408.3810119628906
iter 20, train loss 398.559814453125, val loss 403.04705810546875
iter 30, train loss 391.5489807128906, val loss 396.89776611328125
iter 40, train loss 385.8789367675781, val loss 390.5787658691406
iter 50, train loss 381.9625549316406, val loss 385.6183776855469
iter 60, train loss 380.4324035644531, val loss 384.7129211425781
iter 70, train loss 379.4346923828125, val loss 384.66259765625
iter 80, train loss 378.02520751953125, val loss 384.04925537109375
iter 90, train loss 377.56402587890625, val loss 383.0335998535156
best loss 328.3379211425781
not here
quantized in 32.662875175476074 seconds
36414 MiB free out of 48676 MiB total
20 self_attn.v_proj
Pruning ...
256
iter 0, train loss 173.86489868164062, val loss 179.29103088378906
iter 10, train loss 173.64561462402344, val loss 181.2794189453125
iter 20, train loss 173.6787567138672, val loss 181.285400390625
iter 30, train loss 173.55880737304688, val loss 181.72262573242188
iter 40, train loss 173.0020751953125, val loss 181.7218017578125
iter 50, train loss 172.69085693359375, val loss 181.28521728515625
iter 60, train loss 172.66629028320312, val loss 181.52719116210938
iter 70, train loss 172.51251220703125, val loss 181.70059204101562
iter 80, train loss 172.2673797607422, val loss 181.49990844726562
iter 90, train loss 172.3156280517578, val loss 181.70846557617188
best loss 179.29103088378906
not here
quantized in 30.982556343078613 seconds
36404 MiB free out of 48676 MiB total
20 self_attn.o_proj
Pruning ...
256
iter 0, train loss 13.726323127746582, val loss 10.817793846130371
iter 10, train loss 12.229597091674805, val loss 10.883091926574707
iter 20, train loss 11.707390785217285, val loss 11.579187393188477
iter 30, train loss 11.204734802246094, val loss 11.219966888427734
iter 40, train loss 10.960246086120605, val loss 11.215692520141602
iter 50, train loss 10.845345497131348, val loss 10.831833839416504
iter 60, train loss 10.728567123413086, val loss 10.737508773803711
iter 70, train loss 10.658231735229492, val loss 10.658357620239258
iter 80, train loss 10.594762802124023, val loss 10.831167221069336
iter 90, train loss 10.556722640991211, val loss 10.684036254882812
best loss 10.293413162231445
not here
quantized in 31.845234870910645 seconds
36372 MiB free out of 48676 MiB total
20 mlp.gate_proj
Pruning ...
256
iter 0, train loss 405.3573303222656, val loss 435.91650390625
iter 10, train loss 419.8619079589844, val loss 469.2240295410156
iter 20, train loss 418.8179931640625, val loss 476.53582763671875
iter 30, train loss 416.64007568359375, val loss 470.6877136230469
iter 40, train loss 415.7664794921875, val loss 469.2255554199219
iter 50, train loss 416.1446228027344, val loss 465.5445861816406
iter 60, train loss 415.77899169921875, val loss 466.5921630859375
iter 70, train loss 415.7173156738281, val loss 470.00677490234375
iter 80, train loss 415.49871826171875, val loss 472.728759765625
iter 90, train loss 415.3627014160156, val loss 467.994384765625
best loss 435.91650390625
not here
quantized in 84.7430431842804 seconds
36070 MiB free out of 48676 MiB total
20 mlp.up_proj
Pruning ...
256
iter 0, train loss 345.2950439453125, val loss 378.6033935546875
iter 10, train loss 346.01422119140625, val loss 393.0472717285156
iter 20, train loss 346.5552978515625, val loss 394.0257568359375
iter 30, train loss 346.71661376953125, val loss 395.82672119140625
iter 40, train loss 346.4787902832031, val loss 396.0423889160156
iter 50, train loss 347.0351257324219, val loss 400.349609375
iter 60, train loss 346.857421875, val loss 398.53582763671875
iter 70, train loss 346.9710388183594, val loss 397.56256103515625
iter 80, train loss 346.78424072265625, val loss 399.036376953125
iter 90, train loss 346.7817687988281, val loss 402.6204833984375
best loss 378.6033935546875
not here
quantized in 83.82648062705994 seconds
35876 MiB free out of 48676 MiB total
20 mlp.down_proj
Pruning ...
256
iter 0, train loss 14.591405868530273, val loss 10.679183959960938
iter 10, train loss 14.537454605102539, val loss 10.589509963989258
iter 20, train loss 14.47396469116211, val loss 10.548898696899414
iter 30, train loss 14.434752464294434, val loss 10.628376007080078
iter 40, train loss 14.42612361907959, val loss 10.656974792480469
iter 50, train loss 14.41097354888916, val loss 10.367504119873047
iter 60, train loss 14.398950576782227, val loss 10.323565483093262
iter 70, train loss 14.388548851013184, val loss 10.581785202026367
iter 80, train loss 14.379060745239258, val loss 10.587099075317383
iter 90, train loss 14.380782127380371, val loss 10.725723266601562
best loss 10.256057739257812
not here
quantized in 91.1999020576477 seconds
35682 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35682 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31586 MiB free out of 48676 MiB total
after cast to cpu
37852 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
21 self_attn.q_proj
Pruning ...
256
iter 0, train loss 381.349609375, val loss 337.84515380859375
iter 10, train loss 402.2630310058594, val loss 407.75054931640625
iter 20, train loss 391.60894775390625, val loss 397.66375732421875
iter 30, train loss 382.38653564453125, val loss 391.50970458984375
iter 40, train loss 377.7662353515625, val loss 386.82110595703125
iter 50, train loss 375.69012451171875, val loss 385.5372314453125
iter 60, train loss 373.6707763671875, val loss 382.86126708984375
iter 70, train loss 372.00506591796875, val loss 381.6700134277344
iter 80, train loss 371.36932373046875, val loss 381.6186218261719
iter 90, train loss 371.78717041015625, val loss 381.0122375488281
best loss 337.84515380859375
not here
quantized in 33.88596987724304 seconds
36424 MiB free out of 48676 MiB total
21 self_attn.k_proj
Pruning ...
256
iter 0, train loss 397.4824523925781, val loss 347.8321838378906
iter 10, train loss 421.73907470703125, val loss 420.45147705078125
iter 20, train loss 404.43170166015625, val loss 407.9925537109375
iter 30, train loss 392.63958740234375, val loss 398.54315185546875
iter 40, train loss 390.23583984375, val loss 398.41650390625
iter 50, train loss 388.9669189453125, val loss 397.5513916015625
iter 60, train loss 387.3018798828125, val loss 395.52386474609375
iter 70, train loss 386.7759704589844, val loss 395.83038330078125
iter 80, train loss 386.29034423828125, val loss 394.62811279296875
iter 90, train loss 386.2301025390625, val loss 394.46673583984375
best loss 347.8321838378906
not here
quantized in 32.44936394691467 seconds
36414 MiB free out of 48676 MiB total
21 self_attn.v_proj
Pruning ...
256
iter 0, train loss 207.97265625, val loss 216.7487335205078
iter 10, train loss 207.82191467285156, val loss 218.33108520507812
iter 20, train loss 207.76593017578125, val loss 218.35678100585938
iter 30, train loss 207.5268096923828, val loss 218.24729919433594
iter 40, train loss 207.6403350830078, val loss 218.04971313476562
iter 50, train loss 207.46868896484375, val loss 218.0841064453125
iter 60, train loss 207.4105224609375, val loss 217.90304565429688
iter 70, train loss 207.2722625732422, val loss 218.35931396484375
iter 80, train loss 207.2604217529297, val loss 218.3330078125
iter 90, train loss 207.26553344726562, val loss 218.1375732421875
best loss 216.7487335205078
not here
quantized in 32.14443397521973 seconds
36404 MiB free out of 48676 MiB total
21 self_attn.o_proj
Pruning ...
256
iter 0, train loss 10.482950210571289, val loss 12.770357131958008
iter 10, train loss 9.746792793273926, val loss 13.550859451293945
iter 20, train loss 9.522092819213867, val loss 13.690201759338379
iter 30, train loss 9.325193405151367, val loss 13.801612854003906
iter 40, train loss 9.182048797607422, val loss 13.848962783813477
iter 50, train loss 9.105796813964844, val loss 13.538724899291992
iter 60, train loss 9.073091506958008, val loss 13.052505493164062
iter 70, train loss 9.036858558654785, val loss 13.295955657958984
iter 80, train loss 8.99818229675293, val loss 13.184202194213867
iter 90, train loss 8.98643970489502, val loss 13.41402816772461
best loss 12.770357131958008
not here
quantized in 31.942668914794922 seconds
36372 MiB free out of 48676 MiB total
21 mlp.gate_proj
Pruning ...
256
iter 0, train loss 435.3280334472656, val loss 465.7121276855469
iter 10, train loss 453.25933837890625, val loss 514.9085693359375
iter 20, train loss 452.1707458496094, val loss 511.3217468261719
iter 30, train loss 450.76861572265625, val loss 500.9532470703125
iter 40, train loss 450.849365234375, val loss 493.7542724609375
iter 50, train loss 450.0766296386719, val loss 495.7209167480469
iter 60, train loss 450.1102600097656, val loss 498.146728515625
iter 70, train loss 449.55035400390625, val loss 501.5267333984375
iter 80, train loss 449.7550354003906, val loss 511.0133056640625
iter 90, train loss 449.7166442871094, val loss 508.8708190917969
best loss 465.7121276855469
not here
quantized in 85.3533616065979 seconds
36070 MiB free out of 48676 MiB total
21 mlp.up_proj
Pruning ...
256
iter 0, train loss 366.67626953125, val loss 398.0425720214844
iter 10, train loss 366.6910400390625, val loss 410.0013427734375
iter 20, train loss 367.466552734375, val loss 413.4457702636719
iter 30, train loss 368.0074768066406, val loss 416.3944091796875
iter 40, train loss 368.1370849609375, val loss 423.441650390625
iter 50, train loss 368.2629699707031, val loss 426.68359375
iter 60, train loss 367.9185791015625, val loss 425.1851501464844
iter 70, train loss 368.1302795410156, val loss 423.28662109375
iter 80, train loss 368.48675537109375, val loss 421.2187805175781
iter 90, train loss 368.71337890625, val loss 424.9323425292969
best loss 398.0425720214844
not here
quantized in 84.74405097961426 seconds
35876 MiB free out of 48676 MiB total
21 mlp.down_proj
Pruning ...
256
iter 0, train loss 15.276152610778809, val loss 13.401995658874512
iter 10, train loss 15.219198226928711, val loss 14.110674858093262
iter 20, train loss 15.08993148803711, val loss 13.825565338134766
iter 30, train loss 15.04709243774414, val loss 13.76791000366211
iter 40, train loss 14.996856689453125, val loss 13.9537353515625
iter 50, train loss 14.971552848815918, val loss 13.37997817993164
iter 60, train loss 14.951334953308105, val loss 13.487446784973145
iter 70, train loss 14.93696403503418, val loss 13.689004898071289
iter 80, train loss 14.92232608795166, val loss 13.698954582214355
iter 90, train loss 14.920025825500488, val loss 13.781925201416016
best loss 13.37997817993164
not here
quantized in 91.54249024391174 seconds
35682 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35682 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31586 MiB free out of 48676 MiB total
after cast to cpu
37852 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
22 self_attn.q_proj
Pruning ...
256
iter 0, train loss 409.29779052734375, val loss 364.2857666015625
iter 10, train loss 439.4529113769531, val loss 438.977783203125
iter 20, train loss 417.010986328125, val loss 422.3802185058594
iter 30, train loss 407.94488525390625, val loss 415.294189453125
iter 40, train loss 404.33013916015625, val loss 411.2596435546875
iter 50, train loss 402.6277160644531, val loss 411.6893310546875
iter 60, train loss 399.360595703125, val loss 410.5714111328125
iter 70, train loss 399.4720458984375, val loss 409.3248596191406
iter 80, train loss 397.9691162109375, val loss 409.45648193359375
iter 90, train loss 397.653564453125, val loss 407.65838623046875
best loss 364.2857666015625
not here
quantized in 33.81720757484436 seconds
36424 MiB free out of 48676 MiB total
22 self_attn.k_proj
Pruning ...
256
iter 0, train loss 433.3573303222656, val loss 377.13873291015625
iter 10, train loss 447.85882568359375, val loss 451.0249328613281
iter 20, train loss 439.39202880859375, val loss 445.05816650390625
iter 30, train loss 433.91534423828125, val loss 438.86883544921875
iter 40, train loss 426.3095397949219, val loss 433.83746337890625
iter 50, train loss 427.32818603515625, val loss 434.21466064453125
iter 60, train loss 424.1039733886719, val loss 432.20843505859375
iter 70, train loss 423.1016845703125, val loss 430.55548095703125
iter 80, train loss 422.57171630859375, val loss 431.7493896484375
iter 90, train loss 422.23577880859375, val loss 430.8285217285156
best loss 377.13873291015625
not here
quantized in 32.34256982803345 seconds
36414 MiB free out of 48676 MiB total
22 self_attn.v_proj
Pruning ...
256
iter 0, train loss 216.42201232910156, val loss 223.28646850585938
iter 10, train loss 216.8427276611328, val loss 225.17010498046875
iter 20, train loss 215.7415313720703, val loss 226.21678161621094
iter 30, train loss 216.07315063476562, val loss 226.2744598388672
iter 40, train loss 215.91314697265625, val loss 226.3924102783203
iter 50, train loss 215.92398071289062, val loss 225.3284912109375
iter 60, train loss 215.98956298828125, val loss 225.4908905029297
iter 70, train loss 215.92857360839844, val loss 225.3129119873047
iter 80, train loss 215.68360900878906, val loss 225.43617248535156
iter 90, train loss 215.69876098632812, val loss 225.685546875
best loss 223.28646850585938
not here
quantized in 31.113084316253662 seconds
36404 MiB free out of 48676 MiB total
22 self_attn.o_proj
Pruning ...
256
iter 0, train loss 61.000022888183594, val loss 16.333702087402344
iter 10, train loss 42.537445068359375, val loss 17.896196365356445
iter 20, train loss 34.91828918457031, val loss 19.118610382080078
iter 30, train loss 28.24018096923828, val loss 17.84442138671875
iter 40, train loss 26.750038146972656, val loss 18.017492294311523
iter 50, train loss 24.673477172851562, val loss 16.88043975830078
iter 60, train loss 23.101375579833984, val loss 16.622350692749023
iter 70, train loss 21.659954071044922, val loss 15.776321411132812
iter 80, train loss 20.76198959350586, val loss 15.8460111618042
iter 90, train loss 20.47374725341797, val loss 15.521554946899414
best loss 15.386049270629883
not here
quantized in 32.49789547920227 seconds
36404 MiB free out of 48676 MiB total
22 mlp.gate_proj
Pruning ...
256
iter 0, train loss 450.1153564453125, val loss 497.05316162109375
iter 10, train loss 465.15814208984375, val loss 529.8719482421875
iter 20, train loss 463.680419921875, val loss 529.1188354492188
iter 30, train loss 464.62335205078125, val loss 537.23583984375
iter 40, train loss 464.32305908203125, val loss 531.14892578125
iter 50, train loss 464.03509521484375, val loss 535.6971435546875
iter 60, train loss 462.9577941894531, val loss 534.7847290039062
iter 70, train loss 462.61322021484375, val loss 539.5465087890625
iter 80, train loss 462.0911865234375, val loss 535.345458984375
iter 90, train loss 461.6283874511719, val loss 536.9981079101562
best loss 492.6098937988281
not here
quantized in 85.26864767074585 seconds
36102 MiB free out of 48676 MiB total
22 mlp.up_proj
Pruning ...
256
iter 0, train loss 374.63079833984375, val loss 423.10980224609375
iter 10, train loss 375.4461669921875, val loss 435.56427001953125
iter 20, train loss 375.7636413574219, val loss 432.3747253417969
iter 30, train loss 376.3148193359375, val loss 431.0508728027344
iter 40, train loss 376.74627685546875, val loss 436.8746337890625
iter 50, train loss 377.0077819824219, val loss 438.40484619140625
iter 60, train loss 377.1636962890625, val loss 436.11260986328125
iter 70, train loss 376.74090576171875, val loss 436.5706787109375
iter 80, train loss 376.41473388671875, val loss 437.34539794921875
iter 90, train loss 376.7002258300781, val loss 435.7120666503906
best loss 422.3805847167969
not here
quantized in 83.9665093421936 seconds
35908 MiB free out of 48676 MiB total
22 mlp.down_proj
Pruning ...
256
iter 0, train loss 16.102336883544922, val loss 13.637723922729492
iter 10, train loss 16.14706039428711, val loss 13.41010856628418
iter 20, train loss 16.094345092773438, val loss 13.361371040344238
iter 30, train loss 16.061691284179688, val loss 13.540289878845215
iter 40, train loss 16.038436889648438, val loss 13.670845031738281
iter 50, train loss 16.027347564697266, val loss 13.664657592773438
iter 60, train loss 16.013660430908203, val loss 13.603379249572754
iter 70, train loss 16.008960723876953, val loss 13.676289558410645
iter 80, train loss 16.008710861206055, val loss 13.55038070678711
iter 90, train loss 16.007579803466797, val loss 13.687472343444824
best loss 13.251161575317383
not here
quantized in 91.35263895988464 seconds
35714 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35714 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31618 MiB free out of 48676 MiB total
after cast to cpu
37852 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
23 self_attn.q_proj
Pruning ...
256
iter 0, train loss 445.2655029296875, val loss 410.21429443359375
iter 10, train loss 458.8564147949219, val loss 469.0614929199219
iter 20, train loss 444.43701171875, val loss 453.72918701171875
iter 30, train loss 442.26434326171875, val loss 456.9544677734375
iter 40, train loss 441.653076171875, val loss 457.23162841796875
iter 50, train loss 439.0614929199219, val loss 456.1429443359375
iter 60, train loss 436.1520690917969, val loss 451.22454833984375
iter 70, train loss 433.11517333984375, val loss 449.3546142578125
iter 80, train loss 432.2842712402344, val loss 448.9397277832031
iter 90, train loss 432.1419982910156, val loss 448.02044677734375
best loss 410.21429443359375
not here
quantized in 33.47394609451294 seconds
36424 MiB free out of 48676 MiB total
23 self_attn.k_proj
Pruning ...
256
iter 0, train loss 459.40472412109375, val loss 422.1029052734375
iter 10, train loss 486.9570007324219, val loss 497.9712829589844
iter 20, train loss 468.7478332519531, val loss 482.50054931640625
iter 30, train loss 459.92633056640625, val loss 476.9134521484375
iter 40, train loss 452.5767822265625, val loss 470.44647216796875
iter 50, train loss 451.8365478515625, val loss 469.36376953125
iter 60, train loss 450.2640686035156, val loss 469.3887939453125
iter 70, train loss 449.42840576171875, val loss 468.00909423828125
iter 80, train loss 450.0187072753906, val loss 468.6358337402344
iter 90, train loss 450.839111328125, val loss 468.5889892578125
best loss 422.1029052734375
not here
quantized in 32.31295943260193 seconds
36414 MiB free out of 48676 MiB total
23 self_attn.v_proj
Pruning ...
256
iter 0, train loss 267.9870910644531, val loss 279.26123046875
iter 10, train loss 268.42340087890625, val loss 281.57940673828125
iter 20, train loss 267.70166015625, val loss 282.827880859375
iter 30, train loss 267.37957763671875, val loss 281.265380859375
iter 40, train loss 267.3890380859375, val loss 282.25628662109375
iter 50, train loss 267.5638122558594, val loss 283.2537841796875
iter 60, train loss 267.74993896484375, val loss 281.60894775390625
iter 70, train loss 267.2181091308594, val loss 281.38714599609375
iter 80, train loss 267.0597229003906, val loss 281.529296875
iter 90, train loss 266.8554382324219, val loss 281.6885986328125
best loss 279.26123046875
not here
quantized in 30.96962285041809 seconds
36404 MiB free out of 48676 MiB total
23 self_attn.o_proj
Pruning ...
256
iter 0, train loss 12.14761734008789, val loss 17.815093994140625
iter 10, train loss 12.05180549621582, val loss 19.160160064697266
iter 20, train loss 11.95270824432373, val loss 19.236867904663086
iter 30, train loss 11.90736198425293, val loss 18.97764778137207
iter 40, train loss 11.885086059570312, val loss 19.198442459106445
iter 50, train loss 11.802783966064453, val loss 18.781347274780273
iter 60, train loss 11.78506088256836, val loss 18.830364227294922
iter 70, train loss 11.760025978088379, val loss 19.30776596069336
iter 80, train loss 11.74702262878418, val loss 19.00640106201172
iter 90, train loss 11.73792552947998, val loss 18.97023582458496
best loss 17.79349136352539
not here
quantized in 31.417927742004395 seconds
36372 MiB free out of 48676 MiB total
23 mlp.gate_proj
Pruning ...
256
iter 0, train loss 502.79742431640625, val loss 554.2197265625
iter 10, train loss 515.8569946289062, val loss 595.082763671875
iter 20, train loss 516.3021240234375, val loss 589.6502685546875
iter 30, train loss 514.7239990234375, val loss 592.2138671875
iter 40, train loss 514.4456787109375, val loss 583.8992309570312
iter 50, train loss 514.0432739257812, val loss 579.9962158203125
iter 60, train loss 514.3311767578125, val loss 582.8220825195312
iter 70, train loss 513.8739013671875, val loss 585.384765625
iter 80, train loss 513.4155883789062, val loss 590.5735473632812
iter 90, train loss 512.9371337890625, val loss 589.8336181640625
best loss 554.2197265625
not here
quantized in 83.69896149635315 seconds
36070 MiB free out of 48676 MiB total
23 mlp.up_proj
Pruning ...
256
iter 0, train loss 420.2345275878906, val loss 472.4890441894531
iter 10, train loss 420.28314208984375, val loss 483.7323303222656
iter 20, train loss 420.34490966796875, val loss 492.0269775390625
iter 30, train loss 420.8429870605469, val loss 496.93878173828125
iter 40, train loss 421.118896484375, val loss 496.0056457519531
iter 50, train loss 421.1260070800781, val loss 491.1809387207031
iter 60, train loss 420.95404052734375, val loss 489.5701904296875
iter 70, train loss 420.9072570800781, val loss 486.8948974609375
iter 80, train loss 421.16973876953125, val loss 486.6776428222656
iter 90, train loss 421.0310974121094, val loss 490.223876953125
best loss 472.4890441894531
not here
quantized in 83.13077402114868 seconds
35876 MiB free out of 48676 MiB total
23 mlp.down_proj
Pruning ...
256
iter 0, train loss 18.691835403442383, val loss 15.918750762939453
iter 10, train loss 18.715900421142578, val loss 16.035358428955078
iter 20, train loss 18.658124923706055, val loss 16.137954711914062
iter 30, train loss 18.62014389038086, val loss 16.192289352416992
iter 40, train loss 18.61280632019043, val loss 15.963918685913086
iter 50, train loss 18.599884033203125, val loss 15.932782173156738
iter 60, train loss 18.58226776123047, val loss 15.96364688873291
iter 70, train loss 18.582700729370117, val loss 15.840964317321777
iter 80, train loss 18.57745933532715, val loss 16.03961181640625
iter 90, train loss 18.5793399810791, val loss 16.189010620117188
best loss 15.65359878540039
not here
quantized in 106.99801015853882 seconds
35682 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35682 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31586 MiB free out of 48676 MiB total
after cast to cpu
37852 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
24 self_attn.q_proj
Pruning ...
256
iter 0, train loss 415.3070068359375, val loss 372.1924133300781
iter 10, train loss 427.0030517578125, val loss 434.5029296875
iter 20, train loss 411.4034118652344, val loss 419.2734680175781
iter 30, train loss 412.1007080078125, val loss 419.8218994140625
iter 40, train loss 411.25006103515625, val loss 421.081787109375
iter 50, train loss 408.6033935546875, val loss 417.14056396484375
iter 60, train loss 406.29754638671875, val loss 416.5782165527344
iter 70, train loss 404.4645690917969, val loss 415.8621520996094
iter 80, train loss 404.1162109375, val loss 414.84912109375
iter 90, train loss 403.25689697265625, val loss 414.2774658203125
best loss 372.1924133300781
not here
quantized in 35.50259327888489 seconds
36424 MiB free out of 48676 MiB total
24 self_attn.k_proj
Pruning ...
256
iter 0, train loss 425.572265625, val loss 382.31658935546875
iter 10, train loss 449.03375244140625, val loss 454.880615234375
iter 20, train loss 433.2528991699219, val loss 442.6466064453125
iter 30, train loss 426.0659484863281, val loss 437.7374267578125
iter 40, train loss 421.7027587890625, val loss 434.541015625
iter 50, train loss 420.1659851074219, val loss 434.93603515625
iter 60, train loss 419.0933837890625, val loss 433.6301574707031
iter 70, train loss 418.7632141113281, val loss 433.67413330078125
iter 80, train loss 417.72235107421875, val loss 432.8880615234375
iter 90, train loss 417.5168151855469, val loss 431.5792236328125
best loss 382.31658935546875
not here
quantized in 32.33635854721069 seconds
36414 MiB free out of 48676 MiB total
24 self_attn.v_proj
Pruning ...
256
iter 0, train loss 255.9343719482422, val loss 267.44622802734375
iter 10, train loss 255.51043701171875, val loss 269.8365478515625
iter 20, train loss 255.0675811767578, val loss 269.4375
iter 30, train loss 254.81307983398438, val loss 269.03460693359375
iter 40, train loss 254.55404663085938, val loss 268.5032043457031
iter 50, train loss 254.22439575195312, val loss 269.3012390136719
iter 60, train loss 254.52597045898438, val loss 269.686767578125
iter 70, train loss 254.68093872070312, val loss 269.3228759765625
iter 80, train loss 254.54498291015625, val loss 269.2818908691406
iter 90, train loss 254.60507202148438, val loss 269.3970947265625
best loss 267.44622802734375
not here
quantized in 38.95788049697876 seconds
36404 MiB free out of 48676 MiB total
24 self_attn.o_proj
Pruning ...
256
iter 0, train loss 30.495267868041992, val loss 16.948762893676758
iter 10, train loss 24.02252960205078, val loss 19.423843383789062
iter 20, train loss 20.957622528076172, val loss 20.18695068359375
iter 30, train loss 19.065261840820312, val loss 18.917896270751953
iter 40, train loss 18.10541343688965, val loss 19.055681228637695
iter 50, train loss 17.65377426147461, val loss 19.16238784790039
iter 60, train loss 17.447843551635742, val loss 18.998811721801758
iter 70, train loss 17.359830856323242, val loss 18.900991439819336
iter 80, train loss 17.162181854248047, val loss 18.8231143951416
iter 90, train loss 17.043251037597656, val loss 18.580507278442383
best loss 16.948762893676758
not here
quantized in 50.08744525909424 seconds
36372 MiB free out of 48676 MiB total
24 mlp.gate_proj
Pruning ...
256
iter 0, train loss 525.41162109375, val loss 578.5712280273438
iter 10, train loss 540.3883056640625, val loss 624.594970703125
iter 20, train loss 540.0336303710938, val loss 618.0626831054688
iter 30, train loss 539.787841796875, val loss 619.2955322265625
iter 40, train loss 539.4142456054688, val loss 627.48779296875
iter 50, train loss 538.4189453125, val loss 619.5368041992188
iter 60, train loss 538.3162841796875, val loss 620.8762817382812
iter 70, train loss 538.6396484375, val loss 626.2920532226562
iter 80, train loss 538.95361328125, val loss 624.2803344726562
iter 90, train loss 538.59521484375, val loss 621.8135986328125
best loss 578.5712280273438
not here
quantized in 97.38321566581726 seconds
36070 MiB free out of 48676 MiB total
24 mlp.up_proj
Pruning ...
256
iter 0, train loss 440.789794921875, val loss 515.141845703125
iter 10, train loss 441.1181945800781, val loss 525.9815673828125
iter 20, train loss 441.5902099609375, val loss 526.7591552734375
iter 30, train loss 442.0440673828125, val loss 526.901123046875
iter 40, train loss 442.5774841308594, val loss 521.4630126953125
iter 50, train loss 442.7245788574219, val loss 521.9619140625
iter 60, train loss 442.9566650390625, val loss 522.7777099609375
iter 70, train loss 443.02099609375, val loss 524.6416015625
iter 80, train loss 442.6774597167969, val loss 517.9552001953125
iter 90, train loss 442.728271484375, val loss 521.1796875
best loss 515.141845703125
not here
quantized in 92.62084698677063 seconds
35876 MiB free out of 48676 MiB total
24 mlp.down_proj
Pruning ...
256
iter 0, train loss 19.664216995239258, val loss 19.927936553955078
iter 10, train loss 19.672592163085938, val loss 19.401504516601562
iter 20, train loss 19.608936309814453, val loss 20.005577087402344
iter 30, train loss 19.579837799072266, val loss 19.74545669555664
iter 40, train loss 19.57431983947754, val loss 20.088184356689453
iter 50, train loss 19.662324905395508, val loss 20.284408569335938
iter 60, train loss 19.65380859375, val loss 20.150733947753906
iter 70, train loss 19.66089630126953, val loss 20.24957847595215
iter 80, train loss 19.658851623535156, val loss 20.159507751464844
iter 90, train loss 19.64778709411621, val loss 20.11812973022461
best loss 19.401504516601562
not here
quantized in 100.3507571220398 seconds
35682 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35682 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31586 MiB free out of 48676 MiB total
after cast to cpu
37852 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
25 self_attn.q_proj
Pruning ...
256
iter 0, train loss 484.0798645019531, val loss 448.5653076171875
iter 10, train loss 496.22235107421875, val loss 511.1630554199219
iter 20, train loss 475.33355712890625, val loss 489.0318908691406
iter 30, train loss 479.13177490234375, val loss 496.3432922363281
iter 40, train loss 477.8240966796875, val loss 497.12152099609375
iter 50, train loss 476.58740234375, val loss 495.9970397949219
iter 60, train loss 475.044189453125, val loss 493.03582763671875
iter 70, train loss 472.9578552246094, val loss 492.47509765625
iter 80, train loss 472.09356689453125, val loss 491.654296875
iter 90, train loss 471.39007568359375, val loss 491.12188720703125
best loss 448.5653076171875
not here
quantized in 37.23818063735962 seconds
36424 MiB free out of 48676 MiB total
25 self_attn.k_proj
Pruning ...
256
iter 0, train loss 498.9884948730469, val loss 460.091064453125
iter 10, train loss 526.8224487304688, val loss 535.3652954101562
iter 20, train loss 501.28802490234375, val loss 516.238525390625
iter 30, train loss 493.68780517578125, val loss 510.7879333496094
iter 40, train loss 490.52471923828125, val loss 509.7457275390625
iter 50, train loss 489.0174255371094, val loss 507.5199279785156
iter 60, train loss 488.7334899902344, val loss 507.5743713378906
iter 70, train loss 488.8780517578125, val loss 506.02264404296875
iter 80, train loss 488.1944580078125, val loss 506.000732421875
iter 90, train loss 487.251220703125, val loss 505.7420654296875
best loss 460.091064453125
not here
quantized in 36.00552749633789 seconds
36414 MiB free out of 48676 MiB total
25 self_attn.v_proj
Pruning ...
256
iter 0, train loss 326.497314453125, val loss 342.6864318847656
iter 10, train loss 325.7978820800781, val loss 345.10003662109375
iter 20, train loss 325.2942810058594, val loss 345.7758483886719
iter 30, train loss 325.388427734375, val loss 346.41259765625
iter 40, train loss 325.3763122558594, val loss 346.187744140625
iter 50, train loss 324.8072509765625, val loss 347.1128234863281
iter 60, train loss 324.4103088378906, val loss 347.1131591796875
iter 70, train loss 324.11370849609375, val loss 346.59429931640625
iter 80, train loss 323.6846618652344, val loss 346.1639099121094
iter 90, train loss 323.6354064941406, val loss 346.6499328613281
best loss 342.6864318847656
not here
quantized in 35.0540554523468 seconds
36404 MiB free out of 48676 MiB total
25 self_attn.o_proj
Pruning ...
256
iter 0, train loss 20.007732391357422, val loss 25.4771728515625
iter 10, train loss 20.441438674926758, val loss 27.65390968322754
iter 20, train loss 20.17089080810547, val loss 27.60747718811035
iter 30, train loss 20.196603775024414, val loss 26.91476821899414
iter 40, train loss 20.152956008911133, val loss 26.633586883544922
iter 50, train loss 20.0638370513916, val loss 26.821935653686523
iter 60, train loss 19.912269592285156, val loss 26.323583602905273
iter 70, train loss 19.766124725341797, val loss 26.447025299072266
iter 80, train loss 19.716381072998047, val loss 26.215442657470703
iter 90, train loss 19.67319107055664, val loss 26.666067123413086
best loss 25.4771728515625
not here
quantized in 35.263269901275635 seconds
36372 MiB free out of 48676 MiB total
25 mlp.gate_proj
Pruning ...
256
iter 0, train loss 578.789794921875, val loss 618.1253662109375
iter 10, train loss 594.249267578125, val loss 669.3118896484375
iter 20, train loss 595.2028198242188, val loss 673.6812744140625
iter 30, train loss 594.4664306640625, val loss 680.811767578125
iter 40, train loss 592.693115234375, val loss 673.570556640625
iter 50, train loss 591.15478515625, val loss 667.225830078125
iter 60, train loss 590.831787109375, val loss 663.0297241210938
iter 70, train loss 590.6378173828125, val loss 662.8599243164062
iter 80, train loss 590.5885620117188, val loss 664.760498046875
iter 90, train loss 589.644775390625, val loss 668.0556640625
best loss 618.1253662109375
not here
quantized in 94.42585349082947 seconds
36070 MiB free out of 48676 MiB total
25 mlp.up_proj
Pruning ...
256
iter 0, train loss 486.02691650390625, val loss 544.6771850585938
iter 10, train loss 486.1192626953125, val loss 541.004150390625
iter 20, train loss 486.4635009765625, val loss 545.4448852539062
iter 30, train loss 486.7088623046875, val loss 541.2198486328125
iter 40, train loss 487.580078125, val loss 540.9942626953125
iter 50, train loss 487.084716796875, val loss 550.8689575195312
iter 60, train loss 486.73504638671875, val loss 547.6983032226562
iter 70, train loss 485.920654296875, val loss 551.9347534179688
iter 80, train loss 485.78656005859375, val loss 548.315185546875
iter 90, train loss 485.7037353515625, val loss 550.6648559570312
best loss 538.05859375
not here
quantized in 94.15064573287964 seconds
35876 MiB free out of 48676 MiB total
25 mlp.down_proj
Pruning ...
256
iter 0, train loss 21.491113662719727, val loss 20.576784133911133
iter 10, train loss 21.51759910583496, val loss 21.213787078857422
iter 20, train loss 21.463138580322266, val loss 20.98813247680664
iter 30, train loss 21.43091583251953, val loss 21.129159927368164
iter 40, train loss 21.420902252197266, val loss 20.95004653930664
iter 50, train loss 21.423316955566406, val loss 20.940067291259766
iter 60, train loss 21.42523193359375, val loss 21.382003784179688
iter 70, train loss 21.42542839050293, val loss 21.075979232788086
iter 80, train loss 21.400920867919922, val loss 21.178386688232422
iter 90, train loss 21.399169921875, val loss 21.02621078491211
best loss 20.51944351196289
not here
quantized in 101.4234266281128 seconds
35682 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35682 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31586 MiB free out of 48676 MiB total
after cast to cpu
37852 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
26 self_attn.q_proj
Pruning ...
256
iter 0, train loss 460.579345703125, val loss 409.5677490234375
iter 10, train loss 486.396728515625, val loss 486.6034851074219
iter 20, train loss 464.424072265625, val loss 467.25213623046875
iter 30, train loss 455.29388427734375, val loss 460.73040771484375
iter 40, train loss 452.5953369140625, val loss 457.1137390136719
iter 50, train loss 451.22125244140625, val loss 457.6160888671875
iter 60, train loss 448.695556640625, val loss 456.3153076171875
iter 70, train loss 447.03497314453125, val loss 455.8612060546875
iter 80, train loss 445.24200439453125, val loss 453.589599609375
iter 90, train loss 445.725341796875, val loss 453.1546630859375
best loss 409.5677490234375
not here
quantized in 36.874773025512695 seconds
36424 MiB free out of 48676 MiB total
26 self_attn.k_proj
Pruning ...
256
iter 0, train loss 484.2562255859375, val loss 423.005126953125
iter 10, train loss 510.117431640625, val loss 509.7005920410156
iter 20, train loss 490.76318359375, val loss 495.56951904296875
iter 30, train loss 484.0640869140625, val loss 490.1544494628906
iter 40, train loss 479.8199462890625, val loss 487.6575012207031
iter 50, train loss 475.2977600097656, val loss 482.4670715332031
iter 60, train loss 475.27362060546875, val loss 483.9667053222656
iter 70, train loss 473.7171630859375, val loss 482.93817138671875
iter 80, train loss 472.817626953125, val loss 481.2046203613281
iter 90, train loss 471.273193359375, val loss 479.9743347167969
best loss 423.005126953125
not here
quantized in 35.88085222244263 seconds
36414 MiB free out of 48676 MiB total
26 self_attn.v_proj
Pruning ...
256
iter 0, train loss 320.4081726074219, val loss 328.55145263671875
iter 10, train loss 321.095703125, val loss 331.11004638671875
iter 20, train loss 320.3522033691406, val loss 332.67193603515625
iter 30, train loss 320.3192443847656, val loss 332.20196533203125
iter 40, train loss 320.0196533203125, val loss 332.19677734375
iter 50, train loss 319.2823181152344, val loss 332.5084228515625
iter 60, train loss 319.2687683105469, val loss 332.65020751953125
iter 70, train loss 319.124267578125, val loss 332.62188720703125
iter 80, train loss 318.95697021484375, val loss 332.5120544433594
iter 90, train loss 318.8644104003906, val loss 332.20220947265625
best loss 328.55145263671875
not here
quantized in 34.68172335624695 seconds
36404 MiB free out of 48676 MiB total
26 self_attn.o_proj
Pruning ...
256
iter 0, train loss 41.36690139770508, val loss 27.421722412109375
iter 10, train loss 30.707775115966797, val loss 31.323280334472656
iter 20, train loss 26.09664535522461, val loss 29.46368408203125
iter 30, train loss 24.93830108642578, val loss 28.817296981811523
iter 40, train loss 23.580501556396484, val loss 29.044538497924805
iter 50, train loss 22.92136001586914, val loss 28.416297912597656
iter 60, train loss 22.427757263183594, val loss 28.532119750976562
iter 70, train loss 22.244482040405273, val loss 28.610305786132812
iter 80, train loss 22.14476776123047, val loss 28.88770866394043
iter 90, train loss 22.053985595703125, val loss 29.024761199951172
best loss 27.421722412109375
not here
quantized in 35.271692991256714 seconds
36372 MiB free out of 48676 MiB total
26 mlp.gate_proj
Pruning ...
256
iter 0, train loss 608.72216796875, val loss 655.86083984375
iter 10, train loss 633.137939453125, val loss 723.7966918945312
iter 20, train loss 636.293701171875, val loss 741.1070556640625
iter 30, train loss 634.8846435546875, val loss 734.1187744140625
iter 40, train loss 631.421630859375, val loss 725.9470825195312
iter 50, train loss 630.583984375, val loss 728.208251953125
iter 60, train loss 631.0203247070312, val loss 713.5546264648438
iter 70, train loss 631.043701171875, val loss 713.0531616210938
iter 80, train loss 630.661376953125, val loss 721.6776123046875
iter 90, train loss 630.5472412109375, val loss 719.2203979492188
best loss 655.86083984375
not here
quantized in 94.59400033950806 seconds
36070 MiB free out of 48676 MiB total
26 mlp.up_proj
Pruning ...
256
iter 0, train loss 512.7158813476562, val loss 575.4503784179688
iter 10, train loss 513.9490966796875, val loss 590.4012451171875
iter 20, train loss 514.6441040039062, val loss 592.3536987304688
iter 30, train loss 514.23583984375, val loss 598.697265625
iter 40, train loss 513.9268188476562, val loss 598.8828125
iter 50, train loss 514.482421875, val loss 599.9375
iter 60, train loss 514.7149658203125, val loss 593.44091796875
iter 70, train loss 514.342041015625, val loss 599.9653930664062
iter 80, train loss 513.8597412109375, val loss 596.5845947265625
iter 90, train loss 513.87451171875, val loss 592.2860717773438
best loss 575.4503784179688
not here
quantized in 94.0334107875824 seconds
35876 MiB free out of 48676 MiB total
26 mlp.down_proj
Pruning ...
256
iter 0, train loss 23.33260726928711, val loss 24.465967178344727
iter 10, train loss 23.368873596191406, val loss 24.837953567504883
iter 20, train loss 23.324077606201172, val loss 24.878366470336914
iter 30, train loss 23.295242309570312, val loss 24.5617618560791
iter 40, train loss 23.301376342773438, val loss 24.471057891845703
iter 50, train loss 23.29279327392578, val loss 24.42104148864746
iter 60, train loss 23.302385330200195, val loss 24.166078567504883
iter 70, train loss 23.257160186767578, val loss 23.924346923828125
iter 80, train loss 23.233522415161133, val loss 23.87256622314453
iter 90, train loss 23.2333984375, val loss 24.34807777404785
best loss 23.77947235107422
not here
quantized in 101.25761294364929 seconds
35682 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35682 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31586 MiB free out of 48676 MiB total
after cast to cpu
37852 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
27 self_attn.q_proj
Pruning ...
256
iter 0, train loss 510.3011474609375, val loss 471.1861572265625
iter 10, train loss 545.1441650390625, val loss 556.4226684570312
iter 20, train loss 518.9137573242188, val loss 535.4375
iter 30, train loss 507.93414306640625, val loss 524.8565063476562
iter 40, train loss 506.34710693359375, val loss 523.881103515625
iter 50, train loss 502.04852294921875, val loss 522.1289672851562
iter 60, train loss 501.63525390625, val loss 522.4962158203125
iter 70, train loss 498.7179870605469, val loss 522.0233764648438
iter 80, train loss 498.79815673828125, val loss 518.7297973632812
iter 90, train loss 498.2235107421875, val loss 517.0367431640625
best loss 471.1861572265625
not here
quantized in 45.964417934417725 seconds
36424 MiB free out of 48676 MiB total
27 self_attn.k_proj
Pruning ...
256
iter 0, train loss 553.3070068359375, val loss 485.1773681640625
iter 10, train loss 582.620361328125, val loss 598.05419921875
iter 20, train loss 549.255615234375, val loss 566.9851684570312
iter 30, train loss 535.8637084960938, val loss 553.0118408203125
iter 40, train loss 535.2354125976562, val loss 553.7005615234375
iter 50, train loss 532.2872314453125, val loss 551.341796875
iter 60, train loss 529.0130615234375, val loss 550.1627197265625
iter 70, train loss 528.30419921875, val loss 548.2095947265625
iter 80, train loss 526.9120483398438, val loss 547.6508178710938
iter 90, train loss 526.3267822265625, val loss 546.4505004882812
best loss 485.1773681640625
not here
quantized in 48.36838507652283 seconds
36414 MiB free out of 48676 MiB total
27 self_attn.v_proj
Pruning ...
256
iter 0, train loss 342.2692565917969, val loss 351.5779724121094
iter 10, train loss 341.22882080078125, val loss 357.236328125
iter 20, train loss 341.2272033691406, val loss 357.7646789550781
iter 30, train loss 341.3292236328125, val loss 357.44256591796875
iter 40, train loss 341.5901184082031, val loss 356.6829528808594
iter 50, train loss 340.94342041015625, val loss 357.3118591308594
iter 60, train loss 340.3633728027344, val loss 356.84527587890625
iter 70, train loss 339.9621887207031, val loss 357.0395202636719
iter 80, train loss 340.005126953125, val loss 356.94622802734375
iter 90, train loss 340.257568359375, val loss 357.06842041015625
best loss 351.5779724121094
not here
quantized in 32.92207980155945 seconds
36404 MiB free out of 48676 MiB total
27 self_attn.o_proj
Pruning ...
256
iter 0, train loss 23.03628921508789, val loss 27.035194396972656
iter 10, train loss 21.41315460205078, val loss 30.290307998657227
iter 20, train loss 20.975341796875, val loss 31.99559783935547
iter 30, train loss 20.118494033813477, val loss 30.584096908569336
iter 40, train loss 19.85228729248047, val loss 31.251220703125
iter 50, train loss 19.61725616455078, val loss 30.117937088012695
iter 60, train loss 19.428424835205078, val loss 29.850326538085938
iter 70, train loss 19.304107666015625, val loss 29.02313232421875
iter 80, train loss 19.241287231445312, val loss 29.27191925048828
iter 90, train loss 19.247278213500977, val loss 28.92884063720703
best loss 27.035194396972656
not here
quantized in 35.44167184829712 seconds
36372 MiB free out of 48676 MiB total
27 mlp.gate_proj
Pruning ...
256
iter 0, train loss 669.1624145507812, val loss 732.5556030273438
iter 10, train loss 694.0691528320312, val loss 794.1245727539062
iter 20, train loss 691.92919921875, val loss 779.529052734375
iter 30, train loss 691.1941528320312, val loss 788.5681762695312
iter 40, train loss 691.1541748046875, val loss 782.133056640625
iter 50, train loss 691.7535400390625, val loss 809.5458374023438
iter 60, train loss 690.515380859375, val loss 793.1520385742188
iter 70, train loss 690.1160888671875, val loss 799.9468383789062
iter 80, train loss 690.51806640625, val loss 804.845458984375
iter 90, train loss 690.80224609375, val loss 806.1396484375
best loss 732.5556030273438
not here
quantized in 94.79201531410217 seconds
36070 MiB free out of 48676 MiB total
27 mlp.up_proj
Pruning ...
256
iter 0, train loss 567.5118408203125, val loss 630.3978881835938
iter 10, train loss 567.4359130859375, val loss 625.3707275390625
iter 20, train loss 569.1026611328125, val loss 644.3220825195312
iter 30, train loss 569.484130859375, val loss 657.195068359375
iter 40, train loss 570.39990234375, val loss 660.6484375
iter 50, train loss 570.2815551757812, val loss 658.26708984375
iter 60, train loss 570.355712890625, val loss 665.18359375
iter 70, train loss 570.6635131835938, val loss 662.6524047851562
iter 80, train loss 569.9480590820312, val loss 661.779296875
iter 90, train loss 570.158447265625, val loss 666.5904541015625
best loss 625.3707275390625
not here
quantized in 93.96321702003479 seconds
35876 MiB free out of 48676 MiB total
27 mlp.down_proj
Pruning ...
256
iter 0, train loss 27.885950088500977, val loss 33.22240447998047
iter 10, train loss 27.93585205078125, val loss 33.56232833862305
iter 20, train loss 27.925432205200195, val loss 33.501075744628906
iter 30, train loss 27.848644256591797, val loss 33.15514373779297
iter 40, train loss 27.84568977355957, val loss 33.60260009765625
iter 50, train loss 27.840261459350586, val loss 33.699951171875
iter 60, train loss 27.84307861328125, val loss 33.39796447753906
iter 70, train loss 27.794578552246094, val loss 33.981781005859375
iter 80, train loss 27.799823760986328, val loss 34.08638381958008
iter 90, train loss 27.786542892456055, val loss 34.12790298461914
best loss 32.92988586425781
not here
quantized in 100.26631879806519 seconds
35682 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35682 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31586 MiB free out of 48676 MiB total
after cast to cpu
37852 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
28 self_attn.q_proj
Pruning ...
256
iter 0, train loss 511.3145751953125, val loss 458.7891540527344
iter 10, train loss 548.042724609375, val loss 553.1707153320312
iter 20, train loss 521.296630859375, val loss 529.9663696289062
iter 30, train loss 513.4111328125, val loss 521.405029296875
iter 40, train loss 507.11431884765625, val loss 518.1826171875
iter 50, train loss 503.30364990234375, val loss 513.4419555664062
iter 60, train loss 500.3767395019531, val loss 510.7257385253906
iter 70, train loss 500.0129699707031, val loss 510.3316650390625
iter 80, train loss 498.33489990234375, val loss 510.2705078125
iter 90, train loss 497.81024169921875, val loss 509.8916320800781
best loss 458.7891540527344
not here
quantized in 37.42007327079773 seconds
36424 MiB free out of 48676 MiB total
28 self_attn.k_proj
Pruning ...
256
iter 0, train loss 548.23388671875, val loss 470.71514892578125
iter 10, train loss 587.8321533203125, val loss 596.5963134765625
iter 20, train loss 560.745361328125, val loss 569.5367431640625
iter 30, train loss 548.4712524414062, val loss 559.024658203125
iter 40, train loss 534.47802734375, val loss 544.087890625
iter 50, train loss 531.8927001953125, val loss 544.0288696289062
iter 60, train loss 529.74658203125, val loss 541.7664794921875
iter 70, train loss 527.9871826171875, val loss 540.1449584960938
iter 80, train loss 526.9784545898438, val loss 539.2212524414062
iter 90, train loss 525.271728515625, val loss 538.1171875
best loss 470.71514892578125
not here
quantized in 36.65708255767822 seconds
36414 MiB free out of 48676 MiB total
28 self_attn.v_proj
Pruning ...
256
iter 0, train loss 378.079833984375, val loss 385.1299133300781
iter 10, train loss 378.976318359375, val loss 390.72119140625
iter 20, train loss 377.90960693359375, val loss 391.0158386230469
iter 30, train loss 378.6802062988281, val loss 389.759765625
iter 40, train loss 378.3492736816406, val loss 391.570556640625
iter 50, train loss 377.55755615234375, val loss 391.8332214355469
iter 60, train loss 377.7445068359375, val loss 393.0722351074219
iter 70, train loss 377.43603515625, val loss 392.8471984863281
iter 80, train loss 377.9228515625, val loss 392.6195068359375
iter 90, train loss 377.2840576171875, val loss 392.145751953125
best loss 385.1299133300781
not here
quantized in 34.839086294174194 seconds
36404 MiB free out of 48676 MiB total
28 self_attn.o_proj
Pruning ...
256
iter 0, train loss 33.238800048828125, val loss 35.3739013671875
iter 10, train loss 32.2842903137207, val loss 38.775455474853516
iter 20, train loss 31.981178283691406, val loss 38.94260787963867
iter 30, train loss 31.678606033325195, val loss 39.01325225830078
iter 40, train loss 31.297752380371094, val loss 38.306236267089844
iter 50, train loss 31.14112091064453, val loss 39.368446350097656
iter 60, train loss 30.930675506591797, val loss 39.03203582763672
iter 70, train loss 30.818462371826172, val loss 38.86647415161133
iter 80, train loss 30.789098739624023, val loss 38.29883575439453
iter 90, train loss 30.699167251586914, val loss 38.35856628417969
best loss 35.3739013671875
not here
quantized in 35.705076456069946 seconds
36372 MiB free out of 48676 MiB total
28 mlp.gate_proj
Pruning ...
256
iter 0, train loss 706.8377685546875, val loss 744.9564208984375
iter 10, train loss 736.172119140625, val loss 803.5067138671875
iter 20, train loss 728.66455078125, val loss 823.5147094726562
iter 30, train loss 729.5156860351562, val loss 832.3317260742188
iter 40, train loss 727.6402587890625, val loss 818.6651611328125
iter 50, train loss 726.9859619140625, val loss 823.4576416015625
iter 60, train loss 725.4220581054688, val loss 822.9797973632812
iter 70, train loss 724.5362548828125, val loss 818.7683715820312
iter 80, train loss 724.9435424804688, val loss 821.8966674804688
iter 90, train loss 724.3759155273438, val loss 819.324462890625
best loss 744.9564208984375
not here
quantized in 95.17497110366821 seconds
36070 MiB free out of 48676 MiB total
28 mlp.up_proj
Pruning ...
256
iter 0, train loss 626.5396728515625, val loss 675.595947265625
iter 10, train loss 634.5726318359375, val loss 717.3363037109375
iter 20, train loss 627.1085815429688, val loss 718.1163940429688
iter 30, train loss 626.0955810546875, val loss 724.056396484375
iter 40, train loss 625.9292602539062, val loss 711.0523681640625
iter 50, train loss 624.900390625, val loss 705.0291137695312
iter 60, train loss 624.6998291015625, val loss 716.150634765625
iter 70, train loss 624.587158203125, val loss 712.2301025390625
iter 80, train loss 624.0926513671875, val loss 709.1242065429688
iter 90, train loss 624.1581420898438, val loss 710.9765014648438
best loss 675.595947265625
not here
quantized in 95.62898993492126 seconds
35876 MiB free out of 48676 MiB total
28 mlp.down_proj
Pruning ...
256
iter 0, train loss 34.06479263305664, val loss 39.057708740234375
iter 10, train loss 34.12633514404297, val loss 38.81386947631836
iter 20, train loss 34.00132751464844, val loss 38.973670959472656
iter 30, train loss 33.96878433227539, val loss 38.50676345825195
iter 40, train loss 33.93163299560547, val loss 38.075138092041016
iter 50, train loss 33.928627014160156, val loss 38.852806091308594
iter 60, train loss 33.87443923950195, val loss 38.636131286621094
iter 70, train loss 33.78407669067383, val loss 38.457706451416016
iter 80, train loss 33.79487228393555, val loss 38.393394470214844
iter 90, train loss 33.7919921875, val loss 38.57859420776367
best loss 37.857269287109375
not here
quantized in 100.79595518112183 seconds
35682 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35682 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31586 MiB free out of 48676 MiB total
after cast to cpu
37852 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
29 self_attn.q_proj
Pruning ...
256
iter 0, train loss 471.04986572265625, val loss 406.04742431640625
iter 10, train loss 502.6055908203125, val loss 512.4754028320312
iter 20, train loss 481.91534423828125, val loss 493.72589111328125
iter 30, train loss 467.52484130859375, val loss 480.854248046875
iter 40, train loss 460.70001220703125, val loss 471.8504333496094
iter 50, train loss 457.06085205078125, val loss 469.5185546875
iter 60, train loss 455.48712158203125, val loss 468.54736328125
iter 70, train loss 455.36865234375, val loss 467.305908203125
iter 80, train loss 454.6474304199219, val loss 468.1799011230469
iter 90, train loss 453.8440856933594, val loss 466.3290710449219
best loss 406.04742431640625
not here
quantized in 37.76253604888916 seconds
36424 MiB free out of 48676 MiB total
29 self_attn.k_proj
Pruning ...
256
iter 0, train loss 507.9356689453125, val loss 422.07525634765625
iter 10, train loss 538.6742553710938, val loss 548.5987548828125
iter 20, train loss 524.5453491210938, val loss 536.7831420898438
iter 30, train loss 500.94866943359375, val loss 517.44091796875
iter 40, train loss 494.8990478515625, val loss 511.00885009765625
iter 50, train loss 488.0049743652344, val loss 503.8061218261719
iter 60, train loss 486.5357971191406, val loss 501.7022705078125
iter 70, train loss 484.7210693359375, val loss 500.4674987792969
iter 80, train loss 482.61273193359375, val loss 499.1363525390625
iter 90, train loss 482.28704833984375, val loss 500.3923645019531
best loss 422.07525634765625
not here
quantized in 37.10274887084961 seconds
36414 MiB free out of 48676 MiB total
29 self_attn.v_proj
Pruning ...
256
iter 0, train loss 356.49456787109375, val loss 369.7169189453125
iter 10, train loss 356.66290283203125, val loss 374.04296875
iter 20, train loss 356.5084228515625, val loss 375.40460205078125
iter 30, train loss 356.3880310058594, val loss 375.5406494140625
iter 40, train loss 355.89617919921875, val loss 374.95257568359375
iter 50, train loss 356.3439025878906, val loss 374.96160888671875
iter 60, train loss 356.0533142089844, val loss 375.2225341796875
iter 70, train loss 355.9666442871094, val loss 375.2996520996094
iter 80, train loss 355.4173889160156, val loss 375.7777099609375
iter 90, train loss 354.8006591796875, val loss 375.46600341796875
best loss 369.7169189453125
not here
quantized in 34.602728605270386 seconds
36404 MiB free out of 48676 MiB total
29 self_attn.o_proj
Pruning ...
256
iter 0, train loss 36.47184371948242, val loss 34.58075714111328
iter 10, train loss 29.959552764892578, val loss 41.13081741333008
iter 20, train loss 29.00122833251953, val loss 42.87889862060547
iter 30, train loss 27.825515747070312, val loss 41.64396286010742
iter 40, train loss 27.069881439208984, val loss 41.989295959472656
iter 50, train loss 26.90502166748047, val loss 40.236968994140625
iter 60, train loss 26.71898651123047, val loss 40.63855743408203
iter 70, train loss 26.461923599243164, val loss 40.06740188598633
iter 80, train loss 26.411365509033203, val loss 39.32756805419922
iter 90, train loss 26.25212860107422, val loss 39.84257125854492
best loss 34.58075714111328
not here
quantized in 35.35417366027832 seconds
36372 MiB free out of 48676 MiB total
29 mlp.gate_proj
Pruning ...
256
iter 0, train loss 747.7188110351562, val loss 789.680419921875
iter 10, train loss 781.5044555664062, val loss 892.4010009765625
iter 20, train loss 768.9171142578125, val loss 886.947021484375
iter 30, train loss 772.2072143554688, val loss 895.1749877929688
iter 40, train loss 766.4273071289062, val loss 884.0682373046875
iter 50, train loss 762.577880859375, val loss 883.7745971679688
iter 60, train loss 761.51416015625, val loss 886.57275390625
iter 70, train loss 760.4573974609375, val loss 879.1104736328125
iter 80, train loss 761.18994140625, val loss 888.9860229492188
iter 90, train loss 761.4146728515625, val loss 895.3008422851562
best loss 789.680419921875
not here
quantized in 95.67769241333008 seconds
36070 MiB free out of 48676 MiB total
29 mlp.up_proj
Pruning ...
256
iter 0, train loss 671.4362182617188, val loss 709.0316162109375
iter 10, train loss 695.294677734375, val loss 799.8035888671875
iter 20, train loss 663.18798828125, val loss 753.0366821289062
iter 30, train loss 661.048583984375, val loss 747.0686645507812
iter 40, train loss 657.7373657226562, val loss 762.1585693359375
iter 50, train loss 657.1607055664062, val loss 769.5789794921875
iter 60, train loss 657.43994140625, val loss 772.3967895507812
iter 70, train loss 656.06298828125, val loss 765.4829711914062
iter 80, train loss 655.7182006835938, val loss 762.6450805664062
iter 90, train loss 656.47607421875, val loss 756.5409545898438
best loss 709.0316162109375
not here
quantized in 95.53185200691223 seconds
35876 MiB free out of 48676 MiB total
29 mlp.down_proj
Pruning ...
256
iter 0, train loss 41.25513458251953, val loss 121.22310638427734
iter 10, train loss 41.358245849609375, val loss 116.56997680664062
iter 20, train loss 41.415653228759766, val loss 108.36090087890625
iter 30, train loss 41.38927459716797, val loss 105.12545013427734
iter 40, train loss 41.25651550292969, val loss 102.07514953613281
iter 50, train loss 41.26926040649414, val loss 99.15216064453125
iter 60, train loss 41.34769058227539, val loss 96.37321472167969
iter 70, train loss 41.15290451049805, val loss 93.96719360351562
iter 80, train loss 41.13238525390625, val loss 90.37739562988281
iter 90, train loss 41.059410095214844, val loss 86.80278778076172
best loss 84.42237091064453
not here
quantized in 101.9888756275177 seconds
35682 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35682 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31586 MiB free out of 48676 MiB total
after cast to cpu
37852 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
30 self_attn.q_proj
Pruning ...
256
iter 0, train loss 506.11553955078125, val loss 451.88360595703125
iter 10, train loss 542.03369140625, val loss 563.1026611328125
iter 20, train loss 520.5188598632812, val loss 538.8605346679688
iter 30, train loss 504.77783203125, val loss 527.7471923828125
iter 40, train loss 496.943115234375, val loss 521.4061279296875
iter 50, train loss 491.9443054199219, val loss 518.2522583007812
iter 60, train loss 488.337158203125, val loss 514.7848510742188
iter 70, train loss 486.582763671875, val loss 513.9673461914062
iter 80, train loss 487.81103515625, val loss 512.423828125
iter 90, train loss 486.5670471191406, val loss 513.1949462890625
best loss 451.88360595703125
not here
quantized in 37.75019836425781 seconds
36424 MiB free out of 48676 MiB total
30 self_attn.k_proj
Pruning ...
256
iter 0, train loss 544.8399658203125, val loss 467.3250427246094
iter 10, train loss 576.6177978515625, val loss 599.93310546875
iter 20, train loss 551.8198852539062, val loss 577.3104248046875
iter 30, train loss 530.2103881835938, val loss 560.8743896484375
iter 40, train loss 519.9073486328125, val loss 550.5219116210938
iter 50, train loss 516.086181640625, val loss 545.42138671875
iter 60, train loss 515.1445922851562, val loss 546.31982421875
iter 70, train loss 513.5672607421875, val loss 541.6116943359375
iter 80, train loss 513.3571166992188, val loss 539.739013671875
iter 90, train loss 511.99957275390625, val loss 537.627197265625
best loss 467.3250427246094
not here
quantized in 36.86863589286804 seconds
36414 MiB free out of 48676 MiB total
30 self_attn.v_proj
Pruning ...
256
iter 0, train loss 406.09710693359375, val loss 419.45733642578125
iter 10, train loss 408.3832092285156, val loss 429.8453369140625
iter 20, train loss 407.1532287597656, val loss 430.7230224609375
iter 30, train loss 408.56146240234375, val loss 432.7685546875
iter 40, train loss 408.375732421875, val loss 434.2238464355469
iter 50, train loss 407.2210998535156, val loss 434.14404296875
iter 60, train loss 406.92572021484375, val loss 433.0830993652344
iter 70, train loss 406.7437744140625, val loss 432.7843322753906
iter 80, train loss 407.20306396484375, val loss 432.2030334472656
iter 90, train loss 405.7272033691406, val loss 432.5193786621094
best loss 419.45733642578125
not here
quantized in 34.53132939338684 seconds
36404 MiB free out of 48676 MiB total
30 self_attn.o_proj
Pruning ...
256
iter 0, train loss 43.75307846069336, val loss 36.265869140625
iter 10, train loss 34.90981674194336, val loss 42.802764892578125
iter 20, train loss 33.05585861206055, val loss 41.13770294189453
iter 30, train loss 32.22370147705078, val loss 42.52119064331055
iter 40, train loss 31.58536148071289, val loss 41.19114685058594
iter 50, train loss 30.990455627441406, val loss 41.05970001220703
iter 60, train loss 30.613805770874023, val loss 40.226234436035156
iter 70, train loss 30.74378776550293, val loss 40.08048629760742
iter 80, train loss 30.516155242919922, val loss 40.463523864746094
iter 90, train loss 30.349971771240234, val loss 41.196983337402344
best loss 36.265869140625
not here
quantized in 35.20024871826172 seconds
36372 MiB free out of 48676 MiB total
30 mlp.gate_proj
Pruning ...
256
iter 0, train loss 846.4151611328125, val loss 853.2401123046875
iter 10, train loss 892.5169677734375, val loss 1018.8871459960938
iter 20, train loss 844.9507446289062, val loss 1013.985595703125
iter 30, train loss 835.9577026367188, val loss 971.5112915039062
iter 40, train loss 834.8565673828125, val loss 963.2135009765625
iter 50, train loss 830.5406494140625, val loss 964.9332275390625
iter 60, train loss 829.1671142578125, val loss 958.1187744140625
iter 70, train loss 828.3717041015625, val loss 970.0123901367188
iter 80, train loss 827.083984375, val loss 964.7274169921875
iter 90, train loss 828.337890625, val loss 975.1205444335938
best loss 853.2401123046875
not here
quantized in 95.55755925178528 seconds
36070 MiB free out of 48676 MiB total
30 mlp.up_proj
Pruning ...
256
iter 0, train loss 749.7398681640625, val loss 792.2223510742188
iter 10, train loss 783.421630859375, val loss 932.6229248046875
iter 20, train loss 732.6485595703125, val loss 877.0025634765625
iter 30, train loss 718.4180908203125, val loss 852.47802734375
iter 40, train loss 706.4462890625, val loss 847.57568359375
iter 50, train loss 705.4239501953125, val loss 852.327880859375
iter 60, train loss 703.2169189453125, val loss 843.8185424804688
iter 70, train loss 701.5576171875, val loss 855.8131713867188
iter 80, train loss 701.0389404296875, val loss 851.7051391601562
iter 90, train loss 699.9892578125, val loss 850.03173828125
best loss 792.2223510742188
not here
quantized in 96.75561714172363 seconds
35876 MiB free out of 48676 MiB total
30 mlp.down_proj
Pruning ...
256
iter 0, train loss 58.96807861328125, val loss 73.05001831054688
iter 10, train loss 59.25155258178711, val loss 82.34393310546875
iter 20, train loss 58.75354766845703, val loss 77.28849792480469
iter 30, train loss 58.70059585571289, val loss 79.87855529785156
iter 40, train loss 58.647823333740234, val loss 78.58187866210938
iter 50, train loss 58.0494270324707, val loss 78.17245483398438
iter 60, train loss 58.055931091308594, val loss 76.85531616210938
iter 70, train loss 57.89305114746094, val loss 75.38770294189453
iter 80, train loss 57.84318923950195, val loss 74.45182800292969
iter 90, train loss 57.7254524230957, val loss 73.80054473876953
best loss 73.05001831054688
not here
quantized in 102.5986738204956 seconds
35682 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35682 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31586 MiB free out of 48676 MiB total
after cast to cpu
37852 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
31 self_attn.q_proj
Pruning ...
256
iter 0, train loss 383.0174255371094, val loss 310.8038330078125
iter 10, train loss 399.0013427734375, val loss 406.58465576171875
iter 20, train loss 385.6177062988281, val loss 395.4142150878906
iter 30, train loss 372.84326171875, val loss 382.78631591796875
iter 40, train loss 364.999267578125, val loss 377.2980041503906
iter 50, train loss 365.64678955078125, val loss 374.9746398925781
iter 60, train loss 361.6990966796875, val loss 371.1921081542969
iter 70, train loss 360.0592956542969, val loss 371.0080871582031
iter 80, train loss 358.4969177246094, val loss 369.114013671875
iter 90, train loss 356.44732666015625, val loss 367.1553955078125
best loss 310.8038330078125
not here
quantized in 38.44746232032776 seconds
36424 MiB free out of 48676 MiB total
31 self_attn.k_proj
Pruning ...
256
iter 0, train loss 446.9891052246094, val loss 342.349609375
iter 10, train loss 454.43121337890625, val loss 467.41455078125
iter 20, train loss 449.3908996582031, val loss 464.4458923339844
iter 30, train loss 430.76068115234375, val loss 446.23370361328125
iter 40, train loss 418.8958435058594, val loss 434.3548278808594
iter 50, train loss 410.5489196777344, val loss 426.87432861328125
iter 60, train loss 407.2523193359375, val loss 422.7568664550781
iter 70, train loss 404.1130065917969, val loss 420.2193603515625
iter 80, train loss 403.45208740234375, val loss 419.516357421875
iter 90, train loss 401.5299377441406, val loss 417.92401123046875
best loss 342.349609375
not here
quantized in 37.94503974914551 seconds
36414 MiB free out of 48676 MiB total
31 self_attn.v_proj
Pruning ...
256
iter 0, train loss 231.3527374267578, val loss 239.00613403320312
iter 10, train loss 231.5915985107422, val loss 243.22421264648438
iter 20, train loss 231.57913208007812, val loss 243.3077392578125
iter 30, train loss 230.61514282226562, val loss 244.0631561279297
iter 40, train loss 231.64862060546875, val loss 243.58505249023438
iter 50, train loss 230.28732299804688, val loss 243.4600830078125
iter 60, train loss 230.08749389648438, val loss 244.2178955078125
iter 70, train loss 229.928955078125, val loss 244.25296020507812
iter 80, train loss 229.65013122558594, val loss 243.8412322998047
iter 90, train loss 229.5200653076172, val loss 243.69552612304688
best loss 239.00613403320312
not here
quantized in 34.8999764919281 seconds
36404 MiB free out of 48676 MiB total
31 self_attn.o_proj
Pruning ...
256
iter 0, train loss 187.5784149169922, val loss 31.897069931030273
iter 10, train loss 160.06338500976562, val loss 31.438459396362305
iter 20, train loss 144.90426635742188, val loss 31.500144958496094
iter 30, train loss 132.9398956298828, val loss 29.725563049316406
iter 40, train loss 124.7989273071289, val loss 29.838703155517578
iter 50, train loss 119.45707702636719, val loss 29.398334503173828
iter 60, train loss 116.38874816894531, val loss 28.1669921875
iter 70, train loss 115.84072875976562, val loss 27.776473999023438
iter 80, train loss 113.79976654052734, val loss 27.527870178222656
iter 90, train loss 112.68006896972656, val loss 27.891923904418945
best loss 27.504165649414062
not here
quantized in 36.1778519153595 seconds
36372 MiB free out of 48676 MiB total
31 mlp.gate_proj
Pruning ...
256
iter 0, train loss 773.705322265625, val loss 696.9853515625
iter 10, train loss 794.1755981445312, val loss 843.137451171875
iter 20, train loss 749.150146484375, val loss 795.273681640625
iter 30, train loss 724.643310546875, val loss 764.076171875
iter 40, train loss 723.530029296875, val loss 760.2264404296875
iter 50, train loss 723.502685546875, val loss 769.6414794921875
iter 60, train loss 722.6080932617188, val loss 765.0115966796875
iter 70, train loss 722.2305908203125, val loss 765.018798828125
iter 80, train loss 723.643310546875, val loss 766.987060546875
iter 90, train loss 724.142578125, val loss 769.1156616210938
best loss 696.9853515625
not here
quantized in 97.26165056228638 seconds
36070 MiB free out of 48676 MiB total
31 mlp.up_proj
Pruning ...
256
iter 0, train loss 747.7294311523438, val loss 680.2664184570312
iter 10, train loss 747.0388793945312, val loss 808.017578125
iter 20, train loss 717.2410278320312, val loss 771.6397705078125
iter 30, train loss 687.51904296875, val loss 735.5489501953125
iter 40, train loss 668.494873046875, val loss 710.4833374023438
iter 50, train loss 666.8883056640625, val loss 712.6666259765625
iter 60, train loss 660.7081298828125, val loss 709.2076416015625
iter 70, train loss 657.7759399414062, val loss 708.1431884765625
iter 80, train loss 655.94970703125, val loss 707.403564453125
iter 90, train loss 652.7972412109375, val loss 696.033447265625
best loss 652.58154296875
not here
quantized in 99.52393960952759 seconds
35876 MiB free out of 48676 MiB total
31 mlp.down_proj
Pruning ...
256
iter 0, train loss 116.97614288330078, val loss 1341.581298828125
iter 10, train loss 126.93121337890625, val loss 744.3084716796875
iter 20, train loss 120.32666778564453, val loss 368.92498779296875
iter 30, train loss 112.53465270996094, val loss 282.02880859375
iter 40, train loss 109.70535278320312, val loss 246.64398193359375
iter 50, train loss 106.8106460571289, val loss 231.8163299560547
iter 60, train loss 103.42545318603516, val loss 232.71914672851562
iter 70, train loss 103.31461334228516, val loss 231.80848693847656
iter 80, train loss 102.1884765625, val loss 227.3204345703125
iter 90, train loss 100.99224853515625, val loss 219.36868286132812
best loss 216.07351684570312
not here
quantized in 103.59123039245605 seconds
35682 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35682 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31586 MiB free out of 48676 MiB total
after cast to cpu
37852 MiB free out of 48676 MiB total
Total bits: 13017415680.0 Total params: 6476005376
average bits per value: 2.0100995790155443
total time taken: 13861.664040088654
Loading tokenizer for meta-llama/Llama-2-7b-hf
/home/lliu/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Dataset: wikitext2
Evaluating ...
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
Perplexity: 8.087837
