/home/lliu/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.13s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.39it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.28it/s]
Model loaded. LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
4096
Loading tokenizer for meta-llama/Llama-2-7b-hf
Starting...
38748 MiB free out of 48676 MiB total
attention_mask torch.Size([1, 1, 4096, 4096]) cuda:6 torch.float16
position_ids torch.Size([1, 4096]) cuda:6 torch.int64
position_ids torch.Size([1, 4096])
attention_mask torch.Size([1, 1, 4096, 4096])
Ready.
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
0 self_attn.k_proj
low_ranking ...
using low rank =  512
0 self_attn.v_proj
low_ranking ...
using low rank =  512
0 self_attn.q_proj
low_ranking ...
using low rank =  512
0 self_attn.o_proj
low_ranking ...
using low rank =  512
0 mlp.up_proj
low_ranking ...
0 mlp.down_proj
low_ranking ...
0 mlp.gate_proj
low_ranking ...
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
31330 MiB free out of 48676 MiB total
after cast to cpu
36838 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
1 self_attn.k_proj
low_ranking ...
using low rank =  512
1 self_attn.v_proj
low_ranking ...
using low rank =  512
1 self_attn.q_proj
low_ranking ...
using low rank =  512
1 self_attn.o_proj
low_ranking ...
using low rank =  512
1 mlp.up_proj
low_ranking ...
1 mlp.down_proj
low_ranking ...
1 mlp.gate_proj
low_ranking ...
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
30370 MiB free out of 48676 MiB total
after cast to cpu
35814 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
2 self_attn.k_proj
low_ranking ...
using low rank =  512
2 self_attn.v_proj
low_ranking ...
using low rank =  512
2 self_attn.q_proj
low_ranking ...
using low rank =  512
2 self_attn.o_proj
low_ranking ...
using low rank =  512
2 mlp.up_proj
low_ranking ...
2 mlp.down_proj
low_ranking ...
2 mlp.gate_proj
low_ranking ...
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
29474 MiB free out of 48676 MiB total
after cast to cpu
34726 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
3 self_attn.k_proj
low_ranking ...
using low rank =  512
3 self_attn.v_proj
low_ranking ...
using low rank =  512
3 self_attn.q_proj
low_ranking ...
using low rank =  512
3 self_attn.o_proj
low_ranking ...
using low rank =  512
3 mlp.up_proj
low_ranking ...
3 mlp.down_proj
low_ranking ...
3 mlp.gate_proj
low_ranking ...
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
28578 MiB free out of 48676 MiB total
after cast to cpu
33702 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
4 self_attn.k_proj
low_ranking ...
using low rank =  512
4 self_attn.v_proj
low_ranking ...
using low rank =  512
4 self_attn.q_proj
low_ranking ...
using low rank =  512
4 self_attn.o_proj
low_ranking ...
using low rank =  512
4 mlp.up_proj
low_ranking ...
4 mlp.down_proj
low_ranking ...
4 mlp.gate_proj
low_ranking ...
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
27554 MiB free out of 48676 MiB total
after cast to cpu
32678 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
5 self_attn.k_proj
low_ranking ...
using low rank =  512
5 self_attn.v_proj
low_ranking ...
using low rank =  512
5 self_attn.q_proj
low_ranking ...
using low rank =  512
5 self_attn.o_proj
low_ranking ...
using low rank =  512
5 mlp.up_proj
low_ranking ...
5 mlp.down_proj
low_ranking ...
5 mlp.gate_proj
low_ranking ...
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
26530 MiB free out of 48676 MiB total
after cast to cpu
31654 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
6 self_attn.k_proj
low_ranking ...
using low rank =  512
6 self_attn.v_proj
low_ranking ...
using low rank =  512
6 self_attn.q_proj
low_ranking ...
using low rank =  512
6 self_attn.o_proj
low_ranking ...
using low rank =  512
6 mlp.up_proj
low_ranking ...
6 mlp.down_proj
low_ranking ...
6 mlp.gate_proj
low_ranking ...
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
25506 MiB free out of 48676 MiB total
after cast to cpu
30630 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
7 self_attn.k_proj
low_ranking ...
using low rank =  512
7 self_attn.v_proj
low_ranking ...
using low rank =  512
7 self_attn.q_proj
low_ranking ...
using low rank =  512
7 self_attn.o_proj
low_ranking ...
using low rank =  512
7 mlp.up_proj
low_ranking ...
7 mlp.down_proj
low_ranking ...
7 mlp.gate_proj
low_ranking ...
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
24482 MiB free out of 48676 MiB total
after cast to cpu
29606 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
8 self_attn.k_proj
low_ranking ...
using low rank =  512
8 self_attn.v_proj
low_ranking ...
using low rank =  512
8 self_attn.q_proj
low_ranking ...
using low rank =  512
8 self_attn.o_proj
low_ranking ...
using low rank =  512
8 mlp.up_proj
low_ranking ...
8 mlp.down_proj
low_ranking ...
8 mlp.gate_proj
low_ranking ...
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
23458 MiB free out of 48676 MiB total
after cast to cpu
28582 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
9 self_attn.k_proj
low_ranking ...
using low rank =  512
9 self_attn.v_proj
low_ranking ...
using low rank =  512
9 self_attn.q_proj
low_ranking ...
using low rank =  512
9 self_attn.o_proj
low_ranking ...
using low rank =  512
9 mlp.up_proj
low_ranking ...
9 mlp.down_proj
low_ranking ...
9 mlp.gate_proj
low_ranking ...
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
22434 MiB free out of 48676 MiB total
after cast to cpu
28582 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
10 self_attn.k_proj
low_ranking ...
using low rank =  512
10 self_attn.v_proj
low_ranking ...
using low rank =  512
10 self_attn.q_proj
low_ranking ...
using low rank =  512
10 self_attn.o_proj
low_ranking ...
using low rank =  512
10 mlp.up_proj
low_ranking ...
10 mlp.down_proj
low_ranking ...
10 mlp.gate_proj
low_ranking ...
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
22284 MiB free out of 48676 MiB total
after cast to cpu
27408 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
11 self_attn.k_proj
low_ranking ...
using low rank =  512
11 self_attn.v_proj
low_ranking ...
using low rank =  512
11 self_attn.q_proj
low_ranking ...
using low rank =  512
11 self_attn.o_proj
low_ranking ...
using low rank =  512
11 mlp.up_proj
low_ranking ...
11 mlp.down_proj
low_ranking ...
11 mlp.gate_proj
low_ranking ...
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
21260 MiB free out of 48676 MiB total
after cast to cpu
26384 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
12 self_attn.k_proj
low_ranking ...
using low rank =  512
12 self_attn.v_proj
low_ranking ...
using low rank =  512
12 self_attn.q_proj
low_ranking ...
using low rank =  512
12 self_attn.o_proj
low_ranking ...
using low rank =  512
12 mlp.up_proj
low_ranking ...
12 mlp.down_proj
low_ranking ...
12 mlp.gate_proj
low_ranking ...
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
20236 MiB free out of 48676 MiB total
after cast to cpu
25360 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
13 self_attn.k_proj
low_ranking ...
using low rank =  512
13 self_attn.v_proj
low_ranking ...
using low rank =  512
13 self_attn.q_proj
low_ranking ...
using low rank =  512
13 self_attn.o_proj
low_ranking ...
using low rank =  512
13 mlp.up_proj
low_ranking ...
13 mlp.down_proj
low_ranking ...
13 mlp.gate_proj
low_ranking ...
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
19212 MiB free out of 48676 MiB total
after cast to cpu
24336 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
14 self_attn.k_proj
low_ranking ...
using low rank =  512
14 self_attn.v_proj
low_ranking ...
using low rank =  512
14 self_attn.q_proj
low_ranking ...
using low rank =  512
14 self_attn.o_proj
low_ranking ...
using low rank =  512
14 mlp.up_proj
low_ranking ...
14 mlp.down_proj
low_ranking ...
14 mlp.gate_proj
low_ranking ...
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
18188 MiB free out of 48676 MiB total
after cast to cpu
23312 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
15 self_attn.k_proj
low_ranking ...
using low rank =  512
15 self_attn.v_proj
low_ranking ...
using low rank =  512
15 self_attn.q_proj
low_ranking ...
using low rank =  512
15 self_attn.o_proj
low_ranking ...
using low rank =  512
15 mlp.up_proj
low_ranking ...
15 mlp.down_proj
low_ranking ...
15 mlp.gate_proj
low_ranking ...
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
17164 MiB free out of 48676 MiB total
after cast to cpu
23312 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
16 self_attn.k_proj
low_ranking ...
using low rank =  512
16 self_attn.v_proj
low_ranking ...
using low rank =  512
16 self_attn.q_proj
low_ranking ...
using low rank =  512
16 self_attn.o_proj
low_ranking ...
using low rank =  512
16 mlp.up_proj
low_ranking ...
16 mlp.down_proj
low_ranking ...
16 mlp.gate_proj
low_ranking ...
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
16886 MiB free out of 48676 MiB total
after cast to cpu
22074 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
17 self_attn.k_proj
low_ranking ...
using low rank =  512
17 self_attn.v_proj
low_ranking ...
using low rank =  512
17 self_attn.q_proj
low_ranking ...
using low rank =  512
17 self_attn.o_proj
low_ranking ...
using low rank =  512
17 mlp.up_proj
low_ranking ...
17 mlp.down_proj
low_ranking ...
17 mlp.gate_proj
low_ranking ...
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
15926 MiB free out of 48676 MiB total
after cast to cpu
21050 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
18 self_attn.k_proj
low_ranking ...
using low rank =  512
18 self_attn.v_proj
low_ranking ...
using low rank =  512
18 self_attn.q_proj
low_ranking ...
using low rank =  512
18 self_attn.o_proj
low_ranking ...
using low rank =  512
18 mlp.up_proj
low_ranking ...
18 mlp.down_proj
low_ranking ...
18 mlp.gate_proj
low_ranking ...
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
14902 MiB free out of 48676 MiB total
after cast to cpu
19002 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
19 self_attn.k_proj
low_ranking ...
using low rank =  512
19 self_attn.v_proj
low_ranking ...
using low rank =  512
19 self_attn.q_proj
low_ranking ...
using low rank =  512
19 self_attn.o_proj
low_ranking ...
using low rank =  512
19 mlp.up_proj
low_ranking ...
19 mlp.down_proj
low_ranking ...
19 mlp.gate_proj
low_ranking ...
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
14902 MiB free out of 48676 MiB total
after cast to cpu
19002 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
20 self_attn.k_proj
low_ranking ...
using low rank =  512
20 self_attn.v_proj
low_ranking ...
using low rank =  512
20 self_attn.q_proj
low_ranking ...
using low rank =  512
20 self_attn.o_proj
low_ranking ...
using low rank =  512
20 mlp.up_proj
low_ranking ...
20 mlp.down_proj
low_ranking ...
20 mlp.gate_proj
low_ranking ...
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
12854 MiB free out of 48676 MiB total
after cast to cpu
17978 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
21 self_attn.k_proj
low_ranking ...
using low rank =  512
21 self_attn.v_proj
low_ranking ...
using low rank =  512
21 self_attn.q_proj
low_ranking ...
using low rank =  512
21 self_attn.o_proj
low_ranking ...
using low rank =  512
21 mlp.up_proj
low_ranking ...
21 mlp.down_proj
low_ranking ...
21 mlp.gate_proj
low_ranking ...
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
11830 MiB free out of 48676 MiB total
after cast to cpu
17978 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
22 self_attn.k_proj
low_ranking ...
using low rank =  512
22 self_attn.v_proj
low_ranking ...
using low rank =  512
22 self_attn.q_proj
low_ranking ...
using low rank =  512
22 self_attn.o_proj
low_ranking ...
using low rank =  512
22 mlp.up_proj
low_ranking ...
22 mlp.down_proj
low_ranking ...
22 mlp.gate_proj
low_ranking ...
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
11552 MiB free out of 48676 MiB total
after cast to cpu
16740 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
23 self_attn.k_proj
low_ranking ...
using low rank =  512
23 self_attn.v_proj
low_ranking ...
using low rank =  512
23 self_attn.q_proj
low_ranking ...
using low rank =  512
23 self_attn.o_proj
low_ranking ...
using low rank =  512
23 mlp.up_proj
low_ranking ...
23 mlp.down_proj
low_ranking ...
23 mlp.gate_proj
low_ranking ...
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
10592 MiB free out of 48676 MiB total
after cast to cpu
15716 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
24 self_attn.k_proj
low_ranking ...
using low rank =  512
24 self_attn.v_proj
low_ranking ...
using low rank =  512
24 self_attn.q_proj
low_ranking ...
using low rank =  512
24 self_attn.o_proj
low_ranking ...
using low rank =  512
24 mlp.up_proj
low_ranking ...
24 mlp.down_proj
low_ranking ...
24 mlp.gate_proj
low_ranking ...
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
9568 MiB free out of 48676 MiB total
after cast to cpu
14692 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
25 self_attn.k_proj
low_ranking ...
using low rank =  512
25 self_attn.v_proj
low_ranking ...
using low rank =  512
25 self_attn.q_proj
low_ranking ...
using low rank =  512
25 self_attn.o_proj
low_ranking ...
using low rank =  512
25 mlp.up_proj
low_ranking ...
25 mlp.down_proj
low_ranking ...
25 mlp.gate_proj
low_ranking ...
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
8544 MiB free out of 48676 MiB total
after cast to cpu
13668 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
26 self_attn.k_proj
low_ranking ...
using low rank =  512
26 self_attn.v_proj
low_ranking ...
using low rank =  512
26 self_attn.q_proj
low_ranking ...
using low rank =  512
26 self_attn.o_proj
low_ranking ...
using low rank =  512
26 mlp.up_proj
low_ranking ...
26 mlp.down_proj
low_ranking ...
26 mlp.gate_proj
low_ranking ...
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
7520 MiB free out of 48676 MiB total
after cast to cpu
12644 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
27 self_attn.k_proj
low_ranking ...
using low rank =  512
27 self_attn.v_proj
low_ranking ...
using low rank =  512
27 self_attn.q_proj
low_ranking ...
using low rank =  512
27 self_attn.o_proj
low_ranking ...
using low rank =  512
27 mlp.up_proj
low_ranking ...
27 mlp.down_proj
low_ranking ...
27 mlp.gate_proj
low_ranking ...
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
6496 MiB free out of 48676 MiB total
after cast to cpu
11620 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
28 self_attn.k_proj
low_ranking ...
using low rank =  512
28 self_attn.v_proj
low_ranking ...
using low rank =  512
28 self_attn.q_proj
low_ranking ...
using low rank =  512
28 self_attn.o_proj
low_ranking ...
using low rank =  512
28 mlp.up_proj
low_ranking ...
28 mlp.down_proj
low_ranking ...
28 mlp.gate_proj
low_ranking ...
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
5472 MiB free out of 48676 MiB total
after cast to cpu
10596 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
29 self_attn.k_proj
low_ranking ...
using low rank =  512
29 self_attn.v_proj
low_ranking ...
using low rank =  512
29 self_attn.q_proj
low_ranking ...
using low rank =  512
29 self_attn.o_proj
low_ranking ...
using low rank =  512
29 mlp.up_proj
low_ranking ...
29 mlp.down_proj
low_ranking ...
29 mlp.gate_proj
low_ranking ...
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
4448 MiB free out of 48676 MiB total
after cast to cpu
9572 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
30 self_attn.k_proj
low_ranking ...
using low rank =  512
30 self_attn.v_proj
low_ranking ...
using low rank =  512
30 self_attn.q_proj
low_ranking ...
using low rank =  512
30 self_attn.o_proj
low_ranking ...
using low rank =  512
30 mlp.up_proj
low_ranking ...
30 mlp.down_proj
low_ranking ...
30 mlp.gate_proj
low_ranking ...
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
3424 MiB free out of 48676 MiB total
after cast to cpu
8548 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
31 self_attn.k_proj
low_ranking ...
using low rank =  512
31 self_attn.v_proj
low_ranking ...
using low rank =  512
31 self_attn.q_proj
low_ranking ...
using low rank =  512
31 self_attn.o_proj
low_ranking ...
using low rank =  512
31 mlp.up_proj
low_ranking ...
31 mlp.down_proj
low_ranking ...
31 mlp.gate_proj
low_ranking ...
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
2400 MiB free out of 48676 MiB total
after cast to cpu
7524 MiB free out of 48676 MiB total
Total bits: 77846282240 Total params: 6476005376
average bits per value: 12.020725388601036
total time taken: 1079.6350259780884
Loading tokenizer for meta-llama/Llama-2-7b-hf
/home/lliu/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Dataset: wikitext2
Evaluating ...
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
Perplexity: 4990.756348
