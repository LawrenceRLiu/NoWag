/home/lliu/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.24s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.26it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.16it/s]
Model loaded. LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
4096
Loading tokenizer for meta-llama/Llama-2-7b-hf
Starting...
39011 MiB free out of 48676 MiB total
attention_mask torch.Size([1, 1, 4096, 4096]) cuda:4 torch.float16
position_ids torch.Size([1, 4096]) cuda:4 torch.int64
position_ids torch.Size([1, 4096])
attention_mask torch.Size([1, 1, 4096, 4096])
Ready.
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
0 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.1756, 0.8264, 0.3096,  ..., 0.5891, 0.6551, 0.5814], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.0278, 1.4363, 1.2937,  ..., 1.3905, 1.6662, 0.9913], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
0 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([0.7236, 0.7618, 0.5036,  ..., 0.7654, 0.7673, 0.6849], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.4949, 0.5030, 0.4779,  ..., 0.4897, 0.5060, 0.5031], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
0 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([0.8849, 0.6529, 0.2560,  ..., 0.5371, 0.5559, 0.4512], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.6149, 1.0973, 1.2322,  ..., 1.2838, 1.3711, 0.9672], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
0 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([0.2801, 0.2821, 0.2653,  ..., 0.2623, 0.2674, 0.2648], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.4009, 0.4159, 0.4015,  ..., 0.4328, 0.4183, 0.4196], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
0 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 85737744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 7.585671852439191e-05 val loss: 7.30494643903512e-05
9021 MiB free out of 48676 MiB total
epoch 1 loss: 3.874656732705262e-05 val loss: 9.213116982209613e-05
9013 MiB free out of 48676 MiB total
epoch 2 loss: 2.7163371711935724e-05 val loss: 0.00010411629409645684
9013 MiB free out of 48676 MiB total
epoch 3 loss: 1.9795468460870325e-05 val loss: 0.00011310502077321871
9013 MiB free out of 48676 MiB total
epoch 4 loss: 1.5070127076910467e-05 val loss: 0.0001198773315991275
9013 MiB free out of 48676 MiB total
epoch 5 loss: 1.2165934563768133e-05 val loss: 0.00012507462815847248
9013 MiB free out of 48676 MiB total
early stopping
39011 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
9013 MiB free out of 48676 MiB total
after cast to cpu
38501 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
1 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([2.0254, 1.9664, 2.0364,  ..., 1.6627, 1.8973, 1.8084], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.5419, 1.7741, 1.8750,  ..., 0.6519, 0.6390, 0.5201], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
1 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([0.5438, 0.5367, 0.5507,  ..., 0.6505, 0.5752, 0.6442], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.8513, 0.8723, 0.8107,  ..., 0.3544, 0.3659, 0.3639], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
1 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.9255, 1.9044, 1.8602,  ..., 1.6792, 1.9344, 1.6705], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.9452, 1.6639, 1.9007,  ..., 0.3805, 0.3928, 0.4772], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
1 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([0.8602, 0.8032, 0.6896,  ..., 0.2113, 0.2166, 0.2192], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.5290, 0.5332, 0.5174,  ..., 0.5289, 0.5185, 0.5111], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
1 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 85737744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.027467420482025773 val loss: 0.0005705051844415721
8925 MiB free out of 48676 MiB total
epoch 1 loss: 0.0005696281679092863 val loss: 0.0005912748638365883
8917 MiB free out of 48676 MiB total
epoch 2 loss: 0.00024071057077890146 val loss: 0.0006126931111793965
8917 MiB free out of 48676 MiB total
epoch 3 loss: 0.0001466074771769854 val loss: 0.0006491544882010203
8917 MiB free out of 48676 MiB total
epoch 4 loss: 0.0001253413070685383 val loss: 0.0006998089302214794
8917 MiB free out of 48676 MiB total
epoch 5 loss: 0.00011276772730184348 val loss: 0.000746598161640577
8917 MiB free out of 48676 MiB total
early stopping
38501 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8917 MiB free out of 48676 MiB total
after cast to cpu
38469 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
2 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.7840, 1.7515, 1.8243,  ..., 1.7953, 1.7517, 1.7821], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.7197, 1.3612, 1.5470,  ..., 2.5627, 0.6336, 2.3330], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
2 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([0.9290, 0.9331, 0.9195,  ..., 0.9050, 0.9574, 0.9601], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.9509, 0.9649, 0.9703,  ..., 0.8383, 0.9783, 0.9524], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
2 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.6843, 1.7193, 1.6812,  ..., 1.7487, 1.6237, 1.6486], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.0094, 1.2899, 1.5509,  ..., 2.1117, 0.7452, 2.0198], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
2 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([0.9219, 0.9821, 0.9457,  ..., 0.9478, 0.9625, 0.9246], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.8827, 0.8924, 0.8919,  ..., 0.8741, 0.8675, 0.8812], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
2 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 85737744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0009508978328085504 val loss: 0.0018571321488707326
8925 MiB free out of 48676 MiB total
epoch 1 loss: 0.00040456155443280295 val loss: 0.0018506802443880588
8917 MiB free out of 48676 MiB total
epoch 2 loss: 0.00034531689732375526 val loss: 0.0018475144897820428
8917 MiB free out of 48676 MiB total
epoch 3 loss: 0.00031342718511950807 val loss: 0.001844787344452925
8917 MiB free out of 48676 MiB total
epoch 4 loss: 0.00029375320241342706 val loss: 0.0018490441652829759
8917 MiB free out of 48676 MiB total
epoch 5 loss: 0.0002768148455061237 val loss: 0.0018477401899872348
8917 MiB free out of 48676 MiB total
epoch 6 loss: 0.0002640965886939739 val loss: 0.0018500464429962449
8917 MiB free out of 48676 MiB total
epoch 7 loss: 0.00025271384185998613 val loss: 0.0018499137222534046
8917 MiB free out of 48676 MiB total
epoch 8 loss: 0.0002459173409761206 val loss: 0.001848430409154389
8917 MiB free out of 48676 MiB total
early stopping
38469 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8917 MiB free out of 48676 MiB total
after cast to cpu
38501 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
3 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.6463, 1.6737, 1.6654,  ..., 1.6477, 1.6560, 1.6845], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.1294, 1.3448, 1.3066,  ..., 2.7185, 2.9419, 3.0465], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
3 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([0.8999, 0.8761, 0.8859,  ..., 0.8799, 0.8948, 0.8878], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.8146, 0.8202, 0.8095,  ..., 0.4050, 0.4349, 0.3835], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
3 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.5567, 1.6199, 1.6101,  ..., 1.6199, 1.6003, 1.6185], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.1710, 1.3686, 1.3253,  ..., 2.6906, 2.8910, 3.0339], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
3 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([0.8149, 0.8113, 0.7834,  ..., 0.3370, 0.3588, 0.3251], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.8344, 0.8513, 0.8366,  ..., 0.8515, 0.8315, 0.8545], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
3 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 85737744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.005559518440350075 val loss: 0.005706973373889923
8925 MiB free out of 48676 MiB total
epoch 1 loss: 0.001891454996439279 val loss: 0.005736797640565783
8917 MiB free out of 48676 MiB total
epoch 2 loss: 0.0014175478881952586 val loss: 0.0057416733470745385
8917 MiB free out of 48676 MiB total
epoch 3 loss: 0.0011682735103022424 val loss: 0.0057318203907925636
8917 MiB free out of 48676 MiB total
epoch 4 loss: 0.0010243316323794716 val loss: 0.005718027125112712
8917 MiB free out of 48676 MiB total
epoch 5 loss: 0.0009355523461636039 val loss: 0.005706014868337661
8917 MiB free out of 48676 MiB total
early stopping
38501 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8917 MiB free out of 48676 MiB total
after cast to cpu
38469 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
4 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.6928, 1.7265, 1.7171,  ..., 1.6686, 1.7117, 1.7124], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.9987, 1.3206, 1.3897,  ..., 2.4271, 2.2609, 2.4617], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
4 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([0.9338, 0.9049, 0.9457,  ..., 0.9385, 0.9042, 0.9126], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.8676, 0.8579, 0.8606,  ..., 0.9248, 0.9169, 0.9350], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
4 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.6591, 1.6807, 1.6291,  ..., 1.6611, 1.7065, 1.7137], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.9198, 1.3477, 1.4289,  ..., 2.3017, 2.2600, 2.2052], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
4 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([0.8375, 0.8274, 0.8368,  ..., 0.9084, 0.9081, 0.9244], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.8678, 0.8990, 0.9099,  ..., 0.8872, 0.8625, 0.8959], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
4 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 85737744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.009139806850726018 val loss: 0.005793390300823376
8925 MiB free out of 48676 MiB total
epoch 1 loss: 0.0026065868223668076 val loss: 0.005792565469164401
8917 MiB free out of 48676 MiB total
epoch 2 loss: 0.0018695275830395985 val loss: 0.005799677746836096
8917 MiB free out of 48676 MiB total
epoch 3 loss: 0.0014955915585233015 val loss: 0.005800638115033507
8917 MiB free out of 48676 MiB total
epoch 4 loss: 0.0012760487711602764 val loss: 0.005804069340229034
8917 MiB free out of 48676 MiB total
epoch 5 loss: 0.0011287586476100842 val loss: 0.005802653584396467
8917 MiB free out of 48676 MiB total
early stopping
38469 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8917 MiB free out of 48676 MiB total
after cast to cpu
38501 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
5 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.7228, 1.7929, 1.7542,  ..., 1.6983, 1.7302, 1.7280], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.9647, 1.0825, 1.1445,  ..., 1.4308, 2.6816, 2.3936], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
5 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([0.9815, 0.9241, 0.9573,  ..., 0.9756, 0.9689, 0.9269], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.9928, 0.9724, 0.9818,  ..., 0.8969, 0.9049, 0.9102], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
5 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.6849, 1.6923, 1.6421,  ..., 1.6521, 1.6664, 1.7371], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.0021, 1.0970, 1.1329,  ..., 1.7344, 1.9421, 1.6516], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
5 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([0.9817, 0.9510, 0.9640,  ..., 0.8447, 0.8656, 0.8768], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.9241, 0.9280, 0.9083,  ..., 0.9133, 0.8996, 0.9203], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
5 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 85737744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.01295257229685376 val loss: 0.007350254862103611
8925 MiB free out of 48676 MiB total
epoch 1 loss: 0.003897278407748672 val loss: 0.007325498387217522
8917 MiB free out of 48676 MiB total
epoch 2 loss: 0.0027671316820487846 val loss: 0.007302019250346348
8917 MiB free out of 48676 MiB total
epoch 3 loss: 0.0022128194195829565 val loss: 0.007285827363375574
8917 MiB free out of 48676 MiB total
epoch 4 loss: 0.0018767788833429222 val loss: 0.007264128653332591
8917 MiB free out of 48676 MiB total
epoch 5 loss: 0.0016570707575738197 val loss: 0.00724119643564336
8917 MiB free out of 48676 MiB total
epoch 6 loss: 0.0014982394136495714 val loss: 0.0072266449278686196
8917 MiB free out of 48676 MiB total
epoch 7 loss: 0.0013746222916779516 val loss: 0.007212124852230772
8917 MiB free out of 48676 MiB total
epoch 8 loss: 0.00127559875818406 val loss: 0.007196295715402812
8917 MiB free out of 48676 MiB total
epoch 9 loss: 0.0011942305027332623 val loss: 0.007184510497609153
8917 MiB free out of 48676 MiB total
38501 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8917 MiB free out of 48676 MiB total
after cast to cpu
38469 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
6 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.6115, 1.6701, 1.6337,  ..., 1.6253, 1.6097, 1.6225], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.2987, 1.3126, 1.3289,  ..., 2.0024, 2.0803, 2.1513], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
6 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([0.9260, 0.8385, 0.8766,  ..., 0.9176, 0.8962, 0.8614], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.9808, 0.9636, 0.9873,  ..., 0.9508, 0.9518, 0.9481], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
6 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.5404, 1.5738, 1.6013,  ..., 1.5337, 1.5906, 1.6066], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.3378, 1.3558, 1.3186,  ..., 2.1034, 2.0254, 2.1746], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
6 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([0.9574, 0.9658, 0.9658,  ..., 0.9132, 0.9029, 0.9161], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.8659, 0.8919, 0.8469,  ..., 0.8721, 0.8547, 0.8473], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
6 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 85737744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.02007581005091197 val loss: 0.006714078481309116
8925 MiB free out of 48676 MiB total
epoch 1 loss: 0.006109047908466891 val loss: 0.006706528045469895
8917 MiB free out of 48676 MiB total
epoch 2 loss: 0.004196136920654681 val loss: 0.006704157072817907
8917 MiB free out of 48676 MiB total
epoch 3 loss: 0.003278887953456433 val loss: 0.006702642946038395
8917 MiB free out of 48676 MiB total
epoch 4 loss: 0.002739595324783295 val loss: 0.006701097154291347
8917 MiB free out of 48676 MiB total
epoch 5 loss: 0.0023832533588574734 val loss: 0.006700961064780131
8917 MiB free out of 48676 MiB total
epoch 6 loss: 0.0021294744246915798 val loss: 0.006701145524857566
8917 MiB free out of 48676 MiB total
epoch 7 loss: 0.0019423134208409465 val loss: 0.006702855316689238
8917 MiB free out of 48676 MiB total
epoch 8 loss: 0.0017904800188262016 val loss: 0.006704800442093983
8917 MiB free out of 48676 MiB total
epoch 9 loss: 0.0016651729611112387 val loss: 0.006705172883812338
8917 MiB free out of 48676 MiB total
early stopping
38469 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8917 MiB free out of 48676 MiB total
after cast to cpu
38501 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
7 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.5728, 1.6454, 1.5849,  ..., 1.5643, 1.5614, 1.5966], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.7458, 0.7266, 0.7555,  ..., 1.9018, 2.3899, 2.3772], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
7 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([0.9560, 0.8397, 0.8979,  ..., 0.9439, 0.9081, 0.8830], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.9850, 0.9593, 0.9277,  ..., 0.8599, 0.8627, 0.8792], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
7 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.5591, 1.5747, 1.5777,  ..., 1.5886, 1.5985, 1.6013], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.7524, 0.7359, 0.7657,  ..., 2.0297, 2.7848, 2.7748], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
7 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.0083, 0.9239, 0.9096,  ..., 0.8438, 0.8473, 0.8590], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.8733, 0.9017, 0.8406,  ..., 0.8633, 0.8822, 0.8546], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
7 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 85737744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.02266242895711912 val loss: 0.009905946382787079
8925 MiB free out of 48676 MiB total
epoch 1 loss: 0.007525155378971249 val loss: 0.009961529460269958
8917 MiB free out of 48676 MiB total
epoch 2 loss: 0.005139014747328474 val loss: 0.010025359923020005
8917 MiB free out of 48676 MiB total
epoch 3 loss: 0.00397632023486949 val loss: 0.01007877814117819
8917 MiB free out of 48676 MiB total
epoch 4 loss: 0.003306363389128819 val loss: 0.010122359788510948
8917 MiB free out of 48676 MiB total
epoch 5 loss: 0.002874132756005565 val loss: 0.010158115008380264
8917 MiB free out of 48676 MiB total
early stopping
38501 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8917 MiB free out of 48676 MiB total
after cast to cpu
38469 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
8 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.6222, 1.6256, 1.6392,  ..., 1.5824, 1.5826, 1.6133], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.8323, 1.0179, 0.9911,  ..., 1.9226, 2.3946, 2.4478], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
8 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([0.9597, 0.8698, 0.8955,  ..., 0.9706, 0.9312, 0.9159], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.0011, 1.0007, 0.9877,  ..., 0.8776, 0.8515, 0.8656], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
8 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.5808, 1.6306, 1.5969,  ..., 1.5672, 1.5837, 1.5975], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.8247, 1.0033, 0.9929,  ..., 2.3955, 3.1897, 3.1726], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
8 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([0.9737, 0.9628, 0.9414,  ..., 0.8193, 0.8024, 0.8064], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.9048, 0.9030, 0.8637,  ..., 0.8852, 0.8910, 0.8782], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
8 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 85737744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.02147719266213244 val loss: 0.01064331125235185
8925 MiB free out of 48676 MiB total
epoch 1 loss: 0.007259339534357423 val loss: 0.01064745883923024
8917 MiB free out of 48676 MiB total
epoch 2 loss: 0.004971973969077226 val loss: 0.010647181537933648
8917 MiB free out of 48676 MiB total
epoch 3 loss: 0.0038588004099437967 val loss: 0.010650014621205628
8917 MiB free out of 48676 MiB total
epoch 4 loss: 0.0032123789405886782 val loss: 0.010652610391844064
8917 MiB free out of 48676 MiB total
epoch 5 loss: 0.002790275466395542 val loss: 0.01065581600414589
8917 MiB free out of 48676 MiB total
early stopping
38469 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8917 MiB free out of 48676 MiB total
after cast to cpu
38501 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
9 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.6808, 1.6457, 1.6952,  ..., 1.6183, 1.6024, 1.6369], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.9793, 1.1443, 1.1281,  ..., 2.1393, 2.2283, 2.2846], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
9 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([0.9727, 0.9112, 0.8904,  ..., 0.9682, 0.9803, 0.9293], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.0864, 1.0820, 1.0906,  ..., 0.8114, 0.8140, 0.7966], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
9 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.5445, 1.6070, 1.5710,  ..., 1.6273, 1.5779, 1.6374], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.0707, 1.1604, 1.1292,  ..., 1.9872, 2.0199, 2.1190], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
9 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.0343, 1.0450, 1.0397,  ..., 0.8190, 0.8186, 0.7914], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.9360, 0.9315, 0.8659,  ..., 0.9257, 0.9022, 0.9244], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
9 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 85737744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.02507398995294352 val loss: 0.013447834586258978
8925 MiB free out of 48676 MiB total
epoch 1 loss: 0.008308058659167727 val loss: 0.013466570060700178
8917 MiB free out of 48676 MiB total
epoch 2 loss: 0.0056711699598963605 val loss: 0.013480393681675196
8917 MiB free out of 48676 MiB total
epoch 3 loss: 0.004373005183879286 val loss: 0.013488426920957863
8917 MiB free out of 48676 MiB total
epoch 4 loss: 0.0036141004093224183 val loss: 0.013490165350958705
8917 MiB free out of 48676 MiB total
epoch 5 loss: 0.0031192376218314166 val loss: 0.013489674136508256
8917 MiB free out of 48676 MiB total
early stopping
38501 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8917 MiB free out of 48676 MiB total
after cast to cpu
38469 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
10 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.6543, 1.6721, 1.6871,  ..., 1.5862, 1.6025, 1.6724], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.7476, 0.8385, 0.8215,  ..., 2.1863, 2.1931, 2.1422], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
10 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([0.9680, 0.9046, 0.8915,  ..., 0.9570, 0.9430, 0.9284], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.1302, 1.1081, 1.1169,  ..., 0.9286, 0.9147, 0.9270], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
10 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.5843, 1.6006, 1.5543,  ..., 1.6136, 1.6117, 1.6147], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.8123, 0.8635, 0.8176,  ..., 2.0155, 2.2036, 2.1027], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
10 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.0892, 1.0734, 1.0835,  ..., 0.8955, 0.8985, 0.8911], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.9230, 0.9006, 0.8636,  ..., 0.9054, 0.9069, 0.8883], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
10 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 85737744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.02850998870417243 val loss: 0.018247400992549956
8925 MiB free out of 48676 MiB total
epoch 1 loss: 0.00833974986744579 val loss: 0.018194108735769987
8917 MiB free out of 48676 MiB total
epoch 2 loss: 0.005622205366307753 val loss: 0.018162858090363443
8917 MiB free out of 48676 MiB total
epoch 3 loss: 0.004334553161243093 val loss: 0.018135485472157598
8917 MiB free out of 48676 MiB total
epoch 4 loss: 0.003580773854992003 val loss: 0.0181112892460078
8917 MiB free out of 48676 MiB total
epoch 5 loss: 0.00308350539944513 val loss: 0.018089255318045616
8917 MiB free out of 48676 MiB total
epoch 6 loss: 0.002731313803451485 val loss: 0.018067983794026077
8917 MiB free out of 48676 MiB total
epoch 7 loss: 0.002472745995873993 val loss: 0.01805296668317169
8917 MiB free out of 48676 MiB total
epoch 8 loss: 0.002279360338434344 val loss: 0.018073642975650728
8917 MiB free out of 48676 MiB total
epoch 9 loss: 0.0020927392306475667 val loss: 0.018094351864419878
8917 MiB free out of 48676 MiB total
38469 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8917 MiB free out of 48676 MiB total
after cast to cpu
38501 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
11 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.4919, 1.5258, 1.5711,  ..., 1.5032, 1.4571, 1.5529], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.9077, 0.8226, 0.7986,  ..., 1.9173, 1.8759, 1.8916], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
11 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([0.9840, 0.9265, 0.9024,  ..., 1.0096, 0.9917, 0.9475], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.9041, 0.9214, 0.9175,  ..., 1.0224, 1.0506, 1.0277], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
11 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.5355, 1.5699, 1.5003,  ..., 1.5027, 1.5507, 1.5416], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.9395, 0.8253, 0.8040,  ..., 2.0374, 1.9353, 2.0648], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
11 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([0.8975, 0.9156, 0.8993,  ..., 0.9782, 0.9988, 0.9837], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.9541, 0.9431, 0.8961,  ..., 0.9448, 0.9398, 0.9368], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
11 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 85737744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.04011489808908664 val loss: 0.015971758461091667
8925 MiB free out of 48676 MiB total
epoch 1 loss: 0.013289935108332429 val loss: 0.01600936782779172
8917 MiB free out of 48676 MiB total
epoch 2 loss: 0.008869405104633188 val loss: 0.016037909605074674
8917 MiB free out of 48676 MiB total
epoch 3 loss: 0.00676007218498853 val loss: 0.01605555327842012
8917 MiB free out of 48676 MiB total
epoch 4 loss: 0.005546755419345573 val loss: 0.016066667914856225
8917 MiB free out of 48676 MiB total
epoch 5 loss: 0.0047544349126837915 val loss: 0.016083254013210535
8917 MiB free out of 48676 MiB total
early stopping
38501 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8917 MiB free out of 48676 MiB total
after cast to cpu
38469 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
12 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.6344, 1.6304, 1.6164,  ..., 1.5886, 1.5710, 1.6159], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.8726, 0.9940, 1.0412,  ..., 1.7685, 1.8328, 1.6122], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
12 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([0.9805, 0.8820, 0.9145,  ..., 0.9917, 0.9768, 0.9477], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.9254, 0.9355, 0.9365,  ..., 0.7872, 0.8072, 0.7791], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
12 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.5269, 1.5773, 1.5420,  ..., 1.5242, 1.5672, 1.5658], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.8878, 1.0026, 1.0124,  ..., 1.6680, 1.7091, 1.5400], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
12 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([0.9019, 0.9160, 0.9263,  ..., 0.8095, 0.8280, 0.8232], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.9529, 0.9486, 0.9037,  ..., 0.9460, 0.9320, 0.9379], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
12 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 85737744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.03682624165958259 val loss: 0.016577339032664895
8925 MiB free out of 48676 MiB total
epoch 1 loss: 0.013027381297433749 val loss: 0.016500163357704878
8917 MiB free out of 48676 MiB total
epoch 2 loss: 0.00897652842104435 val loss: 0.01647037942893803
8917 MiB free out of 48676 MiB total
epoch 3 loss: 0.006952957033718121 val loss: 0.01647513103671372
8917 MiB free out of 48676 MiB total
epoch 4 loss: 0.005746019802245428 val loss: 0.016491499729454517
8917 MiB free out of 48676 MiB total
epoch 5 loss: 0.004943655734678032 val loss: 0.016507325577549636
8917 MiB free out of 48676 MiB total
epoch 6 loss: 0.004371420491224853 val loss: 0.01651678909547627
8917 MiB free out of 48676 MiB total
epoch 7 loss: 0.003942084704249282 val loss: 0.016514242626726627
8917 MiB free out of 48676 MiB total
early stopping
38469 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8917 MiB free out of 48676 MiB total
after cast to cpu
38501 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
13 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.6126, 1.5780, 1.5905,  ..., 1.5597, 1.5394, 1.5684], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.8244, 1.0660, 1.0900,  ..., 1.8910, 1.9190, 1.7981], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
13 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.0103, 0.9506, 0.9665,  ..., 0.9982, 1.0047, 0.9755], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.9152, 0.9285, 0.9353,  ..., 0.9549, 0.9411, 0.9663], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
13 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.5021, 1.5756, 1.5207,  ..., 1.5473, 1.5423, 1.5433], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.0422, 1.0638, 1.0815,  ..., 1.8074, 1.7660, 1.7255], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
13 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([0.8974, 0.9448, 0.9435,  ..., 0.9521, 0.9254, 0.9538], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.9631, 0.9864, 0.9225,  ..., 0.9686, 0.9673, 0.9634], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
13 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 85737744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.042206241691019386 val loss: 0.020050460007041693
8925 MiB free out of 48676 MiB total
epoch 1 loss: 0.014831819862592965 val loss: 0.02006665093358606
8917 MiB free out of 48676 MiB total
epoch 2 loss: 0.01037539044409641 val loss: 0.020065231947228312
8917 MiB free out of 48676 MiB total
epoch 3 loss: 0.008097548667137744 val loss: 0.020092233549803495
8917 MiB free out of 48676 MiB total
epoch 4 loss: 0.00673309637568309 val loss: 0.020140313543379307
8917 MiB free out of 48676 MiB total
epoch 5 loss: 0.005824883117384161 val loss: 0.020187693648040295
8917 MiB free out of 48676 MiB total
early stopping
38501 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8917 MiB free out of 48676 MiB total
after cast to cpu
38469 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
14 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.6134, 1.5749, 1.5880,  ..., 1.5348, 1.5272, 1.5489], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.0522, 1.4499, 1.4432,  ..., 1.5537, 1.5905, 1.4958], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
14 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([0.9699, 0.9163, 0.9656,  ..., 0.9915, 0.9925, 0.9866], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.9107, 0.9021, 0.9065,  ..., 1.0683, 1.0652, 1.0858], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
14 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.5147, 1.5503, 1.5152,  ..., 1.5625, 1.5397, 1.5450], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.1204, 1.4476, 1.4390,  ..., 1.4891, 1.5953, 1.5113], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
14 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([0.8922, 0.8743, 0.8909,  ..., 1.0444, 1.0262, 1.0497], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.9329, 0.9440, 0.9126,  ..., 0.9653, 0.9494, 0.9803], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
14 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 85737744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.03828714305564063 val loss: 0.020714199752546847
8925 MiB free out of 48676 MiB total
epoch 1 loss: 0.013283471002068836 val loss: 0.020581345772370696
8917 MiB free out of 48676 MiB total
epoch 2 loss: 0.009255691515136277 val loss: 0.020557401003316045
8917 MiB free out of 48676 MiB total
epoch 3 loss: 0.007243069259857293 val loss: 0.020575031521730125
8917 MiB free out of 48676 MiB total
epoch 4 loss: 0.00603290562685288 val loss: 0.02060492755845189
8917 MiB free out of 48676 MiB total
epoch 5 loss: 0.005236823415543768 val loss: 0.020634144311770797
8917 MiB free out of 48676 MiB total
epoch 6 loss: 0.004681745725974906 val loss: 0.020661322865635157
8917 MiB free out of 48676 MiB total
epoch 7 loss: 0.004257104777934728 val loss: 0.02068789303302765
8917 MiB free out of 48676 MiB total
early stopping
38469 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8917 MiB free out of 48676 MiB total
after cast to cpu
38501 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
15 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.6233, 1.5785, 1.5584,  ..., 1.5217, 1.5787, 1.5373], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.8892, 0.9599, 0.9163,  ..., 1.9989, 2.0357, 2.1068], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
15 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.0155, 0.9771, 0.9901,  ..., 1.0429, 1.0140, 1.0064], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.0221, 1.0299, 1.0216,  ..., 0.9816, 0.9779, 0.9640], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
15 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.4933, 1.5343, 1.4894,  ..., 1.5105, 1.5305, 1.5341], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.8819, 0.9525, 0.9162,  ..., 1.9384, 1.9062, 1.9309], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
15 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([0.9999, 1.0097, 1.0048,  ..., 0.9665, 0.9695, 0.9545], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.9702, 0.9879, 0.9669,  ..., 0.9994, 0.9682, 1.0115], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
15 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 85737744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.04456203176960116 val loss: 0.025186599465087056
8925 MiB free out of 48676 MiB total
epoch 1 loss: 0.015156660112552345 val loss: 0.025153558468446136
8917 MiB free out of 48676 MiB total
epoch 2 loss: 0.010480788972927257 val loss: 0.0251597868045792
8917 MiB free out of 48676 MiB total
epoch 3 loss: 0.008181673350918572 val loss: 0.02519135526381433
8917 MiB free out of 48676 MiB total
epoch 4 loss: 0.0068192492399248295 val loss: 0.025225579156540334
8917 MiB free out of 48676 MiB total
epoch 5 loss: 0.005910681147724972 val loss: 0.025261711445637047
8917 MiB free out of 48676 MiB total
epoch 6 loss: 0.005256527774690767 val loss: 0.025290444726124406
8917 MiB free out of 48676 MiB total
early stopping
38501 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8917 MiB free out of 48676 MiB total
after cast to cpu
38469 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
16 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.5899, 1.5800, 1.5417,  ..., 1.5215, 1.5134, 1.5515], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.1845, 1.2345, 1.2825,  ..., 1.7402, 1.7219, 1.7930], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
16 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.0617, 1.0144, 1.0695,  ..., 1.0903, 1.0521, 1.0318], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.2892, 1.2590, 1.2708,  ..., 1.0814, 1.0957, 1.0983], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
16 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.4894, 1.4845, 1.4596,  ..., 1.4817, 1.5148, 1.4989], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.1535, 1.2194, 1.2913,  ..., 1.6515, 1.6507, 1.6518], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
16 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.2393, 1.2085, 1.2276,  ..., 1.0664, 1.0781, 1.0688], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.0223, 1.0249, 1.0036,  ..., 1.0444, 1.0177, 1.0147], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
16 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 85737744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0572290493582841 val loss: 0.037701883586123586
8925 MiB free out of 48676 MiB total
epoch 1 loss: 0.020472667150897905 val loss: 0.03771016327664256
8917 MiB free out of 48676 MiB total
epoch 2 loss: 0.014190338330081431 val loss: 0.03768706344999373
8917 MiB free out of 48676 MiB total
epoch 3 loss: 0.011070283977460349 val loss: 0.037678351160138845
8917 MiB free out of 48676 MiB total
epoch 4 loss: 0.009204288060573163 val loss: 0.037678411696106195
8917 MiB free out of 48676 MiB total
epoch 5 loss: 0.007959012829815038 val loss: 0.03769024205394089
8917 MiB free out of 48676 MiB total
epoch 6 loss: 0.0070647753491357435 val loss: 0.037709262454882264
8917 MiB free out of 48676 MiB total
epoch 7 loss: 0.0063862348870316055 val loss: 0.03772248048335314
8917 MiB free out of 48676 MiB total
epoch 8 loss: 0.00585681013581052 val loss: 0.03775963373482227
8917 MiB free out of 48676 MiB total
early stopping
38469 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8917 MiB free out of 48676 MiB total
after cast to cpu
38501 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
17 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.5578, 1.5840, 1.5218,  ..., 1.4811, 1.4996, 1.5344], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.6770, 0.7048, 0.7073,  ..., 2.2019, 1.6335, 2.1880], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
17 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.0450, 1.0147, 1.0800,  ..., 1.0646, 1.0705, 1.0441], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.9612, 0.9027, 0.9841,  ..., 1.0400, 1.0164, 1.0498], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
17 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.4791, 1.4705, 1.4639,  ..., 1.5067, 1.5329, 1.4977], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.7025, 0.6941, 0.6939,  ..., 2.0938, 2.2398, 2.0421], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
17 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.0241, 1.0220, 0.9856,  ..., 1.0233, 0.9882, 1.0276], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.0365, 1.0348, 1.0110,  ..., 1.0520, 1.0362, 1.0467], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
17 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 85737744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.05648282516631298 val loss: 0.0800482016056776
8925 MiB free out of 48676 MiB total
epoch 1 loss: 0.02004363787273178 val loss: 0.08133826171979308
8917 MiB free out of 48676 MiB total
epoch 2 loss: 0.01392741258314345 val loss: 0.08232138771563768
8917 MiB free out of 48676 MiB total
epoch 3 loss: 0.010888726166740526 val loss: 0.08302602684125304
8917 MiB free out of 48676 MiB total
epoch 4 loss: 0.009050621338246856 val loss: 0.08359614247456193
8917 MiB free out of 48676 MiB total
epoch 5 loss: 0.007813278236426413 val loss: 0.08408390963450074
8917 MiB free out of 48676 MiB total
early stopping
38501 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8917 MiB free out of 48676 MiB total
after cast to cpu
38469 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
18 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.5252, 1.5307, 1.4716,  ..., 1.4937, 1.4664, 1.4463], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.0710, 1.1842, 1.2079,  ..., 2.1710, 2.3075, 2.3298], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
18 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.0789, 1.0596, 1.0864,  ..., 1.1258, 1.1193, 1.1207], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.2060, 1.2129, 1.2298,  ..., 0.9829, 0.9974, 1.0103], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
18 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.4467, 1.4891, 1.4806,  ..., 1.4545, 1.4652, 1.4646], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.0691, 1.1817, 1.1950,  ..., 1.9673, 2.0273, 2.0164], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
18 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.1691, 1.1865, 1.1922,  ..., 0.9777, 0.9909, 0.9968], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.0622, 1.0683, 1.0524,  ..., 1.0709, 1.0725, 1.0642], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
18 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 85737744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.06829154938168358 val loss: 0.042243220610544086
8925 MiB free out of 48676 MiB total
epoch 1 loss: 0.027228129365539644 val loss: 0.04225553525611758
8917 MiB free out of 48676 MiB total
epoch 2 loss: 0.01949332108051749 val loss: 0.04230392538011074
8917 MiB free out of 48676 MiB total
epoch 3 loss: 0.015419734827446518 val loss: 0.04232686385512352
8917 MiB free out of 48676 MiB total
epoch 4 loss: 0.012896850374090718 val loss: 0.04235130315646529
8917 MiB free out of 48676 MiB total
epoch 5 loss: 0.011180606481502764 val loss: 0.0423596128821373
8917 MiB free out of 48676 MiB total
early stopping
38469 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8917 MiB free out of 48676 MiB total
after cast to cpu
38501 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
19 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.4944, 1.5200, 1.4792,  ..., 1.4335, 1.4468, 1.4712], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.8642, 1.1287, 1.1200,  ..., 2.0235, 2.0642, 2.0212], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
19 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.0984, 1.0858, 1.1186,  ..., 1.1277, 1.1063, 1.0958], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.1414, 1.1464, 1.1453,  ..., 1.1046, 1.0892, 1.0853], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
19 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.4172, 1.4455, 1.4177,  ..., 1.4245, 1.4529, 1.4584], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.9218, 1.1143, 1.1046,  ..., 1.9034, 1.8568, 1.8653], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
19 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.1254, 1.1186, 1.1293,  ..., 1.0896, 1.0747, 1.0727], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.0850, 1.0817, 1.0711,  ..., 1.0988, 1.0905, 1.0791], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
19 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 85737744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.06012414120777976 val loss: 0.04266694374382496
8925 MiB free out of 48676 MiB total
epoch 1 loss: 0.023426715153618716 val loss: 0.04255607048980892
8917 MiB free out of 48676 MiB total
epoch 2 loss: 0.01674527703289641 val loss: 0.042468135012313724
8917 MiB free out of 48676 MiB total
epoch 3 loss: 0.013251113836304285 val loss: 0.04237436642870307
8917 MiB free out of 48676 MiB total
epoch 4 loss: 0.011094734301877907 val loss: 0.042282082606107
8917 MiB free out of 48676 MiB total
epoch 5 loss: 0.009622672652767505 val loss: 0.04221493750810623
8917 MiB free out of 48676 MiB total
epoch 6 loss: 0.008556178006983828 val loss: 0.04219893668778241
8917 MiB free out of 48676 MiB total
epoch 7 loss: 0.007763824731227942 val loss: 0.04220360005274415
8917 MiB free out of 48676 MiB total
epoch 8 loss: 0.007134973551728763 val loss: 0.042233253829181194
8917 MiB free out of 48676 MiB total
epoch 9 loss: 0.006594651626073755 val loss: 0.04229791206307709
8917 MiB free out of 48676 MiB total
38501 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8917 MiB free out of 48676 MiB total
after cast to cpu
38469 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
20 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.5040, 1.5277, 1.4980,  ..., 1.4534, 1.4546, 1.4831], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.5292, 0.5662, 0.5834,  ..., 1.8763, 2.0239, 2.0330], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
20 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.1192, 1.0834, 1.1373,  ..., 1.1337, 1.1346, 1.1265], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.0799, 1.1011, 1.0996,  ..., 1.0353, 1.0458, 1.0728], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
20 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.4390, 1.4916, 1.4176,  ..., 1.4356, 1.4500, 1.4538], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.5440, 0.5997, 0.6395,  ..., 1.7327, 1.8238, 1.5132], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
20 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.0672, 1.0759, 1.0829,  ..., 1.0529, 1.0531, 1.0652], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.1243, 1.1123, 1.0981,  ..., 1.0983, 1.1009, 1.1329], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
20 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 85737744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.07803437949041836 val loss: 0.05551075912080705
8925 MiB free out of 48676 MiB total
epoch 1 loss: 0.029489020038454328 val loss: 0.05522783729247749
8917 MiB free out of 48676 MiB total
epoch 2 loss: 0.020726644310343545 val loss: 0.055080021265894175
8917 MiB free out of 48676 MiB total
epoch 3 loss: 0.01626457778547774 val loss: 0.05495279701426625
8917 MiB free out of 48676 MiB total
epoch 4 loss: 0.013572435709647834 val loss: 0.05484562902711332
8917 MiB free out of 48676 MiB total
epoch 5 loss: 0.011766211995563935 val loss: 0.054742442443966866
8917 MiB free out of 48676 MiB total
epoch 6 loss: 0.010467034022440203 val loss: 0.05462669674307108
8917 MiB free out of 48676 MiB total
epoch 7 loss: 0.00949621955078328 val loss: 0.05448903399519622
8917 MiB free out of 48676 MiB total
epoch 8 loss: 0.008744220955122728 val loss: 0.05445925937965512
8917 MiB free out of 48676 MiB total
epoch 9 loss: 0.008052991070144344 val loss: 0.054417080944404006
8917 MiB free out of 48676 MiB total
38469 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8917 MiB free out of 48676 MiB total
after cast to cpu
38501 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
21 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.4550, 1.4713, 1.4318,  ..., 1.4118, 1.4379, 1.4315], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.8120, 0.9468, 0.9507,  ..., 2.0480, 1.7255, 1.9224], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
21 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.1393, 1.1365, 1.1569,  ..., 1.1675, 1.1572, 1.1763], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.0815, 1.0853, 1.1115,  ..., 1.1556, 1.1620, 1.1422], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
21 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.4144, 1.4360, 1.4147,  ..., 1.3882, 1.4228, 1.3867], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.8719, 0.9523, 0.9489,  ..., 1.8164, 1.4733, 1.9335], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
21 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.0900, 1.0793, 1.1069,  ..., 1.1355, 1.1423, 1.1257], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.1539, 1.1705, 1.1258,  ..., 1.1432, 1.1037, 1.1304], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
21 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 85737744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.09061074271448888 val loss: 0.07572657288983464
8925 MiB free out of 48676 MiB total
epoch 1 loss: 0.03464503644499928 val loss: 0.07567091612145305
8917 MiB free out of 48676 MiB total
epoch 2 loss: 0.024364199096453376 val loss: 0.07565614534541965
8917 MiB free out of 48676 MiB total
epoch 3 loss: 0.01903690527251456 val loss: 0.07563332980498672
8917 MiB free out of 48676 MiB total
epoch 4 loss: 0.015808859963726718 val loss: 0.0755903017707169
8917 MiB free out of 48676 MiB total
epoch 5 loss: 0.013644518694491126 val loss: 0.07552782818675041
8917 MiB free out of 48676 MiB total
epoch 6 loss: 0.012088419647625415 val loss: 0.07547190133482218
8917 MiB free out of 48676 MiB total
epoch 7 loss: 0.010907276293437462 val loss: 0.07541585061699152
8917 MiB free out of 48676 MiB total
epoch 8 loss: 0.009973382664611563 val loss: 0.07536434475332499
8917 MiB free out of 48676 MiB total
epoch 9 loss: 0.009220727442880161 val loss: 0.0753211285918951
8917 MiB free out of 48676 MiB total
38501 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8917 MiB free out of 48676 MiB total
after cast to cpu
38469 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
22 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.4745, 1.4774, 1.4902,  ..., 1.4655, 1.4788, 1.4825], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.4680, 1.4894, 1.5025,  ..., 1.2110, 1.9322, 1.9817], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
22 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.1536, 1.1395, 1.1547,  ..., 1.1419, 1.1616, 1.1692], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.2635, 1.2563, 1.2672,  ..., 1.2306, 1.2221, 1.2212], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
22 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.4339, 1.4647, 1.4494,  ..., 1.4377, 1.4301, 1.4364], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.4712, 1.4866, 1.5007,  ..., 1.9845, 1.9318, 2.1921], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
22 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.2228, 1.2182, 1.2207,  ..., 1.1587, 1.1975, 1.1741], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.1274, 1.1286, 1.1356,  ..., 1.1232, 1.1369, 1.1320], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
22 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 85737744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.10502995766000822 val loss: 0.06762875826098025
8925 MiB free out of 48676 MiB total
epoch 1 loss: 0.03940790613705758 val loss: 0.0676474729552865
8917 MiB free out of 48676 MiB total
epoch 2 loss: 0.027342914807377383 val loss: 0.0676607945933938
8917 MiB free out of 48676 MiB total
epoch 3 loss: 0.02112891836441122 val loss: 0.0675890389829874
8917 MiB free out of 48676 MiB total
epoch 4 loss: 0.017459510600019712 val loss: 0.06752564758062363
8917 MiB free out of 48676 MiB total
epoch 5 loss: 0.015054057039378677 val loss: 0.06750780786387622
8917 MiB free out of 48676 MiB total
epoch 6 loss: 0.013337829321244499 val loss: 0.06748177437111735
8917 MiB free out of 48676 MiB total
epoch 7 loss: 0.012006513181404443 val loss: 0.06748175085522234
8917 MiB free out of 48676 MiB total
epoch 8 loss: 0.010931730761512881 val loss: 0.06752550834789872
8917 MiB free out of 48676 MiB total
epoch 9 loss: 0.010142628103494644 val loss: 0.06762335589155555
8917 MiB free out of 48676 MiB total
38469 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8917 MiB free out of 48676 MiB total
after cast to cpu
38501 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
23 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.4515, 1.4864, 1.4677,  ..., 1.4692, 1.4491, 1.4700], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.7387, 0.7335, 0.7738,  ..., 1.5745, 1.3933, 1.6235], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
23 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.2002, 1.1744, 1.2148,  ..., 1.1841, 1.2298, 1.2296], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.1003, 1.0871, 1.1005,  ..., 1.1547, 1.1473, 1.1477], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
23 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.4343, 1.4455, 1.4322,  ..., 1.4610, 1.4012, 1.4335], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.7342, 0.7344, 0.7733,  ..., 1.6962, 1.9142, 1.6451], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
23 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.0890, 1.0794, 1.0946,  ..., 1.1393, 1.1247, 1.1352], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.1980, 1.2021, 1.1762,  ..., 1.1868, 1.1935, 1.1843], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
23 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 85737744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.13923597792745568 val loss: 0.08392072794958949
8925 MiB free out of 48676 MiB total
epoch 1 loss: 0.05284000633400865 val loss: 0.08390676090493798
8917 MiB free out of 48676 MiB total
epoch 2 loss: 0.03702778941806173 val loss: 0.08398707257583737
8917 MiB free out of 48676 MiB total
epoch 3 loss: 0.028683600248768926 val loss: 0.08409102959558368
8917 MiB free out of 48676 MiB total
epoch 4 loss: 0.023643929867830593 val loss: 0.08419115329161286
8917 MiB free out of 48676 MiB total
epoch 5 loss: 0.02029450457484927 val loss: 0.08427223330363631
8917 MiB free out of 48676 MiB total
epoch 6 loss: 0.017909385278471746 val loss: 0.08433839259669185
8917 MiB free out of 48676 MiB total
early stopping
38501 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8917 MiB free out of 48676 MiB total
after cast to cpu
38469 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
24 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.4107, 1.4404, 1.4210,  ..., 1.3930, 1.3714, 1.3905], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.3660, 1.3716, 1.4155,  ..., 1.6762, 1.7466, 1.7519], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
24 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.2120, 1.1798, 1.1826,  ..., 1.2159, 1.1909, 1.2084], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.2397, 1.2433, 1.2546,  ..., 1.1785, 1.1836, 1.1816], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
24 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.3881, 1.3955, 1.3956,  ..., 1.4213, 1.4150, 1.4089], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.3741, 1.3726, 1.4193,  ..., 1.6439, 1.7088, 1.6957], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
24 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.2159, 1.2214, 1.2327,  ..., 1.1572, 1.1644, 1.1607], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.1998, 1.1973, 1.1921,  ..., 1.1950, 1.1950, 1.1825], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
24 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 85737744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.12218030379153788 val loss: 0.09692760417237878
8925 MiB free out of 48676 MiB total
epoch 1 loss: 0.04619356355397031 val loss: 0.09688495937734842
8917 MiB free out of 48676 MiB total
epoch 2 loss: 0.03222847936558537 val loss: 0.09679986769333482
8917 MiB free out of 48676 MiB total
epoch 3 loss: 0.025035917562490795 val loss: 0.09674073569476604
8917 MiB free out of 48676 MiB total
epoch 4 loss: 0.020706367009552196 val loss: 0.09669652860611677
8917 MiB free out of 48676 MiB total
epoch 5 loss: 0.0178238516964484 val loss: 0.09666665038093925
8917 MiB free out of 48676 MiB total
epoch 6 loss: 0.015764401614433154 val loss: 0.0966346594505012
8917 MiB free out of 48676 MiB total
epoch 7 loss: 0.014207455511495937 val loss: 0.09662613179534674
8917 MiB free out of 48676 MiB total
epoch 8 loss: 0.012973211458302103 val loss: 0.09662491036579013
8917 MiB free out of 48676 MiB total
epoch 9 loss: 0.011959561150433728 val loss: 0.09663926530629396
8917 MiB free out of 48676 MiB total
38469 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8917 MiB free out of 48676 MiB total
after cast to cpu
38501 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
25 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.4192, 1.4690, 1.4127,  ..., 1.4213, 1.3962, 1.4222], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.5660, 0.5315, 0.5776,  ..., 1.6343, 1.7744, 1.7085], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
25 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.2550, 1.2244, 1.2299,  ..., 1.2444, 1.2465, 1.2497], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.3124, 1.3050, 1.3245,  ..., 1.2849, 1.3018, 1.2839], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
25 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.4169, 1.3969, 1.4229,  ..., 1.4106, 1.3910, 1.4140], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.5655, 0.5461, 0.5966,  ..., 1.6563, 1.7658, 1.6648], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
25 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.2653, 1.2502, 1.2811,  ..., 1.2720, 1.2811, 1.2801], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.2426, 1.2505, 1.2296,  ..., 1.2162, 1.2488, 1.2485], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
25 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 85737744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.15920561001985334 val loss: 0.1213445053435862
8925 MiB free out of 48676 MiB total
epoch 1 loss: 0.061421067104674876 val loss: 0.1211701943539083
8917 MiB free out of 48676 MiB total
epoch 2 loss: 0.04309271083911881 val loss: 0.12108037434518337
8917 MiB free out of 48676 MiB total
epoch 3 loss: 0.033536922637722455 val loss: 0.12095768051221967
8917 MiB free out of 48676 MiB total
epoch 4 loss: 0.02780231710494263 val loss: 0.1208124365657568
8917 MiB free out of 48676 MiB total
epoch 5 loss: 0.024003815662581474 val loss: 0.12067120056599379
8917 MiB free out of 48676 MiB total
epoch 6 loss: 0.02129262163362 val loss: 0.12053976953029633
8917 MiB free out of 48676 MiB total
epoch 7 loss: 0.01923727977555245 val loss: 0.12042479496449232
8917 MiB free out of 48676 MiB total
epoch 8 loss: 0.01760316829313524 val loss: 0.12031995924189687
8917 MiB free out of 48676 MiB total
epoch 9 loss: 0.016255355832981877 val loss: 0.12023536302149296
8917 MiB free out of 48676 MiB total
38501 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8917 MiB free out of 48676 MiB total
after cast to cpu
38469 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
26 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.3885, 1.4287, 1.3924,  ..., 1.4006, 1.3950, 1.3818], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.1274, 1.1214, 1.1409,  ..., 1.8750, 1.8001, 1.7344], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
26 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.2695, 1.2609, 1.2552,  ..., 1.2922, 1.2361, 1.2380], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.3299, 1.3174, 1.3327,  ..., 1.2755, 1.2536, 1.2610], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
26 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.3872, 1.3984, 1.4091,  ..., 1.3887, 1.3689, 1.4181], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.1334, 1.1177, 1.1413,  ..., 0.8847, 1.7802, 1.8081], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
26 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.3113, 1.3014, 1.3114,  ..., 1.2673, 1.2453, 1.2514], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.2578, 1.2535, 1.2387,  ..., 1.2614, 1.2616, 1.2322], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
26 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 85737744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.15721210383344442 val loss: 0.1465266291052103
8925 MiB free out of 48676 MiB total
epoch 1 loss: 0.06070995629124809 val loss: 0.14571309741586447
8917 MiB free out of 48676 MiB total
epoch 2 loss: 0.042877913801930845 val loss: 0.1453570332378149
8917 MiB free out of 48676 MiB total
epoch 3 loss: 0.03348607724183239 val loss: 0.1450863080099225
8917 MiB free out of 48676 MiB total
epoch 4 loss: 0.027809798368252814 val loss: 0.144852077588439
8917 MiB free out of 48676 MiB total
epoch 5 loss: 0.02403236051031854 val loss: 0.14465399272739887
8917 MiB free out of 48676 MiB total
epoch 6 loss: 0.021325449517462403 val loss: 0.14453281462192535
8917 MiB free out of 48676 MiB total
epoch 7 loss: 0.019269183940195944 val loss: 0.14439173229038715
8917 MiB free out of 48676 MiB total
epoch 8 loss: 0.017623240397369955 val loss: 0.14426160976290703
8917 MiB free out of 48676 MiB total
epoch 9 loss: 0.016266814127448015 val loss: 0.14415812864899635
8917 MiB free out of 48676 MiB total
38469 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8917 MiB free out of 48676 MiB total
after cast to cpu
38501 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
27 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.4432, 1.4596, 1.4588,  ..., 1.4564, 1.4578, 1.4376], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.2920, 1.3262, 1.3285,  ..., 1.5665, 1.5050, 1.6066], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
27 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.2719, 1.2507, 1.2797,  ..., 1.2662, 1.2425, 1.2610], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.2851, 1.2833, 1.2784,  ..., 1.4586, 1.4483, 1.4834], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
27 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.4110, 1.4780, 1.4380,  ..., 1.4506, 1.4507, 1.4779], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.3049, 1.3313, 1.3302,  ..., 1.5344, 1.4997, 1.0326], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
27 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.2766, 1.2844, 1.2753,  ..., 1.5237, 1.4909, 1.5521], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.2326, 1.2740, 1.2605,  ..., 1.2930, 1.2936, 1.2707], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
27 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 85737744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.20212171162711456 val loss: 0.19790760334581137
8925 MiB free out of 48676 MiB total
epoch 1 loss: 0.07613694792962633 val loss: 0.19710396137088537
8917 MiB free out of 48676 MiB total
epoch 2 loss: 0.05279342888388783 val loss: 0.19543076027184725
8917 MiB free out of 48676 MiB total
epoch 3 loss: 0.040687720611458644 val loss: 0.19419395830482244
8917 MiB free out of 48676 MiB total
epoch 4 loss: 0.03346349278581329 val loss: 0.19335735589265823
8917 MiB free out of 48676 MiB total
epoch 5 loss: 0.02871158506604843 val loss: 0.1927955560386181
8917 MiB free out of 48676 MiB total
epoch 6 loss: 0.02534620661754161 val loss: 0.19238762464374304
8917 MiB free out of 48676 MiB total
epoch 7 loss: 0.022815013566287234 val loss: 0.19205630104988813
8917 MiB free out of 48676 MiB total
epoch 8 loss: 0.020818787248572335 val loss: 0.19182762689888477
8917 MiB free out of 48676 MiB total
epoch 9 loss: 0.0191897553668241 val loss: 0.1915634097531438
8917 MiB free out of 48676 MiB total
38501 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8917 MiB free out of 48676 MiB total
after cast to cpu
38469 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
28 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.4238, 1.4306, 1.4203,  ..., 1.4233, 1.4031, 1.4396], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.7822, 0.7447, 0.7523,  ..., 1.7620, 1.7353, 1.7418], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
28 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.2854, 1.2936, 1.3251,  ..., 1.2990, 1.3177, 1.3131], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.4762, 1.4871, 1.5179,  ..., 1.4363, 1.4516, 1.4401], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
28 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.3899, 1.4050, 1.4191,  ..., 1.3927, 1.3823, 1.4056], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.6978, 0.7669, 0.7800,  ..., 1.5473, 1.6398, 1.7314], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
28 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.5029, 1.4821, 1.5049,  ..., 1.4681, 1.4612, 1.4610], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.2865, 1.2987, 1.3002,  ..., 1.3400, 1.3499, 1.3088], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
28 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 85737744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.24158226733561605 val loss: 0.2624198393896222
8925 MiB free out of 48676 MiB total
epoch 1 loss: 0.09079735178966075 val loss: 0.2605845322832465
8917 MiB free out of 48676 MiB total
epoch 2 loss: 0.06274468237825204 val loss: 0.25960980728268623
8917 MiB free out of 48676 MiB total
epoch 3 loss: 0.04816865197790321 val loss: 0.259067602455616
8917 MiB free out of 48676 MiB total
epoch 4 loss: 0.03958332660840824 val loss: 0.25882129650563
8917 MiB free out of 48676 MiB total
epoch 5 loss: 0.034002222113485914 val loss: 0.25897213350981474
8917 MiB free out of 48676 MiB total
epoch 6 loss: 0.03006094594456954 val loss: 0.259189716540277
8917 MiB free out of 48676 MiB total
epoch 7 loss: 0.027087871138064656 val loss: 0.2594272429123521
8917 MiB free out of 48676 MiB total
epoch 8 loss: 0.02473079702758696 val loss: 0.2596566826105118
8917 MiB free out of 48676 MiB total
epoch 9 loss: 0.02279227904364234 val loss: 0.2599563570693135
8917 MiB free out of 48676 MiB total
early stopping
38469 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8917 MiB free out of 48676 MiB total
after cast to cpu
38501 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
29 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.3682, 1.4166, 1.3816,  ..., 1.4090, 1.3446, 1.3857], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.9166, 0.9375, 0.9786,  ..., 1.5665, 1.7960, 1.9091], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
29 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.3328, 1.2889, 1.3329,  ..., 1.2950, 1.3074, 1.3143], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.2784, 1.2410, 1.2657,  ..., 1.2642, 1.2732, 1.2683], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
29 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.3790, 1.3933, 1.3722,  ..., 1.3567, 1.3229, 1.3394], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.8758, 0.9288, 0.9866,  ..., 1.5234, 1.7083, 1.3679], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
29 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.2741, 1.2564, 1.2702,  ..., 1.2577, 1.2643, 1.2598], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.3297, 1.2958, 1.2935,  ..., 1.3316, 1.3571, 1.3266], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
29 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 85737744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.21319096072693355 val loss: 0.9843608140945435
8925 MiB free out of 48676 MiB total
epoch 1 loss: 0.08138650815817527 val loss: 1.0364257134497166
8917 MiB free out of 48676 MiB total
epoch 2 loss: 0.056540194258559495 val loss: 1.0718034766614437
8917 MiB free out of 48676 MiB total
epoch 3 loss: 0.04370828182436526 val loss: 1.0978709682822227
8917 MiB free out of 48676 MiB total
epoch 4 loss: 0.036038091842783615 val loss: 1.1122590340673923
8917 MiB free out of 48676 MiB total
epoch 5 loss: 0.03099372015276458 val loss: 1.1224939301609993
8917 MiB free out of 48676 MiB total
early stopping
38501 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8917 MiB free out of 48676 MiB total
after cast to cpu
38469 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
30 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.4019, 1.4178, 1.3976,  ..., 1.3864, 1.3602, 1.3972], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.0182, 1.0110, 1.0278,  ..., 1.4493, 1.4746, 1.4525], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
30 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.3517, 1.3246, 1.3371,  ..., 1.3470, 1.3623, 1.3652], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.3536, 1.3972, 1.3478,  ..., 1.4740, 1.4633, 1.5104], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
30 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.3731, 1.4261, 1.3994,  ..., 1.3547, 1.3360, 1.3441], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.0392, 1.0339, 1.0422,  ..., 1.4349, 1.4045, 1.4240], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
30 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.3750, 1.4141, 1.3648,  ..., 1.4676, 1.4893, 1.4816], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.3430, 1.3294, 1.3135,  ..., 1.3429, 1.3585, 1.3574], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
30 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 85737744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 1.8620593550149351 val loss: 73.02658156305552
8925 MiB free out of 48676 MiB total
epoch 1 loss: 0.13031124806730077 val loss: 68.86974281072617
8917 MiB free out of 48676 MiB total
epoch 2 loss: 0.0887647368654143 val loss: 67.04571446031332
8917 MiB free out of 48676 MiB total
epoch 3 loss: 0.06796824242337607 val loss: 68.77907121181488
8917 MiB free out of 48676 MiB total
epoch 4 loss: 0.055534089682623744 val loss: 67.35261699557304
8917 MiB free out of 48676 MiB total
epoch 5 loss: 0.04716397568699904 val loss: 68.3947594165802
8917 MiB free out of 48676 MiB total
epoch 6 loss: 0.04125457761983853 val loss: 69.64515552669764
8917 MiB free out of 48676 MiB total
epoch 7 loss: 0.03686215575726237 val loss: 69.53799995034933
8917 MiB free out of 48676 MiB total
early stopping
38469 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8917 MiB free out of 48676 MiB total
after cast to cpu
38501 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
31 self_attn.k_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.4408, 1.4562, 1.4368,  ..., 1.4340, 1.4106, 1.4039], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.9789, 0.9707, 1.0469,  ..., 1.6500, 1.6098, 1.6455], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
31 self_attn.v_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.1832, 1.2165, 1.2963,  ..., 1.1931, 1.2608, 1.2214], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.1387, 1.1403, 1.1530,  ..., 1.1925, 1.2135, 1.2031], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
31 self_attn.q_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.3578, 1.4003, 1.4142,  ..., 1.4286, 1.3539, 1.3484], device='cuda:4')]
sparse_frac =  0.01
[tensor([0.9185, 0.9745, 1.0542,  ..., 1.5913, 1.5933, 1.6090], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
31 self_attn.o_proj
Pruning ...
using low rank =  512
0.01 ['weight'] 0.01 ['weight']
[tensor([1.2308, 1.2274, 1.2312,  ..., 1.2517, 1.2610, 1.2420], device='cuda:4')]
sparse_frac =  0.01
[tensor([1.1774, 1.2117, 1.2020,  ..., 1.2123, 1.2554, 1.2226], device='cuda:4')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
31 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 85737744
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.19595782354008406 val loss: 26.379242300987244
8925 MiB free out of 48676 MiB total
epoch 1 loss: 0.05761269459617324 val loss: 25.49755549430847
8917 MiB free out of 48676 MiB total
epoch 2 loss: 0.038813251987448893 val loss: 25.31865918636322
8917 MiB free out of 48676 MiB total
epoch 3 loss: 0.029645797447301447 val loss: 25.195823311805725
8917 MiB free out of 48676 MiB total
epoch 4 loss: 0.02428539797256235 val loss: 25.118956208229065
8917 MiB free out of 48676 MiB total
epoch 5 loss: 0.020874242502031848 val loss: 25.093969702720642
8917 MiB free out of 48676 MiB total
epoch 6 loss: 0.018489006695745047 val loss: 24.973408699035645
8917 MiB free out of 48676 MiB total
epoch 7 loss: 0.01678270909906132 val loss: 24.798404455184937
8917 MiB free out of 48676 MiB total
epoch 8 loss: 0.01569017887231894 val loss: 24.565403699874878
8917 MiB free out of 48676 MiB total
epoch 9 loss: 0.014470836438704282 val loss: 24.37854528427124
8917 MiB free out of 48676 MiB total
38501 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8917 MiB free out of 48676 MiB total
after cast to cpu
38469 MiB free out of 48676 MiB total
Total bits: tensor(43901829120, device='cuda:4') Total params: 6476005376
average bits per value: tensor(6.7792, device='cuda:4')
total time taken: 7501.997998952866
Loading tokenizer for meta-llama/Llama-2-7b-hf
/home/lliu/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Dataset: wikitext2
Evaluating ...
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
Perplexity: 100667.476562
