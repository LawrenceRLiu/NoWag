/home/lliu/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.12s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.32it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.23it/s]
Model loaded. LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
4096
Loading tokenizer for meta-llama/Llama-2-7b-hf
Starting...
40099 MiB free out of 48676 MiB total
attention_mask torch.Size([1, 1, 4096, 4096]) cuda:7 torch.float16
position_ids torch.Size([1, 4096]) cuda:7 torch.int64
position_ids torch.Size([1, 4096])
attention_mask torch.Size([1, 1, 4096, 4096])
Ready.
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
layer0: self_attn.q_proj
norm_0 tensor([0.8848, 0.6528, 0.2561,  ..., 0.5371, 0.5562, 0.4512], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.7427, 1.1660, 1.0791,  ..., 1.8926, 1.8721, 1.3779], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 1.2158797979354858, val loss None, lr 0.001
iter 10, train loss 1.0846924781799316, val loss None, lr 0.001
iter 20, train loss 1.0027239322662354, val loss None, lr 0.0009000000000000001
iter 30, train loss 1.0504851341247559, val loss None, lr 0.0006561000000000001
iter 40, train loss 1.007501244544983, val loss None, lr 0.0004782969
iter 50, train loss 0.9262599945068359, val loss None, lr 0.00043046721
iter 60, train loss 0.8700511455535889, val loss None, lr 0.000387420489
iter 70, train loss 0.8021441698074341, val loss None, lr 0.0003486784401
iter 80, train loss 0.7577221393585205, val loss None, lr 0.00031381059609000004
iter 90, train loss 0.7269337177276611, val loss None, lr 0.00028242953648100003
best loss 0.7169129252433777
layer0: self_attn.k_proj
norm_0 tensor([1.1758, 0.8267, 0.3096,  ..., 0.5889, 0.6553, 0.5815], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.8984, 1.2832, 1.1631,  ..., 0.9399, 1.2178, 0.8291], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 0.9143508672714233, val loss None, lr 0.001
iter 10, train loss 0.7954849004745483, val loss None, lr 0.0009000000000000001
iter 20, train loss 0.7319494485855103, val loss None, lr 0.0009000000000000001
iter 30, train loss 0.7112715244293213, val loss None, lr 0.0008100000000000001
iter 40, train loss 0.6914941668510437, val loss None, lr 0.0006561000000000001
iter 50, train loss 0.6711727380752563, val loss None, lr 0.000531441
iter 60, train loss 0.6570463180541992, val loss None, lr 0.0004782969
iter 70, train loss 0.6262837052345276, val loss None, lr 0.00043046721
iter 80, train loss 0.6308207511901855, val loss None, lr 0.00031381059609000004
iter 90, train loss 0.5961980223655701, val loss None, lr 0.00031381059609000004
best loss 0.5836174488067627
layer0: self_attn.v_proj
norm_0 tensor([0.7236, 0.7617, 0.5034,  ..., 0.7656, 0.7676, 0.6851], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.6997, 0.7095, 0.6719,  ..., 0.6890, 0.7085, 0.7065], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 0.09465141594409943, val loss None, lr 0.001
iter 10, train loss 0.08290598541498184, val loss None, lr 0.0009000000000000001
iter 20, train loss 0.08008445054292679, val loss None, lr 0.0006561000000000001
iter 30, train loss 0.07498376071453094, val loss None, lr 0.0006561000000000001
iter 40, train loss 0.07240993529558182, val loss None, lr 0.0006561000000000001
iter 50, train loss 0.07158765196800232, val loss None, lr 0.00059049
iter 60, train loss 0.07160775363445282, val loss None, lr 0.0004782969
iter 70, train loss 0.06971672177314758, val loss None, lr 0.0004782969
iter 80, train loss 0.06978359073400497, val loss None, lr 0.0003486784401
iter 90, train loss 0.06887075304985046, val loss None, lr 0.0003486784401
best loss 0.06874524056911469
layer0: self_attn.o_proj
norm_0 tensor([0.2800, 0.2820, 0.2651,  ..., 0.2622, 0.2673, 0.2649], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.8887, 0.9336, 0.9106,  ..., 0.9902, 0.9199, 0.9688], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 0.010639030486345291, val loss None, lr 0.001
iter 10, train loss 0.00962851196527481, val loss None, lr 0.0009000000000000001
iter 20, train loss 0.009409859776496887, val loss None, lr 0.0006561000000000001
iter 30, train loss 0.008905657567083836, val loss None, lr 0.00059049
iter 40, train loss 0.008444427512586117, val loss None, lr 0.00059049
iter 50, train loss 0.008557084947824478, val loss None, lr 0.0004782969
iter 60, train loss 0.008340716361999512, val loss None, lr 0.000387420489
iter 70, train loss 0.008116870187222958, val loss None, lr 0.000387420489
iter 80, train loss 0.008078556507825851, val loss None, lr 0.0003486784401
iter 90, train loss 0.007998190820217133, val loss None, lr 0.0003486784401
best loss 0.007951216772198677
layer0: mlp.gate_proj
norm_0 tensor([1.6787, 1.7383, 1.7090,  ..., 1.7266, 1.7061, 1.7061], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.6108, 0.6123, 0.6055,  ..., 0.6016, 0.6216, 0.6255], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 3.913414478302002, val loss None, lr 0.001
iter 10, train loss 4.093822479248047, val loss None, lr 0.000729
iter 20, train loss 3.916024684906006, val loss None, lr 0.000531441
iter 30, train loss 3.8952372074127197, val loss None, lr 0.000387420489
iter 40, train loss 3.885502338409424, val loss None, lr 0.00025418658283290005
iter 50, train loss 3.89107346534729, val loss None, lr 0.00018530201888518417
iter 60, train loss 3.8864753246307373, val loss None, lr 0.0001350851717672993
iter 70, train loss 3.865142583847046, val loss None, lr 8.862938119652506e-05
iter 80, train loss 3.867575168609619, val loss None, lr 6.461081889226677e-05
iter 90, train loss 3.85846209526062, val loss None, lr 4.7101286972462485e-05
best loss 3.7355499267578125
layer0: mlp.up_proj
norm_0 tensor([1.6846, 1.6572, 1.7002,  ..., 1.7119, 1.7070, 1.6826], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.6309, 0.6152, 0.6260,  ..., 0.5967, 0.5894, 0.6016], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 3.55928373336792, val loss None, lr 0.001
iter 10, train loss 3.775834321975708, val loss None, lr 0.000729
iter 20, train loss 3.5727782249450684, val loss None, lr 0.000531441
iter 30, train loss 3.5639257431030273, val loss None, lr 0.000387420489
iter 40, train loss 3.5568959712982178, val loss None, lr 0.00025418658283290005
iter 50, train loss 3.5446279048919678, val loss None, lr 0.00018530201888518417
iter 60, train loss 3.543055534362793, val loss None, lr 0.0001350851717672993
iter 70, train loss 3.534613847732544, val loss None, lr 8.862938119652506e-05
iter 80, train loss 3.532475233078003, val loss None, lr 6.461081889226677e-05
iter 90, train loss 3.5308468341827393, val loss None, lr 4.7101286972462485e-05
best loss 3.4052915573120117
layer0: mlp.down_proj
norm_0 tensor([1.0811, 1.0771, 1.0850,  ..., 1.0762, 1.0713, 1.0908], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.6289, 1.6006, 1.6270,  ..., 1.6133, 1.6123, 1.5957], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 0.010795352049171925, val loss None, lr 0.001
iter 10, train loss 0.012044742703437805, val loss None, lr 0.0008100000000000001
iter 20, train loss 0.011671962216496468, val loss None, lr 0.00059049
iter 30, train loss 0.011351259425282478, val loss None, lr 0.00043046721
iter 40, train loss 0.011232768185436726, val loss None, lr 0.00028242953648100003
iter 50, train loss 0.01116756908595562, val loss None, lr 0.00020589113209464906
iter 60, train loss 0.01105300523340702, val loss None, lr 0.0001500946352969992
iter 70, train loss 0.011003783904016018, val loss None, lr 9.847709021836118e-05
iter 80, train loss 0.010975344106554985, val loss None, lr 7.17897987691853e-05
iter 90, train loss 0.010945754125714302, val loss None, lr 5.233476330273609e-05
best loss 0.010184906423091888
40099 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
32049 MiB free out of 48676 MiB total
after cast to cpu
37397 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
layer1: self_attn.q_proj
norm_0 tensor([1.9258, 1.9043, 1.8604,  ..., 1.6787, 1.9346, 1.6709], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.1504, 0.9507, 1.1025,  ..., 0.2090, 0.2136, 0.2512], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 16.828659057617188, val loss None, lr 0.001
iter 10, train loss 14.204252243041992, val loss None, lr 0.001
iter 20, train loss 16.412500381469727, val loss None, lr 0.000729
iter 30, train loss 14.627845764160156, val loss None, lr 0.0004782969
iter 40, train loss 13.795933723449707, val loss None, lr 0.0003486784401
iter 50, train loss 13.374321937561035, val loss None, lr 0.0003486784401
iter 60, train loss 13.393083572387695, val loss None, lr 0.00031381059609000004
iter 70, train loss 13.080036163330078, val loss None, lr 0.00025418658283290005
iter 80, train loss 12.968331336975098, val loss None, lr 0.00020589113209464906
iter 90, train loss 12.714790344238281, val loss None, lr 0.00018530201888518417
best loss 12.468899726867676
layer1: self_attn.k_proj
norm_0 tensor([2.0254, 1.9668, 2.0371,  ..., 1.6631, 1.8975, 1.8086], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.9346, 1.0586, 1.0898,  ..., 0.3875, 0.3777, 0.3123], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 17.43154525756836, val loss None, lr 0.001
iter 10, train loss 14.505926132202148, val loss None, lr 0.0009000000000000001
iter 20, train loss 14.102673530578613, val loss None, lr 0.0008100000000000001
iter 30, train loss 13.450899124145508, val loss None, lr 0.0008100000000000001
iter 40, train loss 13.905128479003906, val loss None, lr 0.0006561000000000001
iter 50, train loss 13.823446273803711, val loss None, lr 0.0004782969
iter 60, train loss 13.437994003295898, val loss None, lr 0.00031381059609000004
iter 70, train loss 13.825958251953125, val loss None, lr 0.00022876792454961005
iter 80, train loss 13.204517364501953, val loss None, lr 0.00018530201888518417
iter 90, train loss 12.85366439819336, val loss None, lr 0.00018530201888518417
best loss 12.710196495056152
layer1: self_attn.v_proj
norm_0 tensor([0.5439, 0.5366, 0.5508,  ..., 0.6504, 0.5752, 0.6440], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.3213, 1.3359, 1.2393,  ..., 0.5474, 0.5635, 0.5596], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 0.9019224643707275, val loss None, lr 0.001
iter 10, train loss 0.928885281085968, val loss None, lr 0.000729
iter 20, train loss 0.8618038892745972, val loss None, lr 0.000531441
iter 30, train loss 0.8378145098686218, val loss None, lr 0.00043046721
iter 40, train loss 0.8287670612335205, val loss None, lr 0.00043046721
iter 50, train loss 0.8177884817123413, val loss None, lr 0.00043046721
iter 60, train loss 0.811314046382904, val loss None, lr 0.00043046721
iter 70, train loss 0.8121711015701294, val loss None, lr 0.000387420489
iter 80, train loss 0.8055757284164429, val loss None, lr 0.0003486784401
iter 90, train loss 0.799203097820282, val loss None, lr 0.0003486784401
best loss 0.7964904308319092
layer1: self_attn.o_proj
norm_0 tensor([0.8604, 0.8032, 0.6895,  ..., 0.2113, 0.2167, 0.2192], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.0117, 1.0215, 0.9888,  ..., 0.9995, 0.9941, 0.9839], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 0.07904624938964844, val loss None, lr 0.001
iter 10, train loss 0.07558432966470718, val loss None, lr 0.0009000000000000001
iter 20, train loss 0.073858842253685, val loss None, lr 0.0009000000000000001
iter 30, train loss 0.0723380595445633, val loss None, lr 0.0009000000000000001
iter 40, train loss 0.07150201499462128, val loss None, lr 0.0008100000000000001
iter 50, train loss 0.0712989941239357, val loss None, lr 0.000729
iter 60, train loss 0.07116132229566574, val loss None, lr 0.000531441
iter 70, train loss 0.07004093378782272, val loss None, lr 0.000531441
iter 80, train loss 0.07062619179487228, val loss None, lr 0.000387420489
iter 90, train loss 0.07014554738998413, val loss None, lr 0.00025418658283290005
best loss 0.06987443566322327
layer1: mlp.gate_proj
norm_0 tensor([1.9492, 1.9355, 1.8818,  ..., 1.9443, 1.8994, 1.8916], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5796, 0.5811, 0.6069,  ..., 0.6011, 0.5981, 0.6099], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 16.306264877319336, val loss None, lr 0.001
iter 10, train loss 17.646411895751953, val loss None, lr 0.000729
iter 20, train loss 17.291959762573242, val loss None, lr 0.000531441
iter 30, train loss 16.97039794921875, val loss None, lr 0.000387420489
iter 40, train loss 16.752111434936523, val loss None, lr 0.00025418658283290005
iter 50, train loss 16.709707260131836, val loss None, lr 0.00018530201888518417
iter 60, train loss 16.719585418701172, val loss None, lr 0.0001350851717672993
iter 70, train loss 16.740283966064453, val loss None, lr 8.862938119652506e-05
iter 80, train loss 16.68865394592285, val loss None, lr 6.461081889226677e-05
iter 90, train loss 16.638957977294922, val loss None, lr 4.7101286972462485e-05
best loss 15.373790740966797
layer1: mlp.up_proj
norm_0 tensor([1.7627, 1.8057, 1.8271,  ..., 1.8008, 1.8066, 1.8203], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.6089, 0.6025, 0.6172,  ..., 0.6206, 0.6221, 0.6230], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 13.262065887451172, val loss None, lr 0.001
iter 10, train loss 13.757955551147461, val loss None, lr 0.000729
iter 20, train loss 13.539440155029297, val loss None, lr 0.000531441
iter 30, train loss 13.563014030456543, val loss None, lr 0.000387420489
iter 40, train loss 13.537673950195312, val loss None, lr 0.00025418658283290005
iter 50, train loss 13.536011695861816, val loss None, lr 0.00018530201888518417
iter 60, train loss 13.529061317443848, val loss None, lr 0.0001350851717672993
iter 70, train loss 13.535325050354004, val loss None, lr 8.862938119652506e-05
iter 80, train loss 13.522665023803711, val loss None, lr 6.461081889226677e-05
iter 90, train loss 13.510978698730469, val loss None, lr 4.7101286972462485e-05
best loss 13.12187671661377
layer1: mlp.down_proj
norm_0 tensor([1.1104, 1.1094, 1.1230,  ..., 1.1250, 1.1289, 1.1299], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.6289, 1.6357, 1.6455,  ..., 1.6387, 1.6582, 1.6318], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 0.08563327044248581, val loss None, lr 0.001
iter 10, train loss 0.3495025634765625, val loss None, lr 0.000729
iter 20, train loss 0.22929280996322632, val loss None, lr 0.000531441
iter 30, train loss 0.19120660424232483, val loss None, lr 0.0003486784401
iter 40, train loss 0.22457048296928406, val loss None, lr 0.00025418658283290005
iter 50, train loss 0.1563878059387207, val loss None, lr 0.00018530201888518417
iter 60, train loss 0.12552917003631592, val loss None, lr 0.00012157665459056936
iter 70, train loss 0.11853008717298508, val loss None, lr 8.862938119652506e-05
iter 80, train loss 0.11134634912014008, val loss None, lr 6.461081889226677e-05
iter 90, train loss 0.10969959199428558, val loss None, lr 4.239115827521624e-05
best loss 0.08563327044248581
37397 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
30657 MiB free out of 48676 MiB total
after cast to cpu
37719 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
layer2: self_attn.q_proj
norm_0 tensor([1.6846, 1.7197, 1.6816,  ..., 1.7490, 1.6240, 1.6484], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5840, 0.7588, 0.9141,  ..., 1.2471, 0.3936, 1.1914], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 61.34341812133789, val loss None, lr 0.001
iter 10, train loss 61.86702346801758, val loss None, lr 0.0009000000000000001
iter 20, train loss 61.668540954589844, val loss None, lr 0.00059049
iter 30, train loss 60.94260787963867, val loss None, lr 0.00043046721
iter 40, train loss 60.3310546875, val loss None, lr 0.00031381059609000004
iter 50, train loss 59.42964553833008, val loss None, lr 0.00020589113209464906
iter 60, train loss 59.22385025024414, val loss None, lr 0.0001500946352969992
iter 70, train loss 59.165916442871094, val loss None, lr 0.00010941898913151243
iter 80, train loss 59.096641540527344, val loss None, lr 7.17897987691853e-05
iter 90, train loss 58.89080047607422, val loss None, lr 5.233476330273609e-05
best loss 56.602882385253906
layer2: self_attn.k_proj
norm_0 tensor([1.7842, 1.7520, 1.8242,  ..., 1.7949, 1.7520, 1.7822], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.3757, 0.7593, 0.8696,  ..., 1.4326, 0.3281, 1.3086], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 72.23794555664062, val loss None, lr 0.001
iter 10, train loss 70.31538391113281, val loss None, lr 0.0009000000000000001
iter 20, train loss 72.36116027832031, val loss None, lr 0.0006561000000000001
iter 30, train loss 71.42979431152344, val loss None, lr 0.00043046721
iter 40, train loss 69.8971939086914, val loss None, lr 0.00031381059609000004
iter 50, train loss 69.25170135498047, val loss None, lr 0.00022876792454961005
iter 60, train loss 68.62010192871094, val loss None, lr 0.0001500946352969992
iter 70, train loss 68.48770141601562, val loss None, lr 0.00010941898913151243
iter 80, train loss 68.13787841796875, val loss None, lr 7.976644307687256e-05
iter 90, train loss 67.94317626953125, val loss None, lr 5.233476330273609e-05
best loss 65.27952575683594
layer2: self_attn.v_proj
norm_0 tensor([0.9292, 0.9331, 0.9194,  ..., 0.9048, 0.9575, 0.9600], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.0322, 1.0566, 1.0547,  ..., 0.9229, 1.0654, 1.0361], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 13.615337371826172, val loss None, lr 0.001
iter 10, train loss 14.19570255279541, val loss None, lr 0.000729
iter 20, train loss 13.713142395019531, val loss None, lr 0.000531441
iter 30, train loss 13.713367462158203, val loss None, lr 0.000387420489
iter 40, train loss 13.66175651550293, val loss None, lr 0.00025418658283290005
iter 50, train loss 13.627195358276367, val loss None, lr 0.00018530201888518417
iter 60, train loss 13.601872444152832, val loss None, lr 0.0001350851717672993
iter 70, train loss 13.589584350585938, val loss None, lr 8.862938119652506e-05
iter 80, train loss 13.578104019165039, val loss None, lr 6.461081889226677e-05
iter 90, train loss 13.548550605773926, val loss None, lr 4.7101286972462485e-05
best loss 13.401386260986328
layer2: self_attn.o_proj
norm_0 tensor([0.9219, 0.9819, 0.9458,  ..., 0.9478, 0.9624, 0.9248], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.9941, 1.0049, 1.0010,  ..., 0.9858, 0.9736, 0.9907], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 0.11607677489519119, val loss None, lr 0.001
iter 10, train loss 0.1158963143825531, val loss None, lr 0.0008100000000000001
iter 20, train loss 0.11320141702890396, val loss None, lr 0.0006561000000000001
iter 30, train loss 0.11390259861946106, val loss None, lr 0.0004782969
iter 40, train loss 0.11304114758968353, val loss None, lr 0.00031381059609000004
iter 50, train loss 0.11254779994487762, val loss None, lr 0.00028242953648100003
iter 60, train loss 0.11230561137199402, val loss None, lr 0.00022876792454961005
iter 70, train loss 0.11226648092269897, val loss None, lr 0.00018530201888518417
iter 80, train loss 0.11203774809837341, val loss None, lr 0.00016677181699666576
iter 90, train loss 0.1120079830288887, val loss None, lr 0.00010941898913151243
best loss 0.11189306527376175
layer2: mlp.gate_proj
norm_0 tensor([1.9473, 1.9883, 1.9746,  ..., 1.9814, 1.9824, 1.9443], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5986, 0.6045, 0.6274,  ..., 0.6235, 0.6045, 0.5771], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 33.34547805786133, val loss None, lr 0.001
iter 10, train loss 34.27564239501953, val loss None, lr 0.0008100000000000001
iter 20, train loss 33.57332229614258, val loss None, lr 0.000531441
iter 30, train loss 33.63923645019531, val loss None, lr 0.000387420489
iter 40, train loss 33.6590690612793, val loss None, lr 0.00028242953648100003
iter 50, train loss 33.63746643066406, val loss None, lr 0.00018530201888518417
iter 60, train loss 33.608158111572266, val loss None, lr 0.0001350851717672993
iter 70, train loss 33.52122497558594, val loss None, lr 9.847709021836118e-05
iter 80, train loss 33.520355224609375, val loss None, lr 6.461081889226677e-05
iter 90, train loss 33.50163650512695, val loss None, lr 4.7101286972462485e-05
best loss 32.82483673095703
layer2: mlp.up_proj
norm_0 tensor([1.8447, 1.8203, 1.8359,  ..., 1.8311, 1.8301, 1.8623], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.6143, 0.6196, 0.6221,  ..., 0.6240, 0.5981, 0.5825], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 27.45980453491211, val loss None, lr 0.001
iter 10, train loss 27.67842674255371, val loss None, lr 0.0008100000000000001
iter 20, train loss 27.752513885498047, val loss None, lr 0.000531441
iter 30, train loss 27.76447296142578, val loss None, lr 0.000387420489
iter 40, train loss 27.752727508544922, val loss None, lr 0.00028242953648100003
iter 50, train loss 27.75208854675293, val loss None, lr 0.00018530201888518417
iter 60, train loss 27.74269676208496, val loss None, lr 0.0001350851717672993
iter 70, train loss 27.765884399414062, val loss None, lr 9.847709021836118e-05
iter 80, train loss 27.757484436035156, val loss None, lr 6.461081889226677e-05
iter 90, train loss 27.731372833251953, val loss None, lr 4.7101286972462485e-05
best loss 27.37354850769043
layer2: mlp.down_proj
norm_0 tensor([1.1338, 1.1387, 1.1426,  ..., 1.1465, 1.1143, 1.0684], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.6309, 1.6396, 1.6504,  ..., 1.6455, 1.6660, 1.6436], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 0.15049609541893005, val loss None, lr 0.001
iter 10, train loss 0.15088512003421783, val loss None, lr 0.000729
iter 20, train loss 0.15057799220085144, val loss None, lr 0.0006561000000000001
iter 30, train loss 0.14976069331169128, val loss None, lr 0.000531441
iter 40, train loss 0.1495329737663269, val loss None, lr 0.00043046721
iter 50, train loss 0.1491970717906952, val loss None, lr 0.00043046721
iter 60, train loss 0.149207204580307, val loss None, lr 0.0003486784401
iter 70, train loss 0.1489972472190857, val loss None, lr 0.00031381059609000004
iter 80, train loss 0.14902043342590332, val loss None, lr 0.00022876792454961005
iter 90, train loss 0.14897972345352173, val loss None, lr 0.00016677181699666576
best loss 0.14893800020217896
37719 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
30379 MiB free out of 48676 MiB total
after cast to cpu
37375 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
layer3: self_attn.q_proj
norm_0 tensor([1.5566, 1.6201, 1.6104,  ..., 1.6201, 1.6006, 1.6182], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.7285, 0.8521, 0.8237,  ..., 1.6670, 1.7910, 1.8848], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 144.64678955078125, val loss None, lr 0.001
iter 10, train loss 151.2626495361328, val loss None, lr 0.000729
iter 20, train loss 147.7422637939453, val loss None, lr 0.000531441
iter 30, train loss 142.7842559814453, val loss None, lr 0.000387420489
iter 40, train loss 141.78048706054688, val loss None, lr 0.00025418658283290005
iter 50, train loss 141.6513671875, val loss None, lr 0.00018530201888518417
iter 60, train loss 141.6826171875, val loss None, lr 0.0001350851717672993
iter 70, train loss 141.3007354736328, val loss None, lr 8.862938119652506e-05
iter 80, train loss 140.8216094970703, val loss None, lr 6.461081889226677e-05
iter 90, train loss 140.63568115234375, val loss None, lr 4.7101286972462485e-05
best loss 137.49388122558594
layer3: self_attn.k_proj
norm_0 tensor([1.6465, 1.6738, 1.6650,  ..., 1.6475, 1.6562, 1.6846], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.6733, 0.8022, 0.7798,  ..., 1.6133, 1.7500, 1.8154], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 163.24856567382812, val loss None, lr 0.001
iter 10, train loss 166.65061950683594, val loss None, lr 0.000729
iter 20, train loss 169.5848846435547, val loss None, lr 0.000531441
iter 30, train loss 165.59812927246094, val loss None, lr 0.000387420489
iter 40, train loss 163.6028594970703, val loss None, lr 0.00025418658283290005
iter 50, train loss 161.90956115722656, val loss None, lr 0.00018530201888518417
iter 60, train loss 161.49429321289062, val loss None, lr 0.0001350851717672993
iter 70, train loss 160.79217529296875, val loss None, lr 8.862938119652506e-05
iter 80, train loss 160.77450561523438, val loss None, lr 6.461081889226677e-05
iter 90, train loss 160.14739990234375, val loss None, lr 4.7101286972462485e-05
best loss 154.175048828125
layer3: self_attn.v_proj
norm_0 tensor([0.8999, 0.8760, 0.8857,  ..., 0.8799, 0.8950, 0.8877], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.9341, 0.9414, 0.9292,  ..., 0.4678, 0.5034, 0.4414], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 35.11549758911133, val loss None, lr 0.001
iter 10, train loss 35.532127380371094, val loss None, lr 0.0008100000000000001
iter 20, train loss 35.154788970947266, val loss None, lr 0.000531441
iter 30, train loss 35.10363006591797, val loss None, lr 0.000387420489
iter 40, train loss 34.97589874267578, val loss None, lr 0.00028242953648100003
iter 50, train loss 34.93735122680664, val loss None, lr 0.00020589113209464906
iter 60, train loss 34.80289840698242, val loss None, lr 0.00020589113209464906
iter 70, train loss 34.83893585205078, val loss None, lr 0.0001500946352969992
iter 80, train loss 34.803382873535156, val loss None, lr 0.00010941898913151243
iter 90, train loss 34.80072784423828, val loss None, lr 7.976644307687256e-05
best loss 34.77879333496094
layer3: self_attn.o_proj
norm_0 tensor([0.8149, 0.8115, 0.7832,  ..., 0.3369, 0.3589, 0.3250], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.9824, 1.0029, 0.9785,  ..., 0.9951, 0.9751, 1.0020], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 0.2913860082626343, val loss None, lr 0.001
iter 10, train loss 0.2628186345100403, val loss None, lr 0.001
iter 20, train loss 0.24896478652954102, val loss None, lr 0.0009000000000000001
iter 30, train loss 0.23891904950141907, val loss None, lr 0.0009000000000000001
iter 40, train loss 0.2372366040945053, val loss None, lr 0.0009000000000000001
iter 50, train loss 0.23531395196914673, val loss None, lr 0.0008100000000000001
iter 60, train loss 0.2317139059305191, val loss None, lr 0.000729
iter 70, train loss 0.2311503291130066, val loss None, lr 0.00059049
iter 80, train loss 0.23202919960021973, val loss None, lr 0.00043046721
iter 90, train loss 0.23010550439357758, val loss None, lr 0.0003486784401
best loss 0.2289050966501236
layer3: mlp.gate_proj
norm_0 tensor([1.9883, 1.9893, 1.9834,  ..., 2.0176, 1.9902, 1.9932], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5977, 0.6079, 0.6357,  ..., 0.5967, 0.5039, 0.6211], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 52.966468811035156, val loss None, lr 0.001
iter 10, train loss 53.901885986328125, val loss None, lr 0.0008100000000000001
iter 20, train loss 53.559532165527344, val loss None, lr 0.000531441
iter 30, train loss 53.277099609375, val loss None, lr 0.000387420489
iter 40, train loss 53.416778564453125, val loss None, lr 0.00028242953648100003
iter 50, train loss 53.37370681762695, val loss None, lr 0.00018530201888518417
iter 60, train loss 53.35752487182617, val loss None, lr 0.0001350851717672993
iter 70, train loss 53.296417236328125, val loss None, lr 9.847709021836118e-05
iter 80, train loss 53.31777572631836, val loss None, lr 6.461081889226677e-05
iter 90, train loss 53.254249572753906, val loss None, lr 4.7101286972462485e-05
best loss 52.54900360107422
layer3: mlp.up_proj
norm_0 tensor([1.8574, 1.8525, 1.8643,  ..., 1.8350, 1.8555, 1.8564], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.6064, 0.6235, 0.6128,  ..., 0.6118, 0.5352, 0.6128], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 43.85385513305664, val loss None, lr 0.001
iter 10, train loss 43.904579162597656, val loss None, lr 0.000729
iter 20, train loss 43.96143341064453, val loss None, lr 0.000531441
iter 30, train loss 44.04617691040039, val loss None, lr 0.0003486784401
iter 40, train loss 44.021995544433594, val loss None, lr 0.00025418658283290005
iter 50, train loss 44.002777099609375, val loss None, lr 0.00018530201888518417
iter 60, train loss 43.994441986083984, val loss None, lr 0.00012157665459056936
iter 70, train loss 43.979408264160156, val loss None, lr 8.862938119652506e-05
iter 80, train loss 43.981842041015625, val loss None, lr 6.461081889226677e-05
iter 90, train loss 43.96120071411133, val loss None, lr 4.239115827521624e-05
best loss 43.85385513305664
layer3: mlp.down_proj
norm_0 tensor([1.1055, 1.1504, 1.1299,  ..., 1.1318, 0.8623, 1.1318], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.6416, 1.6357, 1.6602,  ..., 1.6182, 1.6387, 1.6445], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 0.29241496324539185, val loss None, lr 0.001
iter 10, train loss 0.29456156492233276, val loss None, lr 0.000729
iter 20, train loss 0.293578177690506, val loss None, lr 0.000531441
iter 30, train loss 0.292741596698761, val loss None, lr 0.0003486784401
iter 40, train loss 0.29252636432647705, val loss None, lr 0.00025418658283290005
iter 50, train loss 0.29239508509635925, val loss None, lr 0.00020589113209464906
iter 60, train loss 0.29192447662353516, val loss None, lr 0.00018530201888518417
iter 70, train loss 0.29185163974761963, val loss None, lr 0.00016677181699666576
iter 80, train loss 0.2917798161506653, val loss None, lr 0.00012157665459056936
iter 90, train loss 0.2915409207344055, val loss None, lr 9.847709021836118e-05
best loss 0.29149436950683594
37375 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
29979 MiB free out of 48676 MiB total
after cast to cpu
37095 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
layer4: self_attn.q_proj
norm_0 tensor([1.6592, 1.6807, 1.6289,  ..., 1.6611, 1.7061, 1.7139], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5449, 0.8042, 0.8550,  ..., 1.3770, 1.3496, 1.3174], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 135.12278747558594, val loss None, lr 0.001
iter 10, train loss 143.9078369140625, val loss None, lr 0.0008100000000000001
iter 20, train loss 138.5704345703125, val loss None, lr 0.00059049
iter 30, train loss 137.24330139160156, val loss None, lr 0.00043046721
iter 40, train loss 135.94178771972656, val loss None, lr 0.00028242953648100003
iter 50, train loss 135.51181030273438, val loss None, lr 0.00020589113209464906
iter 60, train loss 135.31832885742188, val loss None, lr 0.0001500946352969992
iter 70, train loss 134.84596252441406, val loss None, lr 9.847709021836118e-05
iter 80, train loss 134.39439392089844, val loss None, lr 7.17897987691853e-05
iter 90, train loss 134.18026733398438, val loss None, lr 5.233476330273609e-05
best loss 130.01953125
layer4: self_attn.k_proj
norm_0 tensor([1.6924, 1.7266, 1.7168,  ..., 1.6689, 1.7119, 1.7129], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5698, 0.7725, 0.8120,  ..., 1.4160, 1.3164, 1.4375], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 148.7257080078125, val loss None, lr 0.001
iter 10, train loss 156.4016876220703, val loss None, lr 0.0008100000000000001
iter 20, train loss 155.18971252441406, val loss None, lr 0.00059049
iter 30, train loss 150.7100830078125, val loss None, lr 0.00043046721
iter 40, train loss 150.09600830078125, val loss None, lr 0.00028242953648100003
iter 50, train loss 149.00413513183594, val loss None, lr 0.00020589113209464906
iter 60, train loss 148.68455505371094, val loss None, lr 0.0001500946352969992
iter 70, train loss 148.23997497558594, val loss None, lr 9.847709021836118e-05
iter 80, train loss 147.69915771484375, val loss None, lr 7.17897987691853e-05
iter 90, train loss 147.59310913085938, val loss None, lr 5.233476330273609e-05
best loss 141.97845458984375
layer4: self_attn.v_proj
norm_0 tensor([0.9341, 0.9048, 0.9458,  ..., 0.9385, 0.9043, 0.9126], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.9438, 0.9316, 0.9370,  ..., 1.0049, 0.9961, 1.0156], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 34.814842224121094, val loss None, lr 0.001
iter 10, train loss 35.2929801940918, val loss None, lr 0.0008100000000000001
iter 20, train loss 34.860198974609375, val loss None, lr 0.000531441
iter 30, train loss 34.74113464355469, val loss None, lr 0.000387420489
iter 40, train loss 34.60349655151367, val loss None, lr 0.00028242953648100003
iter 50, train loss 34.526344299316406, val loss None, lr 0.00022876792454961005
iter 60, train loss 34.5067253112793, val loss None, lr 0.00020589113209464906
iter 70, train loss 34.44200134277344, val loss None, lr 0.00018530201888518417
iter 80, train loss 34.45952224731445, val loss None, lr 0.0001500946352969992
iter 90, train loss 34.43072509765625, val loss None, lr 0.00010941898913151243
best loss 34.36080551147461
layer4: self_attn.o_proj
norm_0 tensor([0.8374, 0.8276, 0.8369,  ..., 0.9082, 0.9082, 0.9243], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.9653, 0.9951, 1.0117,  ..., 0.9888, 0.9629, 0.9980], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 0.6512372493743896, val loss None, lr 0.001
iter 10, train loss 0.580886721611023, val loss None, lr 0.0009000000000000001
iter 20, train loss 0.5485954880714417, val loss None, lr 0.0009000000000000001
iter 30, train loss 0.5288391709327698, val loss None, lr 0.0009000000000000001
iter 40, train loss 0.5164271593093872, val loss None, lr 0.0009000000000000001
iter 50, train loss 0.5097599029541016, val loss None, lr 0.0009000000000000001
iter 60, train loss 0.5016028881072998, val loss None, lr 0.0009000000000000001
iter 70, train loss 0.4999654293060303, val loss None, lr 0.000729
iter 80, train loss 0.4944092631340027, val loss None, lr 0.000729
iter 90, train loss 0.4962230324745178, val loss None, lr 0.0004782969
best loss 0.49202781915664673
layer4: mlp.gate_proj
norm_0 tensor([2.0293, 2.0391, 2.0410,  ..., 2.0449, 2.0215, 2.0449], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5967, 0.5986, 0.5771,  ..., 0.6255, 0.6001, 0.6206], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 78.00985717773438, val loss None, lr 0.001
iter 10, train loss 79.87230682373047, val loss None, lr 0.0008100000000000001
iter 20, train loss 79.15396118164062, val loss None, lr 0.000531441
iter 30, train loss 78.80669403076172, val loss None, lr 0.000387420489
iter 40, train loss 78.80976867675781, val loss None, lr 0.00028242953648100003
iter 50, train loss 78.68701934814453, val loss None, lr 0.00018530201888518417
iter 60, train loss 78.69464111328125, val loss None, lr 0.0001350851717672993
iter 70, train loss 78.58610534667969, val loss None, lr 9.847709021836118e-05
iter 80, train loss 78.5862808227539, val loss None, lr 6.461081889226677e-05
iter 90, train loss 78.54490661621094, val loss None, lr 4.7101286972462485e-05
best loss 77.39315795898438
layer4: mlp.up_proj
norm_0 tensor([1.8447, 1.8350, 1.8320,  ..., 1.8330, 1.8428, 1.8271], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.6138, 0.6162, 0.6216,  ..., 0.6025, 0.6216, 0.6304], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 61.289493560791016, val loss None, lr 0.001
iter 10, train loss 61.43275451660156, val loss None, lr 0.000729
iter 20, train loss 61.50394821166992, val loss None, lr 0.000531441
iter 30, train loss 61.48865509033203, val loss None, lr 0.0003486784401
iter 40, train loss 61.43409729003906, val loss None, lr 0.00025418658283290005
iter 50, train loss 61.45198059082031, val loss None, lr 0.00018530201888518417
iter 60, train loss 61.48027801513672, val loss None, lr 0.00012157665459056936
iter 70, train loss 61.41407012939453, val loss None, lr 8.862938119652506e-05
iter 80, train loss 61.40568161010742, val loss None, lr 6.461081889226677e-05
iter 90, train loss 61.40979766845703, val loss None, lr 4.239115827521624e-05
best loss 61.289493560791016
layer4: mlp.down_proj
norm_0 tensor([1.1270, 1.1289, 1.1396,  ..., 1.0859, 1.1279, 1.1514], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.6475, 1.6377, 1.6514,  ..., 1.6406, 1.6230, 1.6436], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 0.5845831632614136, val loss None, lr 0.001
iter 10, train loss 0.5843977928161621, val loss None, lr 0.000729
iter 20, train loss 0.5827381610870361, val loss None, lr 0.0006561000000000001
iter 30, train loss 0.5844653248786926, val loss None, lr 0.0004782969
iter 40, train loss 0.5824733376502991, val loss None, lr 0.000387420489
iter 50, train loss 0.5813788175582886, val loss None, lr 0.00031381059609000004
iter 60, train loss 0.5808061361312866, val loss None, lr 0.00025418658283290005
iter 70, train loss 0.5816245675086975, val loss None, lr 0.00018530201888518417
iter 80, train loss 0.5815942287445068, val loss None, lr 0.0001350851717672993
iter 90, train loss 0.5808724761009216, val loss None, lr 8.862938119652506e-05
best loss 0.5807238817214966
37095 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
29549 MiB free out of 48676 MiB total
after cast to cpu
36751 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
layer5: self_attn.q_proj
norm_0 tensor([1.6846, 1.6924, 1.6426,  ..., 1.6523, 1.6660, 1.7373], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5879, 0.6504, 0.6724,  ..., 0.9902, 1.1523, 0.9600], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 153.57028198242188, val loss None, lr 0.001
iter 10, train loss 163.7725830078125, val loss None, lr 0.0008100000000000001
iter 20, train loss 158.22113037109375, val loss None, lr 0.00059049
iter 30, train loss 153.62356567382812, val loss None, lr 0.00043046721
iter 40, train loss 154.04244995117188, val loss None, lr 0.00028242953648100003
iter 50, train loss 152.98489379882812, val loss None, lr 0.00020589113209464906
iter 60, train loss 152.67034912109375, val loss None, lr 0.0001500946352969992
iter 70, train loss 152.56219482421875, val loss None, lr 9.847709021836118e-05
iter 80, train loss 152.03225708007812, val loss None, lr 7.17897987691853e-05
iter 90, train loss 151.36105346679688, val loss None, lr 5.233476330273609e-05
best loss 149.00296020507812
layer5: self_attn.k_proj
norm_0 tensor([1.7227, 1.7930, 1.7539,  ..., 1.6982, 1.7305, 1.7275], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5361, 0.6172, 0.6533,  ..., 0.8018, 1.5273, 1.3584], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 179.84521484375, val loss None, lr 0.001
iter 10, train loss 184.80267333984375, val loss None, lr 0.0008100000000000001
iter 20, train loss 185.9928741455078, val loss None, lr 0.00059049
iter 30, train loss 182.23670959472656, val loss None, lr 0.00043046721
iter 40, train loss 178.5279541015625, val loss None, lr 0.00028242953648100003
iter 50, train loss 178.32667541503906, val loss None, lr 0.00020589113209464906
iter 60, train loss 178.25994873046875, val loss None, lr 0.0001500946352969992
iter 70, train loss 176.9840850830078, val loss None, lr 9.847709021836118e-05
iter 80, train loss 175.8892059326172, val loss None, lr 7.17897987691853e-05
iter 90, train loss 175.3056182861328, val loss None, lr 5.233476330273609e-05
best loss 170.218994140625
layer5: self_attn.v_proj
norm_0 tensor([0.9814, 0.9243, 0.9575,  ..., 0.9756, 0.9688, 0.9268], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.0576, 1.0322, 1.0430,  ..., 0.9473, 0.9565, 0.9619], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 41.00061798095703, val loss None, lr 0.001
iter 10, train loss 41.109886169433594, val loss None, lr 0.0008100000000000001
iter 20, train loss 41.040531158447266, val loss None, lr 0.000531441
iter 30, train loss 40.798030853271484, val loss None, lr 0.0004782969
iter 40, train loss 40.78175354003906, val loss None, lr 0.0003486784401
iter 50, train loss 40.728736877441406, val loss None, lr 0.00031381059609000004
iter 60, train loss 40.60113525390625, val loss None, lr 0.00028242953648100003
iter 70, train loss 40.63027572631836, val loss None, lr 0.00022876792454961005
iter 80, train loss 40.549747467041016, val loss None, lr 0.00018530201888518417
iter 90, train loss 40.547454833984375, val loss None, lr 0.00012157665459056936
best loss 40.534141540527344
layer5: self_attn.o_proj
norm_0 tensor([0.9814, 0.9512, 0.9639,  ..., 0.8447, 0.8657, 0.8770], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.9956, 1.0107, 0.9868,  ..., 0.9888, 0.9775, 0.9995], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 0.8682051301002502, val loss None, lr 0.001
iter 10, train loss 0.8372355699539185, val loss None, lr 0.0008100000000000001
iter 20, train loss 0.8275520205497742, val loss None, lr 0.000729
iter 30, train loss 0.8205298185348511, val loss None, lr 0.00059049
iter 40, train loss 0.8155936002731323, val loss None, lr 0.00059049
iter 50, train loss 0.812409520149231, val loss None, lr 0.00059049
iter 60, train loss 0.8114359378814697, val loss None, lr 0.0004782969
iter 70, train loss 0.8091064095497131, val loss None, lr 0.000387420489
iter 80, train loss 0.8063777685165405, val loss None, lr 0.0003486784401
iter 90, train loss 0.8051292300224304, val loss None, lr 0.00031381059609000004
best loss 0.8035396337509155
layer5: mlp.gate_proj
norm_0 tensor([2.0527, 2.0410, 2.0508,  ..., 2.0723, 2.0312, 2.0566], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5820, 0.6226, 0.5684,  ..., 0.6562, 0.5903, 0.5723], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 99.46636199951172, val loss None, lr 0.001
iter 10, train loss 101.51895904541016, val loss None, lr 0.0008100000000000001
iter 20, train loss 100.6754150390625, val loss None, lr 0.000531441
iter 30, train loss 100.57247924804688, val loss None, lr 0.000387420489
iter 40, train loss 100.66140747070312, val loss None, lr 0.00028242953648100003
iter 50, train loss 100.36461639404297, val loss None, lr 0.00018530201888518417
iter 60, train loss 100.28189086914062, val loss None, lr 0.0001350851717672993
iter 70, train loss 100.14007568359375, val loss None, lr 9.847709021836118e-05
iter 80, train loss 100.16399383544922, val loss None, lr 6.461081889226677e-05
iter 90, train loss 100.1952133178711, val loss None, lr 4.7101286972462485e-05
best loss 98.90800476074219
layer5: mlp.up_proj
norm_0 tensor([1.8369, 1.8330, 1.8232,  ..., 1.8311, 1.8477, 1.8047], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.6182, 0.6274, 0.6162,  ..., 0.6235, 0.6250, 0.6138], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 77.32406616210938, val loss None, lr 0.001
iter 10, train loss 77.52816772460938, val loss None, lr 0.000729
iter 20, train loss 77.6187744140625, val loss None, lr 0.000531441
iter 30, train loss 77.6375732421875, val loss None, lr 0.0003486784401
iter 40, train loss 77.54296112060547, val loss None, lr 0.00025418658283290005
iter 50, train loss 77.484619140625, val loss None, lr 0.00018530201888518417
iter 60, train loss 77.48787689208984, val loss None, lr 0.00012157665459056936
iter 70, train loss 77.4977798461914, val loss None, lr 8.862938119652506e-05
iter 80, train loss 77.44227600097656, val loss None, lr 6.461081889226677e-05
iter 90, train loss 77.42385864257812, val loss None, lr 4.239115827521624e-05
best loss 77.32406616210938
layer5: mlp.down_proj
norm_0 tensor([1.1318, 1.1416, 1.1221,  ..., 1.1436, 1.1377, 1.1289], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.6475, 1.6191, 1.6123,  ..., 1.6553, 1.6387, 1.6582], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 0.8634747266769409, val loss None, lr 0.001
iter 10, train loss 0.8663778901100159, val loss None, lr 0.000729
iter 20, train loss 0.8640965223312378, val loss None, lr 0.000531441
iter 30, train loss 0.8628882169723511, val loss None, lr 0.000387420489
iter 40, train loss 0.8608602285385132, val loss None, lr 0.000387420489
iter 50, train loss 0.8592363595962524, val loss None, lr 0.00031381059609000004
iter 60, train loss 0.859034538269043, val loss None, lr 0.00025418658283290005
iter 70, train loss 0.8585925698280334, val loss None, lr 0.00020589113209464906
iter 80, train loss 0.8573910593986511, val loss None, lr 0.00020589113209464906
iter 90, train loss 0.8574033975601196, val loss None, lr 0.0001500946352969992
best loss 0.8572268486022949
36751 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
29141 MiB free out of 48676 MiB total
after cast to cpu
36375 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
layer6: self_attn.q_proj
norm_0 tensor([1.5400, 1.5742, 1.6016,  ..., 1.5332, 1.5908, 1.6064], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.8418, 0.8530, 0.8296,  ..., 1.3203, 1.2705, 1.3652], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 228.86419677734375, val loss None, lr 0.001
iter 10, train loss 244.24111938476562, val loss None, lr 0.000729
iter 20, train loss 241.2165069580078, val loss None, lr 0.000531441
iter 30, train loss 233.25550842285156, val loss None, lr 0.000387420489
iter 40, train loss 229.359375, val loss None, lr 0.00025418658283290005
iter 50, train loss 228.08302307128906, val loss None, lr 0.00018530201888518417
iter 60, train loss 227.1727294921875, val loss None, lr 0.0001350851717672993
iter 70, train loss 226.38775634765625, val loss None, lr 8.862938119652506e-05
iter 80, train loss 226.28794860839844, val loss None, lr 6.461081889226677e-05
iter 90, train loss 225.62789916992188, val loss None, lr 4.7101286972462485e-05
best loss 213.2071533203125
layer6: self_attn.k_proj
norm_0 tensor([1.6113, 1.6699, 1.6338,  ..., 1.6250, 1.6094, 1.6221], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.8003, 0.8091, 0.8184,  ..., 1.2295, 1.2783, 1.3213], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 252.1407012939453, val loss None, lr 0.001
iter 10, train loss 257.8848876953125, val loss None, lr 0.000729
iter 20, train loss 259.0158386230469, val loss None, lr 0.000531441
iter 30, train loss 253.94105529785156, val loss None, lr 0.000387420489
iter 40, train loss 250.93743896484375, val loss None, lr 0.00025418658283290005
iter 50, train loss 247.8011016845703, val loss None, lr 0.00018530201888518417
iter 60, train loss 246.696044921875, val loss None, lr 0.0001350851717672993
iter 70, train loss 246.40020751953125, val loss None, lr 8.862938119652506e-05
iter 80, train loss 245.08937072753906, val loss None, lr 6.461081889226677e-05
iter 90, train loss 244.59295654296875, val loss None, lr 4.7101286972462485e-05
best loss 227.96926879882812
layer6: self_attn.v_proj
norm_0 tensor([0.9263, 0.8384, 0.8765,  ..., 0.9175, 0.8960, 0.8613], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.1201, 1.1016, 1.1309,  ..., 1.0840, 1.0830, 1.0801], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 58.67826843261719, val loss None, lr 0.001
iter 10, train loss 59.068763732910156, val loss None, lr 0.0008100000000000001
iter 20, train loss 58.76049041748047, val loss None, lr 0.000531441
iter 30, train loss 58.43254852294922, val loss None, lr 0.000387420489
iter 40, train loss 58.5216064453125, val loss None, lr 0.00028242953648100003
iter 50, train loss 58.3697509765625, val loss None, lr 0.00022876792454961005
iter 60, train loss 58.306488037109375, val loss None, lr 0.00020589113209464906
iter 70, train loss 58.19485855102539, val loss None, lr 0.00018530201888518417
iter 80, train loss 58.30747985839844, val loss None, lr 0.0001350851717672993
iter 90, train loss 58.206077575683594, val loss None, lr 9.847709021836118e-05
best loss 58.16118621826172
layer6: self_attn.o_proj
norm_0 tensor([0.9575, 0.9658, 0.9658,  ..., 0.9131, 0.9028, 0.9160], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.0029, 1.0332, 0.9849,  ..., 1.0117, 0.9897, 0.9814], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 1.7093833684921265, val loss None, lr 0.001
iter 10, train loss 1.622305154800415, val loss None, lr 0.0009000000000000001
iter 20, train loss 1.5860718488693237, val loss None, lr 0.0009000000000000001
iter 30, train loss 1.5667102336883545, val loss None, lr 0.0009000000000000001
iter 40, train loss 1.5494012832641602, val loss None, lr 0.0008100000000000001
iter 50, train loss 1.5438127517700195, val loss None, lr 0.0008100000000000001
iter 60, train loss 1.5357122421264648, val loss None, lr 0.0006561000000000001
iter 70, train loss 1.5327212810516357, val loss None, lr 0.000531441
iter 80, train loss 1.5286386013031006, val loss None, lr 0.0004782969
iter 90, train loss 1.5235000848770142, val loss None, lr 0.0004782969
best loss 1.5195469856262207
layer6: mlp.gate_proj
norm_0 tensor([2.0645, 2.0723, 2.0859,  ..., 2.0820, 2.0801, 2.0859], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.4995, 0.5005, 0.5156,  ..., 0.5415, 0.5479, 0.6479], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 125.21257019042969, val loss None, lr 0.001
iter 10, train loss 130.7156524658203, val loss None, lr 0.0008100000000000001
iter 20, train loss 127.97344207763672, val loss None, lr 0.000531441
iter 30, train loss 127.61923217773438, val loss None, lr 0.000387420489
iter 40, train loss 127.359619140625, val loss None, lr 0.00028242953648100003
iter 50, train loss 127.34832000732422, val loss None, lr 0.00018530201888518417
iter 60, train loss 127.02767181396484, val loss None, lr 0.0001350851717672993
iter 70, train loss 126.85570526123047, val loss None, lr 9.847709021836118e-05
iter 80, train loss 126.76628112792969, val loss None, lr 6.461081889226677e-05
iter 90, train loss 126.8779296875, val loss None, lr 4.7101286972462485e-05
best loss 124.10615539550781
layer6: mlp.up_proj
norm_0 tensor([1.8457, 1.8311, 1.8223,  ..., 1.8438, 1.8291, 1.8262], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5732, 0.5552, 0.5557,  ..., 0.6118, 0.6104, 0.6094], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 92.92620849609375, val loss None, lr 0.001
iter 10, train loss 93.16424560546875, val loss None, lr 0.000729
iter 20, train loss 93.15834045410156, val loss None, lr 0.000531441
iter 30, train loss 93.17203521728516, val loss None, lr 0.0003486784401
iter 40, train loss 93.07271575927734, val loss None, lr 0.00025418658283290005
iter 50, train loss 92.95204162597656, val loss None, lr 0.00018530201888518417
iter 60, train loss 92.95616149902344, val loss None, lr 0.00012157665459056936
iter 70, train loss 92.95938873291016, val loss None, lr 8.862938119652506e-05
iter 80, train loss 92.99765014648438, val loss None, lr 6.461081889226677e-05
iter 90, train loss 92.903564453125, val loss None, lr 4.7101286972462485e-05
best loss 92.89497375488281
layer6: mlp.down_proj
norm_0 tensor([1.0537, 1.0254, 1.0371,  ..., 1.1191, 1.1172, 1.1006], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.6533, 1.6094, 1.6426,  ..., 1.6611, 1.6182, 1.6289], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 1.3276652097702026, val loss None, lr 0.001
iter 10, train loss 1.3225504159927368, val loss None, lr 0.0008100000000000001
iter 20, train loss 1.3138291835784912, val loss None, lr 0.000729
iter 30, train loss 1.30715012550354, val loss None, lr 0.000729
iter 40, train loss 1.3025052547454834, val loss None, lr 0.000729
iter 50, train loss 1.2998169660568237, val loss None, lr 0.000729
iter 60, train loss 1.298903465270996, val loss None, lr 0.000531441
iter 70, train loss 1.2974061965942383, val loss None, lr 0.0004782969
iter 80, train loss 1.2990820407867432, val loss None, lr 0.000387420489
iter 90, train loss 1.2982614040374756, val loss None, lr 0.00025418658283290005
best loss 1.29649817943573
36375 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
28733 MiB free out of 48676 MiB total
after cast to cpu
35839 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
layer7: self_attn.q_proj
norm_0 tensor([1.5596, 1.5742, 1.5781,  ..., 1.5889, 1.5986, 1.6016], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.4731, 0.4634, 0.4819,  ..., 1.2637, 1.7500, 1.7422], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 249.89340209960938, val loss None, lr 0.001
iter 10, train loss 268.4403076171875, val loss None, lr 0.000729
iter 20, train loss 263.30816650390625, val loss None, lr 0.000531441
iter 30, train loss 255.97283935546875, val loss None, lr 0.000387420489
iter 40, train loss 254.0157012939453, val loss None, lr 0.00025418658283290005
iter 50, train loss 252.57769775390625, val loss None, lr 0.00018530201888518417
iter 60, train loss 251.98268127441406, val loss None, lr 0.0001350851717672993
iter 70, train loss 251.48175048828125, val loss None, lr 8.862938119652506e-05
iter 80, train loss 251.54847717285156, val loss None, lr 6.461081889226677e-05
iter 90, train loss 251.40631103515625, val loss None, lr 4.7101286972462485e-05
best loss 230.04843139648438
layer7: self_attn.k_proj
norm_0 tensor([1.5732, 1.6455, 1.5850,  ..., 1.5645, 1.5615, 1.5967], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.4666, 0.4541, 0.4731,  ..., 1.1797, 1.4951, 1.4863], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 261.969970703125, val loss None, lr 0.001
iter 10, train loss 274.31072998046875, val loss None, lr 0.000729
iter 20, train loss 274.8787841796875, val loss None, lr 0.000531441
iter 30, train loss 266.7262878417969, val loss None, lr 0.000387420489
iter 40, train loss 261.3784484863281, val loss None, lr 0.00025418658283290005
iter 50, train loss 259.0267639160156, val loss None, lr 0.00018530201888518417
iter 60, train loss 258.401123046875, val loss None, lr 0.0001350851717672993
iter 70, train loss 257.6434631347656, val loss None, lr 8.862938119652506e-05
iter 80, train loss 256.24078369140625, val loss None, lr 6.461081889226677e-05
iter 90, train loss 256.5646667480469, val loss None, lr 4.7101286972462485e-05
best loss 238.45477294921875
layer7: self_attn.v_proj
norm_0 tensor([0.9561, 0.8398, 0.8979,  ..., 0.9438, 0.9082, 0.8828], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.1172, 1.0820, 1.0469,  ..., 0.9688, 0.9741, 0.9883], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 65.64635467529297, val loss None, lr 0.001
iter 10, train loss 66.01541900634766, val loss None, lr 0.0008100000000000001
iter 20, train loss 65.63300323486328, val loss None, lr 0.00059049
iter 30, train loss 65.31053161621094, val loss None, lr 0.00043046721
iter 40, train loss 65.44256591796875, val loss None, lr 0.00031381059609000004
iter 50, train loss 65.17997741699219, val loss None, lr 0.00028242953648100003
iter 60, train loss 65.0921401977539, val loss None, lr 0.00025418658283290005
iter 70, train loss 64.9688491821289, val loss None, lr 0.00020589113209464906
iter 80, train loss 65.0444564819336, val loss None, lr 0.00016677181699666576
iter 90, train loss 64.9844970703125, val loss None, lr 0.00010941898913151243
best loss 64.93138885498047
layer7: self_attn.o_proj
norm_0 tensor([1.0088, 0.9238, 0.9097,  ..., 0.8438, 0.8472, 0.8589], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.0029, 1.0322, 0.9668,  ..., 0.9902, 1.0176, 0.9834], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 2.581256866455078, val loss None, lr 0.001
iter 10, train loss 2.4455273151397705, val loss None, lr 0.001
iter 20, train loss 2.427309036254883, val loss None, lr 0.0008100000000000001
iter 30, train loss 2.416480779647827, val loss None, lr 0.0006561000000000001
iter 40, train loss 2.4055962562561035, val loss None, lr 0.000531441
iter 50, train loss 2.386590003967285, val loss None, lr 0.000531441
iter 60, train loss 2.3946027755737305, val loss None, lr 0.000387420489
iter 70, train loss 2.3797457218170166, val loss None, lr 0.00031381059609000004
iter 80, train loss 2.3779053688049316, val loss None, lr 0.00025418658283290005
iter 90, train loss 2.3798344135284424, val loss None, lr 0.00018530201888518417
best loss 2.368506908416748
layer7: mlp.gate_proj
norm_0 tensor([2.0723, 2.0605, 2.0898,  ..., 2.0801, 2.0586, 2.0879], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5020, 0.6572, 0.6641,  ..., 0.6201, 0.4226, 0.5308], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 142.35507202148438, val loss None, lr 0.001
iter 10, train loss 150.16220092773438, val loss None, lr 0.0008100000000000001
iter 20, train loss 146.17535400390625, val loss None, lr 0.000531441
iter 30, train loss 146.0606689453125, val loss None, lr 0.000387420489
iter 40, train loss 145.24862670898438, val loss None, lr 0.00028242953648100003
iter 50, train loss 144.7593536376953, val loss None, lr 0.00018530201888518417
iter 60, train loss 144.42752075195312, val loss None, lr 0.0001350851717672993
iter 70, train loss 144.0063934326172, val loss None, lr 9.847709021836118e-05
iter 80, train loss 143.67678833007812, val loss None, lr 6.461081889226677e-05
iter 90, train loss 143.7085723876953, val loss None, lr 4.7101286972462485e-05
best loss 140.7750244140625
layer7: mlp.up_proj
norm_0 tensor([1.8584, 1.8379, 1.8184,  ..., 1.8516, 1.8643, 1.8301], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5767, 0.5947, 0.5737,  ..., 0.6187, 0.5327, 0.5874], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 106.32403564453125, val loss None, lr 0.001
iter 10, train loss 106.62188720703125, val loss None, lr 0.000729
iter 20, train loss 106.74362182617188, val loss None, lr 0.000531441
iter 30, train loss 106.68991088867188, val loss None, lr 0.0003486784401
iter 40, train loss 106.5445556640625, val loss None, lr 0.00025418658283290005
iter 50, train loss 106.54705047607422, val loss None, lr 0.00018530201888518417
iter 60, train loss 106.3572006225586, val loss None, lr 0.00012157665459056936
iter 70, train loss 106.3337173461914, val loss None, lr 8.862938119652506e-05
iter 80, train loss 106.27831268310547, val loss None, lr 7.976644307687256e-05
iter 90, train loss 106.23374938964844, val loss None, lr 6.461081889226677e-05
best loss 106.20773315429688
layer7: mlp.down_proj
norm_0 tensor([1.0664, 1.0625, 1.0352,  ..., 1.1152, 1.0088, 1.0713], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.6602, 1.6191, 1.6016,  ..., 1.6641, 1.6426, 1.6318], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 1.775736689567566, val loss None, lr 0.001
iter 10, train loss 1.7790054082870483, val loss None, lr 0.000729
iter 20, train loss 1.771316647529602, val loss None, lr 0.00059049
iter 30, train loss 1.7646195888519287, val loss None, lr 0.000531441
iter 40, train loss 1.7612394094467163, val loss None, lr 0.000531441
iter 50, train loss 1.755780577659607, val loss None, lr 0.0004782969
iter 60, train loss 1.7538073062896729, val loss None, lr 0.000387420489
iter 70, train loss 1.7523536682128906, val loss None, lr 0.0003486784401
iter 80, train loss 1.7520774602890015, val loss None, lr 0.00025418658283290005
iter 90, train loss 1.7504611015319824, val loss None, lr 0.00018530201888518417
best loss 1.750424861907959
35839 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
28325 MiB free out of 48676 MiB total
after cast to cpu
35495 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
layer8: self_attn.q_proj
norm_0 tensor([1.5811, 1.6309, 1.5967,  ..., 1.5674, 1.5840, 1.5977], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5137, 0.6250, 0.6187,  ..., 1.4736, 1.9814, 1.9697], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 240.13417053222656, val loss None, lr 0.001
iter 10, train loss 253.20919799804688, val loss None, lr 0.000729
iter 20, train loss 247.146728515625, val loss None, lr 0.000531441
iter 30, train loss 242.3026123046875, val loss None, lr 0.000387420489
iter 40, train loss 241.40966796875, val loss None, lr 0.00025418658283290005
iter 50, train loss 241.2533416748047, val loss None, lr 0.00018530201888518417
iter 60, train loss 240.47744750976562, val loss None, lr 0.0001350851717672993
iter 70, train loss 240.76385498046875, val loss None, lr 8.862938119652506e-05
iter 80, train loss 240.3523712158203, val loss None, lr 6.461081889226677e-05
iter 90, train loss 239.79208374023438, val loss None, lr 4.7101286972462485e-05
best loss 226.07276916503906
layer8: self_attn.k_proj
norm_0 tensor([1.6221, 1.6260, 1.6396,  ..., 1.5820, 1.5830, 1.6133], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5146, 0.6313, 0.6147,  ..., 1.1768, 1.4805, 1.5146], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 251.90435791015625, val loss None, lr 0.001
iter 10, train loss 261.1092834472656, val loss None, lr 0.000729
iter 20, train loss 263.1272888183594, val loss None, lr 0.000531441
iter 30, train loss 256.8726806640625, val loss None, lr 0.000387420489
iter 40, train loss 252.20065307617188, val loss None, lr 0.00025418658283290005
iter 50, train loss 252.28692626953125, val loss None, lr 0.00018530201888518417
iter 60, train loss 251.80538940429688, val loss None, lr 0.0001350851717672993
iter 70, train loss 250.80032348632812, val loss None, lr 8.862938119652506e-05
iter 80, train loss 250.33447265625, val loss None, lr 6.461081889226677e-05
iter 90, train loss 249.69105529785156, val loss None, lr 4.7101286972462485e-05
best loss 238.01803588867188
layer8: self_attn.v_proj
norm_0 tensor([0.9595, 0.8696, 0.8955,  ..., 0.9707, 0.9312, 0.9160], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.0996, 1.1016, 1.0850,  ..., 0.9624, 0.9346, 0.9497], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 67.19867706298828, val loss None, lr 0.001
iter 10, train loss 67.94609069824219, val loss None, lr 0.0008100000000000001
iter 20, train loss 67.37181091308594, val loss None, lr 0.000531441
iter 30, train loss 67.17987060546875, val loss None, lr 0.000387420489
iter 40, train loss 66.86366271972656, val loss None, lr 0.00028242953648100003
iter 50, train loss 66.7703857421875, val loss None, lr 0.00022876792454961005
iter 60, train loss 66.73535919189453, val loss None, lr 0.00020589113209464906
iter 70, train loss 66.63726043701172, val loss None, lr 0.00016677181699666576
iter 80, train loss 66.55165100097656, val loss None, lr 0.00016677181699666576
iter 90, train loss 66.48751831054688, val loss None, lr 0.0001500946352969992
best loss 66.45199584960938
layer8: self_attn.o_proj
norm_0 tensor([0.9736, 0.9629, 0.9414,  ..., 0.8193, 0.8022, 0.8066], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.0117, 1.0107, 0.9648,  ..., 0.9902, 0.9966, 0.9839], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 4.18380069732666, val loss None, lr 0.001
iter 10, train loss 3.9947545528411865, val loss None, lr 0.001
iter 20, train loss 3.8468592166900635, val loss None, lr 0.0009000000000000001
iter 30, train loss 3.7501120567321777, val loss None, lr 0.0009000000000000001
iter 40, train loss 3.6573472023010254, val loss None, lr 0.0009000000000000001
iter 50, train loss 3.595146656036377, val loss None, lr 0.0009000000000000001
iter 60, train loss 3.5740485191345215, val loss None, lr 0.0008100000000000001
iter 70, train loss 3.538618564605713, val loss None, lr 0.000729
iter 80, train loss 3.554395914077759, val loss None, lr 0.000531441
iter 90, train loss 3.5261590480804443, val loss None, lr 0.00043046721
best loss 3.5183210372924805
layer8: mlp.gate_proj
norm_0 tensor([2.0664, 2.0273, 2.0488,  ..., 2.0664, 2.0195, 2.0566], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.7231, 0.6284, 0.5815,  ..., 0.5156, 0.5513, 0.5308], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 147.74618530273438, val loss None, lr 0.001
iter 10, train loss 154.3045654296875, val loss None, lr 0.0008100000000000001
iter 20, train loss 150.57481384277344, val loss None, lr 0.000531441
iter 30, train loss 150.15609741210938, val loss None, lr 0.000387420489
iter 40, train loss 149.41526794433594, val loss None, lr 0.00028242953648100003
iter 50, train loss 149.18873596191406, val loss None, lr 0.00018530201888518417
iter 60, train loss 148.9459686279297, val loss None, lr 0.0001350851717672993
iter 70, train loss 148.866455078125, val loss None, lr 9.847709021836118e-05
iter 80, train loss 148.785888671875, val loss None, lr 6.461081889226677e-05
iter 90, train loss 148.76051330566406, val loss None, lr 4.7101286972462485e-05
best loss 146.58441162109375
layer8: mlp.up_proj
norm_0 tensor([1.8564, 1.8447, 1.8408,  ..., 1.8555, 1.8818, 1.8525], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.6270, 0.6255, 0.6157,  ..., 0.6050, 0.6201, 0.6162], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 117.77993774414062, val loss None, lr 0.001
iter 10, train loss 118.31896209716797, val loss None, lr 0.000729
iter 20, train loss 118.21568298339844, val loss None, lr 0.000531441
iter 30, train loss 118.012451171875, val loss None, lr 0.0003486784401
iter 40, train loss 117.81309509277344, val loss None, lr 0.00025418658283290005
iter 50, train loss 117.71050262451172, val loss None, lr 0.00022876792454961005
iter 60, train loss 117.73818969726562, val loss None, lr 0.0001500946352969992
iter 70, train loss 117.67850494384766, val loss None, lr 0.00010941898913151243
iter 80, train loss 117.63002014160156, val loss None, lr 7.976644307687256e-05
iter 90, train loss 117.68216705322266, val loss None, lr 6.461081889226677e-05
best loss 117.594482421875
layer8: mlp.down_proj
norm_0 tensor([1.1572, 1.1436, 1.1426,  ..., 1.1152, 1.1523, 1.1123], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.6680, 1.6211, 1.5938,  ..., 1.6641, 1.6523, 1.6270], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 2.1578900814056396, val loss None, lr 0.001
iter 10, train loss 2.1622514724731445, val loss None, lr 0.000729
iter 20, train loss 2.1539103984832764, val loss None, lr 0.00059049
iter 30, train loss 2.148916482925415, val loss None, lr 0.000531441
iter 40, train loss 2.1452324390411377, val loss None, lr 0.000531441
iter 50, train loss 2.1380958557128906, val loss None, lr 0.0004782969
iter 60, train loss 2.1370158195495605, val loss None, lr 0.000387420489
iter 70, train loss 2.136162519454956, val loss None, lr 0.00028242953648100003
iter 80, train loss 2.134368896484375, val loss None, lr 0.00022876792454961005
iter 90, train loss 2.1331706047058105, val loss None, lr 0.00020589113209464906
best loss 2.132164239883423
35495 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
27917 MiB free out of 48676 MiB total
after cast to cpu
35151 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
layer9: self_attn.q_proj
norm_0 tensor([1.5449, 1.6074, 1.5713,  ..., 1.6270, 1.5781, 1.6377], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.6655, 0.7217, 0.7026,  ..., 1.2314, 1.2549, 1.3174], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 248.77862548828125, val loss None, lr 0.001
iter 10, train loss 265.84625244140625, val loss None, lr 0.000729
iter 20, train loss 255.3612060546875, val loss None, lr 0.000531441
iter 30, train loss 250.76339721679688, val loss None, lr 0.000387420489
iter 40, train loss 249.19009399414062, val loss None, lr 0.00025418658283290005
iter 50, train loss 250.14303588867188, val loss None, lr 0.00018530201888518417
iter 60, train loss 248.97640991210938, val loss None, lr 0.0001350851717672993
iter 70, train loss 248.5166473388672, val loss None, lr 8.862938119652506e-05
iter 80, train loss 247.5262908935547, val loss None, lr 6.461081889226677e-05
iter 90, train loss 246.8590087890625, val loss None, lr 4.7101286972462485e-05
best loss 236.770263671875
layer9: self_attn.k_proj
norm_0 tensor([1.6807, 1.6455, 1.6953,  ..., 1.6182, 1.6025, 1.6367], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5908, 0.6958, 0.6855,  ..., 1.2959, 1.3516, 1.3848], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 271.9476013183594, val loss None, lr 0.001
iter 10, train loss 284.1688232421875, val loss None, lr 0.000729
iter 20, train loss 282.2259521484375, val loss None, lr 0.000531441
iter 30, train loss 272.4811706542969, val loss None, lr 0.000387420489
iter 40, train loss 270.4476318359375, val loss None, lr 0.00025418658283290005
iter 50, train loss 270.00262451171875, val loss None, lr 0.00018530201888518417
iter 60, train loss 269.9408264160156, val loss None, lr 0.0001350851717672993
iter 70, train loss 270.2115478515625, val loss None, lr 8.862938119652506e-05
iter 80, train loss 269.5718688964844, val loss None, lr 6.461081889226677e-05
iter 90, train loss 269.0552978515625, val loss None, lr 4.7101286972462485e-05
best loss 258.75592041015625
layer9: self_attn.v_proj
norm_0 tensor([0.9727, 0.9111, 0.8906,  ..., 0.9683, 0.9805, 0.9292], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.1680, 1.1650, 1.1729,  ..., 0.8726, 0.8779, 0.8560], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 72.9550552368164, val loss None, lr 0.001
iter 10, train loss 73.41791534423828, val loss None, lr 0.0008100000000000001
iter 20, train loss 73.01463317871094, val loss None, lr 0.000531441
iter 30, train loss 72.6396255493164, val loss None, lr 0.00043046721
iter 40, train loss 72.5980224609375, val loss None, lr 0.0003486784401
iter 50, train loss 72.49163055419922, val loss None, lr 0.00028242953648100003
iter 60, train loss 72.54827117919922, val loss None, lr 0.00025418658283290005
iter 70, train loss 72.59268188476562, val loss None, lr 0.00016677181699666576
iter 80, train loss 72.44692993164062, val loss None, lr 0.00012157665459056936
iter 90, train loss 72.26016235351562, val loss None, lr 0.00012157665459056936
best loss 72.24652099609375
layer9: self_attn.o_proj
norm_0 tensor([1.0342, 1.0449, 1.0400,  ..., 0.8188, 0.8184, 0.7915], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.0186, 1.0156, 0.9429,  ..., 1.0137, 0.9878, 1.0088], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 5.0270771980285645, val loss None, lr 0.001
iter 10, train loss 4.789884567260742, val loss None, lr 0.0009000000000000001
iter 20, train loss 4.7171406745910645, val loss None, lr 0.0008100000000000001
iter 30, train loss 4.66368293762207, val loss None, lr 0.0008100000000000001
iter 40, train loss 4.62687349319458, val loss None, lr 0.000729
iter 50, train loss 4.587089538574219, val loss None, lr 0.0006561000000000001
iter 60, train loss 4.5818681716918945, val loss None, lr 0.000531441
iter 70, train loss 4.554893493652344, val loss None, lr 0.0004782969
iter 80, train loss 4.565862655639648, val loss None, lr 0.000387420489
iter 90, train loss 4.56058406829834, val loss None, lr 0.00028242953648100003
best loss 4.545957088470459
layer9: mlp.gate_proj
norm_0 tensor([2.0605, 2.0234, 2.0059,  ..., 2.0391, 2.0254, 2.0254], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5283, 0.7275, 0.5498,  ..., 0.5586, 0.5161, 0.5200], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 153.99398803710938, val loss None, lr 0.001
iter 10, train loss 160.96356201171875, val loss None, lr 0.0008100000000000001
iter 20, train loss 156.538330078125, val loss None, lr 0.00059049
iter 30, train loss 156.53245544433594, val loss None, lr 0.000387420489
iter 40, train loss 155.68051147460938, val loss None, lr 0.00028242953648100003
iter 50, train loss 155.25196838378906, val loss None, lr 0.00020589113209464906
iter 60, train loss 154.62283325195312, val loss None, lr 0.0001350851717672993
iter 70, train loss 154.47817993164062, val loss None, lr 9.847709021836118e-05
iter 80, train loss 154.43417358398438, val loss None, lr 7.17897987691853e-05
iter 90, train loss 154.3743896484375, val loss None, lr 4.7101286972462485e-05
best loss 152.17884826660156
layer9: mlp.up_proj
norm_0 tensor([1.8604, 1.8662, 1.8672,  ..., 1.8809, 1.8828, 1.8701], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.6807, 0.6201, 0.6006,  ..., 0.6367, 0.6025, 0.5957], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 126.74079895019531, val loss None, lr 0.001
iter 10, train loss 127.4397964477539, val loss None, lr 0.0008100000000000001
iter 20, train loss 127.464599609375, val loss None, lr 0.000531441
iter 30, train loss 127.40690612792969, val loss None, lr 0.000387420489
iter 40, train loss 127.29249572753906, val loss None, lr 0.00028242953648100003
iter 50, train loss 127.1650390625, val loss None, lr 0.00018530201888518417
iter 60, train loss 126.99165344238281, val loss None, lr 0.0001350851717672993
iter 70, train loss 127.02059173583984, val loss None, lr 9.847709021836118e-05
iter 80, train loss 126.87052917480469, val loss None, lr 6.461081889226677e-05
iter 90, train loss 126.92035675048828, val loss None, lr 4.7101286972462485e-05
best loss 126.65758514404297
layer9: mlp.down_proj
norm_0 tensor([1.2451, 1.1426, 1.1172,  ..., 1.1104, 1.0918, 1.1064], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.6816, 1.6230, 1.5801,  ..., 1.6689, 1.6455, 1.6553], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 2.598339080810547, val loss None, lr 0.001
iter 10, train loss 2.595242977142334, val loss None, lr 0.0008100000000000001
iter 20, train loss 2.5913209915161133, val loss None, lr 0.0006561000000000001
iter 30, train loss 2.5810723304748535, val loss None, lr 0.0004782969
iter 40, train loss 2.577676773071289, val loss None, lr 0.000387420489
iter 50, train loss 2.5739574432373047, val loss None, lr 0.0003486784401
iter 60, train loss 2.5673444271087646, val loss None, lr 0.00031381059609000004
iter 70, train loss 2.565870761871338, val loss None, lr 0.00025418658283290005
iter 80, train loss 2.561386823654175, val loss None, lr 0.00022876792454961005
iter 90, train loss 2.5626578330993652, val loss None, lr 0.00016677181699666576
best loss 2.559617042541504
35151 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
27509 MiB free out of 48676 MiB total
after cast to cpu
34743 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
layer10: self_attn.q_proj
norm_0 tensor([1.5840, 1.6006, 1.5547,  ..., 1.6133, 1.6113, 1.6143], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5054, 0.5371, 0.5088,  ..., 1.2539, 1.3711, 1.3086], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 250.7781524658203, val loss None, lr 0.001
iter 10, train loss 265.2108154296875, val loss None, lr 0.000729
iter 20, train loss 256.9420471191406, val loss None, lr 0.000531441
iter 30, train loss 252.23312377929688, val loss None, lr 0.000387420489
iter 40, train loss 250.6046142578125, val loss None, lr 0.00025418658283290005
iter 50, train loss 250.7818145751953, val loss None, lr 0.00018530201888518417
iter 60, train loss 249.875, val loss None, lr 0.0001350851717672993
iter 70, train loss 249.74679565429688, val loss None, lr 8.862938119652506e-05
iter 80, train loss 249.77444458007812, val loss None, lr 6.461081889226677e-05
iter 90, train loss 249.9131317138672, val loss None, lr 4.7101286972462485e-05
best loss 239.98092651367188
layer10: self_attn.k_proj
norm_0 tensor([1.6543, 1.6719, 1.6875,  ..., 1.5859, 1.6025, 1.6729], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.4500, 0.5078, 0.4966,  ..., 1.3223, 1.3271, 1.2959], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 278.5596923828125, val loss None, lr 0.001
iter 10, train loss 296.5545959472656, val loss None, lr 0.0008100000000000001
iter 20, train loss 292.4080810546875, val loss None, lr 0.00059049
iter 30, train loss 285.5135498046875, val loss None, lr 0.00043046721
iter 40, train loss 282.096923828125, val loss None, lr 0.00028242953648100003
iter 50, train loss 281.688232421875, val loss None, lr 0.00020589113209464906
iter 60, train loss 280.7181091308594, val loss None, lr 0.0001500946352969992
iter 70, train loss 280.60986328125, val loss None, lr 9.847709021836118e-05
iter 80, train loss 278.94122314453125, val loss None, lr 7.17897987691853e-05
iter 90, train loss 278.5670166015625, val loss None, lr 5.233476330273609e-05
best loss 267.00408935546875
layer10: self_attn.v_proj
norm_0 tensor([0.9683, 0.9048, 0.8916,  ..., 0.9570, 0.9429, 0.9282], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.2246, 1.2041, 1.2119,  ..., 1.0098, 0.9971, 1.0068], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 72.73236846923828, val loss None, lr 0.001
iter 10, train loss 73.2098159790039, val loss None, lr 0.0008100000000000001
iter 20, train loss 72.75553894042969, val loss None, lr 0.000531441
iter 30, train loss 72.30992126464844, val loss None, lr 0.0004782969
iter 40, train loss 72.31645965576172, val loss None, lr 0.0003486784401
iter 50, train loss 72.20976257324219, val loss None, lr 0.00031381059609000004
iter 60, train loss 72.16258239746094, val loss None, lr 0.00022876792454961005
iter 70, train loss 72.12251281738281, val loss None, lr 0.00018530201888518417
iter 80, train loss 72.09416961669922, val loss None, lr 0.0001500946352969992
iter 90, train loss 72.01332092285156, val loss None, lr 0.00012157665459056936
best loss 71.89521789550781
layer10: self_attn.o_proj
norm_0 tensor([1.0889, 1.0732, 1.0840,  ..., 0.8955, 0.8984, 0.8911], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.0137, 0.9883, 0.9497,  ..., 0.9912, 0.9956, 0.9736], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 7.455657005310059, val loss None, lr 0.001
iter 10, train loss 6.965734481811523, val loss None, lr 0.001
iter 20, train loss 6.809276580810547, val loss None, lr 0.0009000000000000001
iter 30, train loss 6.6087446212768555, val loss None, lr 0.0009000000000000001
iter 40, train loss 6.478270053863525, val loss None, lr 0.0009000000000000001
iter 50, train loss 6.402967929840088, val loss None, lr 0.0009000000000000001
iter 60, train loss 6.381966590881348, val loss None, lr 0.0008100000000000001
iter 70, train loss 6.338979244232178, val loss None, lr 0.000729
iter 80, train loss 6.328197002410889, val loss None, lr 0.00059049
iter 90, train loss 6.302006721496582, val loss None, lr 0.00059049
best loss 6.297307968139648
layer10: mlp.gate_proj
norm_0 tensor([2.0371, 1.9961, 2.0020,  ..., 2.0371, 1.9932, 2.0195], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.6074, 0.6484, 0.5117,  ..., 0.5186, 0.4858, 0.6157], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 160.04098510742188, val loss None, lr 0.001
iter 10, train loss 167.8102569580078, val loss None, lr 0.0008100000000000001
iter 20, train loss 164.13717651367188, val loss None, lr 0.000531441
iter 30, train loss 163.63082885742188, val loss None, lr 0.000387420489
iter 40, train loss 163.04039001464844, val loss None, lr 0.00028242953648100003
iter 50, train loss 162.73477172851562, val loss None, lr 0.00018530201888518417
iter 60, train loss 162.233642578125, val loss None, lr 0.0001350851717672993
iter 70, train loss 162.0119171142578, val loss None, lr 9.847709021836118e-05
iter 80, train loss 161.75656127929688, val loss None, lr 6.461081889226677e-05
iter 90, train loss 161.7502899169922, val loss None, lr 4.7101286972462485e-05
best loss 158.489501953125
layer10: mlp.up_proj
norm_0 tensor([1.8994, 1.8828, 1.8857,  ..., 1.8867, 1.9229, 1.8984], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.6636, 0.6587, 0.5776,  ..., 0.5815, 0.5786, 0.5996], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 135.2626190185547, val loss None, lr 0.001
iter 10, train loss 136.25784301757812, val loss None, lr 0.0008100000000000001
iter 20, train loss 136.42825317382812, val loss None, lr 0.000531441
iter 30, train loss 136.0391082763672, val loss None, lr 0.000387420489
iter 40, train loss 135.95687866210938, val loss None, lr 0.00028242953648100003
iter 50, train loss 135.78749084472656, val loss None, lr 0.00018530201888518417
iter 60, train loss 135.7259521484375, val loss None, lr 0.0001350851717672993
iter 70, train loss 135.74147033691406, val loss None, lr 9.847709021836118e-05
iter 80, train loss 135.7383575439453, val loss None, lr 6.461081889226677e-05
iter 90, train loss 135.6473388671875, val loss None, lr 4.7101286972462485e-05
best loss 134.89048767089844
layer10: mlp.down_proj
norm_0 tensor([1.2129, 1.2090, 1.0928,  ..., 1.1016, 1.0967, 1.0996], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.6562, 1.6104, 1.5859,  ..., 1.6670, 1.6445, 1.6562], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 3.130706310272217, val loss None, lr 0.001
iter 10, train loss 3.130127191543579, val loss None, lr 0.0008100000000000001
iter 20, train loss 3.095533847808838, val loss None, lr 0.0006561000000000001
iter 30, train loss 3.0633740425109863, val loss None, lr 0.0006561000000000001
iter 40, train loss 3.052551746368408, val loss None, lr 0.0006561000000000001
iter 50, train loss 3.0504260063171387, val loss None, lr 0.00043046721
iter 60, train loss 3.048895835876465, val loss None, lr 0.000387420489
iter 70, train loss 3.0379791259765625, val loss None, lr 0.0003486784401
iter 80, train loss 3.040975570678711, val loss None, lr 0.00025418658283290005
iter 90, train loss 3.0275051593780518, val loss None, lr 0.00022876792454961005
best loss 3.0265395641326904
34743 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
27101 MiB free out of 48676 MiB total
after cast to cpu
34333 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
layer11: self_attn.q_proj
norm_0 tensor([1.5352, 1.5703, 1.5000,  ..., 1.5029, 1.5508, 1.5420], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.6152, 0.5410, 0.5269,  ..., 1.3330, 1.2666, 1.3496], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 290.44110107421875, val loss None, lr 0.001
iter 10, train loss 311.06280517578125, val loss None, lr 0.000729
iter 20, train loss 302.56585693359375, val loss None, lr 0.000531441
iter 30, train loss 297.946044921875, val loss None, lr 0.000387420489
iter 40, train loss 294.8778381347656, val loss None, lr 0.00025418658283290005
iter 50, train loss 293.3772888183594, val loss None, lr 0.00018530201888518417
iter 60, train loss 292.03619384765625, val loss None, lr 0.0001350851717672993
iter 70, train loss 291.3187255859375, val loss None, lr 8.862938119652506e-05
iter 80, train loss 292.42254638671875, val loss None, lr 6.461081889226677e-05
iter 90, train loss 292.5311279296875, val loss None, lr 4.7101286972462485e-05
best loss 272.67901611328125
layer11: self_attn.k_proj
norm_0 tensor([1.4922, 1.5254, 1.5713,  ..., 1.5029, 1.4570, 1.5527], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5928, 0.5405, 0.5244,  ..., 1.2598, 1.2256, 1.2422], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 295.6694641113281, val loss None, lr 0.001
iter 10, train loss 311.28369140625, val loss None, lr 0.000729
iter 20, train loss 310.828369140625, val loss None, lr 0.000531441
iter 30, train loss 304.4806823730469, val loss None, lr 0.000387420489
iter 40, train loss 300.3717041015625, val loss None, lr 0.00025418658283290005
iter 50, train loss 298.0647888183594, val loss None, lr 0.00018530201888518417
iter 60, train loss 297.1405334472656, val loss None, lr 0.0001350851717672993
iter 70, train loss 296.43048095703125, val loss None, lr 8.862938119652506e-05
iter 80, train loss 296.23052978515625, val loss None, lr 6.461081889226677e-05
iter 90, train loss 295.3123474121094, val loss None, lr 4.7101286972462485e-05
best loss 279.0497741699219
layer11: self_attn.v_proj
norm_0 tensor([0.9839, 0.9263, 0.9023,  ..., 1.0098, 0.9917, 0.9473], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.9438, 0.9624, 0.9556,  ..., 1.0645, 1.0928, 1.0693], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 99.46204376220703, val loss None, lr 0.001
iter 10, train loss 99.835205078125, val loss None, lr 0.0008100000000000001
iter 20, train loss 99.34292602539062, val loss None, lr 0.000531441
iter 30, train loss 98.5914077758789, val loss None, lr 0.0004782969
iter 40, train loss 98.81977844238281, val loss None, lr 0.0003486784401
iter 50, train loss 98.5327377319336, val loss None, lr 0.00025418658283290005
iter 60, train loss 98.47197723388672, val loss None, lr 0.00020589113209464906
iter 70, train loss 98.34547424316406, val loss None, lr 0.00018530201888518417
iter 80, train loss 98.28997802734375, val loss None, lr 0.00012157665459056936
iter 90, train loss 98.15164184570312, val loss None, lr 0.00012157665459056936
best loss 98.03890228271484
layer11: self_attn.o_proj
norm_0 tensor([0.8975, 0.9155, 0.8994,  ..., 0.9780, 0.9990, 0.9839], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.0078, 0.9941, 0.9434,  ..., 0.9971, 0.9868, 0.9946], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 8.595672607421875, val loss None, lr 0.001
iter 10, train loss 8.48729133605957, val loss None, lr 0.0009000000000000001
iter 20, train loss 8.46530532836914, val loss None, lr 0.000729
iter 30, train loss 8.52853012084961, val loss None, lr 0.00059049
iter 40, train loss 8.41503620147705, val loss None, lr 0.0004782969
iter 50, train loss 8.395045280456543, val loss None, lr 0.000387420489
iter 60, train loss 8.319374084472656, val loss None, lr 0.00031381059609000004
iter 70, train loss 8.284980773925781, val loss None, lr 0.00028242953648100003
iter 80, train loss 8.245347023010254, val loss None, lr 0.00028242953648100003
iter 90, train loss 8.243040084838867, val loss None, lr 0.00022876792454961005
best loss 8.218496322631836
layer11: mlp.gate_proj
norm_0 tensor([2.0176, 2.0156, 2.0137,  ..., 2.0137, 2.0098, 1.9883], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.6592, 0.6074, 0.5933,  ..., 0.6416, 0.4954, 0.5454], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 172.718017578125, val loss None, lr 0.001
iter 10, train loss 182.0983428955078, val loss None, lr 0.0008100000000000001
iter 20, train loss 177.91473388671875, val loss None, lr 0.00059049
iter 30, train loss 177.7130584716797, val loss None, lr 0.000387420489
iter 40, train loss 176.75173950195312, val loss None, lr 0.00028242953648100003
iter 50, train loss 176.12741088867188, val loss None, lr 0.00020589113209464906
iter 60, train loss 175.61691284179688, val loss None, lr 0.0001350851717672993
iter 70, train loss 175.51165771484375, val loss None, lr 9.847709021836118e-05
iter 80, train loss 175.2346649169922, val loss None, lr 7.17897987691853e-05
iter 90, train loss 174.89242553710938, val loss None, lr 4.7101286972462485e-05
best loss 170.6978759765625
layer11: mlp.up_proj
norm_0 tensor([1.9180, 1.8945, 1.8945,  ..., 1.9150, 1.9199, 1.9229], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.6187, 0.6519, 0.6460,  ..., 0.5723, 0.5889, 0.5908], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 149.7044677734375, val loss None, lr 0.001
iter 10, train loss 150.81076049804688, val loss None, lr 0.0008100000000000001
iter 20, train loss 151.32073974609375, val loss None, lr 0.000531441
iter 30, train loss 151.0188751220703, val loss None, lr 0.000387420489
iter 40, train loss 150.66531372070312, val loss None, lr 0.00028242953648100003
iter 50, train loss 150.51112365722656, val loss None, lr 0.00018530201888518417
iter 60, train loss 150.39865112304688, val loss None, lr 0.0001350851717672993
iter 70, train loss 150.33348083496094, val loss None, lr 9.847709021836118e-05
iter 80, train loss 150.30674743652344, val loss None, lr 6.461081889226677e-05
iter 90, train loss 150.32275390625, val loss None, lr 4.7101286972462485e-05
best loss 149.55902099609375
layer11: mlp.down_proj
norm_0 tensor([1.1641, 1.1953, 1.2109,  ..., 1.0713, 1.1064, 1.1221], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.6807, 1.6084, 1.5986,  ..., 1.6611, 1.6416, 1.6543], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 3.4654815196990967, val loss None, lr 0.001
iter 10, train loss 3.4546022415161133, val loss None, lr 0.0008100000000000001
iter 20, train loss 3.438166618347168, val loss None, lr 0.000729
iter 30, train loss 3.4226131439208984, val loss None, lr 0.0006561000000000001
iter 40, train loss 3.414583683013916, val loss None, lr 0.00059049
iter 50, train loss 3.401247024536133, val loss None, lr 0.000531441
iter 60, train loss 3.4014618396759033, val loss None, lr 0.000387420489
iter 70, train loss 3.4022741317749023, val loss None, lr 0.00028242953648100003
iter 80, train loss 3.396974563598633, val loss None, lr 0.00022876792454961005
iter 90, train loss 3.394455909729004, val loss None, lr 0.00020589113209464906
best loss 3.3899264335632324
34333 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
26693 MiB free out of 48676 MiB total
after cast to cpu
33925 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
layer12: self_attn.q_proj
norm_0 tensor([1.5273, 1.5771, 1.5420,  ..., 1.5244, 1.5674, 1.5654], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5688, 0.6431, 0.6504,  ..., 1.0703, 1.0967, 0.9834], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 296.04443359375, val loss None, lr 0.001
iter 10, train loss 315.838623046875, val loss None, lr 0.000729
iter 20, train loss 306.1480712890625, val loss None, lr 0.000531441
iter 30, train loss 302.3908996582031, val loss None, lr 0.000387420489
iter 40, train loss 300.77276611328125, val loss None, lr 0.00025418658283290005
iter 50, train loss 299.8311462402344, val loss None, lr 0.00018530201888518417
iter 60, train loss 299.00848388671875, val loss None, lr 0.0001350851717672993
iter 70, train loss 298.0943603515625, val loss None, lr 8.862938119652506e-05
iter 80, train loss 297.8589782714844, val loss None, lr 6.461081889226677e-05
iter 90, train loss 297.204345703125, val loss None, lr 4.7101286972462485e-05
best loss 281.3887939453125
layer12: self_attn.k_proj
norm_0 tensor([1.6348, 1.6309, 1.6162,  ..., 1.5889, 1.5713, 1.6162], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5381, 0.6201, 0.6494,  ..., 1.0996, 1.1416, 1.0020], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 329.9499816894531, val loss None, lr 0.001
iter 10, train loss 349.7535400390625, val loss None, lr 0.000729
iter 20, train loss 345.36248779296875, val loss None, lr 0.000531441
iter 30, train loss 337.275390625, val loss None, lr 0.000387420489
iter 40, train loss 333.56195068359375, val loss None, lr 0.00025418658283290005
iter 50, train loss 333.0616760253906, val loss None, lr 0.00018530201888518417
iter 60, train loss 330.9536437988281, val loss None, lr 0.0001350851717672993
iter 70, train loss 330.50323486328125, val loss None, lr 8.862938119652506e-05
iter 80, train loss 329.62969970703125, val loss None, lr 6.461081889226677e-05
iter 90, train loss 328.91766357421875, val loss None, lr 4.7101286972462485e-05
best loss 310.5864562988281
layer12: self_attn.v_proj
norm_0 tensor([0.9805, 0.8818, 0.9146,  ..., 0.9917, 0.9771, 0.9478], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.9761, 0.9873, 0.9897,  ..., 0.8320, 0.8545, 0.8218], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 96.6911392211914, val loss None, lr 0.001
iter 10, train loss 97.19219207763672, val loss None, lr 0.0008100000000000001
iter 20, train loss 96.56889343261719, val loss None, lr 0.000531441
iter 30, train loss 96.30087280273438, val loss None, lr 0.00043046721
iter 40, train loss 96.13943481445312, val loss None, lr 0.0003486784401
iter 50, train loss 96.0213623046875, val loss None, lr 0.0003486784401
iter 60, train loss 95.92300415039062, val loss None, lr 0.00028242953648100003
iter 70, train loss 96.05232238769531, val loss None, lr 0.00020589113209464906
iter 80, train loss 96.00940704345703, val loss None, lr 0.0001500946352969992
iter 90, train loss 95.89479064941406, val loss None, lr 0.00010941898913151243
best loss 95.88169860839844
layer12: self_attn.o_proj
norm_0 tensor([0.9019, 0.9160, 0.9263,  ..., 0.8096, 0.8281, 0.8232], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.0186, 1.0127, 0.9609,  ..., 1.0098, 0.9951, 1.0020], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 9.419350624084473, val loss None, lr 0.001
iter 10, train loss 9.319844245910645, val loss None, lr 0.0009000000000000001
iter 20, train loss 9.286184310913086, val loss None, lr 0.000729
iter 30, train loss 9.22750473022461, val loss None, lr 0.000729
iter 40, train loss 9.214190483093262, val loss None, lr 0.000531441
iter 50, train loss 9.225465774536133, val loss None, lr 0.000387420489
iter 60, train loss 9.18736457824707, val loss None, lr 0.00028242953648100003
iter 70, train loss 9.153127670288086, val loss None, lr 0.00025418658283290005
iter 80, train loss 9.10876750946045, val loss None, lr 0.00025418658283290005
iter 90, train loss 9.06656551361084, val loss None, lr 0.00025418658283290005
best loss 9.057074546813965
layer12: mlp.gate_proj
norm_0 tensor([1.9873, 1.9941, 1.9922,  ..., 1.9883, 1.9795, 2.0000], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.7197, 0.5757, 0.6562,  ..., 0.5669, 0.5142, 0.5156], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 183.9884490966797, val loss None, lr 0.001
iter 10, train loss 193.4909210205078, val loss None, lr 0.0008100000000000001
iter 20, train loss 188.51614379882812, val loss None, lr 0.00059049
iter 30, train loss 188.87765502929688, val loss None, lr 0.000387420489
iter 40, train loss 187.883544921875, val loss None, lr 0.00028242953648100003
iter 50, train loss 187.4158935546875, val loss None, lr 0.00020589113209464906
iter 60, train loss 187.17193603515625, val loss None, lr 0.0001350851717672993
iter 70, train loss 186.7762908935547, val loss None, lr 9.847709021836118e-05
iter 80, train loss 186.2612762451172, val loss None, lr 7.17897987691853e-05
iter 90, train loss 186.31961059570312, val loss None, lr 4.7101286972462485e-05
best loss 182.01345825195312
layer12: mlp.up_proj
norm_0 tensor([1.9453, 1.9141, 1.9209,  ..., 1.9414, 1.9346, 1.9307], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.6260, 0.6333, 0.5864,  ..., 0.6069, 0.5874, 0.5762], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 164.05795288085938, val loss None, lr 0.001
iter 10, train loss 164.8375244140625, val loss None, lr 0.0008100000000000001
iter 20, train loss 165.089599609375, val loss None, lr 0.000531441
iter 30, train loss 164.82485961914062, val loss None, lr 0.000387420489
iter 40, train loss 164.7161865234375, val loss None, lr 0.00028242953648100003
iter 50, train loss 164.7864990234375, val loss None, lr 0.00018530201888518417
iter 60, train loss 164.613525390625, val loss None, lr 0.0001350851717672993
iter 70, train loss 164.49713134765625, val loss None, lr 9.847709021836118e-05
iter 80, train loss 164.45059204101562, val loss None, lr 6.461081889226677e-05
iter 90, train loss 164.54249572753906, val loss None, lr 4.7101286972462485e-05
best loss 163.93287658691406
layer12: mlp.down_proj
norm_0 tensor([1.2061, 1.2090, 1.1094,  ..., 1.1533, 1.1279, 1.1035], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.6914, 1.6230, 1.5947,  ..., 1.6689, 1.6689, 1.6318], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 3.8775289058685303, val loss None, lr 0.001
iter 10, train loss 3.894619941711426, val loss None, lr 0.000729
iter 20, train loss 3.8939218521118164, val loss None, lr 0.000531441
iter 30, train loss 3.8918275833129883, val loss None, lr 0.0003486784401
iter 40, train loss 3.88330078125, val loss None, lr 0.00025418658283290005
iter 50, train loss 3.884125232696533, val loss None, lr 0.00018530201888518417
iter 60, train loss 3.8871700763702393, val loss None, lr 0.00012157665459056936
iter 70, train loss 3.8844797611236572, val loss None, lr 8.862938119652506e-05
iter 80, train loss 3.878490447998047, val loss None, lr 6.461081889226677e-05
iter 90, train loss 3.880647659301758, val loss None, lr 4.239115827521624e-05
best loss 3.8775289058685303
33925 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
26285 MiB free out of 48676 MiB total
after cast to cpu
33517 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
layer13: self_attn.q_proj
norm_0 tensor([1.5020, 1.5752, 1.5205,  ..., 1.5469, 1.5420, 1.5430], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.6719, 0.6899, 0.7017,  ..., 1.1709, 1.1445, 1.1143], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 309.99542236328125, val loss None, lr 0.001
iter 10, train loss 327.4581604003906, val loss None, lr 0.000729
iter 20, train loss 314.4801025390625, val loss None, lr 0.000531441
iter 30, train loss 308.6048583984375, val loss None, lr 0.000387420489
iter 40, train loss 307.43634033203125, val loss None, lr 0.00025418658283290005
iter 50, train loss 307.16595458984375, val loss None, lr 0.00018530201888518417
iter 60, train loss 305.7455139160156, val loss None, lr 0.0001350851717672993
iter 70, train loss 305.50823974609375, val loss None, lr 8.862938119652506e-05
iter 80, train loss 305.5419921875, val loss None, lr 6.461081889226677e-05
iter 90, train loss 304.9696044921875, val loss None, lr 4.7101286972462485e-05
best loss 289.4130554199219
layer13: self_attn.k_proj
norm_0 tensor([1.6123, 1.5781, 1.5908,  ..., 1.5596, 1.5391, 1.5684], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5093, 0.6797, 0.6953,  ..., 1.2012, 1.2188, 1.1230], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 333.7035217285156, val loss None, lr 0.001
iter 10, train loss 351.7511291503906, val loss None, lr 0.000729
iter 20, train loss 347.6671142578125, val loss None, lr 0.000531441
iter 30, train loss 338.49639892578125, val loss None, lr 0.000387420489
iter 40, train loss 334.126220703125, val loss None, lr 0.00025418658283290005
iter 50, train loss 329.9734802246094, val loss None, lr 0.00018530201888518417
iter 60, train loss 327.911376953125, val loss None, lr 0.0001350851717672993
iter 70, train loss 327.88067626953125, val loss None, lr 8.862938119652506e-05
iter 80, train loss 326.9219665527344, val loss None, lr 6.461081889226677e-05
iter 90, train loss 325.8216857910156, val loss None, lr 4.7101286972462485e-05
best loss 309.640380859375
layer13: self_attn.v_proj
norm_0 tensor([1.0107, 0.9507, 0.9663,  ..., 0.9980, 1.0049, 0.9756], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.9341, 0.9478, 0.9473,  ..., 0.9712, 0.9614, 0.9858], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 107.50936889648438, val loss None, lr 0.001
iter 10, train loss 107.3650131225586, val loss None, lr 0.0008100000000000001
iter 20, train loss 107.28124237060547, val loss None, lr 0.00059049
iter 30, train loss 107.01298522949219, val loss None, lr 0.0004782969
iter 40, train loss 106.83369445800781, val loss None, lr 0.00043046721
iter 50, train loss 106.7755126953125, val loss None, lr 0.00031381059609000004
iter 60, train loss 106.58715057373047, val loss None, lr 0.00028242953648100003
iter 70, train loss 106.39913940429688, val loss None, lr 0.00022876792454961005
iter 80, train loss 106.38041687011719, val loss None, lr 0.00016677181699666576
iter 90, train loss 106.2524185180664, val loss None, lr 0.0001350851717672993
best loss 106.19625854492188
layer13: self_attn.o_proj
norm_0 tensor([0.8975, 0.9448, 0.9434,  ..., 0.9521, 0.9253, 0.9536], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.9922, 1.0107, 0.9492,  ..., 0.9976, 0.9951, 0.9917], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 9.154712677001953, val loss None, lr 0.001
iter 10, train loss 8.899977684020996, val loss None, lr 0.0009000000000000001
iter 20, train loss 8.802840232849121, val loss None, lr 0.0006561000000000001
iter 30, train loss 8.6136474609375, val loss None, lr 0.0006561000000000001
iter 40, train loss 8.55339527130127, val loss None, lr 0.0006561000000000001
iter 50, train loss 8.527182579040527, val loss None, lr 0.00059049
iter 60, train loss 8.46939754486084, val loss None, lr 0.000531441
iter 70, train loss 8.426523208618164, val loss None, lr 0.00043046721
iter 80, train loss 8.38863754272461, val loss None, lr 0.00043046721
iter 90, train loss 8.404924392700195, val loss None, lr 0.00031381059609000004
best loss 8.384170532226562
layer13: mlp.gate_proj
norm_0 tensor([1.9912, 1.9756, 1.9883,  ..., 1.9971, 2.0098, 1.9844], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.4390, 0.5815, 0.5342,  ..., 0.5488, 0.5723, 0.5757], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 194.43173217773438, val loss None, lr 0.001
iter 10, train loss 203.64300537109375, val loss None, lr 0.0008100000000000001
iter 20, train loss 199.06480407714844, val loss None, lr 0.000531441
iter 30, train loss 197.59796142578125, val loss None, lr 0.000387420489
iter 40, train loss 196.91232299804688, val loss None, lr 0.00028242953648100003
iter 50, train loss 196.18914794921875, val loss None, lr 0.00018530201888518417
iter 60, train loss 195.86788940429688, val loss None, lr 0.0001350851717672993
iter 70, train loss 195.61712646484375, val loss None, lr 9.847709021836118e-05
iter 80, train loss 195.48944091796875, val loss None, lr 6.461081889226677e-05
iter 90, train loss 195.64674377441406, val loss None, lr 4.7101286972462485e-05
best loss 192.29891967773438
layer13: mlp.up_proj
norm_0 tensor([1.9521, 1.9463, 1.9248,  ..., 1.9443, 1.9375, 1.9648], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5557, 0.5957, 0.6099,  ..., 0.5972, 0.5747, 0.6196], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 178.2397003173828, val loss None, lr 0.001
iter 10, train loss 178.86373901367188, val loss None, lr 0.0008100000000000001
iter 20, train loss 179.35064697265625, val loss None, lr 0.000531441
iter 30, train loss 179.1954345703125, val loss None, lr 0.000387420489
iter 40, train loss 178.93362426757812, val loss None, lr 0.00028242953648100003
iter 50, train loss 178.8526611328125, val loss None, lr 0.00018530201888518417
iter 60, train loss 178.7115936279297, val loss None, lr 0.0001350851717672993
iter 70, train loss 178.53334045410156, val loss None, lr 9.847709021836118e-05
iter 80, train loss 178.60293579101562, val loss None, lr 6.461081889226677e-05
iter 90, train loss 178.57113647460938, val loss None, lr 4.7101286972462485e-05
best loss 177.96017456054688
layer13: mlp.down_proj
norm_0 tensor([1.0791, 1.1406, 1.1748,  ..., 1.1504, 1.0996, 1.1953], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.6523, 1.6123, 1.5850,  ..., 1.6445, 1.6318, 1.6611], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 4.74000883102417, val loss None, lr 0.001
iter 10, train loss 4.729389667510986, val loss None, lr 0.0008100000000000001
iter 20, train loss 4.698696136474609, val loss None, lr 0.0008100000000000001
iter 30, train loss 4.647315502166748, val loss None, lr 0.0008100000000000001
iter 40, train loss 4.6323699951171875, val loss None, lr 0.000729
iter 50, train loss 4.61345911026001, val loss None, lr 0.0006561000000000001
iter 60, train loss 4.601228713989258, val loss None, lr 0.00059049
iter 70, train loss 4.586263656616211, val loss None, lr 0.0004782969
iter 80, train loss 4.591028690338135, val loss None, lr 0.0003486784401
iter 90, train loss 4.584598541259766, val loss None, lr 0.00028242953648100003
best loss 4.584292411804199
33517 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
25877 MiB free out of 48676 MiB total
after cast to cpu
33109 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
layer14: self_attn.q_proj
norm_0 tensor([1.5146, 1.5508, 1.5156,  ..., 1.5625, 1.5400, 1.5449], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.7290, 0.9404, 0.9341,  ..., 0.9673, 1.0361, 0.9829], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 324.10906982421875, val loss None, lr 0.001
iter 10, train loss 346.4737548828125, val loss None, lr 0.000729
iter 20, train loss 330.8660888671875, val loss None, lr 0.000531441
iter 30, train loss 325.71533203125, val loss None, lr 0.000387420489
iter 40, train loss 322.6541748046875, val loss None, lr 0.00025418658283290005
iter 50, train loss 321.7769775390625, val loss None, lr 0.00018530201888518417
iter 60, train loss 321.65625, val loss None, lr 0.0001350851717672993
iter 70, train loss 319.86865234375, val loss None, lr 8.862938119652506e-05
iter 80, train loss 320.7686462402344, val loss None, lr 6.461081889226677e-05
iter 90, train loss 321.0335998535156, val loss None, lr 4.7101286972462485e-05
best loss 300.2728576660156
layer14: self_attn.k_proj
norm_0 tensor([1.6133, 1.5752, 1.5879,  ..., 1.5352, 1.5273, 1.5488], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.6665, 0.9277, 0.9243,  ..., 0.9937, 1.0176, 0.9570], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 353.318359375, val loss None, lr 0.001
iter 10, train loss 372.498291015625, val loss None, lr 0.000729
iter 20, train loss 366.8419494628906, val loss None, lr 0.000531441
iter 30, train loss 358.8070068359375, val loss None, lr 0.000387420489
iter 40, train loss 354.7969970703125, val loss None, lr 0.00025418658283290005
iter 50, train loss 352.4020080566406, val loss None, lr 0.00018530201888518417
iter 60, train loss 350.5500183105469, val loss None, lr 0.0001350851717672993
iter 70, train loss 348.8928527832031, val loss None, lr 8.862938119652506e-05
iter 80, train loss 347.7776184082031, val loss None, lr 6.461081889226677e-05
iter 90, train loss 347.744140625, val loss None, lr 4.7101286972462485e-05
best loss 323.34149169921875
layer14: self_attn.v_proj
norm_0 tensor([0.9697, 0.9165, 0.9658,  ..., 0.9917, 0.9927, 0.9868], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.9492, 0.9385, 0.9473,  ..., 1.1113, 1.1074, 1.1270], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 108.32516479492188, val loss None, lr 0.001
iter 10, train loss 107.80633544921875, val loss None, lr 0.0008100000000000001
iter 20, train loss 107.38211059570312, val loss None, lr 0.000729
iter 30, train loss 107.14602661132812, val loss None, lr 0.0006561000000000001
iter 40, train loss 107.10882568359375, val loss None, lr 0.0004782969
iter 50, train loss 106.8791275024414, val loss None, lr 0.00043046721
iter 60, train loss 106.78355407714844, val loss None, lr 0.0003486784401
iter 70, train loss 106.7855224609375, val loss None, lr 0.00028242953648100003
iter 80, train loss 106.85762023925781, val loss None, lr 0.00020589113209464906
iter 90, train loss 106.72676849365234, val loss None, lr 0.0001500946352969992
best loss 106.6780776977539
layer14: self_attn.o_proj
norm_0 tensor([0.8921, 0.8745, 0.8906,  ..., 1.0439, 1.0264, 1.0498], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.9810, 0.9897, 0.9609,  ..., 1.0146, 0.9961, 1.0293], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 12.30004596710205, val loss None, lr 0.001
iter 10, train loss 11.922659873962402, val loss None, lr 0.0009000000000000001
iter 20, train loss 11.663961410522461, val loss None, lr 0.0009000000000000001
iter 30, train loss 11.473016738891602, val loss None, lr 0.0009000000000000001
iter 40, train loss 11.288805961608887, val loss None, lr 0.0009000000000000001
iter 50, train loss 11.233453750610352, val loss None, lr 0.000729
iter 60, train loss 11.15196704864502, val loss None, lr 0.000729
iter 70, train loss 11.10328483581543, val loss None, lr 0.00059049
iter 80, train loss 11.125467300415039, val loss None, lr 0.00043046721
iter 90, train loss 11.100343704223633, val loss None, lr 0.0003486784401
best loss 11.023846626281738
layer14: mlp.gate_proj
norm_0 tensor([1.9795, 1.9805, 1.9893,  ..., 1.9951, 1.9756, 1.9717], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5684, 0.6548, 0.5732,  ..., 0.6045, 0.5820, 0.5410], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 210.32571411132812, val loss None, lr 0.001
iter 10, train loss 221.11732482910156, val loss None, lr 0.0008100000000000001
iter 20, train loss 214.80467224121094, val loss None, lr 0.000531441
iter 30, train loss 214.93116760253906, val loss None, lr 0.000387420489
iter 40, train loss 214.0058135986328, val loss None, lr 0.00028242953648100003
iter 50, train loss 213.30238342285156, val loss None, lr 0.00018530201888518417
iter 60, train loss 213.19369506835938, val loss None, lr 0.0001350851717672993
iter 70, train loss 213.01849365234375, val loss None, lr 9.847709021836118e-05
iter 80, train loss 212.88340759277344, val loss None, lr 6.461081889226677e-05
iter 90, train loss 212.57406616210938, val loss None, lr 4.7101286972462485e-05
best loss 207.65013122558594
layer14: mlp.up_proj
norm_0 tensor([1.9697, 1.9395, 1.9385,  ..., 1.9443, 1.9668, 1.9580], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.6133, 0.6011, 0.6069,  ..., 0.6299, 0.5903, 0.5781], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 192.9155731201172, val loss None, lr 0.001
iter 10, train loss 193.89039611816406, val loss None, lr 0.0008100000000000001
iter 20, train loss 194.2055206298828, val loss None, lr 0.000531441
iter 30, train loss 194.11981201171875, val loss None, lr 0.000387420489
iter 40, train loss 193.87319946289062, val loss None, lr 0.00028242953648100003
iter 50, train loss 193.93643188476562, val loss None, lr 0.00018530201888518417
iter 60, train loss 194.19268798828125, val loss None, lr 0.0001350851717672993
iter 70, train loss 193.84786987304688, val loss None, lr 9.847709021836118e-05
iter 80, train loss 193.78805541992188, val loss None, lr 6.461081889226677e-05
iter 90, train loss 193.78060913085938, val loss None, lr 4.7101286972462485e-05
best loss 192.8556365966797
layer14: mlp.down_proj
norm_0 tensor([1.1758, 1.1631, 1.1572,  ..., 1.2100, 1.1436, 1.1152], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.6904, 1.6191, 1.5908,  ..., 1.6602, 1.6436, 1.6416], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 5.192992210388184, val loss None, lr 0.001
iter 10, train loss 5.203085422515869, val loss None, lr 0.0008100000000000001
iter 20, train loss 5.188638687133789, val loss None, lr 0.0006561000000000001
iter 30, train loss 5.183590888977051, val loss None, lr 0.0004782969
iter 40, train loss 5.177209854125977, val loss None, lr 0.000387420489
iter 50, train loss 5.176992893218994, val loss None, lr 0.00031381059609000004
iter 60, train loss 5.173470973968506, val loss None, lr 0.00022876792454961005
iter 70, train loss 5.165380954742432, val loss None, lr 0.00020589113209464906
iter 80, train loss 5.162161827087402, val loss None, lr 0.00016677181699666576
iter 90, train loss 5.158856391906738, val loss None, lr 0.0001350851717672993
best loss 5.155147552490234
33109 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
25469 MiB free out of 48676 MiB total
after cast to cpu
32701 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
layer15: self_attn.q_proj
norm_0 tensor([1.4932, 1.5342, 1.4893,  ..., 1.5107, 1.5303, 1.5342], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5806, 0.6270, 0.6035,  ..., 1.2744, 1.2510, 1.2686], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 305.5721130371094, val loss None, lr 0.001
iter 10, train loss 321.12152099609375, val loss None, lr 0.000729
iter 20, train loss 306.8840026855469, val loss None, lr 0.000531441
iter 30, train loss 302.77960205078125, val loss None, lr 0.000387420489
iter 40, train loss 299.363525390625, val loss None, lr 0.00025418658283290005
iter 50, train loss 298.7827453613281, val loss None, lr 0.00018530201888518417
iter 60, train loss 297.2142028808594, val loss None, lr 0.0001350851717672993
iter 70, train loss 296.86468505859375, val loss None, lr 8.862938119652506e-05
iter 80, train loss 296.848388671875, val loss None, lr 6.461081889226677e-05
iter 90, train loss 296.51275634765625, val loss None, lr 4.7101286972462485e-05
best loss 282.733642578125
layer15: self_attn.k_proj
norm_0 tensor([1.6230, 1.5781, 1.5586,  ..., 1.5215, 1.5791, 1.5371], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5669, 0.6113, 0.5835,  ..., 1.2627, 1.2861, 1.3330], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 338.5304870605469, val loss None, lr 0.001
iter 10, train loss 357.16107177734375, val loss None, lr 0.000729
iter 20, train loss 358.1534118652344, val loss None, lr 0.000531441
iter 30, train loss 351.73712158203125, val loss None, lr 0.000387420489
iter 40, train loss 343.9259948730469, val loss None, lr 0.00025418658283290005
iter 50, train loss 342.409912109375, val loss None, lr 0.00018530201888518417
iter 60, train loss 339.81500244140625, val loss None, lr 0.0001350851717672993
iter 70, train loss 337.8099365234375, val loss None, lr 8.862938119652506e-05
iter 80, train loss 337.3008117675781, val loss None, lr 6.461081889226677e-05
iter 90, train loss 336.8075256347656, val loss None, lr 4.7101286972462485e-05
best loss 309.3798522949219
layer15: self_attn.v_proj
norm_0 tensor([1.0156, 0.9771, 0.9902,  ..., 1.0430, 1.0137, 1.0068], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.0195, 1.0264, 1.0195,  ..., 0.9795, 0.9761, 0.9629], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 112.74612426757812, val loss None, lr 0.001
iter 10, train loss 112.56571960449219, val loss None, lr 0.0008100000000000001
iter 20, train loss 111.96397399902344, val loss None, lr 0.0008100000000000001
iter 30, train loss 111.6920166015625, val loss None, lr 0.000729
iter 40, train loss 111.3424301147461, val loss None, lr 0.00059049
iter 50, train loss 111.3957290649414, val loss None, lr 0.0004782969
iter 60, train loss 111.10301208496094, val loss None, lr 0.00043046721
iter 70, train loss 111.11579132080078, val loss None, lr 0.00031381059609000004
iter 80, train loss 110.98416900634766, val loss None, lr 0.00025418658283290005
iter 90, train loss 110.9353256225586, val loss None, lr 0.00020589113209464906
best loss 110.6227798461914
layer15: self_attn.o_proj
norm_0 tensor([1.0000, 1.0098, 1.0049,  ..., 0.9663, 0.9692, 0.9546], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.9834, 0.9985, 0.9790,  ..., 1.0117, 0.9805, 1.0205], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 11.22256851196289, val loss None, lr 0.001
iter 10, train loss 11.070968627929688, val loss None, lr 0.0009000000000000001
iter 20, train loss 10.98539924621582, val loss None, lr 0.0008100000000000001
iter 30, train loss 10.928495407104492, val loss None, lr 0.0006561000000000001
iter 40, train loss 10.858745574951172, val loss None, lr 0.0006561000000000001
iter 50, train loss 10.779696464538574, val loss None, lr 0.0006561000000000001
iter 60, train loss 10.743255615234375, val loss None, lr 0.000531441
iter 70, train loss 10.751161575317383, val loss None, lr 0.0004782969
iter 80, train loss 10.748042106628418, val loss None, lr 0.00031381059609000004
iter 90, train loss 10.730812072753906, val loss None, lr 0.00022876792454961005
best loss 10.703580856323242
layer15: mlp.gate_proj
norm_0 tensor([1.9961, 2.0000, 2.0059,  ..., 2.0059, 2.0000, 2.0000], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5493, 0.6406, 0.6123,  ..., 0.5264, 0.5767, 0.5229], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 229.81007385253906, val loss None, lr 0.001
iter 10, train loss 241.5617218017578, val loss None, lr 0.0008100000000000001
iter 20, train loss 235.8414306640625, val loss None, lr 0.000531441
iter 30, train loss 236.33978271484375, val loss None, lr 0.000387420489
iter 40, train loss 235.71701049804688, val loss None, lr 0.00028242953648100003
iter 50, train loss 234.4702911376953, val loss None, lr 0.00018530201888518417
iter 60, train loss 233.7726593017578, val loss None, lr 0.0001350851717672993
iter 70, train loss 233.17306518554688, val loss None, lr 9.847709021836118e-05
iter 80, train loss 232.86370849609375, val loss None, lr 6.461081889226677e-05
iter 90, train loss 232.907958984375, val loss None, lr 4.7101286972462485e-05
best loss 227.06387329101562
layer15: mlp.up_proj
norm_0 tensor([1.9668, 1.9443, 1.9502,  ..., 1.9609, 1.9629, 1.9482], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.6147, 0.6255, 0.6577,  ..., 0.5459, 0.5649, 0.5571], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 211.2053985595703, val loss None, lr 0.001
iter 10, train loss 212.82608032226562, val loss None, lr 0.000729
iter 20, train loss 213.263427734375, val loss None, lr 0.000531441
iter 30, train loss 212.54588317871094, val loss None, lr 0.0003486784401
iter 40, train loss 212.55880737304688, val loss None, lr 0.00025418658283290005
iter 50, train loss 212.32760620117188, val loss None, lr 0.00018530201888518417
iter 60, train loss 212.17822265625, val loss None, lr 0.00012157665459056936
iter 70, train loss 212.13401794433594, val loss None, lr 8.862938119652506e-05
iter 80, train loss 212.06509399414062, val loss None, lr 6.461081889226677e-05
iter 90, train loss 211.8761749267578, val loss None, lr 4.239115827521624e-05
best loss 211.2053985595703
layer15: mlp.down_proj
norm_0 tensor([1.2051, 1.1855, 1.3145,  ..., 1.0889, 1.1104, 1.0977], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.6289, 1.6133, 1.6055,  ..., 1.6436, 1.6494, 1.6436], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 6.445160865783691, val loss None, lr 0.001
iter 10, train loss 6.441795349121094, val loss None, lr 0.0008100000000000001
iter 20, train loss 6.410860061645508, val loss None, lr 0.000729
iter 30, train loss 6.381991386413574, val loss None, lr 0.0006561000000000001
iter 40, train loss 6.3594560623168945, val loss None, lr 0.0006561000000000001
iter 50, train loss 6.352831840515137, val loss None, lr 0.0006561000000000001
iter 60, train loss 6.346092700958252, val loss None, lr 0.000531441
iter 70, train loss 6.3382439613342285, val loss None, lr 0.0004782969
iter 80, train loss 6.334280967712402, val loss None, lr 0.000387420489
iter 90, train loss 6.332061290740967, val loss None, lr 0.00028242953648100003
best loss 6.32901668548584
32701 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
25061 MiB free out of 48676 MiB total
after cast to cpu
32293 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
layer16: self_attn.q_proj
norm_0 tensor([1.4893, 1.4844, 1.4600,  ..., 1.4814, 1.5146, 1.4990], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.7651, 0.8101, 0.8564,  ..., 1.0957, 1.0938, 1.0938], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 317.08282470703125, val loss None, lr 0.001
iter 10, train loss 333.9794006347656, val loss None, lr 0.000729
iter 20, train loss 323.6460876464844, val loss None, lr 0.000531441
iter 30, train loss 315.0188293457031, val loss None, lr 0.000387420489
iter 40, train loss 314.5892028808594, val loss None, lr 0.00025418658283290005
iter 50, train loss 313.89337158203125, val loss None, lr 0.00018530201888518417
iter 60, train loss 311.0683288574219, val loss None, lr 0.0001350851717672993
iter 70, train loss 309.4176330566406, val loss None, lr 8.862938119652506e-05
iter 80, train loss 309.55712890625, val loss None, lr 6.461081889226677e-05
iter 90, train loss 309.7615966796875, val loss None, lr 4.7101286972462485e-05
best loss 293.71014404296875
layer16: self_attn.k_proj
norm_0 tensor([1.5898, 1.5801, 1.5420,  ..., 1.5215, 1.5137, 1.5518], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.7666, 0.7998, 0.8311,  ..., 1.1182, 1.0986, 1.1484], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 355.37200927734375, val loss None, lr 0.001
iter 10, train loss 368.8942565917969, val loss None, lr 0.000729
iter 20, train loss 366.0102233886719, val loss None, lr 0.000531441
iter 30, train loss 358.59759521484375, val loss None, lr 0.000387420489
iter 40, train loss 352.4176330566406, val loss None, lr 0.00025418658283290005
iter 50, train loss 349.8017578125, val loss None, lr 0.00018530201888518417
iter 60, train loss 347.50738525390625, val loss None, lr 0.0001350851717672993
iter 70, train loss 345.9124450683594, val loss None, lr 8.862938119652506e-05
iter 80, train loss 345.43365478515625, val loss None, lr 6.461081889226677e-05
iter 90, train loss 345.36492919921875, val loss None, lr 4.7101286972462485e-05
best loss 321.95965576171875
layer16: self_attn.v_proj
norm_0 tensor([1.0615, 1.0146, 1.0693,  ..., 1.0898, 1.0518, 1.0322], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.2354, 1.2041, 1.2158,  ..., 1.0371, 1.0479, 1.0527], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 127.91207885742188, val loss None, lr 0.001
iter 10, train loss 127.27593231201172, val loss None, lr 0.001
iter 20, train loss 126.67706298828125, val loss None, lr 0.001
iter 30, train loss 126.45960998535156, val loss None, lr 0.0009000000000000001
iter 40, train loss 126.05289459228516, val loss None, lr 0.0008100000000000001
iter 50, train loss 125.73307800292969, val loss None, lr 0.000729
iter 60, train loss 125.61920166015625, val loss None, lr 0.0006561000000000001
iter 70, train loss 125.513916015625, val loss None, lr 0.000531441
iter 80, train loss 125.498779296875, val loss None, lr 0.000387420489
iter 90, train loss 125.39796447753906, val loss None, lr 0.00028242953648100003
best loss 125.1694564819336
layer16: self_attn.o_proj
norm_0 tensor([1.2393, 1.2080, 1.2275,  ..., 1.0664, 1.0781, 1.0684], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.9907, 0.9927, 0.9731,  ..., 1.0098, 0.9839, 0.9854], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 13.503564834594727, val loss None, lr 0.001
iter 10, train loss 13.354928970336914, val loss None, lr 0.0009000000000000001
iter 20, train loss 13.318764686584473, val loss None, lr 0.0009000000000000001
iter 30, train loss 13.231657981872559, val loss None, lr 0.0008100000000000001
iter 40, train loss 13.176345825195312, val loss None, lr 0.000729
iter 50, train loss 13.101190567016602, val loss None, lr 0.0006561000000000001
iter 60, train loss 13.140798568725586, val loss None, lr 0.000531441
iter 70, train loss 13.117240905761719, val loss None, lr 0.000387420489
iter 80, train loss 13.075925827026367, val loss None, lr 0.00028242953648100003
iter 90, train loss 13.029356002807617, val loss None, lr 0.00028242953648100003
best loss 13.010295867919922
layer16: mlp.gate_proj
norm_0 tensor([1.9951, 2.0078, 2.0254,  ..., 2.0195, 2.0273, 2.0137], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5884, 0.5054, 0.5322,  ..., 0.5488, 0.6777, 0.8716], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 266.89508056640625, val loss None, lr 0.001
iter 10, train loss 282.37945556640625, val loss None, lr 0.000729
iter 20, train loss 274.06884765625, val loss None, lr 0.000531441
iter 30, train loss 273.7718200683594, val loss None, lr 0.000387420489
iter 40, train loss 272.57470703125, val loss None, lr 0.00025418658283290005
iter 50, train loss 272.1474304199219, val loss None, lr 0.00018530201888518417
iter 60, train loss 271.587890625, val loss None, lr 0.0001350851717672993
iter 70, train loss 271.3133544921875, val loss None, lr 8.862938119652506e-05
iter 80, train loss 271.1868896484375, val loss None, lr 6.461081889226677e-05
iter 90, train loss 270.71929931640625, val loss None, lr 4.7101286972462485e-05
best loss 261.9439697265625
layer16: mlp.up_proj
norm_0 tensor([1.9805, 1.9512, 1.9395,  ..., 1.9551, 1.9570, 1.9482], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.6250, 0.5552, 0.6064,  ..., 0.6055, 0.5176, 0.6606], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 239.82354736328125, val loss None, lr 0.001
iter 10, train loss 241.14590454101562, val loss None, lr 0.0008100000000000001
iter 20, train loss 241.68408203125, val loss None, lr 0.000531441
iter 30, train loss 241.39378356933594, val loss None, lr 0.000387420489
iter 40, train loss 240.96754455566406, val loss None, lr 0.00028242953648100003
iter 50, train loss 241.32864379882812, val loss None, lr 0.00018530201888518417
iter 60, train loss 241.32217407226562, val loss None, lr 0.0001350851717672993
iter 70, train loss 241.32666015625, val loss None, lr 9.847709021836118e-05
iter 80, train loss 241.11138916015625, val loss None, lr 6.461081889226677e-05
iter 90, train loss 241.0215606689453, val loss None, lr 4.7101286972462485e-05
best loss 239.51527404785156
layer16: mlp.down_proj
norm_0 tensor([1.1953, 1.0918, 1.1787,  ..., 1.1670, 0.9341, 1.2900], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.6504, 1.6055, 1.6289,  ..., 1.6279, 1.6436, 1.6221], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 8.343864440917969, val loss None, lr 0.001
iter 10, train loss 8.361051559448242, val loss None, lr 0.000729
iter 20, train loss 8.327756881713867, val loss None, lr 0.0006561000000000001
iter 30, train loss 8.312163352966309, val loss None, lr 0.00059049
iter 40, train loss 8.299564361572266, val loss None, lr 0.0004782969
iter 50, train loss 8.2942476272583, val loss None, lr 0.000387420489
iter 60, train loss 8.285337448120117, val loss None, lr 0.0003486784401
iter 70, train loss 8.292732238769531, val loss None, lr 0.00025418658283290005
iter 80, train loss 8.279255867004395, val loss None, lr 0.00020589113209464906
iter 90, train loss 8.2772216796875, val loss None, lr 0.00016677181699666576
best loss 8.273518562316895
32293 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
24653 MiB free out of 48676 MiB total
after cast to cpu
31885 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
layer17: self_attn.q_proj
norm_0 tensor([1.4795, 1.4707, 1.4639,  ..., 1.5068, 1.5332, 1.4980], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.4700, 0.4636, 0.4636,  ..., 1.3955, 1.4902, 1.3623], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 329.63665771484375, val loss None, lr 0.001
iter 10, train loss 352.2136535644531, val loss None, lr 0.000729
iter 20, train loss 340.67431640625, val loss None, lr 0.000531441
iter 30, train loss 331.2616882324219, val loss None, lr 0.000387420489
iter 40, train loss 328.0574035644531, val loss None, lr 0.00025418658283290005
iter 50, train loss 327.8960876464844, val loss None, lr 0.00018530201888518417
iter 60, train loss 325.6522216796875, val loss None, lr 0.0001350851717672993
iter 70, train loss 324.608154296875, val loss None, lr 8.862938119652506e-05
iter 80, train loss 324.092041015625, val loss None, lr 6.461081889226677e-05
iter 90, train loss 323.625732421875, val loss None, lr 4.7101286972462485e-05
best loss 304.22442626953125
layer17: self_attn.k_proj
norm_0 tensor([1.5576, 1.5840, 1.5215,  ..., 1.4814, 1.5000, 1.5342], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.4421, 0.4617, 0.4624,  ..., 1.4277, 1.0381, 1.4160], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 362.77032470703125, val loss None, lr 0.001
iter 10, train loss 383.29681396484375, val loss None, lr 0.000729
iter 20, train loss 378.30889892578125, val loss None, lr 0.000531441
iter 30, train loss 371.3564758300781, val loss None, lr 0.000387420489
iter 40, train loss 363.1295166015625, val loss None, lr 0.00025418658283290005
iter 50, train loss 359.8511962890625, val loss None, lr 0.00018530201888518417
iter 60, train loss 359.3887939453125, val loss None, lr 0.0001350851717672993
iter 70, train loss 357.26446533203125, val loss None, lr 8.862938119652506e-05
iter 80, train loss 355.3352355957031, val loss None, lr 6.461081889226677e-05
iter 90, train loss 354.1356201171875, val loss None, lr 4.7101286972462485e-05
best loss 328.24407958984375
layer17: self_attn.v_proj
norm_0 tensor([1.0449, 1.0146, 1.0801,  ..., 1.0645, 1.0703, 1.0439], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.9409, 0.9824, 0.9521,  ..., 1.0010, 0.9766, 1.0088], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 134.41375732421875, val loss None, lr 0.001
iter 10, train loss 134.88784790039062, val loss None, lr 0.000729
iter 20, train loss 134.69154357910156, val loss None, lr 0.000531441
iter 30, train loss 134.3137969970703, val loss None, lr 0.00043046721
iter 40, train loss 134.02789306640625, val loss None, lr 0.0003486784401
iter 50, train loss 133.51400756835938, val loss None, lr 0.0003486784401
iter 60, train loss 133.62635803222656, val loss None, lr 0.00025418658283290005
iter 70, train loss 133.4115447998047, val loss None, lr 0.00018530201888518417
iter 80, train loss 133.4497833251953, val loss None, lr 0.0001350851717672993
iter 90, train loss 133.39407348632812, val loss None, lr 8.862938119652506e-05
best loss 133.22207641601562
layer17: self_attn.o_proj
norm_0 tensor([1.0244, 1.0225, 0.9854,  ..., 1.0234, 0.9883, 1.0273], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.0020, 1.0020, 0.9790,  ..., 1.0176, 1.0029, 1.0137], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 9.952802658081055, val loss None, lr 0.001
iter 10, train loss 9.626879692077637, val loss None, lr 0.0008100000000000001
iter 20, train loss 9.411508560180664, val loss None, lr 0.000729
iter 30, train loss 9.289567947387695, val loss None, lr 0.000729
iter 40, train loss 9.209760665893555, val loss None, lr 0.000729
iter 50, train loss 9.23647689819336, val loss None, lr 0.000531441
iter 60, train loss 9.194576263427734, val loss None, lr 0.000387420489
iter 70, train loss 9.19117546081543, val loss None, lr 0.00031381059609000004
iter 80, train loss 9.192299842834473, val loss None, lr 0.00028242953648100003
iter 90, train loss 9.166518211364746, val loss None, lr 0.00025418658283290005
best loss 9.143516540527344
layer17: mlp.gate_proj
norm_0 tensor([2.0137, 2.0352, 2.0410,  ..., 2.0469, 2.0312, 2.0312], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5410, 0.5557, 0.5542,  ..., 0.5640, 0.6533, 0.5396], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 300.6298522949219, val loss None, lr 0.001
iter 10, train loss 315.3465576171875, val loss None, lr 0.000729
iter 20, train loss 310.96197509765625, val loss None, lr 0.000531441
iter 30, train loss 311.19439697265625, val loss None, lr 0.000387420489
iter 40, train loss 309.703125, val loss None, lr 0.00025418658283290005
iter 50, train loss 308.8892517089844, val loss None, lr 0.00018530201888518417
iter 60, train loss 308.0584716796875, val loss None, lr 0.0001350851717672993
iter 70, train loss 307.1955261230469, val loss None, lr 8.862938119652506e-05
iter 80, train loss 306.49627685546875, val loss None, lr 6.461081889226677e-05
iter 90, train loss 305.8349914550781, val loss None, lr 4.7101286972462485e-05
best loss 295.422119140625
layer17: mlp.up_proj
norm_0 tensor([1.9707, 1.9473, 1.9355,  ..., 1.9434, 1.9570, 1.9453], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.6118, 0.6030, 0.5854,  ..., 0.6094, 0.6396, 0.5801], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 262.715576171875, val loss None, lr 0.001
iter 10, train loss 263.6755676269531, val loss None, lr 0.0008100000000000001
iter 20, train loss 263.900634765625, val loss None, lr 0.000531441
iter 30, train loss 263.521484375, val loss None, lr 0.000387420489
iter 40, train loss 263.56109619140625, val loss None, lr 0.00028242953648100003
iter 50, train loss 264.0601806640625, val loss None, lr 0.00018530201888518417
iter 60, train loss 263.8254699707031, val loss None, lr 0.0001350851717672993
iter 70, train loss 263.8044128417969, val loss None, lr 9.847709021836118e-05
iter 80, train loss 263.81951904296875, val loss None, lr 6.461081889226677e-05
iter 90, train loss 263.45855712890625, val loss None, lr 4.7101286972462485e-05
best loss 262.6193542480469
layer17: mlp.down_proj
norm_0 tensor([1.1807, 1.1660, 1.1338,  ..., 1.1768, 1.2031, 1.1309], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.6416, 1.6201, 1.6396,  ..., 1.6279, 1.6738, 1.6338], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 9.047647476196289, val loss None, lr 0.001
iter 10, train loss 9.041205406188965, val loss None, lr 0.0009000000000000001
iter 20, train loss 8.994775772094727, val loss None, lr 0.0008100000000000001
iter 30, train loss 8.97270679473877, val loss None, lr 0.000729
iter 40, train loss 8.944509506225586, val loss None, lr 0.0006561000000000001
iter 50, train loss 8.91724967956543, val loss None, lr 0.00059049
iter 60, train loss 8.903912544250488, val loss None, lr 0.000531441
iter 70, train loss 8.90707015991211, val loss None, lr 0.00043046721
iter 80, train loss 8.894964218139648, val loss None, lr 0.00031381059609000004
iter 90, train loss 8.87603759765625, val loss None, lr 0.00028242953648100003
best loss 8.86435317993164
31885 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
24245 MiB free out of 48676 MiB total
after cast to cpu
31477 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
layer18: self_attn.q_proj
norm_0 tensor([1.4463, 1.4893, 1.4805,  ..., 1.4541, 1.4648, 1.4648], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.7310, 0.8086, 0.8174,  ..., 1.3447, 1.3838, 1.3779], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 358.2658996582031, val loss None, lr 0.001
iter 10, train loss 377.1255187988281, val loss None, lr 0.000729
iter 20, train loss 361.483154296875, val loss None, lr 0.000531441
iter 30, train loss 351.4034729003906, val loss None, lr 0.000387420489
iter 40, train loss 347.66064453125, val loss None, lr 0.00025418658283290005
iter 50, train loss 344.75872802734375, val loss None, lr 0.00018530201888518417
iter 60, train loss 343.69171142578125, val loss None, lr 0.0001350851717672993
iter 70, train loss 342.0315246582031, val loss None, lr 8.862938119652506e-05
iter 80, train loss 341.5633239746094, val loss None, lr 6.461081889226677e-05
iter 90, train loss 340.4271240234375, val loss None, lr 4.7101286972462485e-05
best loss 326.71295166015625
layer18: self_attn.k_proj
norm_0 tensor([1.5254, 1.5303, 1.4717,  ..., 1.4941, 1.4668, 1.4463], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.7163, 0.7930, 0.8096,  ..., 1.4365, 1.5371, 1.5498], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 384.97418212890625, val loss None, lr 0.001
iter 10, train loss 403.5927734375, val loss None, lr 0.000729
iter 20, train loss 395.59246826171875, val loss None, lr 0.000531441
iter 30, train loss 384.4275207519531, val loss None, lr 0.000387420489
iter 40, train loss 376.4012451171875, val loss None, lr 0.00025418658283290005
iter 50, train loss 371.6565856933594, val loss None, lr 0.00018530201888518417
iter 60, train loss 370.2630920410156, val loss None, lr 0.0001350851717672993
iter 70, train loss 368.4798583984375, val loss None, lr 8.862938119652506e-05
iter 80, train loss 366.8163146972656, val loss None, lr 6.461081889226677e-05
iter 90, train loss 366.5467224121094, val loss None, lr 4.7101286972462485e-05
best loss 347.927490234375
layer18: self_attn.v_proj
norm_0 tensor([1.0791, 1.0596, 1.0859,  ..., 1.1260, 1.1191, 1.1211], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.1074, 1.1133, 1.1289,  ..., 0.9028, 0.9150, 0.9268], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 162.70742797851562, val loss None, lr 0.001
iter 10, train loss 162.03076171875, val loss None, lr 0.0009000000000000001
iter 20, train loss 161.69464111328125, val loss None, lr 0.0008100000000000001
iter 30, train loss 161.88763427734375, val loss None, lr 0.00059049
iter 40, train loss 161.8336639404297, val loss None, lr 0.00043046721
iter 50, train loss 161.57472229003906, val loss None, lr 0.0003486784401
iter 60, train loss 161.18136596679688, val loss None, lr 0.0003486784401
iter 70, train loss 160.79901123046875, val loss None, lr 0.00031381059609000004
iter 80, train loss 160.66807556152344, val loss None, lr 0.00028242953648100003
iter 90, train loss 160.52931213378906, val loss None, lr 0.00022876792454961005
best loss 160.3275909423828
layer18: self_attn.o_proj
norm_0 tensor([1.1689, 1.1865, 1.1924,  ..., 0.9775, 0.9907, 0.9966], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.9873, 0.9946, 0.9844,  ..., 1.0000, 1.0010, 0.9917], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 8.58788776397705, val loss None, lr 0.001
iter 10, train loss 8.337471008300781, val loss None, lr 0.0008100000000000001
iter 20, train loss 8.218941688537598, val loss None, lr 0.000729
iter 30, train loss 8.144594192504883, val loss None, lr 0.0006561000000000001
iter 40, train loss 8.06597900390625, val loss None, lr 0.0006561000000000001
iter 50, train loss 8.033345222473145, val loss None, lr 0.00059049
iter 60, train loss 8.024619102478027, val loss None, lr 0.0004782969
iter 70, train loss 7.9482808113098145, val loss None, lr 0.0004782969
iter 80, train loss 7.928771018981934, val loss None, lr 0.00043046721
iter 90, train loss 7.937669277191162, val loss None, lr 0.00028242953648100003
best loss 7.914593696594238
layer18: mlp.gate_proj
norm_0 tensor([2.0293, 2.0410, 2.0410,  ..., 2.0645, 2.0430, 2.0488], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5918, 0.5991, 0.5757,  ..., 0.7612, 0.6367, 0.6006], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 339.450927734375, val loss None, lr 0.001
iter 10, train loss 353.9068298339844, val loss None, lr 0.000729
iter 20, train loss 350.9935302734375, val loss None, lr 0.000531441
iter 30, train loss 350.9544372558594, val loss None, lr 0.000387420489
iter 40, train loss 350.4724426269531, val loss None, lr 0.00025418658283290005
iter 50, train loss 350.33453369140625, val loss None, lr 0.00018530201888518417
iter 60, train loss 350.2353820800781, val loss None, lr 0.0001350851717672993
iter 70, train loss 349.5262145996094, val loss None, lr 8.862938119652506e-05
iter 80, train loss 348.656982421875, val loss None, lr 6.461081889226677e-05
iter 90, train loss 348.1159973144531, val loss None, lr 4.7101286972462485e-05
best loss 334.6793212890625
layer18: mlp.up_proj
norm_0 tensor([1.9561, 1.9521, 1.9531,  ..., 1.9365, 1.9541, 1.9434], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.6191, 0.6401, 0.6274,  ..., 0.6147, 0.6201, 0.6216], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 287.96624755859375, val loss None, lr 0.001
iter 10, train loss 288.81817626953125, val loss None, lr 0.000729
iter 20, train loss 288.75787353515625, val loss None, lr 0.000531441
iter 30, train loss 288.72857666015625, val loss None, lr 0.0003486784401
iter 40, train loss 289.27593994140625, val loss None, lr 0.00025418658283290005
iter 50, train loss 289.0958251953125, val loss None, lr 0.00018530201888518417
iter 60, train loss 289.0640869140625, val loss None, lr 0.00012157665459056936
iter 70, train loss 288.86328125, val loss None, lr 8.862938119652506e-05
iter 80, train loss 288.8669128417969, val loss None, lr 6.461081889226677e-05
iter 90, train loss 288.94866943359375, val loss None, lr 4.239115827521624e-05
best loss 287.96624755859375
layer18: mlp.down_proj
norm_0 tensor([1.1943, 1.2393, 1.1982,  ..., 1.1797, 1.1914, 1.1836], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.6309, 1.6201, 1.6191,  ..., 1.6230, 1.6641, 1.6348], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 10.370081901550293, val loss None, lr 0.001
iter 10, train loss 10.368148803710938, val loss None, lr 0.0008100000000000001
iter 20, train loss 10.310262680053711, val loss None, lr 0.000729
iter 30, train loss 10.285982131958008, val loss None, lr 0.0006561000000000001
iter 40, train loss 10.280479431152344, val loss None, lr 0.0004782969
iter 50, train loss 10.249021530151367, val loss None, lr 0.00043046721
iter 60, train loss 10.228426933288574, val loss None, lr 0.00043046721
iter 70, train loss 10.200193405151367, val loss None, lr 0.00043046721
iter 80, train loss 10.18113899230957, val loss None, lr 0.000387420489
iter 90, train loss 10.166400909423828, val loss None, lr 0.0003486784401
best loss 10.151667594909668
31477 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
23837 MiB free out of 48676 MiB total
after cast to cpu
31069 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
layer19: self_attn.q_proj
norm_0 tensor([1.4170, 1.4453, 1.4180,  ..., 1.4248, 1.4531, 1.4580], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.6357, 0.7725, 0.7656,  ..., 1.3174, 1.2842, 1.2930], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 338.4326171875, val loss None, lr 0.001
iter 10, train loss 359.64398193359375, val loss None, lr 0.000729
iter 20, train loss 344.9783935546875, val loss None, lr 0.000531441
iter 30, train loss 340.9837341308594, val loss None, lr 0.000387420489
iter 40, train loss 339.6471252441406, val loss None, lr 0.00025418658283290005
iter 50, train loss 337.36444091796875, val loss None, lr 0.00018530201888518417
iter 60, train loss 335.1427001953125, val loss None, lr 0.0001350851717672993
iter 70, train loss 334.2410888671875, val loss None, lr 8.862938119652506e-05
iter 80, train loss 334.2568359375, val loss None, lr 6.461081889226677e-05
iter 90, train loss 334.252197265625, val loss None, lr 4.7101286972462485e-05
best loss 312.977783203125
layer19: self_attn.k_proj
norm_0 tensor([1.4941, 1.5205, 1.4795,  ..., 1.4336, 1.4473, 1.4717], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5762, 0.7686, 0.7637,  ..., 1.3730, 1.3975, 1.3691], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 368.5706481933594, val loss None, lr 0.001
iter 10, train loss 385.5731201171875, val loss None, lr 0.000729
iter 20, train loss 378.1221008300781, val loss None, lr 0.000531441
iter 30, train loss 369.62249755859375, val loss None, lr 0.000387420489
iter 40, train loss 362.9789733886719, val loss None, lr 0.00025418658283290005
iter 50, train loss 359.5314636230469, val loss None, lr 0.00018530201888518417
iter 60, train loss 357.03668212890625, val loss None, lr 0.0001350851717672993
iter 70, train loss 355.3597717285156, val loss None, lr 8.862938119652506e-05
iter 80, train loss 355.1604919433594, val loss None, lr 6.461081889226677e-05
iter 90, train loss 353.8224182128906, val loss None, lr 4.7101286972462485e-05
best loss 335.362548828125
layer19: self_attn.v_proj
norm_0 tensor([1.0986, 1.0859, 1.1182,  ..., 1.1279, 1.1064, 1.0957], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.0381, 1.0420, 1.0430,  ..., 1.0039, 0.9912, 0.9868], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 164.02383422851562, val loss None, lr 0.001
iter 10, train loss 163.72947692871094, val loss None, lr 0.0009000000000000001
iter 20, train loss 163.03378295898438, val loss None, lr 0.000729
iter 30, train loss 162.6675567626953, val loss None, lr 0.000729
iter 40, train loss 162.60003662109375, val loss None, lr 0.00059049
iter 50, train loss 162.75247192382812, val loss None, lr 0.0004782969
iter 60, train loss 162.35667419433594, val loss None, lr 0.0003486784401
iter 70, train loss 161.96658325195312, val loss None, lr 0.00031381059609000004
iter 80, train loss 162.0284881591797, val loss None, lr 0.00022876792454961005
iter 90, train loss 161.8973388671875, val loss None, lr 0.00016677181699666576
best loss 161.8105926513672
layer19: self_attn.o_proj
norm_0 tensor([1.1250, 1.1182, 1.1289,  ..., 1.0898, 1.0742, 1.0723], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.9971, 0.9946, 0.9863,  ..., 1.0146, 1.0020, 0.9941], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 8.952091217041016, val loss None, lr 0.001
iter 10, train loss 8.543684005737305, val loss None, lr 0.0009000000000000001
iter 20, train loss 8.384053230285645, val loss None, lr 0.0008100000000000001
iter 30, train loss 8.368685722351074, val loss None, lr 0.0006561000000000001
iter 40, train loss 8.30584716796875, val loss None, lr 0.00059049
iter 50, train loss 8.247074127197266, val loss None, lr 0.00059049
iter 60, train loss 8.204339027404785, val loss None, lr 0.000531441
iter 70, train loss 8.198999404907227, val loss None, lr 0.00043046721
iter 80, train loss 8.183937072753906, val loss None, lr 0.000387420489
iter 90, train loss 8.187963485717773, val loss None, lr 0.00031381059609000004
best loss 8.170103073120117
layer19: mlp.gate_proj
norm_0 tensor([2.0488, 2.0527, 2.0625,  ..., 2.0684, 2.0508, 2.0605], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5234, 0.5581, 0.5713,  ..., 0.5615, 0.5903, 0.5181], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 359.858154296875, val loss None, lr 0.001
iter 10, train loss 375.49639892578125, val loss None, lr 0.0008100000000000001
iter 20, train loss 372.372314453125, val loss None, lr 0.000531441
iter 30, train loss 372.16168212890625, val loss None, lr 0.000387420489
iter 40, train loss 371.3486633300781, val loss None, lr 0.00028242953648100003
iter 50, train loss 370.6325378417969, val loss None, lr 0.00018530201888518417
iter 60, train loss 369.98040771484375, val loss None, lr 0.0001350851717672993
iter 70, train loss 369.8127136230469, val loss None, lr 9.847709021836118e-05
iter 80, train loss 369.2293701171875, val loss None, lr 6.461081889226677e-05
iter 90, train loss 368.83642578125, val loss None, lr 4.7101286972462485e-05
best loss 355.82220458984375
layer19: mlp.up_proj
norm_0 tensor([1.9551, 1.9395, 1.9385,  ..., 1.9365, 1.9551, 1.9414], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5293, 0.5942, 0.6035,  ..., 0.5835, 0.6016, 0.5757], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 307.8998107910156, val loss None, lr 0.001
iter 10, train loss 307.84259033203125, val loss None, lr 0.000729
iter 20, train loss 308.4776611328125, val loss None, lr 0.00059049
iter 30, train loss 308.4337158203125, val loss None, lr 0.00043046721
iter 40, train loss 308.31964111328125, val loss None, lr 0.00028242953648100003
iter 50, train loss 308.5710144042969, val loss None, lr 0.00020589113209464906
iter 60, train loss 308.30169677734375, val loss None, lr 0.0001500946352969992
iter 70, train loss 308.5260009765625, val loss None, lr 9.847709021836118e-05
iter 80, train loss 308.373291015625, val loss None, lr 7.17897987691853e-05
iter 90, train loss 308.32196044921875, val loss None, lr 5.233476330273609e-05
best loss 307.3008117675781
layer19: mlp.down_proj
norm_0 tensor([1.0137, 1.1533, 1.1680,  ..., 1.1416, 1.1641, 1.1152], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.6338, 1.6182, 1.6416,  ..., 1.6455, 1.6562, 1.6279], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 11.04906940460205, val loss None, lr 0.001
iter 10, train loss 11.045585632324219, val loss None, lr 0.0008100000000000001
iter 20, train loss 11.000171661376953, val loss None, lr 0.0006561000000000001
iter 30, train loss 10.967019081115723, val loss None, lr 0.00059049
iter 40, train loss 10.937039375305176, val loss None, lr 0.000531441
iter 50, train loss 10.924598693847656, val loss None, lr 0.0004782969
iter 60, train loss 10.913129806518555, val loss None, lr 0.00043046721
iter 70, train loss 10.886313438415527, val loss None, lr 0.00043046721
iter 80, train loss 10.871953010559082, val loss None, lr 0.0003486784401
iter 90, train loss 10.864969253540039, val loss None, lr 0.00031381059609000004
best loss 10.86215591430664
31069 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
23429 MiB free out of 48676 MiB total
after cast to cpu
30661 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
layer20: self_attn.q_proj
norm_0 tensor([1.4395, 1.4912, 1.4180,  ..., 1.4355, 1.4502, 1.4541], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.3748, 0.4128, 0.4402,  ..., 1.1904, 1.2539, 1.0312], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 353.01947021484375, val loss None, lr 0.001
iter 10, train loss 373.71954345703125, val loss None, lr 0.000729
iter 20, train loss 362.6557922363281, val loss None, lr 0.000531441
iter 30, train loss 352.5182189941406, val loss None, lr 0.000387420489
iter 40, train loss 345.711669921875, val loss None, lr 0.00025418658283290005
iter 50, train loss 342.7938537597656, val loss None, lr 0.00018530201888518417
iter 60, train loss 341.14935302734375, val loss None, lr 0.0001350851717672993
iter 70, train loss 340.59002685546875, val loss None, lr 8.862938119652506e-05
iter 80, train loss 340.5448913574219, val loss None, lr 6.461081889226677e-05
iter 90, train loss 339.52984619140625, val loss None, lr 4.7101286972462485e-05
best loss 324.9849853515625
layer20: self_attn.k_proj
norm_0 tensor([1.5039, 1.5273, 1.4980,  ..., 1.4531, 1.4551, 1.4834], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.3584, 0.3821, 0.3943,  ..., 1.2578, 1.3662, 1.3652], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 378.75970458984375, val loss None, lr 0.001
iter 10, train loss 398.2294921875, val loss None, lr 0.000729
iter 20, train loss 385.86541748046875, val loss None, lr 0.000531441
iter 30, train loss 380.4410705566406, val loss None, lr 0.000387420489
iter 40, train loss 373.5013732910156, val loss None, lr 0.00025418658283290005
iter 50, train loss 370.3030700683594, val loss None, lr 0.00018530201888518417
iter 60, train loss 368.34552001953125, val loss None, lr 0.0001350851717672993
iter 70, train loss 367.8228454589844, val loss None, lr 8.862938119652506e-05
iter 80, train loss 367.55755615234375, val loss None, lr 6.461081889226677e-05
iter 90, train loss 366.70538330078125, val loss None, lr 4.7101286972462485e-05
best loss 347.178466796875
layer20: self_attn.v_proj
norm_0 tensor([1.1191, 1.0830, 1.1377,  ..., 1.1338, 1.1348, 1.1270], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.9707, 0.9888, 0.9868,  ..., 0.9263, 0.9385, 0.9614], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 169.80978393554688, val loss None, lr 0.001
iter 10, train loss 169.36172485351562, val loss None, lr 0.0009000000000000001
iter 20, train loss 169.08642578125, val loss None, lr 0.000729
iter 30, train loss 168.90771484375, val loss None, lr 0.000531441
iter 40, train loss 168.62066650390625, val loss None, lr 0.000531441
iter 50, train loss 168.57321166992188, val loss None, lr 0.000387420489
iter 60, train loss 168.16688537597656, val loss None, lr 0.000387420489
iter 70, train loss 168.34719848632812, val loss None, lr 0.00028242953648100003
iter 80, train loss 168.27110290527344, val loss None, lr 0.00020589113209464906
iter 90, train loss 167.97410583496094, val loss None, lr 0.00016677181699666576
best loss 167.79000854492188
layer20: self_attn.o_proj
norm_0 tensor([1.0674, 1.0762, 1.0830,  ..., 1.0527, 1.0527, 1.0654], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.0127, 1.0049, 0.9897,  ..., 0.9941, 0.9941, 1.0234], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 13.713027954101562, val loss None, lr 0.001
iter 10, train loss 12.139631271362305, val loss None, lr 0.0009000000000000001
iter 20, train loss 11.229635238647461, val loss None, lr 0.0009000000000000001
iter 30, train loss 10.882728576660156, val loss None, lr 0.0009000000000000001
iter 40, train loss 10.694129943847656, val loss None, lr 0.0009000000000000001
iter 50, train loss 10.518838882446289, val loss None, lr 0.0009000000000000001
iter 60, train loss 10.450590133666992, val loss None, lr 0.0008100000000000001
iter 70, train loss 10.359129905700684, val loss None, lr 0.000729
iter 80, train loss 10.301885604858398, val loss None, lr 0.000729
iter 90, train loss 10.263006210327148, val loss None, lr 0.00059049
best loss 10.221452713012695
layer20: mlp.gate_proj
norm_0 tensor([2.0840, 2.0762, 2.0762,  ..., 2.0781, 2.0684, 2.0410], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.6143, 0.5889, 0.5723,  ..., 0.6572, 0.5854, 0.5718], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 389.33990478515625, val loss None, lr 0.001
iter 10, train loss 406.46875, val loss None, lr 0.0008100000000000001
iter 20, train loss 404.814453125, val loss None, lr 0.000531441
iter 30, train loss 403.2717590332031, val loss None, lr 0.000387420489
iter 40, train loss 402.05712890625, val loss None, lr 0.00028242953648100003
iter 50, train loss 401.413818359375, val loss None, lr 0.00018530201888518417
iter 60, train loss 402.5272216796875, val loss None, lr 0.0001350851717672993
iter 70, train loss 401.7640686035156, val loss None, lr 9.847709021836118e-05
iter 80, train loss 400.9843444824219, val loss None, lr 6.461081889226677e-05
iter 90, train loss 400.7613220214844, val loss None, lr 4.7101286972462485e-05
best loss 385.13427734375
layer20: mlp.up_proj
norm_0 tensor([1.9189, 1.9297, 1.9297,  ..., 1.9404, 1.9395, 1.9697], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.6250, 0.6016, 0.6074,  ..., 0.5781, 0.5952, 0.6099], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 331.05950927734375, val loss None, lr 0.001
iter 10, train loss 331.3820495605469, val loss None, lr 0.0008100000000000001
iter 20, train loss 331.8658142089844, val loss None, lr 0.000531441
iter 30, train loss 332.1715087890625, val loss None, lr 0.000387420489
iter 40, train loss 332.18890380859375, val loss None, lr 0.00028242953648100003
iter 50, train loss 332.2514953613281, val loss None, lr 0.00018530201888518417
iter 60, train loss 332.65814208984375, val loss None, lr 0.0001350851717672993
iter 70, train loss 332.84918212890625, val loss None, lr 9.847709021836118e-05
iter 80, train loss 332.5197448730469, val loss None, lr 6.461081889226677e-05
iter 90, train loss 332.3237609863281, val loss None, lr 4.7101286972462485e-05
best loss 330.48284912109375
layer20: mlp.down_proj
norm_0 tensor([1.1963, 1.1641, 1.1738,  ..., 1.1289, 1.1533, 1.1768], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.6328, 1.6299, 1.5996,  ..., 1.6387, 1.6143, 1.6260], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 13.482467651367188, val loss None, lr 0.001
iter 10, train loss 13.505847930908203, val loss None, lr 0.0008100000000000001
iter 20, train loss 13.460992813110352, val loss None, lr 0.0006561000000000001
iter 30, train loss 13.440633773803711, val loss None, lr 0.0004782969
iter 40, train loss 13.408651351928711, val loss None, lr 0.0004782969
iter 50, train loss 13.395437240600586, val loss None, lr 0.00043046721
iter 60, train loss 13.38679313659668, val loss None, lr 0.000387420489
iter 70, train loss 13.387077331542969, val loss None, lr 0.00025418658283290005
iter 80, train loss 13.38142204284668, val loss None, lr 0.00018530201888518417
iter 90, train loss 13.379451751708984, val loss None, lr 0.0001350851717672993
best loss 13.370390892028809
30661 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
23021 MiB free out of 48676 MiB total
after cast to cpu
30253 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
layer21: self_attn.q_proj
norm_0 tensor([1.4141, 1.4355, 1.4150,  ..., 1.3877, 1.4229, 1.3867], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.6099, 0.6714, 0.6670,  ..., 1.2812, 1.0244, 1.3604], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 362.39300537109375, val loss None, lr 0.001
iter 10, train loss 385.3315734863281, val loss None, lr 0.000729
iter 20, train loss 373.0867614746094, val loss None, lr 0.000531441
iter 30, train loss 365.03546142578125, val loss None, lr 0.000387420489
iter 40, train loss 359.93389892578125, val loss None, lr 0.00025418658283290005
iter 50, train loss 358.02569580078125, val loss None, lr 0.00018530201888518417
iter 60, train loss 355.68017578125, val loss None, lr 0.0001350851717672993
iter 70, train loss 354.1368713378906, val loss None, lr 8.862938119652506e-05
iter 80, train loss 355.2294616699219, val loss None, lr 6.461081889226677e-05
iter 90, train loss 355.51422119140625, val loss None, lr 4.7101286972462485e-05
best loss 341.85272216796875
layer21: self_attn.k_proj
norm_0 tensor([1.4551, 1.4717, 1.4316,  ..., 1.4121, 1.4375, 1.4316], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5571, 0.6606, 0.6641,  ..., 1.4189, 1.1855, 1.3262], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 382.6987609863281, val loss None, lr 0.001
iter 10, train loss 401.340087890625, val loss None, lr 0.000729
iter 20, train loss 384.4898681640625, val loss None, lr 0.000531441
iter 30, train loss 376.3039245605469, val loss None, lr 0.000387420489
iter 40, train loss 372.36773681640625, val loss None, lr 0.00025418658283290005
iter 50, train loss 369.1352233886719, val loss None, lr 0.00018530201888518417
iter 60, train loss 369.18365478515625, val loss None, lr 0.0001350851717672993
iter 70, train loss 366.80029296875, val loss None, lr 8.862938119652506e-05
iter 80, train loss 366.8336181640625, val loss None, lr 6.461081889226677e-05
iter 90, train loss 365.8766174316406, val loss None, lr 4.7101286972462485e-05
best loss 359.7078857421875
layer21: self_attn.v_proj
norm_0 tensor([1.1396, 1.1367, 1.1572,  ..., 1.1680, 1.1572, 1.1758], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.9434, 0.9448, 0.9663,  ..., 1.0039, 1.0078, 0.9932], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 200.40750122070312, val loss None, lr 0.001
iter 10, train loss 198.8023681640625, val loss None, lr 0.001
iter 20, train loss 199.05392456054688, val loss None, lr 0.000729
iter 30, train loss 198.51181030273438, val loss None, lr 0.00059049
iter 40, train loss 198.68601989746094, val loss None, lr 0.00043046721
iter 50, train loss 198.66822814941406, val loss None, lr 0.00031381059609000004
iter 60, train loss 198.46646118164062, val loss None, lr 0.00025418658283290005
iter 70, train loss 198.46717834472656, val loss None, lr 0.00018530201888518417
iter 80, train loss 198.66867065429688, val loss None, lr 0.00012157665459056936
iter 90, train loss 198.53164672851562, val loss None, lr 8.862938119652506e-05
best loss 198.23605346679688
layer21: self_attn.o_proj
norm_0 tensor([1.0898, 1.0791, 1.1064,  ..., 1.1357, 1.1426, 1.1260], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.0186, 1.0312, 0.9946,  ..., 1.0088, 0.9761, 1.0010], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 10.498920440673828, val loss None, lr 0.001
iter 10, train loss 9.799354553222656, val loss None, lr 0.0009000000000000001
iter 20, train loss 9.483465194702148, val loss None, lr 0.0009000000000000001
iter 30, train loss 9.368122100830078, val loss None, lr 0.0008100000000000001
iter 40, train loss 9.197183609008789, val loss None, lr 0.0008100000000000001
iter 50, train loss 9.179977416992188, val loss None, lr 0.000729
iter 60, train loss 9.141971588134766, val loss None, lr 0.0006561000000000001
iter 70, train loss 9.086421012878418, val loss None, lr 0.0006561000000000001
iter 80, train loss 9.050304412841797, val loss None, lr 0.0006561000000000001
iter 90, train loss 9.011300086975098, val loss None, lr 0.000531441
best loss 9.011300086975098
layer21: mlp.gate_proj
norm_0 tensor([2.0898, 2.0723, 2.0703,  ..., 2.0879, 2.0684, 2.0742], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5762, 0.6357, 0.8628,  ..., 0.6323, 0.5581, 0.6060], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 413.570068359375, val loss None, lr 0.001
iter 10, train loss 431.2393798828125, val loss None, lr 0.0008100000000000001
iter 20, train loss 428.70782470703125, val loss None, lr 0.000531441
iter 30, train loss 426.8388671875, val loss None, lr 0.000387420489
iter 40, train loss 425.4957275390625, val loss None, lr 0.00028242953648100003
iter 50, train loss 425.188232421875, val loss None, lr 0.00018530201888518417
iter 60, train loss 425.1963806152344, val loss None, lr 0.0001350851717672993
iter 70, train loss 425.68072509765625, val loss None, lr 9.847709021836118e-05
iter 80, train loss 425.3978271484375, val loss None, lr 6.461081889226677e-05
iter 90, train loss 425.185302734375, val loss None, lr 4.7101286972462485e-05
best loss 410.2914123535156
layer21: mlp.up_proj
norm_0 tensor([1.9199, 1.9414, 1.9492,  ..., 1.9453, 1.9502, 1.9482], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.6128, 0.6001, 0.5967,  ..., 0.6216, 0.5693, 0.6104], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 347.23248291015625, val loss None, lr 0.001
iter 10, train loss 347.52972412109375, val loss None, lr 0.0008100000000000001
iter 20, train loss 348.03961181640625, val loss None, lr 0.00059049
iter 30, train loss 348.27313232421875, val loss None, lr 0.000387420489
iter 40, train loss 348.48681640625, val loss None, lr 0.00028242953648100003
iter 50, train loss 348.2569274902344, val loss None, lr 0.00020589113209464906
iter 60, train loss 348.42779541015625, val loss None, lr 0.0001350851717672993
iter 70, train loss 348.5405578613281, val loss None, lr 9.847709021836118e-05
iter 80, train loss 348.79083251953125, val loss None, lr 7.17897987691853e-05
iter 90, train loss 348.67266845703125, val loss None, lr 4.7101286972462485e-05
best loss 346.9740905761719
layer21: mlp.down_proj
norm_0 tensor([1.1758, 1.1504, 1.1396,  ..., 1.1865, 1.0908, 1.1748], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.6426, 1.6338, 1.6162,  ..., 1.6455, 1.6689, 1.6455], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 13.801167488098145, val loss None, lr 0.001
iter 10, train loss 13.778192520141602, val loss None, lr 0.0008100000000000001
iter 20, train loss 13.707528114318848, val loss None, lr 0.000729
iter 30, train loss 13.661248207092285, val loss None, lr 0.000729
iter 40, train loss 13.659399032592773, val loss None, lr 0.0006561000000000001
iter 50, train loss 13.638500213623047, val loss None, lr 0.000531441
iter 60, train loss 13.612201690673828, val loss None, lr 0.0004782969
iter 70, train loss 13.60920524597168, val loss None, lr 0.0003486784401
iter 80, train loss 13.59901237487793, val loss None, lr 0.00028242953648100003
iter 90, train loss 13.595316886901855, val loss None, lr 0.00020589113209464906
best loss 13.595316886901855
30253 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
22613 MiB free out of 48676 MiB total
after cast to cpu
29845 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
layer22: self_attn.q_proj
norm_0 tensor([1.4336, 1.4648, 1.4492,  ..., 1.4375, 1.4297, 1.4365], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.0156, 1.0264, 1.0361,  ..., 1.3164, 1.3232, 1.5078], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 379.9839782714844, val loss None, lr 0.001
iter 10, train loss 407.575927734375, val loss None, lr 0.000729
iter 20, train loss 389.77392578125, val loss None, lr 0.000531441
iter 30, train loss 382.42926025390625, val loss None, lr 0.000387420489
iter 40, train loss 377.2972412109375, val loss None, lr 0.00025418658283290005
iter 50, train loss 374.9740295410156, val loss None, lr 0.00018530201888518417
iter 60, train loss 373.2287902832031, val loss None, lr 0.0001350851717672993
iter 70, train loss 372.7410583496094, val loss None, lr 8.862938119652506e-05
iter 80, train loss 372.13671875, val loss None, lr 6.461081889226677e-05
iter 90, train loss 371.1260986328125, val loss None, lr 4.7101286972462485e-05
best loss 362.28643798828125
layer22: self_attn.k_proj
norm_0 tensor([1.4746, 1.4775, 1.4902,  ..., 1.4658, 1.4785, 1.4824], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.0010, 1.0156, 1.0244,  ..., 0.8042, 1.3096, 1.3477], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 405.60626220703125, val loss None, lr 0.001
iter 10, train loss 425.02362060546875, val loss None, lr 0.000729
iter 20, train loss 413.5896301269531, val loss None, lr 0.000531441
iter 30, train loss 405.88507080078125, val loss None, lr 0.000387420489
iter 40, train loss 402.8511047363281, val loss None, lr 0.00025418658283290005
iter 50, train loss 398.8048095703125, val loss None, lr 0.00018530201888518417
iter 60, train loss 398.26800537109375, val loss None, lr 0.0001350851717672993
iter 70, train loss 397.68505859375, val loss None, lr 8.862938119652506e-05
iter 80, train loss 398.2269287109375, val loss None, lr 6.461081889226677e-05
iter 90, train loss 397.2462158203125, val loss None, lr 4.7101286972462485e-05
best loss 382.2493896484375
layer22: self_attn.v_proj
norm_0 tensor([1.1533, 1.1396, 1.1543,  ..., 1.1416, 1.1611, 1.1689], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.0938, 1.0869, 1.0977,  ..., 1.0674, 1.0605, 1.0586], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 206.3962860107422, val loss None, lr 0.001
iter 10, train loss 205.50247192382812, val loss None, lr 0.0009000000000000001
iter 20, train loss 205.4371337890625, val loss None, lr 0.000729
iter 30, train loss 204.97850036621094, val loss None, lr 0.000531441
iter 40, train loss 204.61032104492188, val loss None, lr 0.0004782969
iter 50, train loss 204.41256713867188, val loss None, lr 0.0003486784401
iter 60, train loss 204.38681030273438, val loss None, lr 0.00025418658283290005
iter 70, train loss 204.2611541748047, val loss None, lr 0.00020589113209464906
iter 80, train loss 204.1062774658203, val loss None, lr 0.0001350851717672993
iter 90, train loss 203.87319946289062, val loss None, lr 0.00012157665459056936
best loss 203.87319946289062
layer22: self_attn.o_proj
norm_0 tensor([1.2227, 1.2178, 1.2207,  ..., 1.1592, 1.1973, 1.1738], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.9922, 0.9956, 1.0020,  ..., 0.9902, 1.0010, 0.9966], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 56.144264221191406, val loss None, lr 0.001
iter 10, train loss 37.80548095703125, val loss None, lr 0.001
iter 20, train loss 29.25444793701172, val loss None, lr 0.001
iter 30, train loss 22.634735107421875, val loss None, lr 0.001
iter 40, train loss 17.987499237060547, val loss None, lr 0.001
iter 50, train loss 16.064407348632812, val loss None, lr 0.001
iter 60, train loss 15.638822555541992, val loss None, lr 0.001
iter 70, train loss 15.446531295776367, val loss None, lr 0.0009000000000000001
iter 80, train loss 15.104342460632324, val loss None, lr 0.0009000000000000001
iter 90, train loss 15.178081512451172, val loss None, lr 0.0006561000000000001
best loss 14.980034828186035
layer22: mlp.gate_proj
norm_0 tensor([2.1055, 2.0918, 2.1211,  ..., 2.0996, 2.0859, 2.0898], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.6201, 0.5845, 0.5884,  ..., 0.5752, 0.6777, 0.5820], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 421.6530456542969, val loss None, lr 0.001
iter 10, train loss 436.93072509765625, val loss None, lr 0.0008100000000000001
iter 20, train loss 434.6047668457031, val loss None, lr 0.000531441
iter 30, train loss 435.96929931640625, val loss None, lr 0.000387420489
iter 40, train loss 434.8925476074219, val loss None, lr 0.00028242953648100003
iter 50, train loss 433.2568054199219, val loss None, lr 0.00018530201888518417
iter 60, train loss 432.5918884277344, val loss None, lr 0.0001350851717672993
iter 70, train loss 432.64471435546875, val loss None, lr 9.847709021836118e-05
iter 80, train loss 432.231201171875, val loss None, lr 6.461081889226677e-05
iter 90, train loss 432.091552734375, val loss None, lr 4.7101286972462485e-05
best loss 417.3094482421875
layer22: mlp.up_proj
norm_0 tensor([1.9238, 1.9365, 1.9082,  ..., 1.9385, 1.9424, 1.9453], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.6074, 0.5898, 0.6133,  ..., 0.6416, 0.6157, 0.6045], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 350.38409423828125, val loss None, lr 0.001
iter 10, train loss 350.71453857421875, val loss None, lr 0.000729
iter 20, train loss 351.4432067871094, val loss None, lr 0.000531441
iter 30, train loss 351.424072265625, val loss None, lr 0.0003486784401
iter 40, train loss 351.6981506347656, val loss None, lr 0.00025418658283290005
iter 50, train loss 351.91455078125, val loss None, lr 0.00018530201888518417
iter 60, train loss 351.9408264160156, val loss None, lr 0.00012157665459056936
iter 70, train loss 352.1385192871094, val loss None, lr 8.862938119652506e-05
iter 80, train loss 352.05511474609375, val loss None, lr 6.461081889226677e-05
iter 90, train loss 352.02374267578125, val loss None, lr 4.239115827521624e-05
best loss 350.38409423828125
layer22: mlp.down_proj
norm_0 tensor([1.1670, 1.1377, 1.1826,  ..., 1.2158, 1.1846, 1.1650], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.6611, 1.6270, 1.6455,  ..., 1.6406, 1.6670, 1.6357], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 14.518794059753418, val loss None, lr 0.001
iter 10, train loss 14.552071571350098, val loss None, lr 0.000729
iter 20, train loss 14.497169494628906, val loss None, lr 0.0006561000000000001
iter 30, train loss 14.485746383666992, val loss None, lr 0.00059049
iter 40, train loss 14.472540855407715, val loss None, lr 0.00043046721
iter 50, train loss 14.466069221496582, val loss None, lr 0.0003486784401
iter 60, train loss 14.465848922729492, val loss None, lr 0.00028242953648100003
iter 70, train loss 14.477751731872559, val loss None, lr 0.00018530201888518417
iter 80, train loss 14.477043151855469, val loss None, lr 0.0001350851717672993
iter 90, train loss 14.480133056640625, val loss None, lr 9.847709021836118e-05
best loss 14.463908195495605
29845 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
22205 MiB free out of 48676 MiB total
after cast to cpu
29435 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
layer23: self_attn.q_proj
norm_0 tensor([1.4346, 1.4453, 1.4326,  ..., 1.4609, 1.4014, 1.4336], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5103, 0.5107, 0.5371,  ..., 1.1738, 1.3018, 1.1387], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 408.0598449707031, val loss None, lr 0.001
iter 10, train loss 421.89599609375, val loss None, lr 0.000729
iter 20, train loss 404.27093505859375, val loss None, lr 0.000531441
iter 30, train loss 399.1533203125, val loss None, lr 0.000387420489
iter 40, train loss 396.52423095703125, val loss None, lr 0.00025418658283290005
iter 50, train loss 395.8734130859375, val loss None, lr 0.00018530201888518417
iter 60, train loss 395.8481140136719, val loss None, lr 0.0001350851717672993
iter 70, train loss 394.7536315917969, val loss None, lr 8.862938119652506e-05
iter 80, train loss 394.9152526855469, val loss None, lr 6.461081889226677e-05
iter 90, train loss 394.89776611328125, val loss None, lr 4.7101286972462485e-05
best loss 391.69732666015625
layer23: self_attn.k_proj
norm_0 tensor([1.4512, 1.4863, 1.4678,  ..., 1.4688, 1.4492, 1.4697], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5093, 0.5063, 0.5337,  ..., 1.0703, 0.9331, 1.0977], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 425.96392822265625, val loss None, lr 0.001
iter 10, train loss 450.31170654296875, val loss None, lr 0.000729
iter 20, train loss 428.2882385253906, val loss None, lr 0.000531441
iter 30, train loss 423.2550048828125, val loss None, lr 0.000387420489
iter 40, train loss 418.1800537109375, val loss None, lr 0.00025418658283290005
iter 50, train loss 417.2898254394531, val loss None, lr 0.00018530201888518417
iter 60, train loss 416.5433654785156, val loss None, lr 0.0001350851717672993
iter 70, train loss 416.4577331542969, val loss None, lr 8.862938119652506e-05
iter 80, train loss 414.9713439941406, val loss None, lr 6.461081889226677e-05
iter 90, train loss 413.82501220703125, val loss None, lr 4.7101286972462485e-05
best loss 407.12896728515625
layer23: self_attn.v_proj
norm_0 tensor([1.2002, 1.1748, 1.2148,  ..., 1.1846, 1.2295, 1.2295], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.9097, 0.8989, 0.9121,  ..., 0.9551, 0.9502, 0.9492], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 249.24295043945312, val loss None, lr 0.001
iter 10, train loss 248.97293090820312, val loss None, lr 0.0008100000000000001
iter 20, train loss 248.32138061523438, val loss None, lr 0.000729
iter 30, train loss 248.1212615966797, val loss None, lr 0.00059049
iter 40, train loss 247.66476440429688, val loss None, lr 0.0004782969
iter 50, train loss 247.3064727783203, val loss None, lr 0.00043046721
iter 60, train loss 247.1079864501953, val loss None, lr 0.000387420489
iter 70, train loss 247.04782104492188, val loss None, lr 0.00028242953648100003
iter 80, train loss 246.93846130371094, val loss None, lr 0.00020589113209464906
iter 90, train loss 247.01556396484375, val loss None, lr 0.0001500946352969992
best loss 246.694091796875
layer23: self_attn.o_proj
norm_0 tensor([1.0889, 1.0791, 1.0947,  ..., 1.1396, 1.1250, 1.1348], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.0059, 1.0078, 0.9873,  ..., 0.9961, 0.9990, 0.9941], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 11.837925910949707, val loss None, lr 0.001
iter 10, train loss 11.74287223815918, val loss None, lr 0.0008100000000000001
iter 20, train loss 11.63347053527832, val loss None, lr 0.00059049
iter 30, train loss 11.577380180358887, val loss None, lr 0.0004782969
iter 40, train loss 11.490060806274414, val loss None, lr 0.0004782969
iter 50, train loss 11.422890663146973, val loss None, lr 0.00043046721
iter 60, train loss 11.3178071975708, val loss None, lr 0.00043046721
iter 70, train loss 11.317472457885742, val loss None, lr 0.000387420489
iter 80, train loss 11.325824737548828, val loss None, lr 0.00028242953648100003
iter 90, train loss 11.337092399597168, val loss None, lr 0.00020589113209464906
best loss 11.282211303710938
layer23: mlp.gate_proj
norm_0 tensor([2.0957, 2.0938, 2.0859,  ..., 2.1016, 2.0859, 2.1035], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5854, 0.5957, 0.6162,  ..., 0.6245, 0.5713, 0.5938], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 458.5838928222656, val loss None, lr 0.001
iter 10, train loss 470.26904296875, val loss None, lr 0.0008100000000000001
iter 20, train loss 471.36907958984375, val loss None, lr 0.000531441
iter 30, train loss 470.1119384765625, val loss None, lr 0.000387420489
iter 40, train loss 470.1307067871094, val loss None, lr 0.00028242953648100003
iter 50, train loss 469.4142150878906, val loss None, lr 0.00018530201888518417
iter 60, train loss 469.74456787109375, val loss None, lr 0.0001350851717672993
iter 70, train loss 468.9990539550781, val loss None, lr 9.847709021836118e-05
iter 80, train loss 468.3561706542969, val loss None, lr 6.461081889226677e-05
iter 90, train loss 468.0898132324219, val loss None, lr 4.7101286972462485e-05
best loss 455.50048828125
layer23: mlp.up_proj
norm_0 tensor([1.9434, 1.9414, 1.9482,  ..., 1.9385, 1.9551, 1.9414], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.6162, 0.6108, 0.6108,  ..., 0.5962, 0.6040, 0.6133], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 383.02825927734375, val loss None, lr 0.001
iter 10, train loss 383.41656494140625, val loss None, lr 0.000729
iter 20, train loss 383.2601318359375, val loss None, lr 0.000531441
iter 30, train loss 383.4839172363281, val loss None, lr 0.0003486784401
iter 40, train loss 383.7242431640625, val loss None, lr 0.00025418658283290005
iter 50, train loss 383.61224365234375, val loss None, lr 0.00018530201888518417
iter 60, train loss 383.4374084472656, val loss None, lr 0.00012157665459056936
iter 70, train loss 383.6201171875, val loss None, lr 8.862938119652506e-05
iter 80, train loss 383.85968017578125, val loss None, lr 6.461081889226677e-05
iter 90, train loss 383.7601623535156, val loss None, lr 4.239115827521624e-05
best loss 383.0010986328125
layer23: mlp.down_proj
norm_0 tensor([1.1904, 1.1797, 1.1846,  ..., 1.1572, 1.1680, 1.1875], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.6455, 1.6436, 1.6299,  ..., 1.6309, 1.6523, 1.6592], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 16.30789566040039, val loss None, lr 0.001
iter 10, train loss 16.321510314941406, val loss None, lr 0.000729
iter 20, train loss 16.279603958129883, val loss None, lr 0.00059049
iter 30, train loss 16.25996971130371, val loss None, lr 0.00043046721
iter 40, train loss 16.24417495727539, val loss None, lr 0.000387420489
iter 50, train loss 16.223888397216797, val loss None, lr 0.00031381059609000004
iter 60, train loss 16.207176208496094, val loss None, lr 0.00028242953648100003
iter 70, train loss 16.203044891357422, val loss None, lr 0.00022876792454961005
iter 80, train loss 16.19969367980957, val loss None, lr 0.00018530201888518417
iter 90, train loss 16.193193435668945, val loss None, lr 0.0001500946352969992
best loss 16.186986923217773
29435 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
21797 MiB free out of 48676 MiB total
after cast to cpu
29027 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
layer24: self_attn.q_proj
norm_0 tensor([1.3877, 1.3955, 1.3955,  ..., 1.4209, 1.4150, 1.4092], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.9824, 0.9819, 1.0146,  ..., 1.1738, 1.2207, 1.2129], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 374.4782409667969, val loss None, lr 0.001
iter 10, train loss 391.12603759765625, val loss None, lr 0.000729
iter 20, train loss 373.401123046875, val loss None, lr 0.000531441
iter 30, train loss 371.6847229003906, val loss None, lr 0.000387420489
iter 40, train loss 372.388671875, val loss None, lr 0.00025418658283290005
iter 50, train loss 370.0885314941406, val loss None, lr 0.00018530201888518417
iter 60, train loss 368.811279296875, val loss None, lr 0.0001350851717672993
iter 70, train loss 368.19439697265625, val loss None, lr 8.862938119652506e-05
iter 80, train loss 367.255615234375, val loss None, lr 6.461081889226677e-05
iter 90, train loss 367.06561279296875, val loss None, lr 4.7101286972462485e-05
best loss 357.7699890136719
layer24: self_attn.k_proj
norm_0 tensor([1.4111, 1.4404, 1.4209,  ..., 1.3926, 1.3711, 1.3906], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.9712, 0.9785, 1.0078,  ..., 1.1924, 1.2432, 1.2480], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 396.98895263671875, val loss None, lr 0.001
iter 10, train loss 409.7039489746094, val loss None, lr 0.000729
iter 20, train loss 399.86895751953125, val loss None, lr 0.000531441
iter 30, train loss 391.498046875, val loss None, lr 0.000387420489
iter 40, train loss 387.7671203613281, val loss None, lr 0.00025418658283290005
iter 50, train loss 386.0022888183594, val loss None, lr 0.00018530201888518417
iter 60, train loss 384.3834228515625, val loss None, lr 0.0001350851717672993
iter 70, train loss 383.6168518066406, val loss None, lr 8.862938119652506e-05
iter 80, train loss 382.348388671875, val loss None, lr 6.461081889226677e-05
iter 90, train loss 381.86083984375, val loss None, lr 4.7101286972462485e-05
best loss 376.46722412109375
layer24: self_attn.v_proj
norm_0 tensor([1.2119, 1.1797, 1.1826,  ..., 1.2158, 1.1904, 1.2080], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.0342, 1.0391, 1.0469,  ..., 0.9829, 0.9883, 0.9868], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 237.30801391601562, val loss None, lr 0.001
iter 10, train loss 236.0711669921875, val loss None, lr 0.001
iter 20, train loss 235.8616180419922, val loss None, lr 0.0009000000000000001
iter 30, train loss 235.4353790283203, val loss None, lr 0.000729
iter 40, train loss 234.86245727539062, val loss None, lr 0.0006561000000000001
iter 50, train loss 234.96507263183594, val loss None, lr 0.0004782969
iter 60, train loss 234.43435668945312, val loss None, lr 0.00043046721
iter 70, train loss 234.5655517578125, val loss None, lr 0.0003486784401
iter 80, train loss 234.55133056640625, val loss None, lr 0.00025418658283290005
iter 90, train loss 234.55276489257812, val loss None, lr 0.00016677181699666576
best loss 234.03138732910156
layer24: self_attn.o_proj
norm_0 tensor([1.2158, 1.2217, 1.2324,  ..., 1.1572, 1.1641, 1.1611], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.0166, 1.0166, 1.0146,  ..., 1.0166, 1.0156, 1.0049], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 28.979564666748047, val loss None, lr 0.001
iter 10, train loss 22.61246681213379, val loss None, lr 0.001
iter 20, train loss 19.245126724243164, val loss None, lr 0.001
iter 30, train loss 17.430145263671875, val loss None, lr 0.001
iter 40, train loss 17.035831451416016, val loss None, lr 0.001
iter 50, train loss 16.851211547851562, val loss None, lr 0.0009000000000000001
iter 60, train loss 16.646167755126953, val loss None, lr 0.0009000000000000001
iter 70, train loss 16.56637191772461, val loss None, lr 0.0008100000000000001
iter 80, train loss 16.468868255615234, val loss None, lr 0.0008100000000000001
iter 90, train loss 16.4532470703125, val loss None, lr 0.0006561000000000001
best loss 16.31399154663086
layer24: mlp.gate_proj
norm_0 tensor([2.0957, 2.0996, 2.1133,  ..., 2.1172, 2.0742, 2.0898], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5962, 0.5820, 0.5894,  ..., 0.7324, 0.5840, 0.5908], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 475.5913391113281, val loss None, lr 0.001
iter 10, train loss 488.88104248046875, val loss None, lr 0.0008100000000000001
iter 20, train loss 486.1986389160156, val loss None, lr 0.000531441
iter 30, train loss 484.5242919921875, val loss None, lr 0.000387420489
iter 40, train loss 485.2699279785156, val loss None, lr 0.00028242953648100003
iter 50, train loss 485.8380432128906, val loss None, lr 0.00018530201888518417
iter 60, train loss 485.07861328125, val loss None, lr 0.0001350851717672993
iter 70, train loss 484.64532470703125, val loss None, lr 9.847709021836118e-05
iter 80, train loss 484.5968933105469, val loss None, lr 6.461081889226677e-05
iter 90, train loss 484.30889892578125, val loss None, lr 4.7101286972462485e-05
best loss 472.6213073730469
layer24: mlp.up_proj
norm_0 tensor([1.9482, 1.9463, 1.9375,  ..., 1.9424, 1.9717, 1.9648], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.6069, 0.6074, 0.6108,  ..., 0.6602, 0.6094, 0.6113], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 399.6855773925781, val loss None, lr 0.001
iter 10, train loss 400.02691650390625, val loss None, lr 0.000729
iter 20, train loss 400.5735168457031, val loss None, lr 0.000531441
iter 30, train loss 400.9068298339844, val loss None, lr 0.0003486784401
iter 40, train loss 401.40802001953125, val loss None, lr 0.00025418658283290005
iter 50, train loss 401.3162536621094, val loss None, lr 0.00018530201888518417
iter 60, train loss 401.3390808105469, val loss None, lr 0.00012157665459056936
iter 70, train loss 401.18896484375, val loss None, lr 8.862938119652506e-05
iter 80, train loss 401.49505615234375, val loss None, lr 6.461081889226677e-05
iter 90, train loss 401.2781066894531, val loss None, lr 4.239115827521624e-05
best loss 399.6855773925781
layer24: mlp.down_proj
norm_0 tensor([1.1787, 1.1797, 1.1846,  ..., 1.2139, 1.1807, 1.1875], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.6465, 1.6338, 1.6123,  ..., 1.6299, 1.6709, 1.6494], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 17.08684539794922, val loss None, lr 0.001
iter 10, train loss 17.09796714782715, val loss None, lr 0.000729
iter 20, train loss 17.027359008789062, val loss None, lr 0.000729
iter 30, train loss 17.007322311401367, val loss None, lr 0.00059049
iter 40, train loss 16.97991943359375, val loss None, lr 0.000531441
iter 50, train loss 16.958499908447266, val loss None, lr 0.0004782969
iter 60, train loss 16.943649291992188, val loss None, lr 0.00043046721
iter 70, train loss 16.931438446044922, val loss None, lr 0.000387420489
iter 80, train loss 16.919723510742188, val loss None, lr 0.0003486784401
iter 90, train loss 16.922332763671875, val loss None, lr 0.00028242953648100003
best loss 16.91374969482422
29027 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
21389 MiB free out of 48676 MiB total
after cast to cpu
28619 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
layer25: self_attn.q_proj
norm_0 tensor([1.4170, 1.3965, 1.4229,  ..., 1.4102, 1.3906, 1.4141], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.3994, 0.3853, 0.4221,  ..., 1.1709, 1.2480, 1.1719], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 422.89990234375, val loss None, lr 0.001
iter 10, train loss 439.0569763183594, val loss None, lr 0.000729
iter 20, train loss 421.759521484375, val loss None, lr 0.000531441
iter 30, train loss 423.26275634765625, val loss None, lr 0.000387420489
iter 40, train loss 424.2984619140625, val loss None, lr 0.00025418658283290005
iter 50, train loss 421.3172607421875, val loss None, lr 0.00018530201888518417
iter 60, train loss 419.0542907714844, val loss None, lr 0.0001350851717672993
iter 70, train loss 418.33428955078125, val loss None, lr 8.862938119652506e-05
iter 80, train loss 417.3892822265625, val loss None, lr 6.461081889226677e-05
iter 90, train loss 417.0594482421875, val loss None, lr 4.7101286972462485e-05
best loss 410.7151184082031
layer25: self_attn.k_proj
norm_0 tensor([1.4189, 1.4688, 1.4131,  ..., 1.4209, 1.3965, 1.4219], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.3999, 0.3755, 0.4084,  ..., 1.1504, 1.2510, 1.2012], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 445.15667724609375, val loss None, lr 0.001
iter 10, train loss 463.82305908203125, val loss None, lr 0.000729
iter 20, train loss 441.7154541015625, val loss None, lr 0.000531441
iter 30, train loss 434.9685363769531, val loss None, lr 0.000387420489
iter 40, train loss 434.7788391113281, val loss None, lr 0.00025418658283290005
iter 50, train loss 432.39581298828125, val loss None, lr 0.00018530201888518417
iter 60, train loss 433.07708740234375, val loss None, lr 0.0001350851717672993
iter 70, train loss 431.8846435546875, val loss None, lr 8.862938119652506e-05
iter 80, train loss 431.4483947753906, val loss None, lr 6.461081889226677e-05
iter 90, train loss 430.5014953613281, val loss None, lr 4.7101286972462485e-05
best loss 427.24847412109375
layer25: self_attn.v_proj
norm_0 tensor([1.2549, 1.2246, 1.2295,  ..., 1.2441, 1.2461, 1.2500], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.0566, 1.0508, 1.0664,  ..., 1.0342, 1.0488, 1.0332], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 289.43634033203125, val loss None, lr 0.001
iter 10, train loss 288.8369140625, val loss None, lr 0.0008100000000000001
iter 20, train loss 288.1979064941406, val loss None, lr 0.0006561000000000001
iter 30, train loss 288.0721740722656, val loss None, lr 0.000531441
iter 40, train loss 287.83013916015625, val loss None, lr 0.00043046721
iter 50, train loss 287.8569641113281, val loss None, lr 0.00031381059609000004
iter 60, train loss 288.1080017089844, val loss None, lr 0.00020589113209464906
iter 70, train loss 287.91046142578125, val loss None, lr 0.0001500946352969992
iter 80, train loss 287.3388671875, val loss None, lr 0.00012157665459056936
iter 90, train loss 287.3516540527344, val loss None, lr 8.862938119652506e-05
best loss 287.1981201171875
layer25: self_attn.o_proj
norm_0 tensor([1.2656, 1.2500, 1.2812,  ..., 1.2715, 1.2812, 1.2803], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.0146, 1.0244, 1.0039,  ..., 0.9951, 1.0215, 1.0186], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 15.896268844604492, val loss None, lr 0.001
iter 10, train loss 16.04153060913086, val loss None, lr 0.000729
iter 20, train loss 16.222427368164062, val loss None, lr 0.000531441
iter 30, train loss 16.025657653808594, val loss None, lr 0.000387420489
iter 40, train loss 15.885651588439941, val loss None, lr 0.00025418658283290005
iter 50, train loss 15.771045684814453, val loss None, lr 0.00018530201888518417
iter 60, train loss 15.714757919311523, val loss None, lr 0.0001350851717672993
iter 70, train loss 15.630249977111816, val loss None, lr 8.862938119652506e-05
iter 80, train loss 15.559164047241211, val loss None, lr 7.17897987691853e-05
iter 90, train loss 15.500251770019531, val loss None, lr 5.233476330273609e-05
best loss 15.482540130615234
layer25: mlp.gate_proj
norm_0 tensor([2.0938, 2.1016, 2.1172,  ..., 2.1074, 2.0918, 2.1094], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5811, 0.7798, 0.5840,  ..., 0.5850, 0.6807, 0.6221], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 513.5596923828125, val loss None, lr 0.001
iter 10, train loss 528.2342529296875, val loss None, lr 0.0008100000000000001
iter 20, train loss 526.9216918945312, val loss None, lr 0.000531441
iter 30, train loss 525.5521240234375, val loss None, lr 0.000387420489
iter 40, train loss 525.3143310546875, val loss None, lr 0.00028242953648100003
iter 50, train loss 525.8856811523438, val loss None, lr 0.00018530201888518417
iter 60, train loss 523.51220703125, val loss None, lr 0.0001350851717672993
iter 70, train loss 523.3287353515625, val loss None, lr 9.847709021836118e-05
iter 80, train loss 522.8558959960938, val loss None, lr 6.461081889226677e-05
iter 90, train loss 522.578369140625, val loss None, lr 4.7101286972462485e-05
best loss 509.97552490234375
layer25: mlp.up_proj
norm_0 tensor([1.9619, 1.9541, 1.9443,  ..., 1.9619, 1.9629, 1.9561], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.6021, 0.5732, 0.6108,  ..., 0.6089, 0.6016, 0.6147], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 428.2400207519531, val loss None, lr 0.001
iter 10, train loss 427.66363525390625, val loss None, lr 0.0008100000000000001
iter 20, train loss 427.8677062988281, val loss None, lr 0.00059049
iter 30, train loss 428.2354431152344, val loss None, lr 0.00043046721
iter 40, train loss 428.0869140625, val loss None, lr 0.00031381059609000004
iter 50, train loss 428.2481689453125, val loss None, lr 0.00020589113209464906
iter 60, train loss 427.98565673828125, val loss None, lr 0.0001500946352969992
iter 70, train loss 428.0482177734375, val loss None, lr 0.00010941898913151243
iter 80, train loss 427.9964294433594, val loss None, lr 7.17897987691853e-05
iter 90, train loss 428.0736389160156, val loss None, lr 5.233476330273609e-05
best loss 427.4527587890625
layer25: mlp.down_proj
norm_0 tensor([1.1787, 1.1006, 1.1943,  ..., 1.1865, 1.1553, 1.1885], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.6377, 1.6230, 1.6387,  ..., 1.6523, 1.6602, 1.6357], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 17.845722198486328, val loss None, lr 0.001
iter 10, train loss 17.84429359436035, val loss None, lr 0.0008100000000000001
iter 20, train loss 17.808486938476562, val loss None, lr 0.0006561000000000001
iter 30, train loss 17.77908706665039, val loss None, lr 0.000531441
iter 40, train loss 17.757610321044922, val loss None, lr 0.000531441
iter 50, train loss 17.750804901123047, val loss None, lr 0.000387420489
iter 60, train loss 17.748435974121094, val loss None, lr 0.00031381059609000004
iter 70, train loss 17.73733901977539, val loss None, lr 0.00025418658283290005
iter 80, train loss 17.732234954833984, val loss None, lr 0.00018530201888518417
iter 90, train loss 17.73516082763672, val loss None, lr 0.00012157665459056936
best loss 17.726667404174805
28619 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
20981 MiB free out of 48676 MiB total
after cast to cpu
28211 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
layer26: self_attn.q_proj
norm_0 tensor([1.3867, 1.3984, 1.4092,  ..., 1.3887, 1.3691, 1.4180], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.8169, 0.8052, 0.8237,  ..., 0.5854, 1.2822, 1.2910], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 407.9695129394531, val loss None, lr 0.001
iter 10, train loss 430.87066650390625, val loss None, lr 0.000729
iter 20, train loss 414.49407958984375, val loss None, lr 0.000531441
iter 30, train loss 408.1299743652344, val loss None, lr 0.000387420489
iter 40, train loss 402.4433898925781, val loss None, lr 0.00025418658283290005
iter 50, train loss 399.9448547363281, val loss None, lr 0.00018530201888518417
iter 60, train loss 400.65716552734375, val loss None, lr 0.0001350851717672993
iter 70, train loss 398.6802978515625, val loss None, lr 8.862938119652506e-05
iter 80, train loss 398.1116638183594, val loss None, lr 6.461081889226677e-05
iter 90, train loss 397.5784912109375, val loss None, lr 4.7101286972462485e-05
best loss 387.9905700683594
layer26: self_attn.k_proj
norm_0 tensor([1.3887, 1.4287, 1.3926,  ..., 1.4004, 1.3945, 1.3818], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.8066, 0.8022, 0.8164,  ..., 1.2861, 1.2861, 1.2402], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 434.8224182128906, val loss None, lr 0.001
iter 10, train loss 456.8311767578125, val loss None, lr 0.000729
iter 20, train loss 443.19122314453125, val loss None, lr 0.000531441
iter 30, train loss 438.981689453125, val loss None, lr 0.000387420489
iter 40, train loss 433.06158447265625, val loss None, lr 0.00025418658283290005
iter 50, train loss 429.54083251953125, val loss None, lr 0.00018530201888518417
iter 60, train loss 426.38299560546875, val loss None, lr 0.0001350851717672993
iter 70, train loss 424.748291015625, val loss None, lr 8.862938119652506e-05
iter 80, train loss 422.3130798339844, val loss None, lr 6.461081889226677e-05
iter 90, train loss 421.36328125, val loss None, lr 4.7101286972462485e-05
best loss 405.9356689453125
layer26: self_attn.v_proj
norm_0 tensor([1.2695, 1.2607, 1.2549,  ..., 1.2920, 1.2363, 1.2383], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.0547, 1.0449, 1.0547,  ..., 1.0107, 0.9946, 0.9990], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 291.2626647949219, val loss None, lr 0.001
iter 10, train loss 290.46630859375, val loss None, lr 0.0009000000000000001
iter 20, train loss 289.5490417480469, val loss None, lr 0.0008100000000000001
iter 30, train loss 289.0503845214844, val loss None, lr 0.000729
iter 40, train loss 288.115966796875, val loss None, lr 0.0006561000000000001
iter 50, train loss 287.0149230957031, val loss None, lr 0.00059049
iter 60, train loss 286.94622802734375, val loss None, lr 0.0004782969
iter 70, train loss 286.7962646484375, val loss None, lr 0.000387420489
iter 80, train loss 287.189697265625, val loss None, lr 0.00028242953648100003
iter 90, train loss 287.0075988769531, val loss None, lr 0.00020589113209464906
best loss 286.7027587890625
layer26: self_attn.o_proj
norm_0 tensor([1.3115, 1.3018, 1.3115,  ..., 1.2676, 1.2451, 1.2510], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.9995, 0.9941, 0.9927,  ..., 1.0010, 1.0010, 0.9844], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 53.118133544921875, val loss None, lr 0.001
iter 10, train loss 37.86399841308594, val loss None, lr 0.001
iter 20, train loss 29.135761260986328, val loss None, lr 0.001
iter 30, train loss 24.01986312866211, val loss None, lr 0.001
iter 40, train loss 21.407970428466797, val loss None, lr 0.001
iter 50, train loss 20.939558029174805, val loss None, lr 0.001
iter 60, train loss 20.643821716308594, val loss None, lr 0.001
iter 70, train loss 20.515762329101562, val loss None, lr 0.0009000000000000001
iter 80, train loss 20.276439666748047, val loss None, lr 0.0009000000000000001
iter 90, train loss 20.348051071166992, val loss None, lr 0.000729
best loss 20.169010162353516
layer26: mlp.gate_proj
norm_0 tensor([2.1113, 2.0859, 2.1035,  ..., 2.1113, 2.0840, 2.1074], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5879, 0.5894, 0.5073,  ..., 0.5898, 0.5825, 0.6504], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 532.9456176757812, val loss None, lr 0.001
iter 10, train loss 554.7301025390625, val loss None, lr 0.0008100000000000001
iter 20, train loss 553.4942016601562, val loss None, lr 0.000531441
iter 30, train loss 554.2015380859375, val loss None, lr 0.000387420489
iter 40, train loss 552.6920166015625, val loss None, lr 0.00028242953648100003
iter 50, train loss 550.422607421875, val loss None, lr 0.00018530201888518417
iter 60, train loss 549.5135498046875, val loss None, lr 0.0001350851717672993
iter 70, train loss 548.8240966796875, val loss None, lr 9.847709021836118e-05
iter 80, train loss 548.121337890625, val loss None, lr 6.461081889226677e-05
iter 90, train loss 547.87548828125, val loss None, lr 4.7101286972462485e-05
best loss 527.1102905273438
layer26: mlp.up_proj
norm_0 tensor([1.9570, 1.9814, 1.9775,  ..., 1.9756, 1.9824, 1.9707], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.6172, 0.6187, 0.5669,  ..., 0.6133, 0.6094, 0.6104], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 450.3407897949219, val loss None, lr 0.001
iter 10, train loss 450.47747802734375, val loss None, lr 0.0008100000000000001
iter 20, train loss 451.60113525390625, val loss None, lr 0.00059049
iter 30, train loss 451.5955810546875, val loss None, lr 0.000387420489
iter 40, train loss 451.49365234375, val loss None, lr 0.00028242953648100003
iter 50, train loss 451.6144104003906, val loss None, lr 0.00020589113209464906
iter 60, train loss 452.1519775390625, val loss None, lr 0.0001350851717672993
iter 70, train loss 451.97100830078125, val loss None, lr 9.847709021836118e-05
iter 80, train loss 452.19219970703125, val loss None, lr 7.17897987691853e-05
iter 90, train loss 452.29913330078125, val loss None, lr 4.7101286972462485e-05
best loss 450.0860290527344
layer26: mlp.down_proj
norm_0 tensor([1.2051, 1.2080, 1.1055,  ..., 1.2041, 1.1904, 1.1768], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.6514, 1.6426, 1.6309,  ..., 1.6543, 1.6719, 1.6475], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 19.26538848876953, val loss None, lr 0.001
iter 10, train loss 19.283185958862305, val loss None, lr 0.000729
iter 20, train loss 19.24090576171875, val loss None, lr 0.00059049
iter 30, train loss 19.230485916137695, val loss None, lr 0.0004782969
iter 40, train loss 19.216611862182617, val loss None, lr 0.000387420489
iter 50, train loss 19.2017822265625, val loss None, lr 0.000387420489
iter 60, train loss 19.215803146362305, val loss None, lr 0.00028242953648100003
iter 70, train loss 19.189899444580078, val loss None, lr 0.00020589113209464906
iter 80, train loss 19.173776626586914, val loss None, lr 0.00020589113209464906
iter 90, train loss 19.151351928710938, val loss None, lr 0.00018530201888518417
best loss 19.148818969726562
28211 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
20573 MiB free out of 48676 MiB total
after cast to cpu
27803 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
layer27: self_attn.q_proj
norm_0 tensor([1.4111, 1.4785, 1.4375,  ..., 1.4502, 1.4502, 1.4775], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.9072, 0.9258, 0.9243,  ..., 1.0654, 1.0371, 0.6768], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 446.90264892578125, val loss None, lr 0.001
iter 10, train loss 472.7844543457031, val loss None, lr 0.000729
iter 20, train loss 450.6671142578125, val loss None, lr 0.000531441
iter 30, train loss 443.34027099609375, val loss None, lr 0.000387420489
iter 40, train loss 441.3807067871094, val loss None, lr 0.00025418658283290005
iter 50, train loss 438.53875732421875, val loss None, lr 0.00018530201888518417
iter 60, train loss 436.908447265625, val loss None, lr 0.0001350851717672993
iter 70, train loss 434.76812744140625, val loss None, lr 8.862938119652506e-05
iter 80, train loss 434.4025573730469, val loss None, lr 6.461081889226677e-05
iter 90, train loss 433.7634582519531, val loss None, lr 4.7101286972462485e-05
best loss 421.23236083984375
layer27: self_attn.k_proj
norm_0 tensor([1.4434, 1.4600, 1.4590,  ..., 1.4561, 1.4580, 1.4375], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.8877, 0.9116, 0.9131,  ..., 1.0752, 1.0342, 1.0830], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 484.21368408203125, val loss None, lr 0.001
iter 10, train loss 503.04266357421875, val loss None, lr 0.000729
iter 20, train loss 477.8547058105469, val loss None, lr 0.000531441
iter 30, train loss 470.02374267578125, val loss None, lr 0.000387420489
iter 40, train loss 463.55462646484375, val loss None, lr 0.00025418658283290005
iter 50, train loss 461.9599914550781, val loss None, lr 0.00018530201888518417
iter 60, train loss 460.41156005859375, val loss None, lr 0.0001350851717672993
iter 70, train loss 458.2304992675781, val loss None, lr 8.862938119652506e-05
iter 80, train loss 457.564453125, val loss None, lr 6.461081889226677e-05
iter 90, train loss 456.24957275390625, val loss None, lr 4.7101286972462485e-05
best loss 439.214599609375
layer27: self_attn.v_proj
norm_0 tensor([1.2715, 1.2510, 1.2793,  ..., 1.2666, 1.2422, 1.2607], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.0166, 1.0137, 1.0107,  ..., 1.1543, 1.1465, 1.1748], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 297.07513427734375, val loss None, lr 0.001
iter 10, train loss 296.4710693359375, val loss None, lr 0.001
iter 20, train loss 296.1590576171875, val loss None, lr 0.000729
iter 30, train loss 295.0589904785156, val loss None, lr 0.0006561000000000001
iter 40, train loss 294.01312255859375, val loss None, lr 0.00059049
iter 50, train loss 294.1305236816406, val loss None, lr 0.0004782969
iter 60, train loss 294.4606628417969, val loss None, lr 0.00031381059609000004
iter 70, train loss 294.15313720703125, val loss None, lr 0.00022876792454961005
iter 80, train loss 294.4180908203125, val loss None, lr 0.00016677181699666576
iter 90, train loss 294.98236083984375, val loss None, lr 0.00010941898913151243
best loss 293.6144714355469
layer27: self_attn.o_proj
norm_0 tensor([1.2764, 1.2842, 1.2754,  ..., 1.5234, 1.4912, 1.5518], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.9780, 1.0078, 1.0020,  ..., 1.0273, 1.0264, 1.0049], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 18.81145668029785, val loss None, lr 0.001
iter 10, train loss 17.57684326171875, val loss None, lr 0.0009000000000000001
iter 20, train loss 16.660856246948242, val loss None, lr 0.0009000000000000001
iter 30, train loss 16.05780601501465, val loss None, lr 0.0009000000000000001
iter 40, train loss 15.639412879943848, val loss None, lr 0.0009000000000000001
iter 50, train loss 15.554506301879883, val loss None, lr 0.000729
iter 60, train loss 15.452377319335938, val loss None, lr 0.000729
iter 70, train loss 15.376056671142578, val loss None, lr 0.00059049
iter 80, train loss 15.304079055786133, val loss None, lr 0.00059049
iter 90, train loss 15.235444068908691, val loss None, lr 0.000531441
best loss 15.201447486877441
layer27: mlp.gate_proj
norm_0 tensor([2.1191, 2.1113, 2.0977,  ..., 2.1191, 2.0957, 2.1250], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5542, 0.5698, 0.5684,  ..., 0.5898, 0.5835, 0.5747], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 575.0390625, val loss None, lr 0.001
iter 10, train loss 597.41650390625, val loss None, lr 0.0008100000000000001
iter 20, train loss 595.9000244140625, val loss None, lr 0.000531441
iter 30, train loss 597.556640625, val loss None, lr 0.000387420489
iter 40, train loss 598.6712036132812, val loss None, lr 0.00028242953648100003
iter 50, train loss 596.8861083984375, val loss None, lr 0.00018530201888518417
iter 60, train loss 596.772216796875, val loss None, lr 0.0001350851717672993
iter 70, train loss 595.70654296875, val loss None, lr 9.847709021836118e-05
iter 80, train loss 594.461669921875, val loss None, lr 6.461081889226677e-05
iter 90, train loss 592.6058349609375, val loss None, lr 4.7101286972462485e-05
best loss 567.8373413085938
layer27: mlp.up_proj
norm_0 tensor([1.9707, 1.9766, 1.9795,  ..., 1.9707, 1.9590, 1.9639], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.6104, 0.6157, 0.6143,  ..., 0.6162, 0.6138, 0.6060], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 485.3460998535156, val loss None, lr 0.001
iter 10, train loss 486.4553527832031, val loss None, lr 0.0008100000000000001
iter 20, train loss 487.12298583984375, val loss None, lr 0.000531441
iter 30, train loss 485.8743896484375, val loss None, lr 0.000387420489
iter 40, train loss 486.33892822265625, val loss None, lr 0.00028242953648100003
iter 50, train loss 486.8205871582031, val loss None, lr 0.00018530201888518417
iter 60, train loss 486.7174987792969, val loss None, lr 0.0001350851717672993
iter 70, train loss 486.44622802734375, val loss None, lr 9.847709021836118e-05
iter 80, train loss 486.17919921875, val loss None, lr 6.461081889226677e-05
iter 90, train loss 485.8852233886719, val loss None, lr 4.7101286972462485e-05
best loss 483.5572204589844
layer27: mlp.down_proj
norm_0 tensor([1.2051, 1.2148, 1.2158,  ..., 1.2227, 1.2139, 1.1934], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.6436, 1.6357, 1.6455,  ..., 1.6436, 1.6680, 1.6650], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 22.2132568359375, val loss None, lr 0.001
iter 10, train loss 22.244632720947266, val loss None, lr 0.000729
iter 20, train loss 22.200119018554688, val loss None, lr 0.00059049
iter 30, train loss 22.210857391357422, val loss None, lr 0.000387420489
iter 40, train loss 22.20261001586914, val loss None, lr 0.00028242953648100003
iter 50, train loss 22.16846466064453, val loss None, lr 0.00025418658283290005
iter 60, train loss 22.149188995361328, val loss None, lr 0.00020589113209464906
iter 70, train loss 22.15228271484375, val loss None, lr 0.00016677181699666576
iter 80, train loss 22.15694236755371, val loss None, lr 0.00012157665459056936
iter 90, train loss 22.153972625732422, val loss None, lr 8.862938119652506e-05
best loss 22.132808685302734
27803 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
20163 MiB free out of 48676 MiB total
after cast to cpu
27395 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
layer28: self_attn.q_proj
norm_0 tensor([1.3896, 1.4053, 1.4189,  ..., 1.3926, 1.3818, 1.4053], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.4944, 0.5146, 0.5117,  ..., 1.1074, 1.1670, 1.2334], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 448.8726501464844, val loss None, lr 0.001
iter 10, train loss 477.3526306152344, val loss None, lr 0.000729
iter 20, train loss 459.6551513671875, val loss None, lr 0.000531441
iter 30, train loss 449.98773193359375, val loss None, lr 0.000387420489
iter 40, train loss 444.009521484375, val loss None, lr 0.00025418658283290005
iter 50, train loss 440.2033996582031, val loss None, lr 0.00018530201888518417
iter 60, train loss 438.860595703125, val loss None, lr 0.0001350851717672993
iter 70, train loss 438.609619140625, val loss None, lr 8.862938119652506e-05
iter 80, train loss 437.069580078125, val loss None, lr 6.461081889226677e-05
iter 90, train loss 435.60205078125, val loss None, lr 4.7101286972462485e-05
best loss 417.271728515625
layer28: self_attn.k_proj
norm_0 tensor([1.4238, 1.4307, 1.4199,  ..., 1.4238, 1.4033, 1.4395], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5518, 0.5249, 0.5312,  ..., 1.2432, 1.2236, 1.2285], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 482.4118957519531, val loss None, lr 0.001
iter 10, train loss 507.15985107421875, val loss None, lr 0.000729
iter 20, train loss 487.962890625, val loss None, lr 0.000531441
iter 30, train loss 476.66656494140625, val loss None, lr 0.000387420489
iter 40, train loss 469.2671813964844, val loss None, lr 0.00025418658283290005
iter 50, train loss 464.8690185546875, val loss None, lr 0.00018530201888518417
iter 60, train loss 459.7656555175781, val loss None, lr 0.0001350851717672993
iter 70, train loss 457.670166015625, val loss None, lr 8.862938119652506e-05
iter 80, train loss 457.490478515625, val loss None, lr 6.461081889226677e-05
iter 90, train loss 457.3428649902344, val loss None, lr 4.7101286972462485e-05
best loss 435.2722473144531
layer28: self_attn.v_proj
norm_0 tensor([1.2852, 1.2939, 1.3252,  ..., 1.2988, 1.3174, 1.3135], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.1309, 1.1387, 1.1641,  ..., 1.0996, 1.1113, 1.1045], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 329.49285888671875, val loss None, lr 0.001
iter 10, train loss 329.59259033203125, val loss None, lr 0.0009000000000000001
iter 20, train loss 329.15576171875, val loss None, lr 0.0006561000000000001
iter 30, train loss 329.399658203125, val loss None, lr 0.0004782969
iter 40, train loss 329.3427429199219, val loss None, lr 0.00031381059609000004
iter 50, train loss 328.5585632324219, val loss None, lr 0.00022876792454961005
iter 60, train loss 328.2167053222656, val loss None, lr 0.00018530201888518417
iter 70, train loss 328.32843017578125, val loss None, lr 0.0001350851717672993
iter 80, train loss 328.34429931640625, val loss None, lr 9.847709021836118e-05
iter 90, train loss 327.92626953125, val loss None, lr 7.976644307687256e-05
best loss 327.6563720703125
layer28: self_attn.o_proj
norm_0 tensor([1.5029, 1.4824, 1.5049,  ..., 1.4678, 1.4609, 1.4609], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.9937, 1.0020, 1.0000,  ..., 1.0322, 1.0430, 1.0068], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 29.897031784057617, val loss None, lr 0.001
iter 10, train loss 28.979145050048828, val loss None, lr 0.0008100000000000001
iter 20, train loss 28.646320343017578, val loss None, lr 0.00059049
iter 30, train loss 28.26784896850586, val loss None, lr 0.0004782969
iter 40, train loss 28.09766387939453, val loss None, lr 0.00043046721
iter 50, train loss 28.137521743774414, val loss None, lr 0.00031381059609000004
iter 60, train loss 28.01299476623535, val loss None, lr 0.00022876792454961005
iter 70, train loss 27.572010040283203, val loss None, lr 0.00022876792454961005
iter 80, train loss 27.377506256103516, val loss None, lr 0.00022876792454961005
iter 90, train loss 27.33795738220215, val loss None, lr 0.00020589113209464906
best loss 27.273099899291992
layer28: mlp.gate_proj
norm_0 tensor([2.1055, 2.1094, 2.1113,  ..., 2.1152, 2.0996, 2.1133], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5732, 0.5693, 0.5732,  ..., 0.5791, 0.5791, 0.5576], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 603.1054077148438, val loss None, lr 0.001
iter 10, train loss 631.6666259765625, val loss None, lr 0.0008100000000000001
iter 20, train loss 621.1947631835938, val loss None, lr 0.000531441
iter 30, train loss 621.0337524414062, val loss None, lr 0.000387420489
iter 40, train loss 620.223388671875, val loss None, lr 0.00028242953648100003
iter 50, train loss 620.3568725585938, val loss None, lr 0.00018530201888518417
iter 60, train loss 619.6986083984375, val loss None, lr 0.0001350851717672993
iter 70, train loss 618.01904296875, val loss None, lr 9.847709021836118e-05
iter 80, train loss 617.5493774414062, val loss None, lr 6.461081889226677e-05
iter 90, train loss 616.4613037109375, val loss None, lr 4.7101286972462485e-05
best loss 594.94775390625
layer28: mlp.up_proj
norm_0 tensor([1.9922, 1.9785, 1.9844,  ..., 1.9854, 1.9727, 1.9932], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.6123, 0.6172, 0.6143,  ..., 0.6187, 0.6123, 0.6128], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 534.4644165039062, val loss None, lr 0.001
iter 10, train loss 543.0224609375, val loss None, lr 0.0008100000000000001
iter 20, train loss 535.461669921875, val loss None, lr 0.000531441
iter 30, train loss 534.901611328125, val loss None, lr 0.000387420489
iter 40, train loss 534.001708984375, val loss None, lr 0.00028242953648100003
iter 50, train loss 533.21142578125, val loss None, lr 0.00018530201888518417
iter 60, train loss 531.83837890625, val loss None, lr 0.0001350851717672993
iter 70, train loss 532.4195556640625, val loss None, lr 9.847709021836118e-05
iter 80, train loss 532.14111328125, val loss None, lr 6.461081889226677e-05
iter 90, train loss 531.6851196289062, val loss None, lr 4.7101286972462485e-05
best loss 530.858642578125
layer28: mlp.down_proj
norm_0 tensor([1.2217, 1.2285, 1.2178,  ..., 1.2363, 1.2178, 1.2188], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.6611, 1.6377, 1.6562,  ..., 1.6406, 1.6826, 1.6348], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 27.010643005371094, val loss None, lr 0.001
iter 10, train loss 27.017494201660156, val loss None, lr 0.000729
iter 20, train loss 26.99913215637207, val loss None, lr 0.000531441
iter 30, train loss 27.002098083496094, val loss None, lr 0.000387420489
iter 40, train loss 26.946229934692383, val loss None, lr 0.0003486784401
iter 50, train loss 26.90233612060547, val loss None, lr 0.00031381059609000004
iter 60, train loss 26.876323699951172, val loss None, lr 0.00031381059609000004
iter 70, train loss 26.85811424255371, val loss None, lr 0.00020589113209464906
iter 80, train loss 26.861595153808594, val loss None, lr 0.00016677181699666576
iter 90, train loss 26.83631706237793, val loss None, lr 0.0001350851717672993
best loss 26.830875396728516
27395 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
19755 MiB free out of 48676 MiB total
after cast to cpu
26987 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
layer29: self_attn.q_proj
norm_0 tensor([1.3789, 1.3936, 1.3721,  ..., 1.3564, 1.3232, 1.3398], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.6392, 0.6831, 0.7266,  ..., 1.0986, 1.2461, 0.9692], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 410.3201904296875, val loss None, lr 0.001
iter 10, train loss 432.72760009765625, val loss None, lr 0.000729
iter 20, train loss 414.53912353515625, val loss None, lr 0.000531441
iter 30, train loss 408.94757080078125, val loss None, lr 0.000387420489
iter 40, train loss 398.4495849609375, val loss None, lr 0.00025418658283290005
iter 50, train loss 397.4502258300781, val loss None, lr 0.00018530201888518417
iter 60, train loss 398.4521484375, val loss None, lr 0.0001350851717672993
iter 70, train loss 398.37042236328125, val loss None, lr 8.862938119652506e-05
iter 80, train loss 396.9821472167969, val loss None, lr 6.461081889226677e-05
iter 90, train loss 396.16033935546875, val loss None, lr 4.7101286972462485e-05
best loss 372.6174011230469
layer29: self_attn.k_proj
norm_0 tensor([1.3682, 1.4170, 1.3818,  ..., 1.4092, 1.3447, 1.3857], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.6626, 0.6821, 0.7129,  ..., 1.1348, 1.3066, 1.3779], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 442.38214111328125, val loss None, lr 0.001
iter 10, train loss 467.4866943359375, val loss None, lr 0.000729
iter 20, train loss 448.6010437011719, val loss None, lr 0.000531441
iter 30, train loss 439.22607421875, val loss None, lr 0.000387420489
iter 40, train loss 432.82342529296875, val loss None, lr 0.00025418658283290005
iter 50, train loss 428.6737060546875, val loss None, lr 0.00018530201888518417
iter 60, train loss 425.4542541503906, val loss None, lr 0.0001350851717672993
iter 70, train loss 422.7330017089844, val loss None, lr 8.862938119652506e-05
iter 80, train loss 420.52862548828125, val loss None, lr 6.461081889226677e-05
iter 90, train loss 417.53399658203125, val loss None, lr 4.7101286972462485e-05
best loss 389.94403076171875
layer29: self_attn.v_proj
norm_0 tensor([1.3330, 1.2891, 1.3330,  ..., 1.2949, 1.3076, 1.3145], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.9766, 0.9478, 0.9673,  ..., 0.9668, 0.9727, 0.9717], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 309.4982604980469, val loss None, lr 0.001
iter 10, train loss 308.7930908203125, val loss None, lr 0.0008100000000000001
iter 20, train loss 307.71368408203125, val loss None, lr 0.0008100000000000001
iter 30, train loss 307.5303955078125, val loss None, lr 0.0006561000000000001
iter 40, train loss 308.1456298828125, val loss None, lr 0.000531441
iter 50, train loss 307.5747985839844, val loss None, lr 0.000387420489
iter 60, train loss 307.5223693847656, val loss None, lr 0.00031381059609000004
iter 70, train loss 307.1403503417969, val loss None, lr 0.00020589113209464906
iter 80, train loss 306.77178955078125, val loss None, lr 0.00018530201888518417
iter 90, train loss 306.8846740722656, val loss None, lr 0.0001350851717672993
best loss 305.6967468261719
layer29: self_attn.o_proj
norm_0 tensor([1.2744, 1.2568, 1.2705,  ..., 1.2578, 1.2646, 1.2598], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.0117, 0.9854, 0.9849,  ..., 1.0127, 1.0352, 1.0098], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 34.82705307006836, val loss None, lr 0.001
iter 10, train loss 27.910480499267578, val loss None, lr 0.001
iter 20, train loss 26.343524932861328, val loss None, lr 0.001
iter 30, train loss 25.756027221679688, val loss None, lr 0.0009000000000000001
iter 40, train loss 25.294898986816406, val loss None, lr 0.0009000000000000001
iter 50, train loss 25.27021598815918, val loss None, lr 0.0008100000000000001
iter 60, train loss 24.703603744506836, val loss None, lr 0.0006561000000000001
iter 70, train loss 24.704452514648438, val loss None, lr 0.000531441
iter 80, train loss 24.502330780029297, val loss None, lr 0.0004782969
iter 90, train loss 24.245935440063477, val loss None, lr 0.00043046721
best loss 24.226285934448242
layer29: mlp.gate_proj
norm_0 tensor([2.1191, 2.1035, 2.1074,  ..., 2.1055, 2.0938, 2.1074], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5664, 0.5767, 0.6104,  ..., 0.5674, 0.5713, 0.5664], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 629.1699829101562, val loss None, lr 0.001
iter 10, train loss 663.2715454101562, val loss None, lr 0.000729
iter 20, train loss 643.086669921875, val loss None, lr 0.000531441
iter 30, train loss 646.532958984375, val loss None, lr 0.000387420489
iter 40, train loss 643.8587646484375, val loss None, lr 0.00025418658283290005
iter 50, train loss 644.247802734375, val loss None, lr 0.00018530201888518417
iter 60, train loss 643.5761108398438, val loss None, lr 0.0001350851717672993
iter 70, train loss 641.9920043945312, val loss None, lr 8.862938119652506e-05
iter 80, train loss 640.54052734375, val loss None, lr 6.461081889226677e-05
iter 90, train loss 639.6112670898438, val loss None, lr 4.7101286972462485e-05
best loss 613.1209716796875
layer29: mlp.up_proj
norm_0 tensor([2.0000, 2.0293, 2.0215,  ..., 2.0156, 2.0078, 2.0234], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.6172, 0.6235, 0.6426,  ..., 0.6147, 0.6089, 0.6167], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 564.6712646484375, val loss None, lr 0.001
iter 10, train loss 586.9361572265625, val loss None, lr 0.000729
iter 20, train loss 560.8272705078125, val loss None, lr 0.000531441
iter 30, train loss 559.5444946289062, val loss None, lr 0.000387420489
iter 40, train loss 556.3496704101562, val loss None, lr 0.00025418658283290005
iter 50, train loss 556.135986328125, val loss None, lr 0.00018530201888518417
iter 60, train loss 554.8333740234375, val loss None, lr 0.0001350851717672993
iter 70, train loss 554.41015625, val loss None, lr 8.862938119652506e-05
iter 80, train loss 554.2861938476562, val loss None, lr 7.17897987691853e-05
iter 90, train loss 553.9434814453125, val loss None, lr 5.233476330273609e-05
best loss 553.166748046875
layer29: mlp.down_proj
norm_0 tensor([1.2285, 1.2559, 1.2871,  ..., 1.2324, 1.2061, 1.2393], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.6162, 1.6123, 1.6396,  ..., 1.6416, 1.6826, 1.6348], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 32.47041702270508, val loss None, lr 0.001
iter 10, train loss 32.52349853515625, val loss None, lr 0.000729
iter 20, train loss 32.50621032714844, val loss None, lr 0.000531441
iter 30, train loss 32.465091705322266, val loss None, lr 0.000387420489
iter 40, train loss 32.34295654296875, val loss None, lr 0.0003486784401
iter 50, train loss 32.38290023803711, val loss None, lr 0.00025418658283290005
iter 60, train loss 32.25489044189453, val loss None, lr 0.00022876792454961005
iter 70, train loss 32.23396301269531, val loss None, lr 0.00020589113209464906
iter 80, train loss 32.25388717651367, val loss None, lr 0.0001500946352969992
iter 90, train loss 32.148380279541016, val loss None, lr 0.0001350851717672993
best loss 32.117408752441406
26987 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
19347 MiB free out of 48676 MiB total
after cast to cpu
26579 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
layer30: self_attn.q_proj
norm_0 tensor([1.3730, 1.4258, 1.3994,  ..., 1.3545, 1.3359, 1.3438], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.7368, 0.7397, 0.7373,  ..., 1.0498, 1.0098, 1.0273], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 437.87640380859375, val loss None, lr 0.001
iter 10, train loss 468.8826599121094, val loss None, lr 0.000729
iter 20, train loss 449.9178466796875, val loss None, lr 0.000531441
iter 30, train loss 437.8169250488281, val loss None, lr 0.000387420489
iter 40, train loss 429.3473815917969, val loss None, lr 0.00025418658283290005
iter 50, train loss 423.8790283203125, val loss None, lr 0.00018530201888518417
iter 60, train loss 420.6339111328125, val loss None, lr 0.0001350851717672993
iter 70, train loss 420.7742614746094, val loss None, lr 8.862938119652506e-05
iter 80, train loss 419.70233154296875, val loss None, lr 6.461081889226677e-05
iter 90, train loss 418.4306945800781, val loss None, lr 4.7101286972462485e-05
best loss 396.94671630859375
layer30: self_attn.k_proj
norm_0 tensor([1.4023, 1.4180, 1.3975,  ..., 1.3867, 1.3604, 1.3975], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.7329, 0.7285, 0.7402,  ..., 1.0410, 1.0596, 1.0371], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 472.6404724121094, val loss None, lr 0.001
iter 10, train loss 501.95147705078125, val loss None, lr 0.000729
iter 20, train loss 475.61285400390625, val loss None, lr 0.000531441
iter 30, train loss 466.4344787597656, val loss None, lr 0.000387420489
iter 40, train loss 454.8377685546875, val loss None, lr 0.00025418658283290005
iter 50, train loss 445.79473876953125, val loss None, lr 0.00018530201888518417
iter 60, train loss 442.12774658203125, val loss None, lr 0.0001350851717672993
iter 70, train loss 441.8088684082031, val loss None, lr 8.862938119652506e-05
iter 80, train loss 439.836181640625, val loss None, lr 6.461081889226677e-05
iter 90, train loss 438.7777099609375, val loss None, lr 4.7101286972462485e-05
best loss 412.70196533203125
layer30: self_attn.v_proj
norm_0 tensor([1.3516, 1.3242, 1.3369,  ..., 1.3467, 1.3623, 1.3652], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.0098, 1.0420, 1.0059,  ..., 1.1006, 1.0938, 1.1279], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 347.2082214355469, val loss None, lr 0.001
iter 10, train loss 347.39410400390625, val loss None, lr 0.0008100000000000001
iter 20, train loss 345.9909362792969, val loss None, lr 0.0006561000000000001
iter 30, train loss 347.1318054199219, val loss None, lr 0.0004782969
iter 40, train loss 347.16973876953125, val loss None, lr 0.0003486784401
iter 50, train loss 347.0811767578125, val loss None, lr 0.00022876792454961005
iter 60, train loss 345.81390380859375, val loss None, lr 0.00018530201888518417
iter 70, train loss 346.1093444824219, val loss None, lr 0.0001350851717672993
iter 80, train loss 345.7079162597656, val loss None, lr 9.847709021836118e-05
iter 90, train loss 345.36688232421875, val loss None, lr 7.976644307687256e-05
best loss 345.17816162109375
layer30: self_attn.o_proj
norm_0 tensor([1.3750, 1.4141, 1.3652,  ..., 1.4678, 1.4893, 1.4814], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.0010, 0.9873, 0.9775,  ..., 1.0000, 1.0088, 1.0098], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 37.555572509765625, val loss None, lr 0.001
iter 10, train loss 30.900882720947266, val loss None, lr 0.001
iter 20, train loss 28.83094024658203, val loss None, lr 0.0009000000000000001
iter 30, train loss 27.85541534423828, val loss None, lr 0.0009000000000000001
iter 40, train loss 27.096670150756836, val loss None, lr 0.0009000000000000001
iter 50, train loss 27.008235931396484, val loss None, lr 0.0008100000000000001
iter 60, train loss 26.55771827697754, val loss None, lr 0.000729
iter 70, train loss 26.203081130981445, val loss None, lr 0.0006561000000000001
iter 80, train loss 26.449501037597656, val loss None, lr 0.0004782969
iter 90, train loss 25.969451904296875, val loss None, lr 0.00043046721
best loss 25.927841186523438
layer30: mlp.gate_proj
norm_0 tensor([2.1562, 2.1406, 2.1602,  ..., 2.1289, 2.1211, 2.1367], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5620, 0.5479, 0.5527,  ..., 0.6050, 0.7075, 0.5610], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 718.0482788085938, val loss None, lr 0.001
iter 10, train loss 760.7373046875, val loss None, lr 0.000729
iter 20, train loss 712.2061767578125, val loss None, lr 0.000531441
iter 30, train loss 695.9693603515625, val loss None, lr 0.000387420489
iter 40, train loss 689.3649291992188, val loss None, lr 0.00025418658283290005
iter 50, train loss 686.859619140625, val loss None, lr 0.00018530201888518417
iter 60, train loss 686.3395385742188, val loss None, lr 0.0001350851717672993
iter 70, train loss 688.240478515625, val loss None, lr 8.862938119652506e-05
iter 80, train loss 688.2462768554688, val loss None, lr 6.461081889226677e-05
iter 90, train loss 689.1773681640625, val loss None, lr 4.7101286972462485e-05
best loss 658.8624267578125
layer30: mlp.up_proj
norm_0 tensor([2.0352, 2.0371, 2.0508,  ..., 2.0391, 2.0508, 2.0254], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.6172, 0.6138, 0.6118,  ..., 0.5757, 0.6182, 0.6245], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 622.997802734375, val loss None, lr 0.001
iter 10, train loss 649.7973022460938, val loss None, lr 0.000729
iter 20, train loss 618.7076416015625, val loss None, lr 0.000531441
iter 30, train loss 601.8724365234375, val loss None, lr 0.000387420489
iter 40, train loss 598.62841796875, val loss None, lr 0.00025418658283290005
iter 50, train loss 592.5748291015625, val loss None, lr 0.00018530201888518417
iter 60, train loss 591.8323364257812, val loss None, lr 0.0001350851717672993
iter 70, train loss 588.3025512695312, val loss None, lr 8.862938119652506e-05
iter 80, train loss 589.3257446289062, val loss None, lr 6.461081889226677e-05
iter 90, train loss 591.22216796875, val loss None, lr 4.7101286972462485e-05
best loss 581.8858642578125
layer30: mlp.down_proj
norm_0 tensor([1.2627, 1.2412, 1.2354,  ..., 1.1279, 1.0615, 1.2676], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.6152, 1.6250, 1.6465,  ..., 1.6309, 1.6963, 1.6270], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 46.8752326965332, val loss None, lr 0.001
iter 10, train loss 48.19658279418945, val loss None, lr 0.000729
iter 20, train loss 47.07509231567383, val loss None, lr 0.000531441
iter 30, train loss 46.80286407470703, val loss None, lr 0.00043046721
iter 40, train loss 46.72462844848633, val loss None, lr 0.0003486784401
iter 50, train loss 46.48410415649414, val loss None, lr 0.00028242953648100003
iter 60, train loss 46.2993278503418, val loss None, lr 0.00022876792454961005
iter 70, train loss 46.18284606933594, val loss None, lr 0.00018530201888518417
iter 80, train loss 46.13542556762695, val loss None, lr 0.00016677181699666576
iter 90, train loss 46.102264404296875, val loss None, lr 0.00012157665459056936
best loss 45.97459411621094
26579 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
18939 MiB free out of 48676 MiB total
after cast to cpu
26171 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
layer31: self_attn.q_proj
norm_0 tensor([1.3574, 1.4004, 1.4141,  ..., 1.4287, 1.3535, 1.3486], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.6294, 0.7041, 0.7632,  ..., 1.1475, 1.1475, 1.1602], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 335.39892578125, val loss None, lr 0.001
iter 10, train loss 351.37005615234375, val loss None, lr 0.000729
iter 20, train loss 342.0513916015625, val loss None, lr 0.000531441
iter 30, train loss 328.27593994140625, val loss None, lr 0.000387420489
iter 40, train loss 320.6322021484375, val loss None, lr 0.00025418658283290005
iter 50, train loss 318.52374267578125, val loss None, lr 0.00018530201888518417
iter 60, train loss 316.8023376464844, val loss None, lr 0.0001350851717672993
iter 70, train loss 313.9147644042969, val loss None, lr 8.862938119652506e-05
iter 80, train loss 312.682861328125, val loss None, lr 6.461081889226677e-05
iter 90, train loss 313.01776123046875, val loss None, lr 4.7101286972462485e-05
best loss 286.4761962890625
layer31: self_attn.k_proj
norm_0 tensor([1.4404, 1.4561, 1.4365,  ..., 1.4336, 1.4102, 1.4043], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.6567, 0.6768, 0.7300,  ..., 1.1465, 1.1113, 1.1416], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 395.3980712890625, val loss None, lr 0.001
iter 10, train loss 397.44287109375, val loss None, lr 0.000729
iter 20, train loss 395.74951171875, val loss None, lr 0.000531441
iter 30, train loss 377.37103271484375, val loss None, lr 0.000387420489
iter 40, train loss 370.29498291015625, val loss None, lr 0.00025418658283290005
iter 50, train loss 363.7776794433594, val loss None, lr 0.00018530201888518417
iter 60, train loss 360.26812744140625, val loss None, lr 0.0001350851717672993
iter 70, train loss 357.60650634765625, val loss None, lr 8.862938119652506e-05
iter 80, train loss 356.68621826171875, val loss None, lr 6.461081889226677e-05
iter 90, train loss 355.51300048828125, val loss None, lr 4.7101286972462485e-05
best loss 314.51165771484375
layer31: self_attn.v_proj
norm_0 tensor([1.1836, 1.2168, 1.2959,  ..., 1.1934, 1.2607, 1.2217], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.9331, 0.9346, 0.9429,  ..., 0.9785, 0.9932, 0.9839], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 201.19004821777344, val loss None, lr 0.001
iter 10, train loss 201.07601928710938, val loss None, lr 0.0008100000000000001
iter 20, train loss 201.43173217773438, val loss None, lr 0.00059049
iter 30, train loss 202.10177612304688, val loss None, lr 0.000387420489
iter 40, train loss 200.6249237060547, val loss None, lr 0.00028242953648100003
iter 50, train loss 200.38526916503906, val loss None, lr 0.00025418658283290005
iter 60, train loss 200.57061767578125, val loss None, lr 0.00018530201888518417
iter 70, train loss 200.6391143798828, val loss None, lr 0.00012157665459056936
iter 80, train loss 200.1226806640625, val loss None, lr 9.847709021836118e-05
iter 90, train loss 200.3649139404297, val loss None, lr 7.17897987691853e-05
best loss 199.92813110351562
layer31: self_attn.o_proj
norm_0 tensor([1.2305, 1.2275, 1.2314,  ..., 1.2520, 1.2607, 1.2422], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.9502, 0.9829, 0.9736,  ..., 0.9819, 1.0146, 0.9907], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 259.2046203613281, val loss None, lr 0.001
iter 10, train loss 194.2871856689453, val loss None, lr 0.001
iter 20, train loss 160.00241088867188, val loss None, lr 0.001
iter 30, train loss 127.38033294677734, val loss None, lr 0.001
iter 40, train loss 106.56134033203125, val loss None, lr 0.001
iter 50, train loss 94.40309143066406, val loss None, lr 0.001
iter 60, train loss 85.45257568359375, val loss None, lr 0.001
iter 70, train loss 82.60139465332031, val loss None, lr 0.001
iter 80, train loss 83.35636138916016, val loss None, lr 0.0008100000000000001
iter 90, train loss 81.35693359375, val loss None, lr 0.0006561000000000001
best loss 80.23501586914062
layer31: mlp.gate_proj
norm_0 tensor([2.1680, 2.2246, 2.2109,  ..., 2.2520, 2.2559, 2.2422], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.5762, 0.5664, 0.5405,  ..., 0.5767, 0.9365, 0.5073], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 658.294921875, val loss None, lr 0.001
iter 10, train loss 663.822509765625, val loss None, lr 0.000729
iter 20, train loss 637.334228515625, val loss None, lr 0.000531441
iter 30, train loss 617.0614013671875, val loss None, lr 0.000387420489
iter 40, train loss 619.2465209960938, val loss None, lr 0.00025418658283290005
iter 50, train loss 616.2892456054688, val loss None, lr 0.00018530201888518417
iter 60, train loss 616.9837036132812, val loss None, lr 0.0001350851717672993
iter 70, train loss 615.1175537109375, val loss None, lr 8.862938119652506e-05
iter 80, train loss 614.2054443359375, val loss None, lr 6.461081889226677e-05
iter 90, train loss 612.6273803710938, val loss None, lr 4.7101286972462485e-05
best loss 563.819580078125
layer31: mlp.up_proj
norm_0 tensor([2.0586, 2.0996, 2.0996,  ..., 2.1133, 2.1641, 2.0840], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([0.6226, 0.6172, 0.5361,  ..., 0.6191, 0.5962, 0.5386], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 640.0994873046875, val loss None, lr 0.001
iter 10, train loss 639.6349487304688, val loss None, lr 0.000729
iter 20, train loss 616.867431640625, val loss None, lr 0.000531441
iter 30, train loss 589.4927978515625, val loss None, lr 0.000387420489
iter 40, train loss 574.3739624023438, val loss None, lr 0.00025418658283290005
iter 50, train loss 568.3045654296875, val loss None, lr 0.00018530201888518417
iter 60, train loss 565.1116943359375, val loss None, lr 0.0001350851717672993
iter 70, train loss 564.3076171875, val loss None, lr 8.862938119652506e-05
iter 80, train loss 561.503662109375, val loss None, lr 6.461081889226677e-05
iter 90, train loss 558.3689575195312, val loss None, lr 4.7101286972462485e-05
best loss 520.8690185546875
layer31: mlp.down_proj
norm_0 tensor([1.3008, 1.2646, 1.0684,  ..., 1.2900, 1.0322, 1.0645], device='cuda:7',
       dtype=torch.float16)
norm_1 tensor([1.6172, 1.6699, 1.6533,  ..., 1.6094, 1.6836, 1.5586], device='cuda:7',
       dtype=torch.float16)
256
iter 0, train loss 98.92974853515625, val loss None, lr 0.001
iter 10, train loss 106.63619995117188, val loss None, lr 0.0008100000000000001
iter 20, train loss 99.70486450195312, val loss None, lr 0.000531441
iter 30, train loss 93.82101440429688, val loss None, lr 0.00043046721
iter 40, train loss 90.87368774414062, val loss None, lr 0.00043046721
iter 50, train loss 89.07109069824219, val loss None, lr 0.00043046721
iter 60, train loss 87.52090454101562, val loss None, lr 0.000387420489
iter 70, train loss 86.37281799316406, val loss None, lr 0.0003486784401
iter 80, train loss 85.05624389648438, val loss None, lr 0.0003486784401
iter 90, train loss 84.36685180664062, val loss None, lr 0.0003486784401
best loss 83.51571655273438
26171 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
18531 MiB free out of 48676 MiB total
after cast to cpu
25763 MiB free out of 48676 MiB total
Total bits: 12995657728.0 Total params: 6476005376
average bits per value: 2.0067397992227978
total time taken: 14588.975994110107
Loading tokenizer for meta-llama/Llama-2-7b-hf
/home/lliu/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Dataset: wikitext2
Evaluating ...
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
Perplexity: 7.456039
