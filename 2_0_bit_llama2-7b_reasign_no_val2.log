/home/lliu/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.08s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.36it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.27it/s]
Model loaded. LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
4096
Loading tokenizer for meta-llama/Llama-2-7b-hf
Traceback (most recent call last):
  File "/data/lliu/huffman/llama.py", line 661, in <module>
    dataloader, valloader, testloader = get_loaders(
                                        ^^^^^^^^^^^^
  File "/data/lliu/huffman/datautils.py", line 119, in get_loaders
    return get_wikitext2(nsamples_train, nsamples_val, seed, seqlen, model, tokenizer)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/lliu/huffman/datautils.py", line 36, in get_wikitext2
    testenc = tokenizer("\n\n".join(testdata['text']), return_tensors='pt')
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lliu/anaconda3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py", line 2602, in __call__
    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lliu/anaconda3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py", line 2708, in _call_one
    return self.encode_plus(
           ^^^^^^^^^^^^^^^^^
  File "/home/lliu/anaconda3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py", line 2781, in encode_plus
    return self._encode_plus(
           ^^^^^^^^^^^^^^^^^^
  File "/home/lliu/anaconda3/lib/python3.11/site-packages/transformers/tokenization_utils.py", line 656, in _encode_plus
    first_ids = get_input_ids(text)
                ^^^^^^^^^^^^^^^^^^^
  File "/home/lliu/anaconda3/lib/python3.11/site-packages/transformers/tokenization_utils.py", line 623, in get_input_ids
    tokens = self.tokenize(text, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lliu/anaconda3/lib/python3.11/site-packages/transformers/models/llama/tokenization_llama.py", line 206, in tokenize
    tokens = super().tokenize(SPIECE_UNDERLINE + text.replace(SPIECE_UNDERLINE, " "), **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lliu/anaconda3/lib/python3.11/site-packages/transformers/tokenization_utils.py", line 554, in tokenize
    tokenized_text.extend(self._tokenize(token))
                          ^^^^^^^^^^^^^^^^^^^^^
  File "/home/lliu/anaconda3/lib/python3.11/site-packages/transformers/models/llama/tokenization_llama.py", line 228, in _tokenize
    tokens = self.sp_model.encode(self.unk_token + text, out_type=str)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lliu/anaconda3/lib/python3.11/site-packages/sentencepiece/__init__.py", line 552, in Encode
    return self._EncodeAsPieces(input, enable_sampling, nbest_size,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lliu/anaconda3/lib/python3.11/site-packages/sentencepiece/__init__.py", line 322, in _EncodeAsPieces
    return _sentencepiece.SentencePieceProcessor__EncodeAsPieces(self, text, enable_sampling, nbest_size, alpha, add_bos, add_eos, reverse, emit_unk_piece)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
