wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: m6481. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /data/lliu/huffman/wandb/run-20250101_144724-t5h75qce
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sage-donkey-55
wandb: ‚≠êÔ∏è View project at https://wandb.ai/m6481/compression_no_finetune
wandb: üöÄ View run at https://wandb.ai/m6481/compression_no_finetune/runs/t5h75qce
Namespace(models_to_compress=['meta-llama/Llama-2-7b-hf', 'meta-llama/Llama-2-13b-hf', 'meta-llama/Llama-2-70b-hf', 'meta-llama/Meta-Llama-3-8B', 'meta-llama/Meta-Llama-3-70B'], seqlens=[4096, 4096, 4096, 8192, 8192], batch_size=1, hessian_path='/data/lliu/huffman/models/{model_name}/hessians_new/pajama/128/', save_path='/data/lliu/huffman/models/{model_name}/compressed', self_attn_compression_algorithm='quantize', mlp_compression_algorithm='quantize', devices=['cuda:5', 'cuda:6', 'cuda:2', 'cuda:3', 'cuda:4', 'cuda:7'], yaml_path='/data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml', self_attn_yaml_path=None, mlp_yaml_path=None, use_already_done=True, use_wandb=True, wandb_project='compression_no_finetune')
  0%|          | 0/1848 [00:00<?, ?it/s]path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_3/mlp.gate_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_3/mlp.gate_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_3/mlp.gate_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_3/mlp.gate_proj/compressed.log
best_loss 49.43342208862305 running bpv: 2.009131
already done with  meta-llama/Llama-2-7b-hf/layer_3/mlp.gate_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_3/mlp.up_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_3/mlp.up_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_3/mlp.up_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_3/mlp.up_proj/compressed.log
best_loss 41.891727447509766 running bpv: 2.009131
already done with  meta-llama/Llama-2-7b-hf/layer_3/mlp.up_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_3/self_attn.q_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_3/self_attn.q_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_3/self_attn.q_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_3/self_attn.q_proj/compressed.log
best_loss 112.32766723632812 running bpv: 2.009997
already done with  meta-llama/Llama-2-7b-hf/layer_3/self_attn.q_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_3/self_attn.k_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_3/self_attn.k_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_3/self_attn.k_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_3/self_attn.k_proj/compressed.log
best_loss 122.52682495117188 running bpv: 2.010628
already done with  meta-llama/Llama-2-7b-hf/layer_3/self_attn.k_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_3/self_attn.o_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_3/self_attn.o_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_3/self_attn.o_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_3/self_attn.o_proj/compressed.log
best_loss 0.24364137649536133 running bpv: 2.011109
already done with  meta-llama/Llama-2-7b-hf/layer_3/self_attn.o_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_3/mlp.down_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_3/mlp.down_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_3/mlp.down_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_3/mlp.down_proj/compressed.log
best_loss 0.8539788126945496 running bpv: 2.010242
already done with  meta-llama/Llama-2-7b-hf/layer_3/mlp.down_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_3/self_attn.v_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_3/self_attn.v_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_3/self_attn.v_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_3/self_attn.v_proj/compressed.log
best_loss 31.879804611206055 running bpv: 2.010608
already done with  meta-llama/Llama-2-7b-hf/layer_3/self_attn.v_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_5/mlp.gate_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_5/mlp.gate_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_5/mlp.gate_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_5/mlp.gate_proj/compressed.log
best_loss 88.11199951171875 running bpv: 2.010339
already done with  meta-llama/Llama-2-7b-hf/layer_5/mlp.gate_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_5/mlp.up_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_5/mlp.up_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_5/mlp.up_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_5/mlp.up_proj/compressed.log
best_loss 69.80905151367188 running bpv: 2.010153
already done with  meta-llama/Llama-2-7b-hf/layer_5/mlp.up_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_5/self_attn.q_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_5/self_attn.q_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_5/self_attn.q_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_5/self_attn.q_proj/compressed.log
best_loss 121.28446960449219 running bpv: 2.010397
already done with  meta-llama/Llama-2-7b-hf/layer_5/self_attn.q_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_5/self_attn.k_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_5/self_attn.k_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_5/self_attn.k_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_5/self_attn.k_proj/compressed.log
best_loss 132.9383544921875 running bpv: 2.010616
already done with  meta-llama/Llama-2-7b-hf/layer_5/self_attn.k_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_5/self_attn.o_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_5/self_attn.o_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_5/self_attn.o_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_5/self_attn.o_proj/compressed.log
best_loss 0.8941723704338074 running bpv: 2.010813
already done with  meta-llama/Llama-2-7b-hf/layer_5/self_attn.o_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_5/mlp.down_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_5/mlp.down_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_5/mlp.down_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_5/mlp.down_proj/compressed.log
best_loss 2.309817314147949 running bpv: 2.010433
already done with  meta-llama/Llama-2-7b-hf/layer_5/mlp.down_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_5/self_attn.v_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_5/self_attn.v_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_5/self_attn.v_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_5/self_attn.v_proj/compressed.log
best_loss 36.62811279296875 running bpv: 2.010608
already done with  meta-llama/Llama-2-7b-hf/layer_5/self_attn.v_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_8/mlp.gate_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_8/mlp.gate_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_8/mlp.gate_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_8/mlp.gate_proj/compressed.log
best_loss 131.2567901611328 running bpv: 2.01046
already done with  meta-llama/Llama-2-7b-hf/layer_8/mlp.gate_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_8/mlp.up_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_8/mlp.up_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_8/mlp.up_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_8/mlp.up_proj/compressed.log
best_loss 106.80435180664062 running bpv: 2.010339
already done with  meta-llama/Llama-2-7b-hf/layer_8/mlp.up_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_8/self_attn.q_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_8/self_attn.q_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_8/self_attn.q_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_8/self_attn.q_proj/compressed.log
best_loss 191.25511169433594 running bpv: 2.01048
already done with  meta-llama/Llama-2-7b-hf/layer_8/self_attn.q_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_8/self_attn.k_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_8/self_attn.k_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_8/self_attn.k_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_8/self_attn.k_proj/compressed.log
best_loss 193.88648986816406 running bpv: 2.010612
already done with  meta-llama/Llama-2-7b-hf/layer_8/self_attn.k_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_8/self_attn.o_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_8/self_attn.o_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_8/self_attn.o_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_8/self_attn.o_proj/compressed.log
best_loss 2.943741798400879 running bpv: 2.010737
already done with  meta-llama/Llama-2-7b-hf/layer_8/self_attn.o_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_8/mlp.down_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_8/mlp.down_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_8/mlp.down_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_8/mlp.down_proj/compressed.log
best_loss 5.640310287475586 running bpv: 2.010493
already done with  meta-llama/Llama-2-7b-hf/layer_8/mlp.down_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_8/self_attn.v_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_8/self_attn.v_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_8/self_attn.v_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_8/self_attn.v_proj/compressed.log
best_loss 59.480464935302734 running bpv: 2.010608
already done with  meta-llama/Llama-2-7b-hf/layer_8/self_attn.v_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_12/mlp.gate_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_12/mlp.gate_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_12/mlp.gate_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_12/mlp.gate_proj/compressed.log
best_loss 162.8937225341797 running bpv: 2.010506
already done with  meta-llama/Llama-2-7b-hf/layer_12/mlp.gate_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_12/mlp.up_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_12/mlp.up_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_12/mlp.up_proj/compressed.pt
reading log   1%|‚ñè         | 26/1848 [00:00<00:07, 256.77it/s]/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_12/mlp.up_proj/compressed.log
best_loss 148.902099609375 running bpv: 2.010417
already done with  meta-llama/Llama-2-7b-hf/layer_12/mlp.up_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_12/self_attn.q_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_12/self_attn.q_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_12/self_attn.q_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_12/self_attn.q_proj/compressed.log
best_loss 240.7495574951172 running bpv: 2.010516
already done with  meta-llama/Llama-2-7b-hf/layer_12/self_attn.q_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_12/self_attn.k_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_12/self_attn.k_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_12/self_attn.k_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_12/self_attn.k_proj/compressed.log
best_loss 257.9990539550781 running bpv: 2.010611
already done with  meta-llama/Llama-2-7b-hf/layer_12/self_attn.k_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_12/self_attn.o_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_12/self_attn.o_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_12/self_attn.o_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_12/self_attn.o_proj/compressed.log
best_loss 6.915116310119629 running bpv: 2.010702
already done with  meta-llama/Llama-2-7b-hf/layer_12/self_attn.o_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_12/mlp.down_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_12/mlp.down_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_12/mlp.down_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_12/mlp.down_proj/compressed.log
best_loss 10.196646690368652 running bpv: 2.010522
already done with  meta-llama/Llama-2-7b-hf/layer_12/mlp.down_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_12/self_attn.v_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_12/self_attn.v_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_12/self_attn.v_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_12/self_attn.v_proj/compressed.log
best_loss 86.48735046386719 running bpv: 2.010608
already done with  meta-llama/Llama-2-7b-hf/layer_12/self_attn.v_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_1/mlp.gate_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_1/mlp.gate_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_1/mlp.gate_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_1/mlp.gate_proj/compressed.log
best_loss 12.91602897644043 running bpv: 2.01053
already done with  meta-llama/Llama-2-7b-hf/layer_1/mlp.gate_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_1/mlp.up_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_1/mlp.up_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_1/mlp.up_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_1/mlp.up_proj/compressed.log
best_loss 11.325397491455078 running bpv: 2.01046
already done with  meta-llama/Llama-2-7b-hf/layer_1/mlp.up_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_1/self_attn.q_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_1/self_attn.q_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_1/self_attn.q_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_1/self_attn.q_proj/compressed.log
best_loss 6.516097068786621 running bpv: 2.010536
already done with  meta-llama/Llama-2-7b-hf/layer_1/self_attn.q_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_1/self_attn.k_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_1/self_attn.k_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_1/self_attn.k_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_1/self_attn.k_proj/compressed.log
best_loss 6.616118431091309 running bpv: 2.01061
already done with  meta-llama/Llama-2-7b-hf/layer_1/self_attn.k_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_1/self_attn.o_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_1/self_attn.o_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_1/self_attn.o_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_1/self_attn.o_proj/compressed.log
best_loss 0.0628652349114418 running bpv: 2.010682
already done with  meta-llama/Llama-2-7b-hf/layer_1/self_attn.o_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_1/mlp.down_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_1/mlp.down_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_1/mlp.down_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_1/mlp.down_proj/compressed.log
best_loss 0.21109314262866974 running bpv: 2.010539
already done with  meta-llama/Llama-2-7b-hf/layer_1/mlp.down_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_1/self_attn.v_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_1/self_attn.v_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_1/self_attn.v_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_1/self_attn.v_proj/compressed.log
best_loss 0.6918058395385742 running bpv: 2.010608
already done with  meta-llama/Llama-2-7b-hf/layer_1/self_attn.v_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_7/mlp.gate_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_7/mlp.gate_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_7/mlp.gate_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_7/mlp.gate_proj/compressed.log
best_loss 126.69721221923828 running bpv: 2.010545
already done with  meta-llama/Llama-2-7b-hf/layer_7/mlp.gate_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_7/mlp.up_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_7/mlp.up_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_7/mlp.up_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_7/mlp.up_proj/compressed.log
best_loss 97.25100708007812 running bpv: 2.010487
already done with  meta-llama/Llama-2-7b-hf/layer_7/mlp.up_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_7/self_attn.q_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_7/self_attn.q_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_7/self_attn.q_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_7/self_attn.q_proj/compressed.log
best_loss 194.13951110839844 running bpv: 2.010549
already done with  meta-llama/Llama-2-7b-hf/layer_7/self_attn.q_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_7/self_attn.k_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_7/self_attn.k_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_7/self_attn.k_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_7/self_attn.k_proj/compressed.log
best_loss 196.28851318359375 running bpv: 2.01061
already done with  meta-llama/Llama-2-7b-hf/layer_7/self_attn.k_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_7/self_attn.o_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_7/self_attn.o_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_7/self_attn.o_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_7/self_attn.o_proj/compressed.log
best_loss 1.8761067390441895 running bpv: 2.010669
already done with  meta-llama/Llama-2-7b-hf/layer_7/self_attn.o_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_7/mlp.down_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_7/mlp.down_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_7/mlp.down_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_7/mlp.down_proj/compressed.log
best_loss 4.567765235900879 running bpv: 2.010551
already done with  meta-llama/Llama-2-7b-hf/layer_7/mlp.down_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_7/self_attn.v_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_7/self_attn.v_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_7/self_attn.v_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_7/self_attn.v_proj/compressed.log
best_loss 58.0523567199707 running bpv: 2.010608
already done with  meta-llama/Llama-2-7b-hf/layer_7/self_attn.v_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_0/mlp.gate_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_0/mlp.gate_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_0/mlp.gate_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_0/mlp.gate_proj/compressed.log
best_loss 2.9977715015411377 running bpv: 2.010555
already done with  meta-llama/Llama-2-7b-hf/layer_0/mlp.gate_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_0/mlp.up_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_0/mlp.up_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_0/mlp.up_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_0/mlp.up_proj/compressed.log
best_loss 2.8641300201416016 running bpv: 2.010506
already done with  meta-llama/Llama-2-7b-hf/layer_0/mlp.up_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_0/self_attn.q_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_0/self_attn.q_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_0/self_attn.q_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_0/self_attn.q_proj/compressed.log
best_loss 0.36603060364723206 running bpv: 2.010558
already done with  meta-llama/Llama-2-7b-hf/layer_0/self_attn.q_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_0/self_attn.k_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_0/self_attn.k_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_0/self_attn.k_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_0/self_attn.k_proj/compressed.log
best_loss 0.2853046655654907 running bpv: 2.01061
already done with  meta-llama/Llama-2-7b-hf/layer_0/self_attn.k_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_0/self_attn.o_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_0/self_attn.o_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_0/self_attn.o_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_0/self_attn.o_proj/compressed.log
best_loss 0.005404493305832148 running bpv: 2.01066
already done with  meta-llama/Llama-2-7b-hf/layer_0/self_attn.o_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_0/mlp.down_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_0/mlp.down_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_0/mlp.down_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_0/mlp.down_proj/compressed.log
best_loss 0.02371348813176155 running bpv: 2.010559
already done with  meta-llama/Llama-2-7b-hf/layer_0/mlp.down_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_0/self_attn.v_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_0/self_attn.v_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_0/self_attn.v_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_0/self_attn.v_proj/compressed.log
best_loss 0.05582483485341072 running bpv: 2.010608
already done with  meta-llama/Llama-2-7b-hf/layer_0/self_attn.v_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_31/mlp.gate_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_31/mlp.gate_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_31/mlp.gate_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_31/mlp.gate_proj/compressed.log
best_loss 585.796630859375 running bpv: 2.010562
already done with  meta-llama/Llama-2-7b-hf/layer_31/mlp.gate_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_31/mlp.up_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_31/mlp.up_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_31/mlp.up_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_31/mlp.up_proj/compressed.log
best_loss 518.9898681640625 running bpv: 2.010519
already done with  meta-llama/Llama-2-7b-hf/layer_31/mlp.up_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_31/self_attn.q_proj/compressed_args.yaml
  3%|‚ñé         | 52/1848 [00:00<00:06, 257.81it/s]yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_31/self_attn.q_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_31/self_attn.q_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_31/self_attn.q_proj/compressed.log
best_loss 270.0703430175781 running bpv: 2.010565
already done with  meta-llama/Llama-2-7b-hf/layer_31/self_attn.q_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_31/self_attn.k_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_31/self_attn.k_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_31/self_attn.k_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_31/self_attn.k_proj/compressed.log
best_loss 292.4201354980469 running bpv: 2.010609
already done with  meta-llama/Llama-2-7b-hf/layer_31/self_attn.k_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_31/self_attn.o_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_31/self_attn.o_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_31/self_attn.o_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_31/self_attn.o_proj/compressed.log
best_loss 42.25667953491211 running bpv: 2.010653
already done with  meta-llama/Llama-2-7b-hf/layer_31/self_attn.o_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_31/mlp.down_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_31/mlp.down_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_31/mlp.down_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_31/mlp.down_proj/compressed.log
best_loss 284.68499755859375 running bpv: 2.010565
already done with  meta-llama/Llama-2-7b-hf/layer_31/mlp.down_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_31/self_attn.v_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_31/self_attn.v_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_31/self_attn.v_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_31/self_attn.v_proj/compressed.log
best_loss 200.59046936035156 running bpv: 2.010608
already done with  meta-llama/Llama-2-7b-hf/layer_31/self_attn.v_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_21/mlp.gate_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_21/mlp.gate_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_21/mlp.gate_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_21/mlp.gate_proj/compressed.log
best_loss 399.2372131347656 running bpv: 2.010568
already done with  meta-llama/Llama-2-7b-hf/layer_21/mlp.gate_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_21/mlp.up_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_21/mlp.up_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_21/mlp.up_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_21/mlp.up_proj/compressed.log
best_loss 341.3741760253906 running bpv: 2.01053
already done with  meta-llama/Llama-2-7b-hf/layer_21/mlp.up_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_21/self_attn.q_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_21/self_attn.q_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_21/self_attn.q_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_21/self_attn.q_proj/compressed.log
best_loss 308.9530334472656 running bpv: 2.01057
already done with  meta-llama/Llama-2-7b-hf/layer_21/self_attn.q_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_21/self_attn.k_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_21/self_attn.k_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_21/self_attn.k_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_21/self_attn.k_proj/compressed.log
best_loss 317.4786071777344 running bpv: 2.010609
already done with  meta-llama/Llama-2-7b-hf/layer_21/self_attn.k_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_21/self_attn.o_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_21/self_attn.o_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_21/self_attn.o_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_21/self_attn.o_proj/compressed.log
best_loss 11.116890907287598 running bpv: 2.010648
already done with  meta-llama/Llama-2-7b-hf/layer_21/self_attn.o_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_21/mlp.down_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_21/mlp.down_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_21/mlp.down_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_21/mlp.down_proj/compressed.log
best_loss 43.497642517089844 running bpv: 2.01057
already done with  meta-llama/Llama-2-7b-hf/layer_21/mlp.down_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_21/self_attn.v_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_21/self_attn.v_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_21/self_attn.v_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_21/self_attn.v_proj/compressed.log
best_loss 194.8931121826172 running bpv: 2.010608
already done with  meta-llama/Llama-2-7b-hf/layer_21/self_attn.v_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_26/mlp.gate_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_26/mlp.gate_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_26/mlp.gate_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_26/mlp.gate_proj/compressed.log
best_loss 537.7118530273438 running bpv: 2.010572
already done with  meta-llama/Llama-2-7b-hf/layer_26/mlp.gate_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_26/mlp.up_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_26/mlp.up_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_26/mlp.up_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_26/mlp.up_proj/compressed.log
best_loss 462.7989807128906 running bpv: 2.010538
already done with  meta-llama/Llama-2-7b-hf/layer_26/mlp.up_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_26/self_attn.q_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_26/self_attn.q_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_26/self_attn.q_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_26/self_attn.q_proj/compressed.log
best_loss 363.3192138671875 running bpv: 2.010574
already done with  meta-llama/Llama-2-7b-hf/layer_26/self_attn.q_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_26/self_attn.k_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_26/self_attn.k_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_26/self_attn.k_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_26/self_attn.k_proj/compressed.log
best_loss 371.9248962402344 running bpv: 2.010609
already done with  meta-llama/Llama-2-7b-hf/layer_26/self_attn.k_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_26/self_attn.o_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_26/self_attn.o_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_26/self_attn.o_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_26/self_attn.o_proj/compressed.log
best_loss 21.499774932861328 running bpv: 2.010644
already done with  meta-llama/Llama-2-7b-hf/layer_26/self_attn.o_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_26/mlp.down_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_26/mlp.down_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_26/mlp.down_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_26/mlp.down_proj/compressed.log
best_loss 64.91046905517578 running bpv: 2.010574
already done with  meta-llama/Llama-2-7b-hf/layer_26/mlp.down_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_26/self_attn.v_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_26/self_attn.v_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_26/self_attn.v_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_26/self_attn.v_proj/compressed.log
best_loss 289.975830078125 running bpv: 2.010608
already done with  meta-llama/Llama-2-7b-hf/layer_26/self_attn.v_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_14/mlp.gate_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_14/mlp.gate_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_14/mlp.gate_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_14/mlp.gate_proj/compressed.log
best_loss 188.02056884765625 running bpv: 2.010575
already done with  meta-llama/Llama-2-7b-hf/layer_14/mlp.gate_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_14/mlp.up_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_14/mlp.up_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_14/mlp.up_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_14/mlp.up_proj/compressed.log
best_loss 177.93357849121094 running bpv: 2.010545
already done with  meta-llama/Llama-2-7b-hf/layer_14/mlp.up_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_14/self_attn.q_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_14/self_attn.q_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_14/self_attn.q_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_14/self_attn.q_proj/compressed.log
best_loss 257.5028076171875 running bpv: 2.010577
already done with  meta-llama/Llama-2-7b-hf/layer_14/self_attn.q_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_14/self_attn.k_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_14/self_attn.k_proj
loading from   4%|‚ñç         | 78/1848 [00:00<00:06, 258.69it/s]/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_14/self_attn.k_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_14/self_attn.k_proj/compressed.log
best_loss 269.93011474609375 running bpv: 2.010609
already done with  meta-llama/Llama-2-7b-hf/layer_14/self_attn.k_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_14/self_attn.o_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_14/self_attn.o_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_14/self_attn.o_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/gallant-elevator-52/meta-llama/Llama-2-7b-hf/layer_14/self_attn.o_proj/compressed.log
best_loss 9.668866157531738 running bpv: 2.01064
already done with  meta-llama/Llama-2-7b-hf/layer_14/self_attn.o_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_14/mlp.down_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_14/mlp.down_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_14/mlp.down_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_14/mlp.down_proj/compressed.log
best_loss 14.621866226196289 running bpv: 2.010577
already done with  meta-llama/Llama-2-7b-hf/layer_14/mlp.down_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_14/self_attn.v_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_14/self_attn.v_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_14/self_attn.v_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_14/self_attn.v_proj/compressed.log
best_loss 97.36930847167969 running bpv: 2.010608
already done with  meta-llama/Llama-2-7b-hf/layer_14/self_attn.v_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_15/mlp.gate_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_15/mlp.gate_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_15/mlp.gate_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_15/mlp.gate_proj/compressed.log
best_loss 207.4346160888672 running bpv: 2.010578
already done with  meta-llama/Llama-2-7b-hf/layer_15/mlp.gate_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_15/mlp.up_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_15/mlp.up_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_15/mlp.up_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_15/mlp.up_proj/compressed.log
best_loss 196.49502563476562 running bpv: 2.01055
already done with  meta-llama/Llama-2-7b-hf/layer_15/mlp.up_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_15/self_attn.q_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_15/self_attn.q_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_15/self_attn.q_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_15/self_attn.q_proj/compressed.log
best_loss 240.59933471679688 running bpv: 2.01058
already done with  meta-llama/Llama-2-7b-hf/layer_15/self_attn.q_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_15/self_attn.k_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_15/self_attn.k_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_15/self_attn.k_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_15/self_attn.k_proj/compressed.log
best_loss 260.69268798828125 running bpv: 2.010609
already done with  meta-llama/Llama-2-7b-hf/layer_15/self_attn.k_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_15/self_attn.o_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_15/self_attn.o_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_15/self_attn.o_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_15/self_attn.o_proj/compressed.log
best_loss 10.165802001953125 running bpv: 2.010637
already done with  meta-llama/Llama-2-7b-hf/layer_15/self_attn.o_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_15/mlp.down_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_15/mlp.down_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_15/mlp.down_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_15/mlp.down_proj/compressed.log
best_loss 18.33635139465332 running bpv: 2.010579
already done with  meta-llama/Llama-2-7b-hf/layer_15/mlp.down_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_15/self_attn.v_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_15/self_attn.v_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_15/self_attn.v_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_15/self_attn.v_proj/compressed.log
best_loss 101.61927795410156 running bpv: 2.010608
already done with  meta-llama/Llama-2-7b-hf/layer_15/self_attn.v_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_4/mlp.gate_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_4/mlp.gate_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_4/mlp.gate_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_4/mlp.gate_proj/compressed.log
best_loss 69.46524047851562 running bpv: 2.010581
already done with  meta-llama/Llama-2-7b-hf/layer_4/mlp.gate_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_4/mlp.up_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_4/mlp.up_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_4/mlp.up_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_4/mlp.up_proj/compressed.log
best_loss 55.66865158081055 running bpv: 2.010555
already done with  meta-llama/Llama-2-7b-hf/layer_4/mlp.up_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_4/self_attn.q_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_4/self_attn.q_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_4/self_attn.q_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_4/self_attn.q_proj/compressed.log
best_loss 107.25277709960938 running bpv: 2.010582
already done with  meta-llama/Llama-2-7b-hf/layer_4/self_attn.q_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_4/self_attn.k_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_4/self_attn.k_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_4/self_attn.k_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_4/self_attn.k_proj/compressed.log
best_loss 112.94963836669922 running bpv: 2.010609
already done with  meta-llama/Llama-2-7b-hf/layer_4/self_attn.k_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_4/self_attn.o_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_4/self_attn.o_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_4/self_attn.o_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_4/self_attn.o_proj/compressed.log
best_loss 0.536568284034729 running bpv: 2.010635
already done with  meta-llama/Llama-2-7b-hf/layer_4/self_attn.o_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_4/mlp.down_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_4/mlp.down_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_4/mlp.down_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_4/mlp.down_proj/compressed.log
best_loss 1.5733578205108643 running bpv: 2.010582
already done with  meta-llama/Llama-2-7b-hf/layer_4/mlp.down_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_4/self_attn.v_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_4/self_attn.v_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_4/self_attn.v_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_4/self_attn.v_proj/compressed.log
best_loss 31.05900764465332 running bpv: 2.010608
already done with  meta-llama/Llama-2-7b-hf/layer_4/self_attn.v_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_27/mlp.gate_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_27/mlp.gate_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_27/mlp.gate_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_27/mlp.gate_proj/compressed.log
best_loss 572.5789794921875 running bpv: 2.010583
already done with  meta-llama/Llama-2-7b-hf/layer_27/mlp.gate_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_27/mlp.up_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_27/mlp.up_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_27/mlp.up_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_27/mlp.up_proj/compressed.log
best_loss 496.35308837890625 running bpv: 2.010559
already done with  meta-llama/Llama-2-7b-hf/layer_27/mlp.up_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_27/self_attn.q_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_27/self_attn.q_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_27/self_attn.q_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_27/self_attn.q_proj/compressed.log
best_loss 403.30023193359375 running bpv: 2.010584
already done with  meta-llama/Llama-2-7b-hf/layer_27/self_attn.q_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_27/self_attn.k_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_27/self_attn.k_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_27/self_attn.k_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_27/self_attn.k_proj/compressed.log
best_loss 413.33197021484375 running bpv: 2.010609
already done with  meta-llama/Llama-2-7b-hf/layer_27/self_attn.k_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_27/self_attn.o_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_27/self_attn.o_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_27/self_attn.o_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_27/self_attn.o_proj/compressed.log
best_loss 19.273479461669922 running bpv: 2.010633
already done with  meta-llama/Llama-2-7b-hf/layer_27/self_attn.o_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_27/mlp.down_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_27/mlp.down_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_27/mlp.down_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_27/mlp.down_proj/compressed.log
best_loss 74.24616241455078 running bpv: 2.010584
already done with  meta-llama/Llama-2-7b-hf/layer_27/mlp.down_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_27/self_attn.v_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_27/self_attn.v_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_27/self_attn.v_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_27/self_attn.v_proj/compressed.log
best_loss 302.3028869628906 running bpv: 2.010608
already done with  meta-llama/Llama-2-7b-hf/layer_27/self_attn.v_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_2/mlp.gate_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_2/mlp.gate_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_2/mlp.gate_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_2/mlp.gate_proj/compressed.log
best_loss 29.31281852722168 running bpv: 2.010584
already done with  meta-llama/Llama-2-7b-hf/layer_2/mlp.gate_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_2/mlp.up_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_2/mlp.up_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_2/mlp.up_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_2/mlp.up_proj/compressed.log
best_loss 25.18877601623535 running bpv: 2.010562
already done with  meta-llama/Llama-2-7b-hf/layer_2/mlp.up_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_2/self_attn.q_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_2/self_attn.q_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_2/self_attn.q_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_2/self_attn.q_proj/compressed.log
best_loss 41.967960357666016 running bpv: 2.010585
already done with  meta-llama/Llama-2-7b-hf/layer_2/self_attn.q_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_2/self_attn.k_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_2/self_attn.k_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_2/self_attn.k_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_2/self_attn.k_proj/compressed.log
best_loss 46.972328186035156 running bpv: 2.010608
already done with  meta-llama/Llama-2-7b-hf/layer_2/self_attn.k_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_2/self_attn.o_proj/compressed_args.yaml
yaml_args   6%|‚ñå         | 104/1848 [00:00<00:06, 253.75it/s]{'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_2/self_attn.o_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_2/self_attn.o_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_2/self_attn.o_proj/compressed.log
best_loss 0.14825434982776642 running bpv: 2.010631
already done with  meta-llama/Llama-2-7b-hf/layer_2/self_attn.o_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/fragrant-frost-54/meta-llama/Llama-2-7b-hf/layer_2/mlp.down_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_2/mlp.down_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/fragrant-frost-54/meta-llama/Llama-2-7b-hf/layer_2/mlp.down_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/fragrant-frost-54/meta-llama/Llama-2-7b-hf/layer_2/mlp.down_proj/compressed.log
best_loss 0.42012080550193787 running bpv: 2.010585
already done with  meta-llama/Llama-2-7b-hf/layer_2/mlp.down_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_2/self_attn.v_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_2/self_attn.v_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_2/self_attn.v_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_2/self_attn.v_proj/compressed.log
best_loss 11.816667556762695 running bpv: 2.010608
already done with  meta-llama/Llama-2-7b-hf/layer_2/self_attn.v_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/fragrant-frost-54/meta-llama/Llama-2-7b-hf/layer_28/mlp.gate_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_28/mlp.gate_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/fragrant-frost-54/meta-llama/Llama-2-7b-hf/layer_28/mlp.gate_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/fragrant-frost-54/meta-llama/Llama-2-7b-hf/layer_28/mlp.gate_proj/compressed.log
best_loss 595.8999633789062 running bpv: 2.010586
already done with  meta-llama/Llama-2-7b-hf/layer_28/mlp.gate_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/fragrant-frost-54/meta-llama/Llama-2-7b-hf/layer_28/mlp.up_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_28/mlp.up_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/fragrant-frost-54/meta-llama/Llama-2-7b-hf/layer_28/mlp.up_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/fragrant-frost-54/meta-llama/Llama-2-7b-hf/layer_28/mlp.up_proj/compressed.log
best_loss 532.6217041015625 running bpv: 2.010565
already done with  meta-llama/Llama-2-7b-hf/layer_28/mlp.up_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_28/self_attn.q_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_28/self_attn.q_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_28/self_attn.q_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/glorious-tree-53/meta-llama/Llama-2-7b-hf/layer_28/self_attn.q_proj/compressed.log
best_loss 397.3584289550781 running bpv: 2.010587
already done with  meta-llama/Llama-2-7b-hf/layer_28/self_attn.q_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/fragrant-frost-54/meta-llama/Llama-2-7b-hf/layer_28/self_attn.k_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_28/self_attn.k_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/fragrant-frost-54/meta-llama/Llama-2-7b-hf/layer_28/self_attn.k_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/fragrant-frost-54/meta-llama/Llama-2-7b-hf/layer_28/self_attn.k_proj/compressed.log
best_loss 407.24334716796875 running bpv: 2.010608
already done with  meta-llama/Llama-2-7b-hf/layer_28/self_attn.k_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/fragrant-frost-54/meta-llama/Llama-2-7b-hf/layer_28/self_attn.o_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_28/self_attn.o_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/fragrant-frost-54/meta-llama/Llama-2-7b-hf/layer_28/self_attn.o_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/fragrant-frost-54/meta-llama/Llama-2-7b-hf/layer_28/self_attn.o_proj/compressed.log
best_loss 26.552471160888672 running bpv: 2.01063
already done with  meta-llama/Llama-2-7b-hf/layer_28/self_attn.o_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/fragrant-frost-54/meta-llama/Llama-2-7b-hf/layer_28/mlp.down_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_28/mlp.down_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/fragrant-frost-54/meta-llama/Llama-2-7b-hf/layer_28/mlp.down_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/fragrant-frost-54/meta-llama/Llama-2-7b-hf/layer_28/mlp.down_proj/compressed.log
best_loss 90.18383026123047 running bpv: 2.010587
already done with  meta-llama/Llama-2-7b-hf/layer_28/mlp.down_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/fragrant-frost-54/meta-llama/Llama-2-7b-hf/layer_28/self_attn.v_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_28/self_attn.v_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/fragrant-frost-54/meta-llama/Llama-2-7b-hf/layer_28/self_attn.v_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/fragrant-frost-54/meta-llama/Llama-2-7b-hf/layer_28/self_attn.v_proj/compressed.log
best_loss 334.60162353515625 running bpv: 2.010608
already done with  meta-llama/Llama-2-7b-hf/layer_28/self_attn.v_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/fragrant-frost-54/meta-llama/Llama-2-7b-hf/layer_18/mlp.gate_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_18/mlp.gate_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/fragrant-frost-54/meta-llama/Llama-2-7b-hf/layer_18/mlp.gate_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/fragrant-frost-54/meta-llama/Llama-2-7b-hf/layer_18/mlp.gate_proj/compressed.log
best_loss 316.8416442871094 running bpv: 2.010587
already done with  meta-llama/Llama-2-7b-hf/layer_18/mlp.gate_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/fragrant-frost-54/meta-llama/Llama-2-7b-hf/layer_18/mlp.up_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_18/mlp.up_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/fragrant-frost-54/meta-llama/Llama-2-7b-hf/layer_18/mlp.up_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/fragrant-frost-54/meta-llama/Llama-2-7b-hf/layer_18/mlp.up_proj/compressed.log
best_loss 280.7430114746094 running bpv: 2.010568
already done with  meta-llama/Llama-2-7b-hf/layer_18/mlp.up_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/fragrant-frost-54/meta-llama/Llama-2-7b-hf/layer_18/self_attn.q_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_18/self_attn.q_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/fragrant-frost-54/meta-llama/Llama-2-7b-hf/layer_18/self_attn.q_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/fragrant-frost-54/meta-llama/Llama-2-7b-hf/layer_18/self_attn.q_proj/compressed.log
best_loss 286.10357666015625 running bpv: 2.010588
already done with  meta-llama/Llama-2-7b-hf/layer_18/self_attn.q_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/fragrant-frost-54/meta-llama/Llama-2-7b-hf/layer_18/self_attn.k_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_18/self_attn.k_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/fragrant-frost-54/meta-llama/Llama-2-7b-hf/layer_18/self_attn.k_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/fragrant-frost-54/meta-llama/Llama-2-7b-hf/layer_18/self_attn.k_proj/compressed.log
best_loss 301.54583740234375 running bpv: 2.010608
already done with  meta-llama/Llama-2-7b-hf/layer_18/self_attn.k_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/fragrant-frost-54/meta-llama/Llama-2-7b-hf/layer_18/self_attn.o_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_18/self_attn.o_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/fragrant-frost-54/meta-llama/Llama-2-7b-hf/layer_18/self_attn.o_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/fragrant-frost-54/meta-llama/Llama-2-7b-hf/layer_18/self_attn.o_proj/compressed.log
best_loss 10.608203887939453 running bpv: 2.010628
already done with  meta-llama/Llama-2-7b-hf/layer_18/self_attn.o_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/fragrant-frost-54/meta-llama/Llama-2-7b-hf/layer_18/mlp.down_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_18/mlp.down_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/fragrant-frost-54/meta-llama/Llama-2-7b-hf/layer_18/mlp.down_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/fragrant-frost-54/meta-llama/Llama-2-7b-hf/layer_18/mlp.down_proj/compressed.log
best_loss 31.79154396057129 running bpv: 2.010588
already done with  meta-llama/Llama-2-7b-hf/layer_18/mlp.down_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/fragrant-frost-54/meta-llama/Llama-2-7b-hf/layer_18/self_attn.v_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_18/self_attn.v_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/fragrant-frost-54/meta-llama/Llama-2-7b-hf/layer_18/self_attn.v_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/fragrant-frost-54/meta-llama/Llama-2-7b-hf/layer_18/self_attn.v_proj/compressed.log
best_loss 154.01303100585938 running bpv: 2.010608
already done with  meta-llama/Llama-2-7b-hf/layer_18/self_attn.v_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/fragrant-frost-54/meta-llama/Llama-2-7b-hf/layer_25/mlp.gate_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_25/mlp.gate_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/fragrant-frost-54/meta-llama/Llama-2-7b-hf/layer_25/mlp.gate_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/fragrant-frost-54/meta-llama/Llama-2-7b-hf/layer_25/mlp.gate_proj/compressed.log
best_loss 506.6246032714844 running bpv: 2.010589
already done with  meta-llama/Llama-2-7b-hf/layer_25/mlp.gate_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/fragrant-frost-54/meta-llama/Llama-2-7b-hf/layer_25/mlp.up_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_25/mlp.up_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/fragrant-frost-54/meta-llama/Llama-2-7b-hf/layer_25/mlp.up_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/fragrant-frost-54/meta-llama/Llama-2-7b-hf/layer_25/mlp.up_proj/compressed.log
best_loss 434.90582275390625 running bpv: 2.01057
already done with  meta-llama/Llama-2-7b-hf/layer_25/mlp.up_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/fragrant-frost-54/meta-llama/Llama-2-7b-hf/layer_25/self_attn.q_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_25/self_attn.q_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/fragrant-frost-54/meta-llama/Llama-2-7b-hf/layer_25/self_attn.q_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/fragrant-frost-54/meta-llama/Llama-2-7b-hf/layer_25/self_attn.q_proj/compressed.log
best_loss 391.51904296875 running bpv: 2.010589
already done with  meta-llama/Llama-2-7b-hf/layer_25/self_attn.q_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/fragrant-frost-54/meta-llama/Llama-2-7b-hf/layer_25/self_attn.k_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_25/self_attn.k_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/fragrant-frost-54/meta-llama/Llama-2-7b-hf/layer_25/self_attn.k_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/fragrant-frost-54/meta-llama/Llama-2-7b-hf/layer_25/self_attn.k_proj/compressed.log
best_loss 395.7146911621094 running bpv: 2.010608
already done with  meta-llama/Llama-2-7b-hf/layer_25/self_attn.k_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/fragrant-frost-54/meta-llama/Llama-2-7b-hf/layer_25/self_attn.o_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_25/self_attn.o_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/fragrant-frost-54/meta-llama/Llama-2-7b-hf/layer_25/self_attn.o_proj/compressed.pt
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/fragrant-frost-54/meta-llama/Llama-2-7b-hf/layer_25/self_attn.o_proj/compressed.log
best_loss 13.131237983703613 running bpv: 2.010627
already done with  meta-llama/Llama-2-7b-hf/layer_25/self_attn.o_proj
path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/fragrant-frost-54/meta-llama/Llama-2-7b-hf/layer_25/self_attn.v_proj/compressed_args.yaml
yaml_args {'quantizer_kwargs': {'d': 5, 'n_bits': 2, 'cluster_ignore_norms': True, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'seed': 0}
other_args {'alignment_kwargs': {'clip_grad': 0.1, 'discrete_update_every': -1, 'low_bound': 1e-05, 'lr': 0.001, 'lr_multiplier': 0.3333, 'n_iters': 100, 'patience': 100, 'patience_scheduler': 10000, 'verbose': 10}, 'compression_type': 'quantized', 'quantizer_kwargs': {'cluster_ignore_norms': True, 'd': 5, 'n_bits': 2, 'n_iters': 100, 'norm_order': [0, 1], 'zero': [False, False]}, 'seed': 0}
is_same True
already done with  meta-llama/Llama-2-7b-hf/layer_25/self_attn.v_proj
loading from /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/fragrant-frost-54/meta-llama/Llama-2-7b-hf/layer_25/self_attn.v_proj/compressed.pt
reading log   7%|‚ñã         | 125/1848 [00:19<00:06, 253.75it/s]  7%|‚ñã         | 126/1848 [01:50<46:31,  1.62s/it]   7%|‚ñã         | 129/1848 [03:15<1:31:46,  3.20s/it]/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/fragrant-frost-54/meta-llama/Llama-2-7b-hf/layer_25/self_attn.v_proj/compressed.log
best_loss 294.0787353515625 running bpv: 2.010646
already done with  meta-llama/Llama-2-7b-hf/layer_25/self_attn.v_proj
n_commands 1723
sample command python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_25/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_25/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_25/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_25/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_25/mlp.down_proj/compressed.log 2>&1 &
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_30/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_30/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_30/mlp.gate_proj/compressed.log 2>&1 &
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_30/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_30/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:2 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_30/mlp.up_proj/compressed.log 2>&1 &
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_30/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_30/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:3 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_30/self_attn.q_proj/compressed.log 2>&1 &
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_30/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_30/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_30/self_attn.k_proj/compressed.log 2>&1 &
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_30/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_30/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_30/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_30/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_30/self_attn.q_proj/compressed.log
best_loss 381.9583740234375 running bpv: 2.010665
COMMANDS_FINISHED 1 n_commands 1723
meta-llama/Llama-2-7b-hf/layer_30/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_30/self_attn.k_proj/compressed.log
best_loss 393.1234436035156 running bpv: 2.010683
COMMANDS_FINISHED 2 n_commands 1723
meta-llama/Llama-2-7b-hf/layer_30/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_30/self_attn.o_proj/compressed.log
best_loss 33.642051696777344 running bpv: 2.010701
COMMANDS_FINISHED 3 n_commands 1723
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_30/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_30/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:3 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_30/mlp.down_proj/compressed.log 2>&1 &
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_30/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_30/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_30/self_attn.v_proj/compressed.log 2>&1 &
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_29/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_29/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_29/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_25/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_25/mlp.down_proj/compressed.log
best_loss 58.50953674316406 running bpv: 2.010663
COMMANDS_FINISHED 4 n_commands 1723
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_29/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_29/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_29/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_30/mlp.gate_proj is done
reading log   7%|‚ñã         | 130/1848 [03:30<1:40:58,  3.53s/it]  7%|‚ñã         | 133/1848 [05:05<3:05:24,  6.49s/it]  7%|‚ñã         | 136/1848 [05:30<3:11:59,  6.73s/it]  7%|‚ñã         | 138/1848 [06:40<4:40:54,  9.86s/it]/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_30/mlp.gate_proj/compressed.log
best_loss 654.3162841796875 running bpv: 2.010644
COMMANDS_FINISHED 5 n_commands 1723
meta-llama/Llama-2-7b-hf/layer_30/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_30/mlp.up_proj/compressed.log
best_loss 585.7929077148438 running bpv: 2.010626
COMMANDS_FINISHED 6 n_commands 1723
meta-llama/Llama-2-7b-hf/layer_30/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_30/self_attn.v_proj/compressed.log
best_loss 352.8522033691406 running bpv: 2.010644
COMMANDS_FINISHED 7 n_commands 1723
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_29/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_29/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_29/self_attn.q_proj/compressed.log 2>&1 &
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_29/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_29/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:2 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_29/self_attn.k_proj/compressed.log 2>&1 &
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_29/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_29/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_29/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_29/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_29/self_attn.q_proj/compressed.log
best_loss 351.9922790527344 running bpv: 2.010662
COMMANDS_FINISHED 8 n_commands 1723
meta-llama/Llama-2-7b-hf/layer_29/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_29/self_attn.k_proj/compressed.log
best_loss 361.2091369628906 running bpv: 2.010679
COMMANDS_FINISHED 9 n_commands 1723
meta-llama/Llama-2-7b-hf/layer_30/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_30/mlp.down_proj/compressed.log
best_loss 151.99533081054688 running bpv: 2.010643
COMMANDS_FINISHED 10 n_commands 1723
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_29/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_29/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_29/mlp.down_proj/compressed.log 2>&1 &
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_29/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_29/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:2 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_29/self_attn.v_proj/compressed.log 2>&1 &
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_16/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_16/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:3 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_16/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_29/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_29/self_attn.o_proj/compressed.log
best_loss 24.44049072265625 running bpv: 2.01066
COMMANDS_FINISHED 11 n_commands 1723
meta-llama/Llama-2-7b-hf/layer_29/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_29/mlp.gate_proj/compressed.log
best_loss 621.0830078125 running bpv: 2.010642
COMMANDS_FINISHED 12 n_commands 1723
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_16/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_16/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_16/mlp.up_proj/compressed.log 2>&1 &
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_16/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_16/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_16/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_29/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_29/mlp.up_proj/compressed.log
best_loss 561.5757446289062 running bpv: 2.010625
COMMANDS_FINISHED 13 n_commands 1723
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_16/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_16/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_16/self_attn.k_proj/compressed.log 2>&1 &
  8%|‚ñä         | 139/1848 [06:55<4:51:29, 10.23s/it]  8%|‚ñä         | 140/1848 [07:10<5:04:10, 10.69s/it]  8%|‚ñä         | 141/1848 [08:25<8:42:06, 18.35s/it]  8%|‚ñä         | 142/1848 [08:40<8:27:57, 17.87s/it]  8%|‚ñä         | 144/1848 [09:00<7:21:49, 15.56s/it]  8%|‚ñä         | 145/1848 [09:15<7:18:50, 15.46s/it]  8%|‚ñä         | 146/1848 [10:00<10:04:57, 21.33s/it]  8%|‚ñä         | 147/1848 [10:45<12:32:57, 26.56s/it]meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_29/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_29/self_attn.v_proj/compressed.log
best_loss 313.60357666015625 running bpv: 2.010642
COMMANDS_FINISHED 14 n_commands 1723
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_16/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_16/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:2 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_16/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_16/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_16/self_attn.q_proj/compressed.log
best_loss 253.4866943359375 running bpv: 2.010659
COMMANDS_FINISHED 15 n_commands 1723
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_16/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_16/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_16/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_16/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_16/self_attn.k_proj/compressed.log
best_loss 270.81134033203125 running bpv: 2.010676
COMMANDS_FINISHED 16 n_commands 1723
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_16/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_16/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_16/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_29/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_29/mlp.down_proj/compressed.log
best_loss 109.30132293701172 running bpv: 2.010641
COMMANDS_FINISHED 17 n_commands 1723
meta-llama/Llama-2-7b-hf/layer_16/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_16/self_attn.o_proj/compressed.log
best_loss 13.430116653442383 running bpv: 2.010657
COMMANDS_FINISHED 18 n_commands 1723
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_13/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_13/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_13/mlp.gate_proj/compressed.log 2>&1 &
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_13/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_13/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:2 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_13/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_16/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_16/mlp.gate_proj/compressed.log
best_loss 241.6373291015625 running bpv: 2.010641
COMMANDS_FINISHED 19 n_commands 1723
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_13/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_13/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:3 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_13/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_16/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_16/mlp.up_proj/compressed.log
best_loss 225.00425720214844 running bpv: 2.010624
COMMANDS_FINISHED 20 n_commands 1723
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_13/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_13/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_13/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_16/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_16/self_attn.v_proj/compressed.log
best_loss 117.02839660644531 running bpv: 2.010641
COMMANDS_FINISHED 21 n_commands 1723
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_13/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_13/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_13/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_13/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_13/self_attn.q_proj/compressed.log
best_loss 246.3892822265625 running bpv: 2.010656
COMMANDS_FINISHED 22 n_commands 1723
meta-llama/Llama-2-7b-hf/layer_16/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_16/mlp.down_proj/compressed.log
best_loss 24.68352699279785 running bpv: 2.010624
COMMANDS_FINISHED 23 n_commands 1723
  8%|‚ñä         | 149/1848 [11:05<9:30:41, 20.15s/it]   8%|‚ñä         | 150/1848 [11:50<12:02:40, 25.54s/it]  8%|‚ñä         | 151/1848 [12:25<13:05:33, 27.77s/it]  8%|‚ñä         | 152/1848 [12:40<11:33:59, 24.55s/it]  8%|‚ñä         | 154/1848 [14:00<14:35:59, 31.03s/it]  8%|‚ñä         | 155/1848 [14:15<12:51:21, 27.34s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_13/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_13/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:3 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_13/mlp.down_proj/compressed.log 2>&1 &
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_13/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_13/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_13/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_13/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_13/self_attn.k_proj/compressed.log
best_loss 256.6785888671875 running bpv: 2.010639
COMMANDS_FINISHED 24 n_commands 1723
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_10/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_10/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_10/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_13/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_13/self_attn.o_proj/compressed.log
best_loss 7.756601333618164 running bpv: 2.010655
COMMANDS_FINISHED 25 n_commands 1723
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_10/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_10/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_10/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_13/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_13/mlp.gate_proj/compressed.log
best_loss 173.1314697265625 running bpv: 2.010639
COMMANDS_FINISHED 26 n_commands 1723
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_10/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_10/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_10/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_13/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_13/mlp.up_proj/compressed.log
best_loss 162.82406616210938 running bpv: 2.010624
COMMANDS_FINISHED 27 n_commands 1723
meta-llama/Llama-2-7b-hf/layer_13/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_13/self_attn.v_proj/compressed.log
best_loss 96.61712646484375 running bpv: 2.010639
COMMANDS_FINISHED 28 n_commands 1723
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_10/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_10/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:2 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_10/self_attn.k_proj/compressed.log 2>&1 &
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_10/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_10/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_10/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_10/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_10/self_attn.q_proj/compressed.log
best_loss 206.27691650390625 running bpv: 2.010654
COMMANDS_FINISHED 29 n_commands 1723
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_10/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_10/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_10/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_10/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_10/self_attn.k_proj/compressed.log
best_loss 220.53269958496094 running bpv: 2.010669
COMMANDS_FINISHED 30 n_commands 1723
meta-llama/Llama-2-7b-hf/layer_13/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_13/mlp.down_proj/compressed.log
best_loss 12.640562057495117 running bpv: 2.010638
COMMANDS_FINISHED 31 n_commands 1723
meta-llama/Llama-2-7b-hf/layer_10/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_10/self_attn.o_proj/compressed.log
best_loss 6.174777507781982 running bpv: 2.010653
COMMANDS_FINISHED 32 n_commands 1723
  9%|‚ñä         | 158/1848 [14:40<8:24:10, 17.90s/it]   9%|‚ñä         | 159/1848 [15:15<9:55:59, 21.17s/it]  9%|‚ñä         | 160/1848 [15:50<11:19:10, 24.14s/it]  9%|‚ñä         | 161/1848 [16:15<11:24:28, 24.34s/it]  9%|‚ñâ         | 162/1848 [16:50<12:39:17, 27.02s/it]  9%|‚ñâ         | 163/1848 [17:15<12:23:54, 26.49s/it]  9%|‚ñâ         | 164/1848 [17:30<10:55:11, 23.34s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_10/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_10/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:2 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_10/self_attn.v_proj/compressed.log 2>&1 &
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_19/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_19/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:3 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_19/mlp.gate_proj/compressed.log 2>&1 &
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_19/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_19/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_19/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_10/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_10/mlp.gate_proj/compressed.log
best_loss 144.21078491210938 running bpv: 2.010638
COMMANDS_FINISHED 33 n_commands 1723
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_19/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_19/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_19/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_10/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_10/mlp.up_proj/compressed.log
best_loss 124.62789916992188 running bpv: 2.010623
COMMANDS_FINISHED 34 n_commands 1723
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_19/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_19/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_19/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_10/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_10/self_attn.v_proj/compressed.log
best_loss 65.67193603515625 running bpv: 2.010638
COMMANDS_FINISHED 35 n_commands 1723
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_19/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_19/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:2 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_19/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_19/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_19/self_attn.q_proj/compressed.log
best_loss 279.0536804199219 running bpv: 2.010652
COMMANDS_FINISHED 36 n_commands 1723
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_19/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_19/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_19/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_19/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_19/self_attn.k_proj/compressed.log
best_loss 292.7369384765625 running bpv: 2.010667
COMMANDS_FINISHED 37 n_commands 1723
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_19/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_19/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_19/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_10/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_10/mlp.down_proj/compressed.log
best_loss 7.830939292907715 running bpv: 2.010637
COMMANDS_FINISHED 38 n_commands 1723
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_6/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_6/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_6/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_19/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_19/self_attn.o_proj/compressed.log
best_loss 10.021682739257812 running bpv: 2.010651
COMMANDS_FINISHED 39 n_commands 1723
  9%|‚ñâ         | 165/1848 [17:45<9:49:02, 21.00s/it]   9%|‚ñâ         | 167/1848 [18:25<9:35:57, 20.56s/it]  9%|‚ñâ         | 168/1848 [19:20<13:28:29, 28.87s/it]  9%|‚ñâ         | 169/1848 [19:35<11:48:29, 25.32s/it]  9%|‚ñâ         | 170/1848 [19:50<10:30:44, 22.55s/it]  9%|‚ñâ         | 171/1848 [20:05<9:31:56, 20.46s/it]   9%|‚ñâ         | 172/1848 [20:50<12:46:00, 27.42s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_6/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_6/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:2 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_6/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_19/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_19/mlp.gate_proj/compressed.log
best_loss 341.4483947753906 running bpv: 2.010636
COMMANDS_FINISHED 40 n_commands 1723
meta-llama/Llama-2-7b-hf/layer_19/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_19/mlp.up_proj/compressed.log
best_loss 300.86920166015625 running bpv: 2.010622
COMMANDS_FINISHED 41 n_commands 1723
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_6/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_6/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:3 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_6/self_attn.q_proj/compressed.log 2>&1 &
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_6/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_6/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_6/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_19/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_19/self_attn.v_proj/compressed.log
best_loss 156.53829956054688 running bpv: 2.010636
COMMANDS_FINISHED 42 n_commands 1723
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_6/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_6/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_6/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_6/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_6/self_attn.q_proj/compressed.log
best_loss 178.04071044921875 running bpv: 2.01065
COMMANDS_FINISHED 43 n_commands 1723
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_6/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_6/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:3 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_6/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_6/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_6/self_attn.k_proj/compressed.log
best_loss 185.38763427734375 running bpv: 2.010664
COMMANDS_FINISHED 44 n_commands 1723
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_6/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_6/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_6/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_19/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_19/mlp.down_proj/compressed.log
best_loss 35.231529235839844 running bpv: 2.010635
COMMANDS_FINISHED 45 n_commands 1723
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_11/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_11/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_11/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_6/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_6/self_attn.o_proj/compressed.log
best_loss 1.2577875852584839 running bpv: 2.010649
COMMANDS_FINISHED 46 n_commands 1723
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_11/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_11/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_11/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_6/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_6/mlp.gate_proj/compressed.log
best_loss 110.373046875 running bpv: 2.010635
COMMANDS_FINISHED 47 n_commands 1723
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_11/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_11/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_11/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_6/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_6/mlp.up_proj/compressed.log
best_loss   9%|‚ñâ         | 173/1848 [21:05<11:05:38, 23.84s/it]  9%|‚ñâ         | 174/1848 [21:20<9:53:19, 21.27s/it]   9%|‚ñâ         | 175/1848 [22:35<17:13:47, 37.08s/it] 10%|‚ñâ         | 176/1848 [22:50<14:11:12, 30.55s/it] 10%|‚ñâ         | 178/1848 [23:10<9:48:17, 21.14s/it]  10%|‚ñâ         | 179/1848 [23:25<9:05:53, 19.62s/it] 10%|‚ñâ         | 180/1848 [23:40<8:32:07, 18.42s/it] 10%|‚ñâ         | 181/1848 [24:36<13:07:28, 28.34s/it]83.9266586303711 running bpv: 2.010622
COMMANDS_FINISHED 48 n_commands 1723
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_11/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_11/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:2 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_11/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_6/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_6/self_attn.v_proj/compressed.log
best_loss 51.76110076904297 running bpv: 2.010635
COMMANDS_FINISHED 49 n_commands 1723
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_11/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_11/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_11/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_11/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_11/self_attn.q_proj/compressed.log
best_loss 231.41769409179688 running bpv: 2.010649
COMMANDS_FINISHED 50 n_commands 1723
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_11/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_11/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_11/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_11/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_11/self_attn.k_proj/compressed.log
best_loss 231.53173828125 running bpv: 2.010662
COMMANDS_FINISHED 51 n_commands 1723
meta-llama/Llama-2-7b-hf/layer_6/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_6/mlp.down_proj/compressed.log
best_loss 3.40524959564209 running bpv: 2.010634
COMMANDS_FINISHED 52 n_commands 1723
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_11/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_11/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:2 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_11/self_attn.v_proj/compressed.log 2>&1 &
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_20/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_20/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:3 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_20/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_11/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_11/self_attn.o_proj/compressed.log
best_loss 6.377998352050781 running bpv: 2.010647
COMMANDS_FINISHED 53 n_commands 1723
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_20/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_20/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_20/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_11/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_11/mlp.gate_proj/compressed.log
best_loss 154.31858825683594 running bpv: 2.010634
COMMANDS_FINISHED 54 n_commands 1723
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_20/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_20/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_20/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_11/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_11/mlp.up_proj/compressed.log
best_loss 136.99771118164062 running bpv: 2.010621
COMMANDS_FINISHED 55 n_commands 1723
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_20/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_20/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_20/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_11/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_11/self_attn.v_proj/compressed.log
best_loss 88.64358520507812 running bpv: 2.010634
COMMANDS_FINISHED 56 n_commands 1723
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_20/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_20/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:2 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_20/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
 10%|‚ñâ         | 182/1848 [25:11<13:58:40, 30.20s/it] 10%|‚ñâ         | 183/1848 [25:26<11:57:54, 25.87s/it] 10%|‚ñâ         | 184/1848 [26:21<15:51:25, 34.31s/it] 10%|‚ñà         | 186/1848 [26:41<10:45:34, 23.31s/it] 10%|‚ñà         | 187/1848 [26:56<9:49:03, 21.28s/it]  10%|‚ñà         | 188/1848 [27:11<9:03:47, 19.66s/it] 10%|‚ñà         | 189/1848 [28:26<15:56:33, 34.60s/it] 10%|‚ñà         | 190/1848 [28:41<13:25:21, 29.14s/it]meta-llama/Llama-2-7b-hf/layer_20/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_20/self_attn.q_proj/compressed.log
best_loss 287.87615966796875 running bpv: 2.010647
COMMANDS_FINISHED 57 n_commands 1723
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_20/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_20/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_20/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_20/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_20/self_attn.k_proj/compressed.log
best_loss 301.86676025390625 running bpv: 2.01066
COMMANDS_FINISHED 58 n_commands 1723
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_20/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_20/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_20/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_11/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_11/mlp.down_proj/compressed.log
best_loss 8.862733840942383 running bpv: 2.010633
COMMANDS_FINISHED 59 n_commands 1723
meta-llama/Llama-2-7b-hf/layer_20/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_20/self_attn.o_proj/compressed.log
best_loss 13.759693145751953 running bpv: 2.010646
COMMANDS_FINISHED 60 n_commands 1723
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_23/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_23/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_23/mlp.gate_proj/compressed.log 2>&1 &
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_23/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_23/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:2 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_23/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_20/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_20/mlp.gate_proj/compressed.log
best_loss 370.33929443359375 running bpv: 2.010633
COMMANDS_FINISHED 61 n_commands 1723
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_23/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_23/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:3 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_23/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_20/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_20/mlp.up_proj/compressed.log
best_loss 322.443603515625 running bpv: 2.010621
COMMANDS_FINISHED 62 n_commands 1723
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_23/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_23/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_23/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_20/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_20/self_attn.v_proj/compressed.log
best_loss 162.9111785888672 running bpv: 2.010633
COMMANDS_FINISHED 63 n_commands 1723
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_23/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_23/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_23/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_23/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_23/self_attn.q_proj/compressed.log
best_loss 363.3808898925781 running bpv: 2.010646
COMMANDS_FINISHED 64 n_commands 1723
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_23/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_23/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:3 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_23/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_23/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_23/self_attn.k_proj/compressed.log
best_loss 371.1566467285156 running bpv: 2.010658
COMMANDS_FINISHED 65 n_commands 1723
 10%|‚ñà         | 191/1848 [28:56<11:33:48, 25.12s/it] 10%|‚ñà         | 193/1848 [30:06<13:35:14, 29.56s/it] 10%|‚ñà         | 194/1848 [30:21<11:57:27, 26.03s/it] 11%|‚ñà         | 196/1848 [31:51<15:30:36, 33.80s/it] 11%|‚ñà         | 197/1848 [32:06<13:33:03, 29.55s/it] 11%|‚ñà         | 198/1848 [32:21<11:54:53, 26.00s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_23/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_23/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_23/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_23/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_23/self_attn.o_proj/compressed.log
best_loss 13.302600860595703 running bpv: 2.01067
COMMANDS_FINISHED 66 n_commands 1723
meta-llama/Llama-2-7b-hf/layer_20/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_20/mlp.down_proj/compressed.log
best_loss 42.207576751708984 running bpv: 2.010645
COMMANDS_FINISHED 67 n_commands 1723
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_17/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_17/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_17/mlp.gate_proj/compressed.log 2>&1 &
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_17/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_17/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_17/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_23/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_23/mlp.gate_proj/compressed.log
best_loss 452.44000244140625 running bpv: 2.010632
COMMANDS_FINISHED 68 n_commands 1723
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_17/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_17/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_17/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_23/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_23/mlp.up_proj/compressed.log
best_loss 386.1899719238281 running bpv: 2.01062
COMMANDS_FINISHED 69 n_commands 1723
meta-llama/Llama-2-7b-hf/layer_23/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_23/self_attn.v_proj/compressed.log
best_loss 248.58941650390625 running bpv: 2.010632
COMMANDS_FINISHED 70 n_commands 1723
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_17/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_17/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:2 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_17/self_attn.k_proj/compressed.log 2>&1 &
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_17/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_17/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_17/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_17/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_17/self_attn.q_proj/compressed.log
best_loss 268.37554931640625 running bpv: 2.010644
COMMANDS_FINISHED 71 n_commands 1723
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_17/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_17/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_17/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_17/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_17/self_attn.k_proj/compressed.log
best_loss 282.61614990234375 running bpv: 2.010656
COMMANDS_FINISHED 72 n_commands 1723
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_17/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_17/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:2 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_17/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_23/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_23/mlp.down_proj/compressed.log
best_loss 51.89737319946289 running bpv: 2.010631
COMMANDS_FINISHED 73 n_commands 1723
meta-llama/Llama-2-7b-hf/layer_17/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/sage-donkey-55/meta-llama/Llama-2-7b-hf/layer_17/self_attn.o_proj/compressed.log
best_loss 9.753259658813477 running bpv: 2.010643
COMMANDS_FINISHED 74 n_commands 1723
                                                        