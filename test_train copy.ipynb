{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "import yaml\n",
    "from src.utils import model_utils\n",
    "from src.utils import quantized_model\n",
    "from src.model import llama\n",
    "from transformers import LlamaForCausalLM as OrigLlama\n",
    "import os\n",
    "from src import data\n",
    "import tqdm\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !export CUDA_VISIBLE_DEVICES=0,1\n",
    "# !export CUDA_LAUNCH_BLOCKING=1\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "# torch.distributed.init_process_group(backend='nccl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#config\n",
    "ft_n_train = 16\n",
    "ft_n_val = 8\n",
    "ft_dataset = \"pajama\"\n",
    "base_model = \"meta-llama/Llama-2-7b-hf\"\n",
    "seqlen = 4096\n",
    "batch_size = 1\n",
    "per_device_train_batch_size = 2\n",
    "use_embedding = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d8c05b87ed445c69a5d0cf4ace35d2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "orig_model = OrigLlama.from_pretrained(base_model,\n",
    "                                       device_map=\"auto\", torch_dtype=torch.float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Red Pajama: 100%|██████████| 24/24 [00:01<00:00, 18.14it/s]\n"
     ]
    }
   ],
   "source": [
    "overall_data:list[torch.FloatTensor] = data.get_loaders(ft_dataset, nsamples = ft_n_train+ft_n_val\n",
    "                                  , model = base_model, train_test = \"train\",\n",
    "                                  seqlen=seqlen)\n",
    "\n",
    "overall_data = torch.stack([_[0][0] for _ in overall_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def calculate_logits(model: llama.LlamaForCausalLM, devset, batch_size):\n",
    "    logits = []\n",
    "    for i in tqdm.tqdm(range(len(devset) // batch_size), desc = \"Calculating logits\"):\n",
    "        logits.append(\n",
    "            model(devset[i * batch_size:(i + 1) *\n",
    "                         batch_size].cuda())['logits'].cpu())\n",
    "    logits = torch.concat(logits, dim=0)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating logits:   0%|          | 0/24 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating logits: 100%|██████████| 24/24 [01:31<00:00,  3.81s/it]\n"
     ]
    }
   ],
   "source": [
    "overall_out = calculate_logits(orig_model,overall_data, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([24, 4096, 32000])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_out = overall_out[:, :-1].contiguous().softmax(dim=-1).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[2.2354e-10, 5.7025e-09, 1.6306e-05,  ..., 1.0361e-08,\n",
       "          1.3468e-09, 3.6893e-09],\n",
       "         [1.4929e-10, 1.2274e-09, 1.1974e-05,  ..., 2.2617e-08,\n",
       "          7.3949e-09, 2.8877e-09],\n",
       "         [2.0296e-10, 6.5319e-10, 8.4720e-06,  ..., 1.0241e-08,\n",
       "          4.6695e-09, 1.4944e-09],\n",
       "         ...,\n",
       "         [1.3458e-11, 4.6223e-14, 3.6947e-07,  ..., 1.2013e-10,\n",
       "          1.8254e-09, 1.7071e-09],\n",
       "         [4.1465e-12, 1.3264e-13, 8.3513e-07,  ..., 1.8017e-10,\n",
       "          3.2149e-10, 4.0146e-10],\n",
       "         [1.6634e-10, 4.3322e-12, 1.4700e-07,  ..., 1.1152e-09,\n",
       "          9.0015e-10, 4.6064e-08]],\n",
       "\n",
       "        [[2.3169e-09, 2.6811e-09, 1.5956e-04,  ..., 2.8206e-08,\n",
       "          1.2517e-08, 6.9140e-09],\n",
       "         [2.9108e-08, 1.1318e-08, 3.8498e-04,  ..., 2.5980e-07,\n",
       "          5.6099e-07, 1.5601e-07],\n",
       "         [6.9376e-09, 4.8868e-08, 2.5782e-05,  ..., 2.3716e-07,\n",
       "          6.9378e-08, 7.1069e-08],\n",
       "         ...,\n",
       "         [4.4877e-13, 1.4639e-13, 1.6864e-06,  ..., 5.0638e-11,\n",
       "          8.6549e-11, 5.4382e-13],\n",
       "         [9.2573e-13, 4.4149e-12, 8.7405e-05,  ..., 2.7172e-12,\n",
       "          7.7543e-12, 1.6094e-11],\n",
       "         [5.1139e-14, 1.2592e-12, 4.6475e-08,  ..., 2.0294e-12,\n",
       "          1.2609e-12, 2.0178e-12]],\n",
       "\n",
       "        [[1.0240e-09, 2.5624e-08, 4.9998e-05,  ..., 1.5376e-08,\n",
       "          6.7018e-09, 6.4041e-09],\n",
       "         [5.1205e-11, 2.3449e-12, 1.3119e-05,  ..., 3.1350e-09,\n",
       "          9.4135e-10, 1.0464e-09],\n",
       "         [5.6161e-10, 1.3632e-09, 1.9426e-05,  ..., 4.0450e-09,\n",
       "          8.6924e-09, 6.2800e-09],\n",
       "         ...,\n",
       "         [3.0847e-13, 8.9059e-13, 5.1944e-07,  ..., 1.4215e-11,\n",
       "          1.9417e-12, 3.7111e-13],\n",
       "         [1.6150e-11, 3.5429e-11, 2.9994e-06,  ..., 2.3962e-10,\n",
       "          1.4292e-10, 1.6546e-10],\n",
       "         [3.7463e-13, 1.3205e-12, 5.8962e-07,  ..., 8.7061e-12,\n",
       "          2.2495e-12, 3.6792e-12]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[1.3796e-09, 5.6666e-09, 1.1125e-05,  ..., 2.6530e-08,\n",
       "          1.1740e-08, 3.4487e-08],\n",
       "         [1.3419e-10, 5.4767e-11, 5.7598e-06,  ..., 7.6373e-08,\n",
       "          4.2280e-09, 1.0704e-08],\n",
       "         [1.4752e-10, 5.4390e-11, 1.0947e-04,  ..., 1.6760e-08,\n",
       "          1.3526e-09, 6.5279e-10],\n",
       "         ...,\n",
       "         [3.2174e-10, 3.6822e-11, 8.9189e-09,  ..., 3.2797e-09,\n",
       "          5.0912e-09, 6.3332e-09],\n",
       "         [7.5704e-14, 4.0579e-13, 4.4623e-08,  ..., 2.2770e-12,\n",
       "          8.7381e-13, 2.7094e-12],\n",
       "         [5.0499e-10, 1.4079e-09, 1.6660e-08,  ..., 2.1051e-08,\n",
       "          5.0322e-08, 2.0080e-08]],\n",
       "\n",
       "        [[2.6412e-09, 2.1561e-08, 4.6094e-03,  ..., 1.0764e-06,\n",
       "          1.6072e-07, 2.4791e-07],\n",
       "         [5.6429e-09, 1.2534e-09, 7.7496e-04,  ..., 6.3260e-09,\n",
       "          6.3933e-08, 1.2575e-08],\n",
       "         [6.3262e-10, 3.6363e-11, 1.9574e-05,  ..., 1.9984e-09,\n",
       "          7.2509e-09, 6.7358e-09],\n",
       "         ...,\n",
       "         [6.1187e-14, 1.3612e-16, 2.9647e-10,  ..., 1.8212e-14,\n",
       "          9.3962e-15, 2.3023e-15],\n",
       "         [1.4603e-13, 2.7574e-15, 1.8004e-09,  ..., 1.3934e-12,\n",
       "          4.5172e-14, 8.1534e-14],\n",
       "         [4.9919e-12, 1.5273e-12, 2.5180e-05,  ..., 5.2879e-12,\n",
       "          4.2949e-12, 4.7808e-12]],\n",
       "\n",
       "        [[5.9651e-09, 1.7621e-07, 4.9357e-04,  ..., 1.5588e-07,\n",
       "          7.7389e-08, 1.2223e-07],\n",
       "         [9.2601e-11, 4.4080e-10, 5.4572e-04,  ..., 3.3702e-10,\n",
       "          2.4662e-10, 3.5711e-10],\n",
       "         [1.4853e-10, 3.7155e-10, 5.8267e-04,  ..., 5.8037e-10,\n",
       "          5.4838e-10, 6.1695e-10],\n",
       "         ...,\n",
       "         [1.8579e-12, 9.5462e-12, 2.4011e-07,  ..., 7.5892e-11,\n",
       "          1.1971e-11, 5.4337e-11],\n",
       "         [3.1876e-11, 4.2586e-12, 1.7025e-06,  ..., 2.0437e-10,\n",
       "          3.5500e-09, 9.5269e-09],\n",
       "         [1.0544e-11, 1.1537e-11, 5.8979e-07,  ..., 3.4485e-10,\n",
       "          7.5115e-11, 9.2315e-11]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "del orig_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import utils\n",
    "\n",
    "\n",
    "\n",
    "utils.clean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n",
      "device None dtype torch.float32\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3433178b6e49465c9f8d1d5fb5871548",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = llama.LlamaForCausalLM.from_pretrained(\"/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed_hf/run_38\",\n",
    "                                               device_map=\"auto\",\n",
    "                                                  torch_dtype=torch.float32,\n",
    "                                                    low_cpu_mem_usage=True)\n",
    "\n",
    "# model = model_utils.get_llama(\"meta-llama/Llama-2-7b-hf\",\n",
    "#                                device_map=\"auto\",\n",
    "#                                 dtype=torch.float32)\n",
    "                          \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    temp_out = model(overall_data[[0]].cuda())['logits'].contiguous().softmax(dim=-1).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[4.6737e-07, 2.1708e-05, 6.8499e-04,  ..., 4.6186e-07,\n",
       "          2.2432e-07, 1.3042e-06],\n",
       "         [2.0906e-10, 6.5542e-10, 9.4043e-06,  ..., 2.3275e-08,\n",
       "          5.9380e-09, 2.9740e-09],\n",
       "         [1.7926e-10, 3.2220e-10, 7.7861e-06,  ..., 1.0558e-08,\n",
       "          3.6094e-09, 1.0416e-09],\n",
       "         ...,\n",
       "         [1.5693e-11, 3.6049e-13, 2.3453e-06,  ..., 5.4415e-10,\n",
       "          1.1082e-09, 5.7102e-10],\n",
       "         [5.7886e-10, 4.3732e-11, 1.7115e-06,  ..., 3.1174e-09,\n",
       "          9.5443e-09, 9.1943e-08],\n",
       "         [9.4791e-10, 2.3938e-11, 1.8201e-07,  ..., 2.9170e-09,\n",
       "          5.9034e-08, 2.2966e-08]]], device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[2.2354e-10, 5.7025e-09, 1.6306e-05,  ..., 1.0361e-08,\n",
       "          1.3468e-09, 3.6893e-09],\n",
       "         [1.4929e-10, 1.2274e-09, 1.1974e-05,  ..., 2.2617e-08,\n",
       "          7.3949e-09, 2.8877e-09],\n",
       "         [2.0296e-10, 6.5319e-10, 8.4720e-06,  ..., 1.0241e-08,\n",
       "          4.6695e-09, 1.4944e-09],\n",
       "         ...,\n",
       "         [1.3458e-11, 4.6223e-14, 3.6947e-07,  ..., 1.2013e-10,\n",
       "          1.8254e-09, 1.7071e-09],\n",
       "         [4.1465e-12, 1.3264e-13, 8.3513e-07,  ..., 1.8017e-10,\n",
       "          3.2149e-10, 4.0146e-10],\n",
       "         [1.6634e-10, 4.3322e-12, 1.4700e-07,  ..., 1.1152e-09,\n",
       "          9.0015e-10, 4.6064e-08]]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_out[[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-10581.0996, device='cuda:0')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kld = torch.nn.KLDivLoss(reduction=\"batchmean\")\n",
    "kld(temp_out[:,:-1], overall_out[[0]].cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_embeddings = model.model.embed_tokens(overall_data.cuda()).detach()\n",
    "                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_embeddings.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([24, 4095, 32000])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.modeling_attn_mask_utils import \\\n",
    "    _prepare_4d_causal_attention_mask\n",
    "    \n",
    "position_ids = torch.arange(seqlen, dtype=torch.int32)[None, :] + \\\n",
    "    torch.zeros(per_device_train_batch_size, seqlen, dtype=torch.int32)\n",
    "attention_mask = _prepare_4d_causal_attention_mask(\n",
    "    None, (per_device_train_batch_size, seqlen), overall_embeddings[:per_device_train_batch_size], 0)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from typing import Tuple\n",
    "\n",
    "class SimpleDataset(Dataset):\n",
    "\n",
    "    def __init__(self, inputs_embeds, soft_labels,attention_mask, position_ids):\n",
    "        self.inputs_embeds = inputs_embeds\n",
    "        self.soft_labels = soft_labels\n",
    "        self.attention_mask = attention_mask[0]\n",
    "        self.position_ids = position_ids[0]\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs_embeds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'inputs_embeds': self.inputs_embeds[idx],\n",
    "            'labels': self.soft_labels[idx],\n",
    "            'attention_mask': self.attention_mask,\n",
    "            'position_ids': self.position_ids\n",
    "        }\n",
    "    \n",
    "def make_datasets(X:torch.FloatTensor, Y:torch.FloatTensor, n_val:int,\n",
    "                    attention_mask, position_ids) -> Tuple[Dataset, Dataset]:\n",
    "    \n",
    "\n",
    "    #make the indices\n",
    "    idxs = torch.randperm(len(X))\n",
    "    train_idxs = idxs[:-n_val]\n",
    "\n",
    "    train_ds = SimpleDataset(X[train_idxs], Y[train_idxs], attention_mask, position_ids)\n",
    "    valid_ds = SimpleDataset(X[idxs[-n_val:]], Y[idxs[-n_val:]], attention_mask, position_ids)\n",
    "    return train_ds, valid_ds\n",
    "\n",
    "traindataset, validdataset = make_datasets(overall_embeddings, overall_out, ft_n_val, attention_mask, position_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 8)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(traindataset), len(validdataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#custom kld loss\n",
    "def custom_kld_loss(outputs, labels, num_items_in_batch):\n",
    "    \n",
    "    logits = outputs['logits'][:,:-1]\n",
    "    print(logits.shape, labels.shape, num_items_in_batch)\n",
    "    \n",
    "    #do kld on the last dim\n",
    "    loss = torch.nn.KLDivLoss(reduction='sum')(logits, labels)\n",
    "    if num_items_in_batch == 0 or num_items_in_batch is None:\n",
    "        l = loss/logits.numel()\n",
    "        print(l)\n",
    "        return l\n",
    "    else:\n",
    "        print(loss / num_items_in_batch)\n",
    "        return loss / num_items_in_batch\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lliu/miniconda3/envs/NoWAC-VQ/lib/python3.13/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/lliu/miniconda3/envs/NoWAC-VQ/lib/python3.13/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4095, 32000]) torch.Size([2, 4095, 32000]) tensor(262080000, device='cuda:0')\n",
      "tensor(-0.0006, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='49' max='80' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [49/80 03:23 < 02:14, 0.23 it/s, Epoch 6/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-0.000600</td>\n",
       "      <td>-0.000625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-0.000600</td>\n",
       "      <td>-0.000654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>-0.000700</td>\n",
       "      <td>-0.000679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>-0.000700</td>\n",
       "      <td>-0.000701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>-0.000700</td>\n",
       "      <td>-0.000719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>-0.000800</td>\n",
       "      <td>-0.000734</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4095, 32000]) torch.Size([2, 4095, 32000]) tensor(262080000, device='cuda:0')\n",
      "tensor(-0.0006, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "torch.Size([2, 4095, 32000]) torch.Size([2, 4095, 32000]) tensor(262080000, device='cuda:0')\n",
      "tensor(-0.0006, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "torch.Size([2, 4095, 32000]) torch.Size([2, 4095, 32000]) tensor(262080000, device='cuda:0')\n",
      "tensor(-0.0006, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "torch.Size([2, 4095, 32000]) torch.Size([2, 4095, 32000]) tensor(262080000, device='cuda:0')\n",
      "tensor(-0.0006, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "torch.Size([2, 4095, 32000]) torch.Size([2, 4095, 32000]) tensor(262080000, device='cuda:0')\n",
      "tensor(-0.0006, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "torch.Size([2, 4095, 32000]) torch.Size([2, 4095, 32000]) tensor(262080000, device='cuda:0')\n",
      "tensor(-0.0006, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "torch.Size([2, 4095, 32000]) torch.Size([2, 4095, 32000]) tensor(262080000, device='cuda:0')\n",
      "tensor(-0.0006, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "torch.Size([8, 4095, 32000]) torch.Size([8, 4095, 32000]) None\n",
      "tensor(-0.0006, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lliu/miniconda3/envs/NoWAC-VQ/lib/python3.13/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4095, 32000]) torch.Size([2, 4095, 32000]) tensor(262080000, device='cuda:0')\n",
      "tensor(-0.0006, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "torch.Size([2, 4095, 32000]) torch.Size([2, 4095, 32000]) tensor(262080000, device='cuda:0')\n",
      "tensor(-0.0007, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "torch.Size([2, 4095, 32000]) torch.Size([2, 4095, 32000]) tensor(262080000, device='cuda:0')\n",
      "tensor(-0.0006, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "torch.Size([2, 4095, 32000]) torch.Size([2, 4095, 32000]) tensor(262080000, device='cuda:0')\n",
      "tensor(-0.0006, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "torch.Size([2, 4095, 32000]) torch.Size([2, 4095, 32000]) tensor(262080000, device='cuda:0')\n",
      "tensor(-0.0006, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "torch.Size([2, 4095, 32000]) torch.Size([2, 4095, 32000]) tensor(262080000, device='cuda:0')\n",
      "tensor(-0.0006, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "torch.Size([2, 4095, 32000]) torch.Size([2, 4095, 32000]) tensor(262080000, device='cuda:0')\n",
      "tensor(-0.0006, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "torch.Size([2, 4095, 32000]) torch.Size([2, 4095, 32000]) tensor(262080000, device='cuda:0')\n",
      "tensor(-0.0006, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "torch.Size([8, 4095, 32000]) torch.Size([8, 4095, 32000]) None\n",
      "tensor(-0.0007, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lliu/miniconda3/envs/NoWAC-VQ/lib/python3.13/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4095, 32000]) torch.Size([2, 4095, 32000]) tensor(262080000, device='cuda:0')\n",
      "tensor(-0.0006, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "torch.Size([2, 4095, 32000]) torch.Size([2, 4095, 32000]) tensor(262080000, device='cuda:0')\n",
      "tensor(-0.0007, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "torch.Size([2, 4095, 32000]) torch.Size([2, 4095, 32000]) tensor(262080000, device='cuda:0')\n",
      "tensor(-0.0007, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "torch.Size([2, 4095, 32000]) torch.Size([2, 4095, 32000]) tensor(262080000, device='cuda:0')\n",
      "tensor(-0.0006, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "torch.Size([2, 4095, 32000]) torch.Size([2, 4095, 32000]) tensor(262080000, device='cuda:0')\n",
      "tensor(-0.0007, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "torch.Size([2, 4095, 32000]) torch.Size([2, 4095, 32000]) tensor(262080000, device='cuda:0')\n",
      "tensor(-0.0007, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "torch.Size([2, 4095, 32000]) torch.Size([2, 4095, 32000]) tensor(262080000, device='cuda:0')\n",
      "tensor(-0.0007, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "torch.Size([2, 4095, 32000]) torch.Size([2, 4095, 32000]) tensor(262080000, device='cuda:0')\n",
      "tensor(-0.0007, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "torch.Size([8, 4095, 32000]) torch.Size([8, 4095, 32000]) None\n",
      "tensor(-0.0007, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lliu/miniconda3/envs/NoWAC-VQ/lib/python3.13/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4095, 32000]) torch.Size([2, 4095, 32000]) tensor(262080000, device='cuda:0')\n",
      "tensor(-0.0006, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "torch.Size([2, 4095, 32000]) torch.Size([2, 4095, 32000]) tensor(262080000, device='cuda:0')\n",
      "tensor(-0.0007, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "torch.Size([2, 4095, 32000]) torch.Size([2, 4095, 32000]) tensor(262080000, device='cuda:0')\n",
      "tensor(-0.0006, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "torch.Size([2, 4095, 32000]) torch.Size([2, 4095, 32000]) tensor(262080000, device='cuda:0')\n",
      "tensor(-0.0007, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "torch.Size([2, 4095, 32000]) torch.Size([2, 4095, 32000]) tensor(262080000, device='cuda:0')\n",
      "tensor(-0.0007, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "torch.Size([2, 4095, 32000]) torch.Size([2, 4095, 32000]) tensor(262080000, device='cuda:0')\n",
      "tensor(-0.0007, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "torch.Size([2, 4095, 32000]) torch.Size([2, 4095, 32000]) tensor(262080000, device='cuda:0')\n",
      "tensor(-0.0007, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "torch.Size([2, 4095, 32000]) torch.Size([2, 4095, 32000]) tensor(262080000, device='cuda:0')\n",
      "tensor(-0.0007, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "torch.Size([8, 4095, 32000]) torch.Size([8, 4095, 32000]) None\n",
      "tensor(-0.0007, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lliu/miniconda3/envs/NoWAC-VQ/lib/python3.13/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4095, 32000]) torch.Size([2, 4095, 32000]) tensor(262080000, device='cuda:0')\n",
      "tensor(-0.0007, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "torch.Size([2, 4095, 32000]) torch.Size([2, 4095, 32000]) tensor(262080000, device='cuda:0')\n",
      "tensor(-0.0007, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "torch.Size([2, 4095, 32000]) torch.Size([2, 4095, 32000]) tensor(262080000, device='cuda:0')\n",
      "tensor(-0.0007, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "torch.Size([2, 4095, 32000]) torch.Size([2, 4095, 32000]) tensor(262080000, device='cuda:0')\n",
      "tensor(-0.0007, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "torch.Size([2, 4095, 32000]) torch.Size([2, 4095, 32000]) tensor(262080000, device='cuda:0')\n",
      "tensor(-0.0007, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "torch.Size([2, 4095, 32000]) torch.Size([2, 4095, 32000]) tensor(262080000, device='cuda:0')\n",
      "tensor(-0.0007, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "torch.Size([2, 4095, 32000]) torch.Size([2, 4095, 32000]) tensor(262080000, device='cuda:0')\n",
      "tensor(-0.0007, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "torch.Size([2, 4095, 32000]) torch.Size([2, 4095, 32000]) tensor(262080000, device='cuda:0')\n",
      "tensor(-0.0007, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "torch.Size([8, 4095, 32000]) torch.Size([8, 4095, 32000]) None\n",
      "tensor(-0.0007, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lliu/miniconda3/envs/NoWAC-VQ/lib/python3.13/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4095, 32000]) torch.Size([2, 4095, 32000]) tensor(262080000, device='cuda:0')\n",
      "tensor(-0.0008, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "torch.Size([2, 4095, 32000]) torch.Size([2, 4095, 32000]) tensor(262080000, device='cuda:0')\n",
      "tensor(-0.0007, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "torch.Size([2, 4095, 32000]) torch.Size([2, 4095, 32000]) tensor(262080000, device='cuda:0')\n",
      "tensor(-0.0007, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "torch.Size([2, 4095, 32000]) torch.Size([2, 4095, 32000]) tensor(262080000, device='cuda:0')\n",
      "tensor(-0.0007, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "torch.Size([2, 4095, 32000]) torch.Size([2, 4095, 32000]) tensor(262080000, device='cuda:0')\n",
      "tensor(-0.0007, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "torch.Size([2, 4095, 32000]) torch.Size([2, 4095, 32000]) tensor(262080000, device='cuda:0')\n",
      "tensor(-0.0008, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "torch.Size([2, 4095, 32000]) torch.Size([2, 4095, 32000]) tensor(262080000, device='cuda:0')\n",
      "tensor(-0.0007, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "torch.Size([2, 4095, 32000]) torch.Size([2, 4095, 32000]) tensor(262080000, device='cuda:0')\n",
      "tensor(-0.0008, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "torch.Size([8, 4095, 32000]) torch.Size([8, 4095, 32000]) None\n",
      "tensor(-0.0007, device='cuda:0')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#train the model on the dataset with transformers trainer\u001b[39;00m\n\u001b[32m      3\u001b[39m trainer = transformers.Trainer(\n\u001b[32m      4\u001b[39m     model=model,\n\u001b[32m      5\u001b[39m     args=transformers.TrainingArguments(\n\u001b[32m   (...)\u001b[39m\u001b[32m     32\u001b[39m     compute_loss_func=custom_kld_loss,\n\u001b[32m     33\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/NoWAC-VQ/lib/python3.13/site-packages/transformers/trainer.py:2245\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2243\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2244\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2246\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2250\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/NoWAC-VQ/lib/python3.13/site-packages/transformers/trainer.py:2647\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2644\u001b[39m     \u001b[38;5;28mself\u001b[39m.control.should_training_stop = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2646\u001b[39m \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_epoch_end(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n\u001b[32m-> \u001b[39m\u001b[32m2647\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2649\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m DebugOption.TPU_METRICS_DEBUG \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.debug:\n\u001b[32m   2650\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_xla_available():\n\u001b[32m   2651\u001b[39m         \u001b[38;5;66;03m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/NoWAC-VQ/lib/python3.13/site-packages/transformers/trainer.py:3100\u001b[39m, in \u001b[36mTrainer._maybe_log_save_evaluate\u001b[39m\u001b[34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time)\u001b[39m\n\u001b[32m   3097\u001b[39m         \u001b[38;5;28mself\u001b[39m.control.should_save = is_new_best_metric\n\u001b[32m   3099\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.control.should_save:\n\u001b[32m-> \u001b[39m\u001b[32m3100\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_save_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3101\u001b[39m     \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_save(\u001b[38;5;28mself\u001b[39m.args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/NoWAC-VQ/lib/python3.13/site-packages/transformers/trainer.py:3197\u001b[39m, in \u001b[36mTrainer._save_checkpoint\u001b[39m\u001b[34m(self, model, trial)\u001b[39m\n\u001b[32m   3195\u001b[39m run_dir = \u001b[38;5;28mself\u001b[39m._get_output_dir(trial=trial)\n\u001b[32m   3196\u001b[39m output_dir = os.path.join(run_dir, checkpoint_folder)\n\u001b[32m-> \u001b[39m\u001b[32m3197\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msave_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_internal_call\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   3199\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.save_strategy \u001b[38;5;129;01min\u001b[39;00m [SaveStrategy.STEPS, SaveStrategy.EPOCH] \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.best_global_step:\n\u001b[32m   3200\u001b[39m     best_checkpoint_folder = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPREFIX_CHECKPOINT_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.state.best_global_step\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/NoWAC-VQ/lib/python3.13/site-packages/transformers/trainer.py:3884\u001b[39m, in \u001b[36mTrainer.save_model\u001b[39m\u001b[34m(self, output_dir, _internal_call)\u001b[39m\n\u001b[32m   3881\u001b[39m         \u001b[38;5;28mself\u001b[39m.model_wrapped.save_checkpoint(output_dir)\n\u001b[32m   3883\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.should_save:\n\u001b[32m-> \u001b[39m\u001b[32m3884\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3886\u001b[39m \u001b[38;5;66;03m# Push to the Hub when `save_model` is called by the user.\u001b[39;00m\n\u001b[32m   3887\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.push_to_hub \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _internal_call:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/NoWAC-VQ/lib/python3.13/site-packages/transformers/trainer.py:3988\u001b[39m, in \u001b[36mTrainer._save\u001b[39m\u001b[34m(self, output_dir, state_dict)\u001b[39m\n\u001b[32m   3986\u001b[39m             torch.save(state_dict, os.path.join(output_dir, WEIGHTS_NAME))\n\u001b[32m   3987\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3988\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3989\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_serialization\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_safetensors\u001b[49m\n\u001b[32m   3990\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3992\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.processing_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3993\u001b[39m     \u001b[38;5;28mself\u001b[39m.processing_class.save_pretrained(output_dir)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/NoWAC-VQ/lib/python3.13/site-packages/transformers/modeling_utils.py:3578\u001b[39m, in \u001b[36mPreTrainedModel.save_pretrained\u001b[39m\u001b[34m(self, save_directory, is_main_process, state_dict, save_function, push_to_hub, max_shard_size, safe_serialization, variant, token, save_peft_format, **kwargs)\u001b[39m\n\u001b[32m   3573\u001b[39m     gc.collect()\n\u001b[32m   3575\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m safe_serialization:\n\u001b[32m   3576\u001b[39m     \u001b[38;5;66;03m# At some point we will need to deal better with save_function (used for TPU and other distributed\u001b[39;00m\n\u001b[32m   3577\u001b[39m     \u001b[38;5;66;03m# joyfulness), but for now this enough.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3578\u001b[39m     \u001b[43msafe_save_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshard\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshard_file\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mformat\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3579\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3580\u001b[39m     save_function(shard, os.path.join(save_directory, shard_file))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/NoWAC-VQ/lib/python3.13/site-packages/safetensors/torch.py:286\u001b[39m, in \u001b[36msave_file\u001b[39m\u001b[34m(tensors, filename, metadata)\u001b[39m\n\u001b[32m    255\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msave_file\u001b[39m(\n\u001b[32m    256\u001b[39m     tensors: Dict[\u001b[38;5;28mstr\u001b[39m, torch.Tensor],\n\u001b[32m    257\u001b[39m     filename: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike],\n\u001b[32m    258\u001b[39m     metadata: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    259\u001b[39m ):\n\u001b[32m    260\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    261\u001b[39m \u001b[33;03m    Saves a dictionary of tensors into raw bytes in safetensors format.\u001b[39;00m\n\u001b[32m    262\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    284\u001b[39m \u001b[33;03m    ```\u001b[39;00m\n\u001b[32m    285\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m     serialize_file(\u001b[43m_flatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m, filename, metadata=metadata)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/NoWAC-VQ/lib/python3.13/site-packages/safetensors/torch.py:500\u001b[39m, in \u001b[36m_flatten\u001b[39m\u001b[34m(tensors)\u001b[39m\n\u001b[32m    487\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m failing:\n\u001b[32m    488\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    489\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m    490\u001b[39m \u001b[33m        Some tensors share memory, this will lead to duplicate memory on disk and potential differences when loading them again: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfailing\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    493\u001b[39m \u001b[33m        \u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m    494\u001b[39m     )\n\u001b[32m    496\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m    497\u001b[39m     k: {\n\u001b[32m    498\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(v.dtype).split(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)[-\u001b[32m1\u001b[39m],\n\u001b[32m    499\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mshape\u001b[39m\u001b[33m\"\u001b[39m: v.shape,\n\u001b[32m--> \u001b[39m\u001b[32m500\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m: \u001b[43m_tobytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    501\u001b[39m     }\n\u001b[32m    502\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m tensors.items()\n\u001b[32m    503\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/NoWAC-VQ/lib/python3.13/site-packages/safetensors/torch.py:422\u001b[39m, in \u001b[36m_tobytes\u001b[39m\u001b[34m(tensor, name)\u001b[39m\n\u001b[32m    414\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    415\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mYou are trying to save a non contiguous tensor: `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` which is not allowed. It either means you\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    416\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m are trying to save tensors which are reference of each other in which case it\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms recommended to save\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    417\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m only the full tensors, and reslice at load time, or simply call `.contiguous()` on your tensor to\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    418\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m pack it before saving.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    419\u001b[39m     )\n\u001b[32m    420\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tensor.device.type != \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    421\u001b[39m     \u001b[38;5;66;03m# Moving tensor to cpu before saving\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m422\u001b[39m     tensor = \u001b[43mtensor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcpu\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    424\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mctypes\u001b[39;00m\n\u001b[32m    426\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#train the model on the dataset with transformers trainer\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        gradient_accumulation_steps=1,\n",
    "        gradient_checkpointing=True,\n",
    "        fp16=True,\n",
    "        logging_steps=1,\n",
    "        output_dir=\"./output\",\n",
    "        num_train_epochs=10,\n",
    "        save_total_limit=3,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        # eval_steps=100,\n",
    "        # per_device_train_batch_size=32,\n",
    "        load_best_model_at_end=True,\n",
    "        #add a tqdm progress bar\n",
    "        #set the lr to 1e-5\n",
    "        learning_rate=1e-4,\n",
    "        warmup_steps=0,\n",
    "        dataloader_pin_memory=False,\n",
    "        #set the logging dir to ./logs\n",
    "        logging_dir=\"./logs\",\n",
    "        #log to wandb\n",
    "        report_to=\"wandb\",\n",
    "        run_name=\"llama-2-7b-hf\",\n",
    "    ),\n",
    "    train_dataset=traindataset,\n",
    "    eval_dataset=validdataset,\n",
    "    compute_loss_func=custom_kld_loss,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./output\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Mar 21 21:09:03 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.216.01             Driver Version: 535.216.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA RTX A6000               Off | 00000000:05:00.0 Off |                  Off |\n",
      "| 47%   70C    P2             284W / 300W |  48538MiB / 49140MiB |    100%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA RTX A6000               Off | 00000000:06:00.0 Off |                  Off |\n",
      "| 50%   76C    P2             261W / 300W |  47560MiB / 49140MiB |     98%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA RTX A6000               Off | 00000000:45:00.0 Off |                  Off |\n",
      "| 44%   72C    P2             294W / 300W |  47558MiB / 49140MiB |    100%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA RTX A6000               Off | 00000000:46:00.0 Off |                  Off |\n",
      "| 37%   66C    P2             263W / 300W |  47590MiB / 49140MiB |     99%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   4  NVIDIA A100 80GB PCIe          Off | 00000000:85:00.0 Off |                    0 |\n",
      "| N/A   31C    P0              63W / 300W |   4462MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   5  NVIDIA A100 80GB PCIe          Off | 00000000:86:00.0 Off |                    0 |\n",
      "| N/A   33C    P0              64W / 300W |   5036MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   6  NVIDIA RTX A6000               Off | 00000000:C5:00.0 Off |                  Off |\n",
      "| 44%   72C    P2             290W / 300W |  47542MiB / 49140MiB |     98%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   7  NVIDIA RTX A6000               Off | 00000000:C6:00.0 Off |                  Off |\n",
      "| 35%   64C    P2             233W / 300W |  47032MiB / 49140MiB |     97%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      4114      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "|    0   N/A  N/A     29797      C   ...miniconda3/envs/NoWAC-VQ/bin/python     3186MiB |\n",
      "|    0   N/A  N/A     33782      C   python                                    45334MiB |\n",
      "|    1   N/A  N/A      4114      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "|    1   N/A  N/A     29797      C   ...miniconda3/envs/NoWAC-VQ/bin/python     4828MiB |\n",
      "|    1   N/A  N/A     32913      C   python                                    42714MiB |\n",
      "|    2   N/A  N/A      4114      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "|    2   N/A  N/A     29797      C   ...miniconda3/envs/NoWAC-VQ/bin/python     4828MiB |\n",
      "|    2   N/A  N/A     32577      C   python                                    42712MiB |\n",
      "|    3   N/A  N/A      4114      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "|    3   N/A  N/A     29797      C   ...miniconda3/envs/NoWAC-VQ/bin/python     2324MiB |\n",
      "|    3   N/A  N/A     33282      C   python                                    45248MiB |\n",
      "|    4   N/A  N/A      4114      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "|    4   N/A  N/A     29797      C   ...miniconda3/envs/NoWAC-VQ/bin/python     4440MiB |\n",
      "|    5   N/A  N/A      4114      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "|    5   N/A  N/A     29797      C   ...miniconda3/envs/NoWAC-VQ/bin/python     5014MiB |\n",
      "|    6   N/A  N/A      4114      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "|    6   N/A  N/A     29797      C   ...miniconda3/envs/NoWAC-VQ/bin/python     4828MiB |\n",
      "|    6   N/A  N/A     31994      C   python                                    42696MiB |\n",
      "|    7   N/A  N/A      4114      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "|    7   N/A  N/A     29797      C   ...miniconda3/envs/NoWAC-VQ/bin/python     4300MiB |\n",
      "|    7   N/A  N/A     31064      C   python                                    42714MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(13.3660, device='cuda:0', grad_fn=<ToCopyBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(valset[0][\"input_ids\"][...,:-2].cuda(),\n",
    "      labels=valset[0][\"input_ids\"][...,1:].cuda()[...,:-1]  # shift labels\n",
    "      ).loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import tqdm\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1848\n"
     ]
    }
   ],
   "source": [
    "paths = glob.glob(\"/data/lliu/huffman/models/meta-llama/*/hessianDiags/seed_0/pajama/128/*/*.pt\")\n",
    "print(len(paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1848/1848 [00:01<00:00, 979.71it/s] \n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for p in tqdm.tqdm(paths):\n",
    "    hessianDiag = torch.load(p)\n",
    "    if \"hessianDiag\" in hessianDiag:\n",
    "        continue\n",
    "    torch.save({\"hessianDiag\": hessianDiag[\"hessian\"]}, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hessianDiag': tensor([0.0067, 0.0076, 0.0071,  ..., 0.0070, 0.0077, 0.0074], device='cuda:1',\n",
       "        dtype=torch.float16)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.load(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing B\n",
      "fn1_B\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.B at 0x7a053f3f42f0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "class A:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.a = 1\n",
    "        self.b = 2\n",
    "        self.c = 3\n",
    "        \n",
    "    def fn1(self):\n",
    "        print(\"fn1_A\")\n",
    "        \n",
    "    @classmethod\n",
    "    def fn1_static(cls):\n",
    "        c = cls()\n",
    "        c.fn1()\n",
    "        return c\n",
    "    \n",
    "class B(A):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        print(\"initializing B\")\n",
    "        self.d = 4\n",
    "        \n",
    "    def fn1(self):\n",
    "        print(\"fn1_B\")\n",
    "        \n",
    "B.fn1_static()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NoWAC-VQ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
