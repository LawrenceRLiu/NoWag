/home/lliu/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.28s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.25it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.14it/s]
Model loaded. LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
4096
Loading tokenizer for meta-llama/Llama-2-7b-hf
Starting...
/home/lliu/anaconda3/lib/python3.11/site-packages/torch/cuda/__init__.py:611: UserWarning: Can't initialize NVML
  warnings.warn("Can't initialize NVML")
39011 MiB free out of 48676 MiB total
attention_mask torch.Size([1, 1, 4096, 4096]) cuda:7 torch.float16
position_ids torch.Size([1, 4096]) cuda:7 torch.int64
position_ids torch.Size([1, 4096])
attention_mask torch.Size([1, 1, 4096, 4096])
Ready.
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
0 self_attn.q_proj
Pruning ...
realigning
initial loss 1.56924569606781
final loss 1.1642513275146484
quantized
not here
quantized in 23.013914346694946 seconds
36577 MiB free out of 48676 MiB total
0 self_attn.k_proj
Pruning ...
realigning
initial loss 1.8915133476257324
final loss 1.5419642925262451
quantized
not here
quantized in 20.4504873752594 seconds
36641 MiB free out of 48676 MiB total
0 self_attn.v_proj
Pruning ...
realigning
initial loss 0.11391407996416092
final loss 0.09255972504615784
quantized
not here
quantized in 19.63995361328125 seconds
36705 MiB free out of 48676 MiB total
0 self_attn.o_proj
Pruning ...
realigning
initial loss 0.016694191843271255
final loss 0.01346983015537262
quantized
not here
quantized in 19.282963275909424 seconds
36705 MiB free out of 48676 MiB total
0 mlp.gate_proj
Pruning ...
realigning
initial loss 5.378171443939209
final loss 4.615377426147461
quantized
not here
quantized in 54.749664068222046 seconds
36425 MiB free out of 48676 MiB total
0 mlp.up_proj
Pruning ...
realigning
initial loss 3.686795473098755
final loss 3.4293406009674072
quantized
not here
quantized in 52.094900369644165 seconds
36145 MiB free out of 48676 MiB total
0 mlp.down_proj
Pruning ...
realigning
initial loss 0.020199565216898918
final loss 0.016655195504426956
quantized
not here
quantized in 50.334370374679565 seconds
36037 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 8.524706316848096e-06 val loss: 7.2420090191371855e-06
8385 MiB free out of 48676 MiB total
epoch 1 loss: 6.963146272909171e-06 val loss: 6.613146325662456e-06
8385 MiB free out of 48676 MiB total
epoch 2 loss: 6.5559819901750416e-06 val loss: 6.356890025926987e-06
8385 MiB free out of 48676 MiB total
epoch 3 loss: 6.354178932355126e-06 val loss: 6.198482907393554e-06
8385 MiB free out of 48676 MiB total
epoch 4 loss: 6.219573904786557e-06 val loss: 6.08395069434664e-06
8385 MiB free out of 48676 MiB total
36037 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8385 MiB free out of 48676 MiB total
after cast to cpu
38579 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
1 self_attn.q_proj
Pruning ...
realigning
initial loss 23.14562225341797
final loss 17.854764938354492
quantized
not here
quantized in 22.61652374267578 seconds
36543 MiB free out of 48676 MiB total
1 self_attn.k_proj
Pruning ...
realigning
initial loss 24.939071655273438
final loss 19.801544189453125
quantized
not here
quantized in 21.47237277030945 seconds
36629 MiB free out of 48676 MiB total
1 self_attn.v_proj
Pruning ...
realigning
initial loss 1.0507758855819702
final loss 0.9199187159538269
quantized
not here
quantized in 20.24949288368225 seconds
36715 MiB free out of 48676 MiB total
1 self_attn.o_proj
Pruning ...
realigning
initial loss 0.08773843199014664
final loss 0.08273354172706604
quantized
not here
quantized in 19.272284507751465 seconds
36801 MiB free out of 48676 MiB total
1 mlp.gate_proj
Pruning ...
realigning
initial loss 25.727558135986328
final loss 18.473936080932617
quantized
not here
quantized in 54.64858555793762 seconds
36521 MiB free out of 48676 MiB total
1 mlp.up_proj
Pruning ...
realigning
initial loss 14.278097152709961
final loss 13.456363677978516
quantized
not here
quantized in 53.87771987915039 seconds
36241 MiB free out of 48676 MiB total
1 mlp.down_proj
Pruning ...
realigning
initial loss 0.1126033365726471
final loss 0.09226929396390915
quantized
not here
quantized in 55.620386362075806 seconds
35961 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0956418986315839 val loss: 0.0003320495507068699
8309 MiB free out of 48676 MiB total
epoch 1 loss: 0.04830801127536688 val loss: 0.0005926625926804263
8309 MiB free out of 48676 MiB total
epoch 2 loss: 0.020548518854411668 val loss: 0.0007858712124289013
8309 MiB free out of 48676 MiB total
epoch 3 loss: 0.006841140283995628 val loss: 0.0009189236916427035
8309 MiB free out of 48676 MiB total
epoch 4 loss: 0.0026769089135996182 val loss: 0.0008748341351747513
8309 MiB free out of 48676 MiB total
35961 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8309 MiB free out of 48676 MiB total
after cast to cpu
38579 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
2 self_attn.q_proj
Pruning ...
realigning
initial loss 74.73880767822266
final loss 63.034488677978516
quantized
not here
quantized in 22.75564742088318 seconds
36575 MiB free out of 48676 MiB total
2 self_attn.k_proj
Pruning ...
realigning
initial loss 92.23189544677734
final loss 71.81217956542969
quantized
not here
quantized in 22.88983964920044 seconds
36661 MiB free out of 48676 MiB total
2 self_attn.v_proj
Pruning ...
realigning
initial loss 14.237030029296875
final loss 13.633190155029297
quantized
not here
quantized in 19.94627594947815 seconds
36747 MiB free out of 48676 MiB total
2 self_attn.o_proj
Pruning ...
realigning
initial loss 0.20712126791477203
final loss 0.1866941899061203
quantized
not here
quantized in 19.882916688919067 seconds
36811 MiB free out of 48676 MiB total
2 mlp.gate_proj
Pruning ...
realigning
initial loss 37.695343017578125
final loss 35.23109436035156
quantized
not here
quantized in 54.59354329109192 seconds
36531 MiB free out of 48676 MiB total
2 mlp.up_proj
Pruning ...
realigning
initial loss 28.16720199584961
final loss 27.750104904174805
quantized
not here
quantized in 51.93908667564392 seconds
36251 MiB free out of 48676 MiB total
2 mlp.down_proj
Pruning ...
realigning
initial loss 0.15605150163173676
final loss 0.15501798689365387
quantized
not here
quantized in 50.91102719306946 seconds
35971 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.00014442847668760805 val loss: 0.0007368051883531734
8319 MiB free out of 48676 MiB total
epoch 1 loss: 0.00013688217541130143 val loss: 0.0007350856394623406
8319 MiB free out of 48676 MiB total
epoch 2 loss: 0.00013498615908247302 val loss: 0.0007331614178838208
8319 MiB free out of 48676 MiB total
epoch 3 loss: 0.00013390269947421984 val loss: 0.0007318203824979719
8319 MiB free out of 48676 MiB total
epoch 4 loss: 0.00013313274376969275 val loss: 0.0007308287931664381
8319 MiB free out of 48676 MiB total
35971 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8319 MiB free out of 48676 MiB total
after cast to cpu
38579 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
3 self_attn.q_proj
Pruning ...
realigning
initial loss 175.34593200683594
final loss 155.7941436767578
quantized
not here
quantized in 22.776503801345825 seconds
36575 MiB free out of 48676 MiB total
3 self_attn.k_proj
Pruning ...
realigning
initial loss 207.64328002929688
final loss 169.66824340820312
quantized
not here
quantized in 22.516947269439697 seconds
36661 MiB free out of 48676 MiB total
3 self_attn.v_proj
Pruning ...
realigning
initial loss 36.10301971435547
final loss 35.425559997558594
quantized
not here
quantized in 20.197932243347168 seconds
36661 MiB free out of 48676 MiB total
3 self_attn.o_proj
Pruning ...
realigning
initial loss 0.3616723120212555
final loss 0.28115853667259216
quantized
not here
quantized in 20.934626817703247 seconds
36725 MiB free out of 48676 MiB total
3 mlp.gate_proj
Pruning ...
realigning
initial loss 60.49747085571289
final loss 58.33769989013672
quantized
not here
quantized in 54.57942986488342 seconds
36445 MiB free out of 48676 MiB total
3 mlp.up_proj
Pruning ...
realigning
initial loss 46.40015411376953
final loss 46.14289855957031
quantized
not here
quantized in 52.59647560119629 seconds
36165 MiB free out of 48676 MiB total
3 mlp.down_proj
Pruning ...
realigning
initial loss 0.35951176285743713
final loss 0.3566865921020508
quantized
not here
quantized in 49.15838575363159 seconds
35885 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0003629567088410113 val loss: 0.0018869923806050792
9257 MiB free out of 48676 MiB total
epoch 1 loss: 0.00032114023315443774 val loss: 0.0018852181019610725
9257 MiB free out of 48676 MiB total
epoch 2 loss: 0.00031066777319210814 val loss: 0.0018753775730147026
9257 MiB free out of 48676 MiB total
epoch 3 loss: 0.0003061095053453755 val loss: 0.0018675128376344219
9257 MiB free out of 48676 MiB total
epoch 4 loss: 0.0003034804010439984 val loss: 0.0018616983870742843
9257 MiB free out of 48676 MiB total
35885 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
9257 MiB free out of 48676 MiB total
after cast to cpu
38579 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
4 self_attn.q_proj
Pruning ...
realigning
initial loss 150.35292053222656
final loss 138.24395751953125
quantized
not here
quantized in 22.6130850315094 seconds
36575 MiB free out of 48676 MiB total
4 self_attn.k_proj
Pruning ...
realigning
initial loss 171.4586639404297
final loss 149.26687622070312
quantized
not here
quantized in 22.08642601966858 seconds
36661 MiB free out of 48676 MiB total
4 self_attn.v_proj
Pruning ...
realigning
initial loss 34.68959426879883
final loss 34.02410888671875
quantized
not here
quantized in 20.419080018997192 seconds
36747 MiB free out of 48676 MiB total
4 self_attn.o_proj
Pruning ...
realigning
initial loss 0.6784555315971375
final loss 0.5777918100357056
quantized
not here
quantized in 21.010968685150146 seconds
36833 MiB free out of 48676 MiB total
4 mlp.gate_proj
Pruning ...
realigning
initial loss 86.38809967041016
final loss 81.23365783691406
quantized
not here
quantized in 54.403526306152344 seconds
36553 MiB free out of 48676 MiB total
4 mlp.up_proj
Pruning ...
realigning
initial loss 60.215091705322266
final loss 59.6048583984375
quantized
not here
quantized in 52.727251291275024 seconds
36273 MiB free out of 48676 MiB total
4 mlp.down_proj
Pruning ...
realigning
initial loss 0.6638007164001465
final loss 0.6465693116188049
quantized
not here
quantized in 52.29778337478638 seconds
35993 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0005732650515710702 val loss: 0.0021977264404995367
8341 MiB free out of 48676 MiB total
epoch 1 loss: 0.0005440162462946319 val loss: 0.0022009256936144084
8341 MiB free out of 48676 MiB total
epoch 2 loss: 0.0005341159981071542 val loss: 0.002201268755015917
8341 MiB free out of 48676 MiB total
epoch 3 loss: 0.0005288311149342917 val loss: 0.0022003025806043297
8341 MiB free out of 48676 MiB total
epoch 4 loss: 0.0005252558371466876 val loss: 0.002198866699473001
8341 MiB free out of 48676 MiB total
35993 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8341 MiB free out of 48676 MiB total
after cast to cpu
38579 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
5 self_attn.q_proj
Pruning ...
realigning
initial loss 171.9491729736328
final loss 154.53695678710938
quantized
not here
quantized in 22.71438193321228 seconds
36575 MiB free out of 48676 MiB total
5 self_attn.k_proj
Pruning ...
realigning
initial loss 214.30938720703125
final loss 177.16014099121094
quantized
not here
quantized in 21.894585847854614 seconds
36661 MiB free out of 48676 MiB total
5 self_attn.v_proj
Pruning ...
realigning
initial loss 40.240074157714844
final loss 39.76367950439453
quantized
not here
quantized in 20.22534441947937 seconds
36661 MiB free out of 48676 MiB total
5 self_attn.o_proj
Pruning ...
realigning
initial loss 1.2311172485351562
final loss 1.0308657884597778
quantized
not here
quantized in 20.99730372428894 seconds
36725 MiB free out of 48676 MiB total
5 mlp.gate_proj
Pruning ...
realigning
initial loss 105.74771881103516
final loss 101.08625793457031
quantized
not here
quantized in 53.59604525566101 seconds
36445 MiB free out of 48676 MiB total
5 mlp.up_proj
Pruning ...
realigning
initial loss 74.73980712890625
final loss 74.22325134277344
quantized
not here
quantized in 52.72990822792053 seconds
36165 MiB free out of 48676 MiB total
5 mlp.down_proj
Pruning ...
realigning
initial loss 1.1530262231826782
final loss 1.1239897012710571
quantized
not here
quantized in 52.943700551986694 seconds
36057 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0009193084979415289 val loss: 0.002614952973090112
9429 MiB free out of 48676 MiB total
