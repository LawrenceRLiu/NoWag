/home/lliu/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.40s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.16it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.06it/s]
Model loaded. LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
4096
Loading tokenizer for meta-llama/Llama-2-7b-hf
Starting...
40099 MiB free out of 48676 MiB total
attention_mask torch.Size([1, 1, 4096, 4096]) cuda:6 torch.float16
position_ids torch.Size([1, 4096]) cuda:6 torch.int64
position_ids torch.Size([1, 4096])
attention_mask torch.Size([1, 1, 4096, 4096])
Ready.
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
layer0: self_attn.q_proj
norm_0 tensor([0.8848, 0.6528, 0.2561,  ..., 0.5371, 0.5562, 0.4512], device='cuda:6',
       dtype=torch.float16)
norm_1 tensor([0.7427, 1.1660, 1.0791,  ..., 1.8926, 1.8721, 1.3779], device='cuda:6',
       dtype=torch.float16)
256
iter 0, train loss 1.2158797979354858, val loss None
iter 10, train loss 1.0846924781799316, val loss None
iter 20, train loss 1.0027239322662354, val loss None
iter 30, train loss 1.0504851341247559, val loss None
iter 40, train loss 1.007501244544983, val loss None
iter 50, train loss 0.9262599945068359, val loss None
iter 60, train loss 0.8700511455535889, val loss None
iter 70, train loss 0.8021441698074341, val loss None
iter 80, train loss 0.7577221393585205, val loss None
iter 90, train loss 0.7269337177276611, val loss None
best loss 0.7169129252433777
layer0: self_attn.k_proj
norm_0 tensor([1.1758, 0.8267, 0.3096,  ..., 0.5889, 0.6553, 0.5815], device='cuda:6',
       dtype=torch.float16)
norm_1 tensor([0.8984, 1.2832, 1.1631,  ..., 0.9399, 1.2178, 0.8291], device='cuda:6',
       dtype=torch.float16)
256
iter 0, train loss 0.9143508672714233, val loss None
iter 10, train loss 0.7954849004745483, val loss None
iter 20, train loss 0.7319494485855103, val loss None
iter 30, train loss 0.7112715244293213, val loss None
iter 40, train loss 0.6914941668510437, val loss None
iter 50, train loss 0.6711727380752563, val loss None
iter 60, train loss 0.6570463180541992, val loss None
iter 70, train loss 0.6262837052345276, val loss None
iter 80, train loss 0.6308207511901855, val loss None
iter 90, train loss 0.5961980223655701, val loss None
best loss 0.5836174488067627
layer0: self_attn.v_proj
norm_0 tensor([0.7236, 0.7617, 0.5034,  ..., 0.7656, 0.7676, 0.6851], device='cuda:6',
       dtype=torch.float16)
norm_1 tensor([0.6997, 0.7095, 0.6719,  ..., 0.6890, 0.7085, 0.7065], device='cuda:6',
       dtype=torch.float16)
256
iter 0, train loss 0.09465141594409943, val loss None
iter 10, train loss 0.08290598541498184, val loss None
iter 20, train loss 0.08008445054292679, val loss None
iter 30, train loss 0.07498376071453094, val loss None
iter 40, train loss 0.07240993529558182, val loss None
iter 50, train loss 0.07158765196800232, val loss None
iter 60, train loss 0.07160775363445282, val loss None
iter 70, train loss 0.06971672177314758, val loss None
iter 80, train loss 0.06978359073400497, val loss None
iter 90, train loss 0.06887075304985046, val loss None
best loss 0.06874524056911469
layer0: self_attn.o_proj
norm_0 tensor([0.2800, 0.2820, 0.2651,  ..., 0.2622, 0.2673, 0.2649], device='cuda:6',
       dtype=torch.float16)
norm_1 tensor([0.8887, 0.9336, 0.9106,  ..., 0.9902, 0.9199, 0.9688], device='cuda:6',
       dtype=torch.float16)
256
iter 0, train loss 0.010639030486345291, val loss None
iter 10, train loss 0.00962851196527481, val loss None
iter 20, train loss 0.009409859776496887, val loss None
iter 30, train loss 0.008905657567083836, val loss None
iter 40, train loss 0.008444427512586117, val loss None
iter 50, train loss 0.008557084947824478, val loss None
iter 60, train loss 0.008340716361999512, val loss None
iter 70, train loss 0.008116870187222958, val loss None
iter 80, train loss 0.008078556507825851, val loss None
iter 90, train loss 0.007998190820217133, val loss None
best loss 0.007951216772198677
layer0: mlp.gate_proj
norm_0 tensor([1.6787, 1.7383, 1.7090,  ..., 1.7266, 1.7061, 1.7061], device='cuda:6',
       dtype=torch.float16)
norm_1 tensor([0.6108, 0.6123, 0.6055,  ..., 0.6016, 0.6216, 0.6255], device='cuda:6',
       dtype=torch.float16)
256
iter 0, train loss 3.913414478302002, val loss None
iter 10, train loss 4.093822479248047, val loss None
iter 20, train loss 3.916024684906006, val loss None
iter 30, train loss 3.8952372074127197, val loss None
iter 40, train loss 3.885502338409424, val loss None
iter 50, train loss 3.89107346534729, val loss None
iter 60, train loss 3.8864753246307373, val loss None
iter 70, train loss 3.865142583847046, val loss None
iter 80, train loss 3.867575168609619, val loss None
iter 90, train loss 3.85846209526062, val loss None
best loss 3.7355499267578125
layer0: mlp.up_proj
norm_0 tensor([1.6846, 1.6572, 1.7002,  ..., 1.7119, 1.7070, 1.6826], device='cuda:6',
       dtype=torch.float16)
norm_1 tensor([0.6309, 0.6152, 0.6260,  ..., 0.5967, 0.5894, 0.6016], device='cuda:6',
       dtype=torch.float16)
256
iter 0, train loss 3.55928373336792, val loss None
iter 10, train loss 3.775834321975708, val loss None
iter 20, train loss 3.5727782249450684, val loss None
iter 30, train loss 3.5639257431030273, val loss None
iter 40, train loss 3.5568959712982178, val loss None
iter 50, train loss 3.5446279048919678, val loss None
iter 60, train loss 3.543055534362793, val loss None
iter 70, train loss 3.534613847732544, val loss None
iter 80, train loss 3.532475233078003, val loss None
iter 90, train loss 3.5308468341827393, val loss None
best loss 3.4052915573120117
layer0: mlp.down_proj
norm_0 tensor([1.0811, 1.0771, 1.0850,  ..., 1.0762, 1.0713, 1.0908], device='cuda:6',
       dtype=torch.float16)
norm_1 tensor([1.6289, 1.6006, 1.6270,  ..., 1.6133, 1.6123, 1.5957], device='cuda:6',
       dtype=torch.float16)
256
iter 0, train loss 0.010795352049171925, val loss None
iter 10, train loss 0.012044742703437805, val loss None
iter 20, train loss 0.011671962216496468, val loss None
iter 30, train loss 0.011351259425282478, val loss None
iter 40, train loss 0.011232768185436726, val loss None
iter 50, train loss 0.01116756908595562, val loss None
iter 60, train loss 0.01105300523340702, val loss None
iter 70, train loss 0.011003783904016018, val loss None
iter 80, train loss 0.010975344106554985, val loss None
iter 90, train loss 0.010945754125714302, val loss None
best loss 0.010184906423091888
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.quantizer.norms_1', 'self_attn.q_proj.quantizer.norms_0', 'self_attn.q_proj.quantizer.codebook', 'self_attn.k_proj.quantizer.norms_1', 'self_attn.k_proj.quantizer.norms_0', 'self_attn.k_proj.quantizer.codebook', 'self_attn.v_proj.quantizer.norms_1', 'self_attn.v_proj.quantizer.norms_0', 'self_attn.v_proj.quantizer.codebook', 'self_attn.o_proj.quantizer.norms_1', 'self_attn.o_proj.quantizer.norms_0', 'self_attn.o_proj.quantizer.codebook', 'mlp.gate_proj.quantizer.norms_1', 'mlp.gate_proj.quantizer.norms_0', 'mlp.gate_proj.quantizer.codebook', 'mlp.up_proj.quantizer.norms_1', 'mlp.up_proj.quantizer.norms_0', 'mlp.up_proj.quantizer.codebook', 'mlp.down_proj.quantizer.norms_1', 'mlp.down_proj.quantizer.norms_0', 'mlp.down_proj.quantizer.codebook', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 8.277617098428891e-06
8111 MiB free out of 48676 MiB total
epoch 1 loss: 6.741138960819626e-06
8111 MiB free out of 48676 MiB total
epoch 2 loss: 6.528668059502252e-06
8111 MiB free out of 48676 MiB total
epoch 3 loss: 6.3885239960370654e-06
8111 MiB free out of 48676 MiB total
epoch 4 loss: 6.264182164983367e-06
8111 MiB free out of 48676 MiB total
40099 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
8111 MiB free out of 48676 MiB total
after cast to cpu
37397 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
layer1: self_attn.q_proj
norm_0 tensor([1.9258, 1.9043, 1.8604,  ..., 1.6787, 1.9346, 1.6709], device='cuda:6',
       dtype=torch.float16)
norm_1 tensor([1.1504, 0.9507, 1.1025,  ..., 0.2090, 0.2136, 0.2512], device='cuda:6',
       dtype=torch.float16)
256
iter 0, train loss 17.203693389892578, val loss None
iter 10, train loss 14.338228225708008, val loss None
iter 20, train loss 16.579362869262695, val loss None
iter 30, train loss 14.616277694702148, val loss None
iter 40, train loss 14.05722427368164, val loss None
iter 50, train loss 13.412734985351562, val loss None
iter 60, train loss 13.434897422790527, val loss None
iter 70, train loss 13.54958724975586, val loss None
iter 80, train loss 13.482721328735352, val loss None
iter 90, train loss 13.37403678894043, val loss None
best loss 13.352876663208008
layer1: self_attn.k_proj
norm_0 tensor([2.0254, 1.9668, 2.0371,  ..., 1.6631, 1.8975, 1.8086], device='cuda:6',
       dtype=torch.float16)
norm_1 tensor([0.9346, 1.0586, 1.0898,  ..., 0.3875, 0.3777, 0.3123], device='cuda:6',
       dtype=torch.float16)
256
iter 0, train loss 17.70287322998047, val loss None
iter 10, train loss 15.017335891723633, val loss None
iter 20, train loss 14.44526195526123, val loss None
iter 30, train loss 13.675546646118164, val loss None
iter 40, train loss 13.926172256469727, val loss None
iter 50, train loss 14.023147583007812, val loss None
iter 60, train loss 14.199722290039062, val loss None
iter 70, train loss 13.553361892700195, val loss None
iter 80, train loss 13.377462387084961, val loss None
iter 90, train loss 13.250850677490234, val loss None
best loss 13.202320098876953
layer1: self_attn.v_proj
norm_0 tensor([0.5439, 0.5366, 0.5508,  ..., 0.6504, 0.5752, 0.6440], device='cuda:6',
       dtype=torch.float16)
norm_1 tensor([1.3213, 1.3359, 1.2393,  ..., 0.5474, 0.5635, 0.5596], device='cuda:6',
       dtype=torch.float16)
256
iter 0, train loss 0.9291166663169861, val loss None
iter 10, train loss 0.9578298330307007, val loss None
iter 20, train loss 0.8865253925323486, val loss None
iter 30, train loss 0.8645322322845459, val loss None
iter 40, train loss 0.8552848100662231, val loss None
iter 50, train loss 0.8438889384269714, val loss None
iter 60, train loss 0.8386945128440857, val loss None
iter 70, train loss 0.830859363079071, val loss None
iter 80, train loss 0.8308244943618774, val loss None
iter 90, train loss 0.8259369134902954, val loss None
best loss 0.8251498937606812
layer1: self_attn.o_proj
norm_0 tensor([0.8604, 0.8032, 0.6895,  ..., 0.2113, 0.2167, 0.2192], device='cuda:6',
       dtype=torch.float16)
norm_1 tensor([1.0117, 1.0215, 0.9888,  ..., 0.9995, 0.9941, 0.9839], device='cuda:6',
       dtype=torch.float16)
256
iter 0, train loss 0.08557378500699997, val loss None
iter 10, train loss 0.0821322575211525, val loss None
iter 20, train loss 0.079596608877182, val loss None
iter 30, train loss 0.07722003757953644, val loss None
iter 40, train loss 0.07787340134382248, val loss None
iter 50, train loss 0.07741433382034302, val loss None
iter 60, train loss 0.076294906437397, val loss None
iter 70, train loss 0.0763738751411438, val loss None
iter 80, train loss 0.075947105884552, val loss None
iter 90, train loss 0.07571753859519958, val loss None
best loss 0.07534442842006683
layer1: mlp.gate_proj
norm_0 tensor([1.9492, 1.9355, 1.8818,  ..., 1.9443, 1.8994, 1.8916], device='cuda:6',
       dtype=torch.float16)
norm_1 tensor([0.5796, 0.5811, 0.6069,  ..., 0.6011, 0.5981, 0.6099], device='cuda:6',
       dtype=torch.float16)
256
iter 0, train loss 16.483089447021484, val loss None
iter 10, train loss 17.67661476135254, val loss None
iter 20, train loss 17.123302459716797, val loss None
iter 30, train loss 16.900421142578125, val loss None
iter 40, train loss 16.804969787597656, val loss None
iter 50, train loss 16.795419692993164, val loss None
iter 60, train loss 16.78487205505371, val loss None
iter 70, train loss 16.835880279541016, val loss None
iter 80, train loss 16.791725158691406, val loss None
iter 90, train loss 16.753633499145508, val loss None
best loss 15.464435577392578
layer1: mlp.up_proj
norm_0 tensor([1.7627, 1.8057, 1.8271,  ..., 1.8008, 1.8066, 1.8203], device='cuda:6',
       dtype=torch.float16)
norm_1 tensor([0.6089, 0.6025, 0.6172,  ..., 0.6206, 0.6221, 0.6230], device='cuda:6',
       dtype=torch.float16)
256
iter 0, train loss 13.269944190979004, val loss None
iter 10, train loss 13.752877235412598, val loss None
iter 20, train loss 13.452796936035156, val loss None
iter 30, train loss 13.513496398925781, val loss None
iter 40, train loss 13.494794845581055, val loss None
iter 50, train loss 13.48374080657959, val loss None
iter 60, train loss 13.477538108825684, val loss None
iter 70, train loss 13.481563568115234, val loss None
iter 80, train loss 13.455235481262207, val loss None
iter 90, train loss 13.443851470947266, val loss None
best loss 13.129438400268555
layer1: mlp.down_proj
norm_0 tensor([1.1104, 1.1094, 1.1230,  ..., 1.1250, 1.1289, 1.1299], device='cuda:6',
       dtype=torch.float16)
norm_1 tensor([1.6289, 1.6357, 1.6455,  ..., 1.6387, 1.6582, 1.6318], device='cuda:6',
       dtype=torch.float16)
256
iter 0, train loss 0.08828063309192657, val loss None
iter 10, train loss 0.1937798261642456, val loss None
iter 20, train loss 0.2958511412143707, val loss None
iter 30, train loss 0.19862890243530273, val loss None
iter 40, train loss 0.16045725345611572, val loss None
iter 50, train loss 0.16130037605762482, val loss None
iter 60, train loss 0.15214921534061432, val loss None
iter 70, train loss 0.12267037481069565, val loss None
iter 80, train loss 0.11529897153377533, val loss None
iter 90, train loss 0.11278682947158813, val loss None
best loss 0.08828063309192657
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.quantizer.norms_1', 'self_attn.q_proj.quantizer.norms_0', 'self_attn.q_proj.quantizer.codebook', 'self_attn.k_proj.quantizer.norms_1', 'self_attn.k_proj.quantizer.norms_0', 'self_attn.k_proj.quantizer.codebook', 'self_attn.v_proj.quantizer.norms_1', 'self_attn.v_proj.quantizer.norms_0', 'self_attn.v_proj.quantizer.codebook', 'self_attn.o_proj.quantizer.norms_1', 'self_attn.o_proj.quantizer.norms_0', 'self_attn.o_proj.quantizer.codebook', 'mlp.gate_proj.quantizer.norms_1', 'mlp.gate_proj.quantizer.norms_0', 'mlp.gate_proj.quantizer.codebook', 'mlp.up_proj.quantizer.norms_1', 'mlp.up_proj.quantizer.norms_0', 'mlp.up_proj.quantizer.codebook', 'mlp.down_proj.quantizer.norms_1', 'mlp.down_proj.quantizer.norms_0', 'mlp.down_proj.quantizer.codebook', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.006549813039782748
6751 MiB free out of 48676 MiB total
epoch 1 loss: 0.0009980197474419583
6751 MiB free out of 48676 MiB total
epoch 2 loss: 0.0003684217488739705
6751 MiB free out of 48676 MiB total
Terminated
