/home/lliu/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.37s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.19it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.09it/s]
Model loaded. LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
4096
Loading tokenizer for meta-llama/Llama-2-7b-hf
Starting...
39011 MiB free out of 48676 MiB total
attention_mask torch.Size([1, 1, 4096, 4096]) cuda:5 torch.float16
position_ids torch.Size([1, 4096]) cuda:5 torch.int64
position_ids torch.Size([1, 4096])
attention_mask torch.Size([1, 1, 4096, 4096])
Ready.
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj'], ['self_attn.o_proj'], ['mlp.up_proj', 'mlp.gate_proj'], ['mlp.down_proj']]
finished adding batches
subset {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
0 self_attn.k_proj
Pruning ...
realigning
initial loss 1.8915133476257324
final loss 1.5419642925262451
quantized
not here
quantized in 21.571797847747803 seconds
37255 MiB free out of 48676 MiB total
0 self_attn.v_proj
Pruning ...
realigning
initial loss 0.11391407996416092
final loss 0.09255972504615784
quantized
not here
quantized in 18.590261220932007 seconds
37319 MiB free out of 48676 MiB total
0 self_attn.q_proj
Pruning ...
realigning
initial loss 1.56924569606781
final loss 1.1642513275146484
quantized
not here
quantized in 19.641972541809082 seconds
37383 MiB free out of 48676 MiB total
finished: {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 27648
optimizing the following parameters: dict_keys(['self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 152051712
n accumulation steps: 1
n batches: 128
epoch 0 loss: 6.124211113789357e-07 val loss: 3.560433654570261e-07
7339 MiB free out of 48676 MiB total
epoch 1 loss: 2.9747138197144807e-07 val loss: 2.5085913701872187e-07
5291 MiB free out of 48676 MiB total
epoch 2 loss: 2.2992374648644898e-07 val loss: 2.1107953074306351e-07
5291 MiB free out of 48676 MiB total
epoch 3 loss: 2.007969511774732e-07 val loss: 1.9121411565237167e-07
5291 MiB free out of 48676 MiB total
epoch 4 loss: 1.851115805084902e-07 val loss: 1.7939570540193017e-07
5291 MiB free out of 48676 MiB total
finished adding batches
subset {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
0 self_attn.o_proj
Pruning ...
realigning
initial loss 0.016807274892926216
final loss 0.013278797268867493
quantized
not here
quantized in 20.729443788528442 seconds
37479 MiB free out of 48676 MiB total
finished: {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 36864
optimizing the following parameters: dict_keys(['mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135274496
n accumulation steps: 1
n batches: 128
epoch 0 loss: 1.6526248556303358e-06 val loss: 1.5170449358947735e-06
7241 MiB free out of 48676 MiB total
epoch 1 loss: 1.4523200029259442e-06 val loss: 1.3720437905817562e-06
7241 MiB free out of 48676 MiB total
epoch 2 loss: 1.330203279081843e-06 val loss: 1.2698266758093268e-06
7241 MiB free out of 48676 MiB total
epoch 3 loss: 1.241809218122114e-06 val loss: 1.1927951959478378e-06
7241 MiB free out of 48676 MiB total
epoch 4 loss: 1.1745293555520675e-06 val loss: 1.133874881986685e-06
7241 MiB free out of 48676 MiB total
finished adding batches
subset {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
0 mlp.up_proj
Pruning ...
realigning
initial loss 3.701059341430664
final loss 3.4530630111694336
quantized
not here
quantized in 58.24631428718567 seconds
36369 MiB free out of 48676 MiB total
0 mlp.gate_proj
Pruning ...
realigning
initial loss 5.41966438293457
final loss 4.636141300201416
quantized
not here
quantized in 56.579376459121704 seconds
36111 MiB free out of 48676 MiB total
finished: {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 69120
optimizing the following parameters: dict_keys(['mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 45096960
n accumulation steps: 1
n batches: 128
epoch 0 loss: 2.201201058760205e-06 val loss: 1.9191230506976353e-06
8457 MiB free out of 48676 MiB total
epoch 1 loss: 1.8086624713475885e-06 val loss: 1.7027637539968055e-06
6409 MiB free out of 48676 MiB total
epoch 2 loss: 1.6713625212716465e-06 val loss: 1.6144652974503515e-06
6409 MiB free out of 48676 MiB total
epoch 3 loss: 1.6033232093093375e-06 val loss: 1.5609418895223826e-06
6409 MiB free out of 48676 MiB total
epoch 4 loss: 1.558539517887425e-06 val loss: 1.5225836804688697e-06
6409 MiB free out of 48676 MiB total
finished adding batches
subset {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
0 mlp.down_proj
Pruning ...
realigning
initial loss 0.0175836980342865
final loss 0.014217210933566093
quantized
not here
quantized in 50.46501326560974 seconds
35173 MiB free out of 48676 MiB total
finished: {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 3.831954686006611e-06 val loss: 3.538544433467905e-06
6255 MiB free out of 48676 MiB total
epoch 1 loss: 3.506268729580597e-06 val loss: 3.4033216849138626e-06
6255 MiB free out of 48676 MiB total
epoch 2 loss: 3.4074282648788312e-06 val loss: 3.328629077259393e-06
6255 MiB free out of 48676 MiB total
epoch 3 loss: 3.3467681781473857e-06 val loss: 3.2787805395173564e-06
6255 MiB free out of 48676 MiB total
epoch 4 loss: 3.3052716332804266e-06 val loss: 3.2441167689967187e-06
6255 MiB free out of 48676 MiB total
35173 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
6255 MiB free out of 48676 MiB total
after cast to cpu
37865 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj'], ['self_attn.o_proj'], ['mlp.up_proj', 'mlp.gate_proj'], ['mlp.down_proj']]
finished adding batches
subset {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
1 self_attn.k_proj
Pruning ...
realigning
initial loss 24.864742279052734
final loss 19.714046478271484
quantized
not here
quantized in 21.05607509613037 seconds
37863 MiB free out of 48676 MiB total
1 self_attn.v_proj
Pruning ...
realigning
initial loss 1.0546290874481201
final loss 0.9229445457458496
quantized
not here
quantized in 19.173576593399048 seconds
37799 MiB free out of 48676 MiB total
1 self_attn.q_proj
Pruning ...
realigning
initial loss 23.2281551361084
final loss 17.99720001220703
quantized
not here
quantized in 20.354122161865234 seconds
37735 MiB free out of 48676 MiB total
finished: {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 27648
optimizing the following parameters: dict_keys(['self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 152051712
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0006613294802022551 val loss: 7.251944498420926e-05
7187 MiB free out of 48676 MiB total
epoch 1 loss: 0.0002002435611814235 val loss: 7.729386925348081e-05
7187 MiB free out of 48676 MiB total
epoch 2 loss: 0.00018462387239281952 val loss: 7.522784153479734e-05
7187 MiB free out of 48676 MiB total
epoch 3 loss: 0.00023962715251713007 val loss: 7.295552359209978e-05
7187 MiB free out of 48676 MiB total
epoch 4 loss: 0.000181687645984141 val loss: 7.363570011875709e-05
7187 MiB free out of 48676 MiB total
finished adding batches
subset {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
1 self_attn.o_proj
Pruning ...
realigning
initial loss 0.08045915514230728
final loss 0.0749443769454956
quantized
not here
quantized in 20.914422273635864 seconds
37447 MiB free out of 48676 MiB total
finished: {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 36864
optimizing the following parameters: dict_keys(['mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135274496
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.00023155757158122015 val loss: 2.415254448351334e-05
7241 MiB free out of 48676 MiB total
epoch 1 loss: 0.00012615846698338373 val loss: 2.4211245658989355e-05
7241 MiB free out of 48676 MiB total
epoch 2 loss: 9.885386172214794e-05 val loss: 2.4348900865334144e-05
7241 MiB free out of 48676 MiB total
epoch 3 loss: 7.774966105245085e-05 val loss: 2.4504368411726318e-05
7241 MiB free out of 48676 MiB total
epoch 4 loss: 6.282065204743503e-05 val loss: 2.4704598331481975e-05
7241 MiB free out of 48676 MiB total
finished adding batches
subset {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
1 mlp.up_proj
Pruning ...
realigning
initial loss 13.815547943115234
final loss 13.2343111038208
quantized
not here
quantized in 57.95439314842224 seconds
36423 MiB free out of 48676 MiB total
1 mlp.gate_proj
Pruning ...
realigning
initial loss 21.96585464477539
final loss 17.89333724975586
quantized
not here
quantized in 54.651694536209106 seconds
36079 MiB free out of 48676 MiB total
finished: {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 69120
optimizing the following parameters: dict_keys(['mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 45096960
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.15227855305420235 val loss: 7.721691144979559e-05
8425 MiB free out of 48676 MiB total
epoch 1 loss: 0.14265254337806255 val loss: 7.385196522591286e-05
6377 MiB free out of 48676 MiB total
epoch 2 loss: 0.13329138210974634 val loss: 7.130485482775839e-05
6377 MiB free out of 48676 MiB total
epoch 3 loss: 0.12411796260857955 val loss: 6.946134681129479e-05
6377 MiB free out of 48676 MiB total
epoch 4 loss: 0.11517708467727061 val loss: 6.818893143645255e-05
6377 MiB free out of 48676 MiB total
finished adding batches
subset {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
1 mlp.down_proj
Pruning ...
realigning
initial loss 0.08815260231494904
final loss 0.08815260231494904
quantized
not here
quantized in 53.306946992874146 seconds
36079 MiB free out of 48676 MiB total
finished: {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0001903567245449267 val loss: 6.990166184550617e-05
6153 MiB free out of 48676 MiB total
epoch 1 loss: 2.4973001359285263e-05 val loss: 6.865578598080901e-05
6153 MiB free out of 48676 MiB total
epoch 2 loss: 2.4231857992162986e-05 val loss: 6.891166640343727e-05
6153 MiB free out of 48676 MiB total
epoch 3 loss: 2.418580466212461e-05 val loss: 6.897124603710836e-05
6153 MiB free out of 48676 MiB total
epoch 4 loss: 2.4077012866996483e-05 val loss: 6.913383231221815e-05
6153 MiB free out of 48676 MiB total
36079 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
6153 MiB free out of 48676 MiB total
after cast to cpu
37725 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj'], ['self_attn.o_proj'], ['mlp.up_proj', 'mlp.gate_proj'], ['mlp.down_proj']]
finished adding batches
subset {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
2 self_attn.k_proj
Pruning ...
realigning
initial loss 90.03482055664062
final loss 71.28511047363281
quantized
not here
quantized in 22.479306936264038 seconds
37723 MiB free out of 48676 MiB total
2 self_attn.v_proj
Pruning ...
realigning
initial loss 14.029500007629395
final loss 13.418258666992188
quantized
not here
quantized in 18.56728458404541 seconds
37659 MiB free out of 48676 MiB total
2 self_attn.q_proj
Pruning ...
realigning
initial loss 74.0782241821289
final loss 62.59196472167969
quantized
not here
quantized in 20.856037616729736 seconds
37531 MiB free out of 48676 MiB total
finished: {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 27648
optimizing the following parameters: dict_keys(['self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 152051712
n accumulation steps: 1
n batches: 128
epoch 0 loss: 2.1920975477485172e-05 val loss: 0.0002758919999905629
7375 MiB free out of 48676 MiB total
epoch 1 loss: 1.3040519682760987e-05 val loss: 0.0002764104137895629
7375 MiB free out of 48676 MiB total
epoch 2 loss: 1.1146037039111434e-05 val loss: 0.0002771681665763026
7375 MiB free out of 48676 MiB total
epoch 3 loss: 1.0220248732650816e-05 val loss: 0.00027820939612865914
7375 MiB free out of 48676 MiB total
epoch 4 loss: 9.641817136696318e-06 val loss: 0.00027938457242271397
7375 MiB free out of 48676 MiB total
finished adding batches
subset {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
2 self_attn.o_proj
Pruning ...
realigning
initial loss 0.06671703606843948
final loss 0.06432859599590302
quantized
not here
quantized in 20.596935272216797 seconds
37361 MiB free out of 48676 MiB total
finished: {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 36864
optimizing the following parameters: dict_keys(['mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135274496
n accumulation steps: 1
n batches: 128
epoch 0 loss: 1.0826760366455801e-05 val loss: 0.00025897755404002964
8275 MiB free out of 48676 MiB total
epoch 1 loss: 9.654401075920305e-06 val loss: 0.00025941836429410614
6227 MiB free out of 48676 MiB total
epoch 2 loss: 9.03192577794698e-06 val loss: 0.00025961612391256494
6227 MiB free out of 48676 MiB total
epoch 3 loss: 8.63766024394863e-06 val loss: 0.00025973986248573055
6227 MiB free out of 48676 MiB total
epoch 4 loss: 8.363346790218884e-06 val loss: 0.00025982905845012283
6227 MiB free out of 48676 MiB total
finished adding batches
subset {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
2 mlp.up_proj
Pruning ...
realigning
initial loss 29.359020233154297
final loss 29.05937957763672
quantized
not here
quantized in 51.192877769470215 seconds
36337 MiB free out of 48676 MiB total
2 mlp.gate_proj
Pruning ...
realigning
initial loss 37.66706466674805
final loss 36.66929626464844
quantized
not here
quantized in 52.17660045623779 seconds
35993 MiB free out of 48676 MiB total
finished: {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 69120
optimizing the following parameters: dict_keys(['mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 45096960
n accumulation steps: 1
n batches: 128
epoch 0 loss: 6.18554828406559e-05 val loss: 0.00015419222563650692
8339 MiB free out of 48676 MiB total
epoch 1 loss: 5.9360958857723745e-05 val loss: 0.00015350346075138077
6291 MiB free out of 48676 MiB total
epoch 2 loss: 5.762688118693404e-05 val loss: 0.00015319036265282193
6291 MiB free out of 48676 MiB total
epoch 3 loss: 5.636732751668205e-05 val loss: 0.0001531251418782631
6291 MiB free out of 48676 MiB total
epoch 4 loss: 5.541748907944566e-05 val loss: 0.00015316717599489493
6291 MiB free out of 48676 MiB total
finished adding batches
subset {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
2 mlp.down_proj
Pruning ...
realigning
initial loss 0.1379254162311554
final loss 0.1379254162311554
quantized
not here
quantized in 52.784611225128174 seconds
35993 MiB free out of 48676 MiB total
finished: {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 4.413330128727466e-05 val loss: 0.00011558251162568922
6645 MiB free out of 48676 MiB total
epoch 1 loss: 4.352287157871615e-05 val loss: 0.0001159920429927297
6645 MiB free out of 48676 MiB total
epoch 2 loss: 4.341707082744506e-05 val loss: 0.00011622184956650017
6645 MiB free out of 48676 MiB total
epoch 3 loss: 4.3349684290205914e-05 val loss: 0.00011634517522907117
6645 MiB free out of 48676 MiB total
epoch 4 loss: 4.329743339326342e-05 val loss: 0.00011641447690635687
6645 MiB free out of 48676 MiB total
35993 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
6645 MiB free out of 48676 MiB total
after cast to cpu
37897 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj'], ['self_attn.o_proj'], ['mlp.up_proj', 'mlp.gate_proj'], ['mlp.down_proj']]
finished adding batches
subset {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
3 self_attn.k_proj
Pruning ...
realigning
initial loss 205.14120483398438
final loss 173.57424926757812
quantized
not here
quantized in 23.732005834579468 seconds
37809 MiB free out of 48676 MiB total
3 self_attn.v_proj
Pruning ...
realigning
initial loss 37.44446563720703
final loss 36.62053298950195
quantized
not here
quantized in 20.655348777770996 seconds
37745 MiB free out of 48676 MiB total
3 self_attn.q_proj
Pruning ...
realigning
initial loss 179.91513061523438
final loss 161.4221649169922
quantized
not here
quantized in 22.036357164382935 seconds
37681 MiB free out of 48676 MiB total
finished: {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 27648
optimizing the following parameters: dict_keys(['self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 152051712
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.00012212165546543474 val loss: 0.0007482735818484798
7209 MiB free out of 48676 MiB total
epoch 1 loss: 9.01187204362941e-05 val loss: 0.0007393750493065454
7209 MiB free out of 48676 MiB total
epoch 2 loss: 7.89710887261208e-05 val loss: 0.000732676031475421
7209 MiB free out of 48676 MiB total
epoch 3 loss: 7.230285660853042e-05 val loss: 0.0007264448504429311
7209 MiB free out of 48676 MiB total
epoch 4 loss: 6.777630903798126e-05 val loss: 0.0007208346796687692
7209 MiB free out of 48676 MiB total
finished adding batches
subset {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
3 self_attn.o_proj
Pruning ...
realigning
initial loss 0.9192160367965698
final loss 0.546825647354126
quantized
not here
quantized in 24.948843240737915 seconds
37597 MiB free out of 48676 MiB total
finished: {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 36864
optimizing the following parameters: dict_keys(['mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135274496
n accumulation steps: 1
n batches: 128
epoch 0 loss: 6.440620884973214e-05 val loss: 0.0006739070631738286
7263 MiB free out of 48676 MiB total
epoch 1 loss: 5.421899348334591e-05 val loss: 0.0006735069728165399
7263 MiB free out of 48676 MiB total
epoch 2 loss: 5.058084246911676e-05 val loss: 0.0006731883768225089
7263 MiB free out of 48676 MiB total
epoch 3 loss: 4.831796314874737e-05 val loss: 0.0006729868582624476
7263 MiB free out of 48676 MiB total
epoch 4 loss: 4.666280739229478e-05 val loss: 0.0006728329026373103
7263 MiB free out of 48676 MiB total
finished adding batches
subset {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
3 mlp.up_proj
Pruning ...
realigning
initial loss 41.92567443847656
final loss 41.57143783569336
quantized
not here
quantized in 56.79161548614502 seconds
36509 MiB free out of 48676 MiB total
3 mlp.gate_proj
Pruning ...
realigning
initial loss 56.13510513305664
final loss 52.81269836425781
quantized
not here
quantized in 56.967549085617065 seconds
36229 MiB free out of 48676 MiB total
finished: {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 69120
optimizing the following parameters: dict_keys(['mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 45096960
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.00011148825353757275 val loss: 0.00039681893031229265
8575 MiB free out of 48676 MiB total
epoch 1 loss: 0.00010523936640538523 val loss: 0.0003964220577472588
6527 MiB free out of 48676 MiB total
epoch 2 loss: 0.00010209692027274286 val loss: 0.0003963735252909828
6527 MiB free out of 48676 MiB total
epoch 3 loss: 9.992793980018178e-05 val loss: 0.0003967673492297763
6527 MiB free out of 48676 MiB total
epoch 4 loss: 9.824901627553118e-05 val loss: 0.0003971583446400473
6527 MiB free out of 48676 MiB total
finished adding batches
subset {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
3 mlp.down_proj
Pruning ...
realigning
initial loss 0.328909695148468
final loss 0.30780163407325745
quantized
not here
quantized in 58.752676010131836 seconds
35291 MiB free out of 48676 MiB total
finished: {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 8.853706543732187e-05 val loss: 0.0002524576239011367
6117 MiB free out of 48676 MiB total
epoch 1 loss: 8.534690874739681e-05 val loss: 0.0002522404856790672
6117 MiB free out of 48676 MiB total
epoch 2 loss: 8.472653843227818e-05 val loss: 0.00025167325566144427
6117 MiB free out of 48676 MiB total
epoch 3 loss: 8.43578478679774e-05 val loss: 0.00025107678993663285
6117 MiB free out of 48676 MiB total
epoch 4 loss: 8.408017458805261e-05 val loss: 0.0002505840975572937
6117 MiB free out of 48676 MiB total
35291 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
6117 MiB free out of 48676 MiB total
after cast to cpu
37897 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj'], ['self_attn.o_proj'], ['mlp.up_proj', 'mlp.gate_proj'], ['mlp.down_proj']]
finished adding batches
subset {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
4 self_attn.k_proj
Pruning ...
realigning
initial loss 178.1652374267578
final loss 149.0334930419922
quantized
not here
quantized in 23.451508045196533 seconds
37895 MiB free out of 48676 MiB total
4 self_attn.v_proj
Pruning ...
realigning
initial loss 34.61722946166992
final loss 34.04546356201172
quantized
not here
quantized in 21.64383888244629 seconds
37831 MiB free out of 48676 MiB total
4 self_attn.q_proj
Pruning ...
realigning
initial loss 152.99147033691406
final loss 137.5843963623047
quantized
not here
quantized in 22.229546308517456 seconds
37767 MiB free out of 48676 MiB total
finished: {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 27648
optimizing the following parameters: dict_keys(['self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 152051712
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0001221202313104186 val loss: 0.0007216162775876
7187 MiB free out of 48676 MiB total
epoch 1 loss: 0.00010121643344973563 val loss: 0.0007176334329415113
7187 MiB free out of 48676 MiB total
epoch 2 loss: 9.36660468937589e-05 val loss: 0.0007151263598643709
7187 MiB free out of 48676 MiB total
epoch 3 loss: 8.890729094446215e-05 val loss: 0.0007132945975172333
7187 MiB free out of 48676 MiB total
epoch 4 loss: 8.549077671204941e-05 val loss: 0.0007120687259885017
7187 MiB free out of 48676 MiB total
finished adding batches
subset {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
4 self_attn.o_proj
Pruning ...
realigning
initial loss 0.9114055633544922
final loss 0.8576030135154724
quantized
not here
quantized in 22.589135885238647 seconds
37479 MiB free out of 48676 MiB total
finished: {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 36864
optimizing the following parameters: dict_keys(['mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135274496
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0001090551609195245 val loss: 0.0006769794272258878
7241 MiB free out of 48676 MiB total
epoch 1 loss: 9.72802024534758e-05 val loss: 0.0006778504757676274
7241 MiB free out of 48676 MiB total
epoch 2 loss: 9.234385339595974e-05 val loss: 0.000678106771374587
7241 MiB free out of 48676 MiB total
epoch 3 loss: 8.90352047235865e-05 val loss: 0.0006780120274925139
7241 MiB free out of 48676 MiB total
epoch 4 loss: 8.64893392531485e-05 val loss: 0.0006778157912776805
7241 MiB free out of 48676 MiB total
finished adding batches
subset {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
4 mlp.up_proj
Pruning ...
realigning
initial loss 64.46638488769531
final loss 63.96978759765625
quantized
not here
quantized in 56.47572612762451 seconds
36369 MiB free out of 48676 MiB total
4 mlp.gate_proj
Pruning ...
realigning
initial loss 91.34458923339844
final loss 86.7093505859375
quantized
not here
quantized in 56.693790912628174 seconds
36111 MiB free out of 48676 MiB total
finished: {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 69120
optimizing the following parameters: dict_keys(['mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 45096960
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0002538200906201382 val loss: 0.0005567689186136704
8457 MiB free out of 48676 MiB total
epoch 1 loss: 0.00024067082972578646 val loss: 0.0005511699200724252
6409 MiB free out of 48676 MiB total
epoch 2 loss: 0.0002326663882286084 val loss: 0.0005475114485307131
6409 MiB free out of 48676 MiB total
epoch 3 loss: 0.00022700036845435534 val loss: 0.0005451566757983528
6409 MiB free out of 48676 MiB total
epoch 4 loss: 0.00022262893435254227 val loss: 0.0005438119842438027
6409 MiB free out of 48676 MiB total
finished adding batches
subset {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
4 mlp.down_proj
Pruning ...
realigning
initial loss 0.5977031588554382
final loss 0.5872515439987183
quantized
not here
quantized in 56.59508275985718 seconds
35173 MiB free out of 48676 MiB total
finished: {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0001827143382797658 val loss: 0.0003625057124736486
6255 MiB free out of 48676 MiB total
epoch 1 loss: 0.00017796727672703128 val loss: 0.0003569051459635375
6255 MiB free out of 48676 MiB total
epoch 2 loss: 0.00017676842264791048 val loss: 0.0003548184977262281
6255 MiB free out of 48676 MiB total
epoch 3 loss: 0.00017608765233489976 val loss: 0.00035387215757509694
6255 MiB free out of 48676 MiB total
epoch 4 loss: 0.0001755880648488528 val loss: 0.00035329550883034244
6255 MiB free out of 48676 MiB total
35173 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
6255 MiB free out of 48676 MiB total
after cast to cpu
37725 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj'], ['self_attn.o_proj'], ['mlp.up_proj', 'mlp.gate_proj'], ['mlp.down_proj']]
finished adding batches
subset {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
5 self_attn.k_proj
Pruning ...
realigning
initial loss 218.6624298095703
final loss 179.73007202148438
quantized
not here
quantized in 21.473807096481323 seconds
37723 MiB free out of 48676 MiB total
5 self_attn.v_proj
Pruning ...
realigning
initial loss 41.03158187866211
final loss 40.588951110839844
quantized
not here
quantized in 18.8660888671875 seconds
37595 MiB free out of 48676 MiB total
5 self_attn.q_proj
Pruning ...
realigning
initial loss 176.1708526611328
final loss 157.336181640625
quantized
not here
quantized in 20.205543279647827 seconds
37595 MiB free out of 48676 MiB total
finished: {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 27648
optimizing the following parameters: dict_keys(['self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 152051712
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.00015466275544895325 val loss: 0.000989597654552199
7187 MiB free out of 48676 MiB total
epoch 1 loss: 0.00012476792034021855 val loss: 0.0009781511216715444
7187 MiB free out of 48676 MiB total
epoch 2 loss: 0.00011554102115951537 val loss: 0.0009711627171782311
7187 MiB free out of 48676 MiB total
epoch 3 loss: 0.00010960271674775868 val loss: 0.0009662719203333836
7187 MiB free out of 48676 MiB total
epoch 4 loss: 0.00010519638402683995 val loss: 0.0009620637465559412
7187 MiB free out of 48676 MiB total
finished adding batches
subset {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
5 self_attn.o_proj
Pruning ...
realigning
initial loss 1.2516226768493652
final loss 1.0173957347869873
quantized
not here
quantized in 22.657228469848633 seconds
37479 MiB free out of 48676 MiB total
finished: {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 36864
optimizing the following parameters: dict_keys(['mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135274496
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.00011859934869562494 val loss: 0.0008892192126950249
7241 MiB free out of 48676 MiB total
epoch 1 loss: 0.00010449342784113469 val loss: 0.0008885225724952761
7241 MiB free out of 48676 MiB total
epoch 2 loss: 9.823776889561486e-05 val loss: 0.0008876738647813909
5193 MiB free out of 48676 MiB total
epoch 3 loss: 9.393989080308529e-05 val loss: 0.0008867974138411228
5193 MiB free out of 48676 MiB total
epoch 4 loss: 9.062474634902173e-05 val loss: 0.000886005462234607
5193 MiB free out of 48676 MiB total
finished adding batches
subset {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
5 mlp.up_proj
Pruning ...
realigning
initial loss 76.37278747558594
final loss 75.98175811767578
quantized
not here
quantized in 51.3324716091156 seconds
36219 MiB free out of 48676 MiB total
5 mlp.gate_proj
Pruning ...
realigning
initial loss 106.30545043945312
final loss 102.91361999511719
quantized
not here
quantized in 50.792163610458374 seconds
36111 MiB free out of 48676 MiB total
finished: {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 69120
optimizing the following parameters: dict_keys(['mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 45096960
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.00033869820504150994 val loss: 0.0006502422438643407
8457 MiB free out of 48676 MiB total
epoch 1 loss: 0.0003243671490054112 val loss: 0.000644850359094562
6409 MiB free out of 48676 MiB total
epoch 2 loss: 0.0003159764225983963 val loss: 0.000642239669105038
6409 MiB free out of 48676 MiB total
epoch 3 loss: 0.0003098616962233791 val loss: 0.0006409191300917882
6409 MiB free out of 48676 MiB total
epoch 4 loss: 0.00030510297256114427 val loss: 0.0006404094638128299
6409 MiB free out of 48676 MiB total
finished adding batches
subset {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
5 mlp.down_proj
Pruning ...
realigning
initial loss 0.7950983643531799
final loss 0.7921397089958191
quantized
not here
quantized in 49.183582067489624 seconds
35087 MiB free out of 48676 MiB total
finished: {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.00025069585262826877 val loss: 0.00038169116851349827
6407 MiB free out of 48676 MiB total
epoch 1 loss: 0.00024678254044374626 val loss: 0.00038348548514477443
6407 MiB free out of 48676 MiB total
epoch 2 loss: 0.00024577907254297315 val loss: 0.0003836939522443572
6407 MiB free out of 48676 MiB total
epoch 3 loss: 0.00024523023546407785 val loss: 0.00038347247573256027
6407 MiB free out of 48676 MiB total
epoch 4 loss: 0.00024483284914822434 val loss: 0.0003832138809229946
6407 MiB free out of 48676 MiB total
35087 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
6407 MiB free out of 48676 MiB total
after cast to cpu
37725 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj'], ['self_attn.o_proj'], ['mlp.up_proj', 'mlp.gate_proj'], ['mlp.down_proj']]
finished adding batches
subset {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
6 self_attn.k_proj
Pruning ...
realigning
initial loss 307.3803405761719
final loss 252.41415405273438
quantized
not here
quantized in 21.5473473072052 seconds
37723 MiB free out of 48676 MiB total
6 self_attn.v_proj
Pruning ...
realigning
initial loss 58.44287872314453
final loss 57.567649841308594
quantized
not here
quantized in 18.986500024795532 seconds
37659 MiB free out of 48676 MiB total
6 self_attn.q_proj
Pruning ...
realigning
initial loss 279.3810119628906
final loss 246.20272827148438
quantized
not here
quantized in 20.08016872406006 seconds
37595 MiB free out of 48676 MiB total
finished: {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 27648
optimizing the following parameters: dict_keys(['self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 152051712
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.00037269595077304984 val loss: 0.001158100858447142
8211 MiB free out of 48676 MiB total
epoch 1 loss: 0.0002961114191748493 val loss: 0.0011584146486711688
6163 MiB free out of 48676 MiB total
epoch 2 loss: 0.0002651489647860217 val loss: 0.0011625132901826873
6163 MiB free out of 48676 MiB total
epoch 3 loss: 0.0002449490340268312 val loss: 0.001167308000731282
6163 MiB free out of 48676 MiB total
epoch 4 loss: 0.00022989277897522697 val loss: 0.0011734199288184755
6163 MiB free out of 48676 MiB total
finished adding batches
subset {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
6 self_attn.o_proj
Pruning ...
realigning
initial loss 1.566681146621704
final loss 1.3268206119537354
quantized
not here
quantized in 22.63902473449707 seconds
37393 MiB free out of 48676 MiB total
finished: {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 36864
optimizing the following parameters: dict_keys(['mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135274496
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0001412348453300183 val loss: 0.0010084468049171846
7219 MiB free out of 48676 MiB total
epoch 1 loss: 0.00012223403217603845 val loss: 0.001007412101898808
7219 MiB free out of 48676 MiB total
epoch 2 loss: 0.00011401110310771401 val loss: 0.0010067063485621475
7219 MiB free out of 48676 MiB total
epoch 3 loss: 0.00010851376987375261 val loss: 0.0010063730478577781
7219 MiB free out of 48676 MiB total
epoch 4 loss: 0.00010438649729849203 val loss: 0.0010060916029033251
7219 MiB free out of 48676 MiB total
finished adding batches
subset {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
6 mlp.up_proj
Pruning ...
realigning
initial loss 90.36874389648438
final loss 89.75182342529297
quantized
not here
quantized in 52.475253105163574 seconds
37221 MiB free out of 48676 MiB total
6 mlp.gate_proj
Pruning ...
realigning
initial loss 132.8593292236328
final loss 127.56635284423828
quantized
not here
quantized in 50.71970987319946 seconds
36877 MiB free out of 48676 MiB total
finished: {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 69120
optimizing the following parameters: dict_keys(['mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 45096960
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0004979114298748755 val loss: 0.000762373092584312
6843 MiB free out of 48676 MiB total
epoch 1 loss: 0.00047299192397076695 val loss: 0.0007588241714984179
6843 MiB free out of 48676 MiB total
epoch 2 loss: 0.0004583168952194683 val loss: 0.0007572123031422962
6843 MiB free out of 48676 MiB total
epoch 3 loss: 0.0004476024068935658 val loss: 0.0007565854757558554
6843 MiB free out of 48676 MiB total
epoch 4 loss: 0.0004393352990064159 val loss: 0.0007563311301055364
6843 MiB free out of 48676 MiB total
finished adding batches
subset {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
6 mlp.down_proj
Pruning ...
realigning
initial loss 1.2085912227630615
final loss 1.201511025428772
quantized
not here
quantized in 49.50733828544617 seconds
35853 MiB free out of 48676 MiB total
finished: {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.00037845920883228246 val loss: 0.0004331875352363568
6077 MiB free out of 48676 MiB total
epoch 1 loss: 0.00037076720536788343 val loss: 0.0004347271751612425
6077 MiB free out of 48676 MiB total
epoch 2 loss: 0.0003685918543396838 val loss: 0.00043549659494601656
6077 MiB free out of 48676 MiB total
epoch 3 loss: 0.00036746511432284024 val loss: 0.00043591986104729585
6077 MiB free out of 48676 MiB total
epoch 4 loss: 0.00036668362349701056 val loss: 0.0004363265397842042
6077 MiB free out of 48676 MiB total
35853 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
6077 MiB free out of 48676 MiB total
after cast to cpu
37725 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj'], ['self_attn.o_proj'], ['mlp.up_proj', 'mlp.gate_proj'], ['mlp.down_proj']]
finished adding batches
subset {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
7 self_attn.k_proj
Pruning ...
realigning
initial loss 313.3668212890625
final loss 263.4111328125
quantized
not here
quantized in 21.24637198448181 seconds
37723 MiB free out of 48676 MiB total
7 self_attn.v_proj
Pruning ...
realigning
initial loss 65.1014633178711
final loss 64.15679931640625
quantized
not here
quantized in 19.021329879760742 seconds
37659 MiB free out of 48676 MiB total
7 self_attn.q_proj
Pruning ...
realigning
initial loss 312.963134765625
final loss 271.0570068359375
quantized
not here
quantized in 20.1493182182312 seconds
37595 MiB free out of 48676 MiB total
finished: {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 27648
optimizing the following parameters: dict_keys(['self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 152051712
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0004843289339078183 val loss: 0.0020436891718418337
8199 MiB free out of 48676 MiB total
epoch 1 loss: 0.00039183552576105285 val loss: 0.002043871885689441
6151 MiB free out of 48676 MiB total
epoch 2 loss: 0.00035472039417072665 val loss: 0.0020508116504061036
6151 MiB free out of 48676 MiB total
epoch 3 loss: 0.0003299480117675557 val loss: 0.0020591683423845097
6151 MiB free out of 48676 MiB total
epoch 4 loss: 0.0003113799784841831 val loss: 0.00206740452995291
6151 MiB free out of 48676 MiB total
finished adding batches
subset {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
7 self_attn.o_proj
Pruning ...
realigning
initial loss 2.283613681793213
final loss 1.9006531238555908
quantized
not here
quantized in 22.53015685081482 seconds
37339 MiB free out of 48676 MiB total
finished: {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 36864
optimizing the following parameters: dict_keys(['mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135274496
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.00019169006930042087 val loss: 0.001523709019238595
7229 MiB free out of 48676 MiB total
epoch 1 loss: 0.00016433116957159655 val loss: 0.00152279318717774
7229 MiB free out of 48676 MiB total
epoch 2 loss: 0.00015224086030229955 val loss: 0.0015216459869407117
7229 MiB free out of 48676 MiB total
epoch 3 loss: 0.00014416866304145515 val loss: 0.001520571815490257
7229 MiB free out of 48676 MiB total
epoch 4 loss: 0.00013815178454024135 val loss: 0.001519731646112632
7229 MiB free out of 48676 MiB total
finished adding batches
subset {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
7 mlp.up_proj
Pruning ...
realigning
initial loss 103.78662109375
final loss 103.10093688964844
quantized
not here
quantized in 52.31116580963135 seconds
36229 MiB free out of 48676 MiB total
7 mlp.gate_proj
Pruning ...
realigning
initial loss 151.13880920410156
final loss 145.44578552246094
quantized
not here
quantized in 50.91223502159119 seconds
35971 MiB free out of 48676 MiB total
finished: {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 69120
optimizing the following parameters: dict_keys(['mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 45096960
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0006847777794973808 val loss: 0.0010729464920586906
8317 MiB free out of 48676 MiB total
epoch 1 loss: 0.0006437261881728773 val loss: 0.0010715367170632817
6269 MiB free out of 48676 MiB total
epoch 2 loss: 0.0006206405728335085 val loss: 0.0010713360243244097
6269 MiB free out of 48676 MiB total
epoch 3 loss: 0.0006041453143552644 val loss: 0.0010719185520429164
6269 MiB free out of 48676 MiB total
epoch 4 loss: 0.00059155948474654 val loss: 0.0010728620400186628
6269 MiB free out of 48676 MiB total
finished adding batches
subset {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
7 mlp.down_proj
Pruning ...
realigning
initial loss 1.7461464405059814
final loss 1.7379908561706543
quantized
not here
quantized in 49.53115487098694 seconds
36057 MiB free out of 48676 MiB total
finished: {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0005493633784681151 val loss: 0.0006194505440362263
6181 MiB free out of 48676 MiB total
epoch 1 loss: 0.0005381766836762836 val loss: 0.0006316102262644563
6181 MiB free out of 48676 MiB total
epoch 2 loss: 0.0005348462195797765 val loss: 0.000638338562566787
6181 MiB free out of 48676 MiB total
epoch 3 loss: 0.0005330865938049101 val loss: 0.000642135099042207
6181 MiB free out of 48676 MiB total
epoch 4 loss: 0.0005318507842275721 val loss: 0.0006448355852626264
6181 MiB free out of 48676 MiB total
36057 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
6181 MiB free out of 48676 MiB total
after cast to cpu
37725 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj'], ['self_attn.o_proj'], ['mlp.up_proj', 'mlp.gate_proj'], ['mlp.down_proj']]
finished adding batches
subset {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
8 self_attn.k_proj
Pruning ...
realigning
initial loss 289.807373046875
final loss 256.5369567871094
quantized
not here
quantized in 21.336894750595093 seconds
37723 MiB free out of 48676 MiB total
8 self_attn.v_proj
Pruning ...
realigning
initial loss 65.39326477050781
final loss 64.38316345214844
quantized
not here
quantized in 18.854174613952637 seconds
37723 MiB free out of 48676 MiB total
8 self_attn.q_proj
Pruning ...
realigning
initial loss 298.24786376953125
final loss 269.5333251953125
quantized
not here
quantized in 20.28217911720276 seconds
37659 MiB free out of 48676 MiB total
finished: {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 27648
optimizing the following parameters: dict_keys(['self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 152051712
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0006261013309085683 val loss: 0.0020961548216291703
7219 MiB free out of 48676 MiB total
epoch 1 loss: 0.0005126459147959395 val loss: 0.002079142250295263
7219 MiB free out of 48676 MiB total
epoch 2 loss: 0.0004674108797644294 val loss: 0.002066829096293077
7219 MiB free out of 48676 MiB total
epoch 3 loss: 0.0004373662541183876 val loss: 0.0020564558071782812
7219 MiB free out of 48676 MiB total
epoch 4 loss: 0.00041478518960502697 val loss: 0.0020476108766160905
7219 MiB free out of 48676 MiB total
finished adding batches
subset {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
8 self_attn.o_proj
Pruning ...
realigning
initial loss 3.7576982975006104
final loss 3.64044189453125
quantized
not here
quantized in 22.54225516319275 seconds
37543 MiB free out of 48676 MiB total
finished: {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 36864
optimizing the following parameters: dict_keys(['mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135274496
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0003091485045843001 val loss: 0.0014316243614302948
7273 MiB free out of 48676 MiB total
epoch 1 loss: 0.0002580453289056095 val loss: 0.0014275408393586986
7273 MiB free out of 48676 MiB total
epoch 2 loss: 0.00023746684269099205 val loss: 0.0014237144350772724
7273 MiB free out of 48676 MiB total
epoch 3 loss: 0.00022405567608529964 val loss: 0.0014203869068296626
7273 MiB free out of 48676 MiB total
epoch 4 loss: 0.0002141632086249956 val loss: 0.0014173689924064092
7273 MiB free out of 48676 MiB total
finished adding batches
subset {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
8 mlp.up_proj
Pruning ...
realigning
initial loss 114.18800354003906
final loss 113.1070556640625
quantized
not here
quantized in 53.533835887908936 seconds
36519 MiB free out of 48676 MiB total
8 mlp.gate_proj
Pruning ...
realigning
initial loss 160.0994873046875
final loss 152.07354736328125
quantized
not here
quantized in 52.0937442779541 seconds
36175 MiB free out of 48676 MiB total
finished: {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 69120
optimizing the following parameters: dict_keys(['mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 45096960
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0008432777540292591 val loss: 0.001032453597872518
8521 MiB free out of 48676 MiB total
epoch 1 loss: 0.0007908720508567058 val loss: 0.0010354312616982497
6473 MiB free out of 48676 MiB total
epoch 2 loss: 0.0007617425812895817 val loss: 0.0010382909313193522
6473 MiB free out of 48676 MiB total
epoch 3 loss: 0.0007409249756165082 val loss: 0.001040580951666925
6473 MiB free out of 48676 MiB total
epoch 4 loss: 0.000724792567325494 val loss: 0.0010421945262351073
6473 MiB free out of 48676 MiB total
finished adding batches
subset {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
8 mlp.down_proj
Pruning ...
realigning
initial loss 2.17795467376709
final loss 2.1689634323120117
quantized
not here
quantized in 55.078186988830566 seconds
35151 MiB free out of 48676 MiB total
finished: {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0006859086001895776 val loss: 0.0006225942088349257
6233 MiB free out of 48676 MiB total
epoch 1 loss: 0.000671557998884964 val loss: 0.0006301951361820102
6233 MiB free out of 48676 MiB total
epoch 2 loss: 0.000666651316350908 val loss: 0.0006348898314172402
6233 MiB free out of 48676 MiB total
epoch 3 loss: 0.0006640862256972468 val loss: 0.0006373242213157937
6233 MiB free out of 48676 MiB total
epoch 4 loss: 0.0006624066600124934 val loss: 0.0006384831467585173
6233 MiB free out of 48676 MiB total
35151 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
6233 MiB free out of 48676 MiB total
after cast to cpu
37725 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj'], ['self_attn.o_proj'], ['mlp.up_proj', 'mlp.gate_proj'], ['mlp.down_proj']]
finished adding batches
subset {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
9 self_attn.k_proj
Pruning ...
realigning
initial loss 311.17138671875
final loss 273.43658447265625
quantized
not here
quantized in 21.210688829421997 seconds
37723 MiB free out of 48676 MiB total
9 self_attn.v_proj
Pruning ...
realigning
initial loss 71.4471435546875
final loss 70.62506103515625
quantized
not here
quantized in 19.157012701034546 seconds
37659 MiB free out of 48676 MiB total
9 self_attn.q_proj
Pruning ...
realigning
initial loss 279.09521484375
final loss 254.77362060546875
quantized
not here
quantized in 20.0264310836792 seconds
37703 MiB free out of 48676 MiB total
finished: {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 27648
optimizing the following parameters: dict_keys(['self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 152051712
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0006302314159256639 val loss: 0.0024533784308005124
7391 MiB free out of 48676 MiB total
epoch 1 loss: 0.0005183276855404984 val loss: 0.002427089464617893
5343 MiB free out of 48676 MiB total
epoch 2 loss: 0.00047460319501624326 val loss: 0.0024106951168505475
5343 MiB free out of 48676 MiB total
epoch 3 loss: 0.00044609454062083387 val loss: 0.0023991597699932754
5343 MiB free out of 48676 MiB total
epoch 4 loss: 0.00042515645804996893 val loss: 0.0023909212759463117
5343 MiB free out of 48676 MiB total
finished adding batches
subset {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
9 self_attn.o_proj
Pruning ...
realigning
initial loss 5.394854545593262
final loss 5.115607261657715
quantized
not here
quantized in 22.93281126022339 seconds
37361 MiB free out of 48676 MiB total
finished: {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 36864
optimizing the following parameters: dict_keys(['mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135274496
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0004240225216562976 val loss: 0.001756215060595423
7251 MiB free out of 48676 MiB total
epoch 1 loss: 0.0003538175737958227 val loss: 0.0017517373271402903
7251 MiB free out of 48676 MiB total
epoch 2 loss: 0.0003264942311034247 val loss: 0.001748160670103971
7251 MiB free out of 48676 MiB total
epoch 3 loss: 0.00030846662571093475 val loss: 0.001745354755257722
5203 MiB free out of 48676 MiB total
epoch 4 loss: 0.0002950713792415627 val loss: 0.0017427592174499296
5203 MiB free out of 48676 MiB total
finished adding batches
subset {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
9 mlp.up_proj
Pruning ...
realigning
initial loss 124.67083740234375
final loss 123.52789306640625
quantized
not here
quantized in 52.072813749313354 seconds
36337 MiB free out of 48676 MiB total
9 mlp.gate_proj
Pruning ...
realigning
initial loss 168.01023864746094
final loss 160.98590087890625
quantized
not here
quantized in 51.56129789352417 seconds
35993 MiB free out of 48676 MiB total
finished: {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 69120
optimizing the following parameters: dict_keys(['mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 45096960
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0009644472170293739 val loss: 0.001249369204742834
8339 MiB free out of 48676 MiB total
epoch 1 loss: 0.0009073905171135266 val loss: 0.0012451685397536494
6291 MiB free out of 48676 MiB total
epoch 2 loss: 0.0008760594705563562 val loss: 0.00124291379324859
6291 MiB free out of 48676 MiB total
epoch 3 loss: 0.0008539741420463542 val loss: 0.0012416191966622137
6291 MiB free out of 48676 MiB total
epoch 4 loss: 0.0008369504625989066 val loss: 0.0012410050621838309
6291 MiB free out of 48676 MiB total
finished adding batches
subset {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
9 mlp.down_proj
Pruning ...
realigning
initial loss 2.3199143409729004
final loss 2.317168712615967
quantized
not here
quantized in 49.59190130233765 seconds
35055 MiB free out of 48676 MiB total
finished: {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0007520820108766202 val loss: 0.0007840609396225773
6203 MiB free out of 48676 MiB total
epoch 1 loss: 0.0007457891210833623 val loss: 0.0007888443469710182
6203 MiB free out of 48676 MiB total
epoch 2 loss: 0.0007434201343130553 val loss: 0.0007912821129139047
6203 MiB free out of 48676 MiB total
epoch 3 loss: 0.0007422084049721889 val loss: 0.0007922323529783171
6203 MiB free out of 48676 MiB total
epoch 4 loss: 0.0007413997818730422 val loss: 0.000792564038420096
6203 MiB free out of 48676 MiB total
35055 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
6203 MiB free out of 48676 MiB total
after cast to cpu
37725 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj'], ['self_attn.o_proj'], ['mlp.up_proj', 'mlp.gate_proj'], ['mlp.down_proj']]
finished adding batches
subset {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
10 self_attn.k_proj
Pruning ...
realigning
initial loss 320.94952392578125
final loss 287.00250244140625
quantized
not here
quantized in 21.14258861541748 seconds
37895 MiB free out of 48676 MiB total
10 self_attn.v_proj
Pruning ...
realigning
initial loss 71.25209045410156
final loss 70.23687744140625
quantized
not here
quantized in 18.680996417999268 seconds
37895 MiB free out of 48676 MiB total
10 self_attn.q_proj
Pruning ...
realigning
initial loss 288.4970703125
final loss 261.9779968261719
quantized
not here
quantized in 20.186465740203857 seconds
37767 MiB free out of 48676 MiB total
finished: {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 27648
optimizing the following parameters: dict_keys(['self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 152051712
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0007207963794826355 val loss: 0.003530565620167181
8211 MiB free out of 48676 MiB total
epoch 1 loss: 0.0005972661956548109 val loss: 0.003517888515489176
6163 MiB free out of 48676 MiB total
epoch 2 loss: 0.0005459873632389645 val loss: 0.003520511105307378
6163 MiB free out of 48676 MiB total
epoch 3 loss: 0.0005120975642967096 val loss: 0.003505736283841543
6163 MiB free out of 48676 MiB total
epoch 4 loss: 0.0004870026612024958 val loss: 0.003515101969242096
6163 MiB free out of 48676 MiB total
finished adding batches
subset {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
10 self_attn.o_proj
Pruning ...
realigning
initial loss 7.755082130432129
final loss 7.277129650115967
quantized
not here
quantized in 22.842817306518555 seconds
37393 MiB free out of 48676 MiB total
finished: {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 36864
optimizing the following parameters: dict_keys(['mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135274496
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0005823220344609581 val loss: 0.0023568494361825287
8243 MiB free out of 48676 MiB total
epoch 1 loss: 0.00047993723751460493 val loss: 0.002355394884943962
6195 MiB free out of 48676 MiB total
epoch 2 loss: 0.00043782812713288877 val loss: 0.002352721174247563
6195 MiB free out of 48676 MiB total
epoch 3 loss: 0.0004102352675090515 val loss: 0.0023505266435677186
6195 MiB free out of 48676 MiB total
epoch 4 loss: 0.00038991004430499743 val loss: 0.0023483073455281556
6195 MiB free out of 48676 MiB total
finished adding batches
subset {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
10 mlp.up_proj
Pruning ...
realigning
initial loss 133.61935424804688
final loss 132.19662475585938
quantized
not here
quantized in 51.891335010528564 seconds
36369 MiB free out of 48676 MiB total
10 mlp.gate_proj
Pruning ...
realigning
initial loss 181.05516052246094
final loss 172.00733947753906
quantized
not here
quantized in 51.56802010536194 seconds
36025 MiB free out of 48676 MiB total
finished: {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 69120
optimizing the following parameters: dict_keys(['mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 45096960
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0011053306398025597 val loss: 0.0019059862534049898
8371 MiB free out of 48676 MiB total
epoch 1 loss: 0.001042274087012629 val loss: 0.001888697013782803
6323 MiB free out of 48676 MiB total
epoch 2 loss: 0.001005687036922609 val loss: 0.0018759325903374702
6323 MiB free out of 48676 MiB total
epoch 3 loss: 0.0009794210877771548 val loss: 0.0018673669910640456
6323 MiB free out of 48676 MiB total
epoch 4 loss: 0.0009590089425728365 val loss: 0.0018619295951793902
6323 MiB free out of 48676 MiB total
finished adding batches
subset {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
10 mlp.down_proj
Pruning ...
realigning
initial loss 2.890094041824341
final loss 2.868124485015869
quantized
not here
quantized in 52.35999035835266 seconds
36111 MiB free out of 48676 MiB total
finished: {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0008994706504381611 val loss: 0.0011111109561170451
6235 MiB free out of 48676 MiB total
epoch 1 loss: 0.0008737693801776913 val loss: 0.001133716395997908
6235 MiB free out of 48676 MiB total
epoch 2 loss: 0.0008636474644845293 val loss: 0.0011439860754762776
6235 MiB free out of 48676 MiB total
epoch 3 loss: 0.0008581195620536164 val loss: 0.0011486140720080584
6235 MiB free out of 48676 MiB total
epoch 4 loss: 0.0008547425941287656 val loss: 0.0011507059243740514
6235 MiB free out of 48676 MiB total
36111 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
6235 MiB free out of 48676 MiB total
after cast to cpu
37725 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj'], ['self_attn.o_proj'], ['mlp.up_proj', 'mlp.gate_proj'], ['mlp.down_proj']]
finished adding batches
subset {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
11 self_attn.k_proj
Pruning ...
realigning
initial loss 350.35205078125
final loss 313.03375244140625
quantized
not here
quantized in 21.16855549812317 seconds
37723 MiB free out of 48676 MiB total
11 self_attn.v_proj
Pruning ...
realigning
initial loss 97.8787841796875
final loss 96.72805786132812
quantized
not here
quantized in 18.831950902938843 seconds
37659 MiB free out of 48676 MiB total
11 self_attn.q_proj
Pruning ...
realigning
initial loss 361.9478759765625
final loss 327.2144470214844
quantized
not here
quantized in 20.501572608947754 seconds
37595 MiB free out of 48676 MiB total
finished: {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 27648
optimizing the following parameters: dict_keys(['self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 152051712
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0009217462061315018 val loss: 0.0033281013602390885
7219 MiB free out of 48676 MiB total
epoch 1 loss: 0.000772191957821633 val loss: 0.003308259620098397
5171 MiB free out of 48676 MiB total
epoch 2 loss: 0.0007075245348460157 val loss: 0.003290263790404424
5171 MiB free out of 48676 MiB total
epoch 3 loss: 0.0006639902690039889 val loss: 0.0032735159038566053
5171 MiB free out of 48676 MiB total
epoch 4 loss: 0.000631244990927371 val loss: 0.0032591732015134767
5171 MiB free out of 48676 MiB total
finished adding batches
subset {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
11 self_attn.o_proj
Pruning ...
realigning
initial loss 7.108124732971191
final loss 6.594928741455078
quantized
not here
quantized in 22.63785481452942 seconds
37511 MiB free out of 48676 MiB total
finished: {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 36864
optimizing the following parameters: dict_keys(['mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135274496
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0005253707468000357 val loss: 0.002191919556935318
7273 MiB free out of 48676 MiB total
epoch 1 loss: 0.0004362924071301677 val loss: 0.002181595060392283
7273 MiB free out of 48676 MiB total
epoch 2 loss: 0.0003996201382960862 val loss: 0.002174314548028633
7273 MiB free out of 48676 MiB total
epoch 3 loss: 0.00037561126691798563 val loss: 0.002169035782571882
7273 MiB free out of 48676 MiB total
epoch 4 loss: 0.00035793841448139574 val loss: 0.0021647175162797794
7273 MiB free out of 48676 MiB total
finished adding batches
subset {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
11 mlp.up_proj
Pruning ...
realigning
initial loss 146.8174285888672
final loss 145.5152587890625
quantized
not here
quantized in 51.791512966156006 seconds
36401 MiB free out of 48676 MiB total
11 mlp.gate_proj
Pruning ...
realigning
initial loss 193.5806121826172
final loss 182.81980895996094
quantized
not here
quantized in 52.49250054359436 seconds
36143 MiB free out of 48676 MiB total
finished: {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 69120
optimizing the following parameters: dict_keys(['mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 45096960
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0012688186561717885 val loss: 0.0015289876100723632
8489 MiB free out of 48676 MiB total
epoch 1 loss: 0.0011946498098041047 val loss: 0.0015269596115103923
6441 MiB free out of 48676 MiB total
epoch 2 loss: 0.0011519424406287726 val loss: 0.0015262225933838636
6441 MiB free out of 48676 MiB total
epoch 3 loss: 0.0011212758172405302 val loss: 0.001526173589809332
6441 MiB free out of 48676 MiB total
epoch 4 loss: 0.0010974369279210805 val loss: 0.001526335094240494
6441 MiB free out of 48676 MiB total
finished adding batches
subset {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
11 mlp.down_proj
Pruning ...
realigning
initial loss 3.2165260314941406
final loss 3.2088775634765625
quantized
not here
quantized in 52.012388944625854 seconds
36229 MiB free out of 48676 MiB total
finished: {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0010300391136297549 val loss: 0.000873541761393426
6287 MiB free out of 48676 MiB total
epoch 1 loss: 0.001013817892271618 val loss: 0.0008922557426558342
6287 MiB free out of 48676 MiB total
epoch 2 loss: 0.001007508180464356 val loss: 0.0009038619828061201
6287 MiB free out of 48676 MiB total
epoch 3 loss: 0.0010039669436991971 val loss: 0.0009113146334129851
6287 MiB free out of 48676 MiB total
epoch 4 loss: 0.0010015785856012371 val loss: 0.0009160774716292508
6287 MiB free out of 48676 MiB total
36229 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
6287 MiB free out of 48676 MiB total
after cast to cpu
37725 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj'], ['self_attn.o_proj'], ['mlp.up_proj', 'mlp.gate_proj'], ['mlp.down_proj']]
finished adding batches
subset {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
12 self_attn.k_proj
Pruning ...
realigning
initial loss 378.8927917480469
final loss 336.2227783203125
quantized
not here
quantized in 21.244677305221558 seconds
37723 MiB free out of 48676 MiB total
12 self_attn.v_proj
Pruning ...
realigning
initial loss 94.97642517089844
final loss 94.06954193115234
quantized
not here
quantized in 19.03107261657715 seconds
37659 MiB free out of 48676 MiB total
12 self_attn.q_proj
Pruning ...
realigning
initial loss 344.45123291015625
final loss 315.3419494628906
quantized
not here
quantized in 20.16446805000305 seconds
37595 MiB free out of 48676 MiB total
finished: {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 27648
optimizing the following parameters: dict_keys(['self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 152051712
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0009649428939155769 val loss: 0.0035595457593444735
8231 MiB free out of 48676 MiB total
epoch 1 loss: 0.0008140048453242343 val loss: 0.00354302475170698
6183 MiB free out of 48676 MiB total
epoch 2 loss: 0.0007488338605980971 val loss: 0.0035274843394290656
6183 MiB free out of 48676 MiB total
epoch 3 loss: 0.0007052226810628781 val loss: 0.003512183335260488
6183 MiB free out of 48676 MiB total
epoch 4 loss: 0.0006726208239342668 val loss: 0.0034992966247955337
6183 MiB free out of 48676 MiB total
finished adding batches
subset {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
12 self_attn.o_proj
Pruning ...
realigning
initial loss 8.65662670135498
final loss 8.305002212524414
quantized
not here
quantized in 22.653240203857422 seconds
37425 MiB free out of 48676 MiB total
finished: {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 36864
optimizing the following parameters: dict_keys(['mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135274496
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.000668934792656728 val loss: 0.0026423114904901013
8307 MiB free out of 48676 MiB total
epoch 1 loss: 0.0005554849451527843 val loss: 0.0026370839186711237
6259 MiB free out of 48676 MiB total
epoch 2 loss: 0.0005088434447770851 val loss: 0.0026338885218137875
6259 MiB free out of 48676 MiB total
epoch 3 loss: 0.000478289750390104 val loss: 0.0026314072165405378
6259 MiB free out of 48676 MiB total
epoch 4 loss: 0.0004557697375275893 val loss: 0.002629456765134819
6259 MiB free out of 48676 MiB total
finished adding batches
subset {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
12 mlp.up_proj
Pruning ...
realigning
initial loss 159.26351928710938
final loss 157.74642944335938
quantized
not here
quantized in 52.42527651786804 seconds
36229 MiB free out of 48676 MiB total
12 mlp.gate_proj
Pruning ...
realigning
initial loss 200.34408569335938
final loss 190.7201385498047
quantized
not here
quantized in 52.68340563774109 seconds
35885 MiB free out of 48676 MiB total
finished: {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 69120
optimizing the following parameters: dict_keys(['mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 45096960
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0014623112911067437 val loss: 0.0015889061687630601
7207 MiB free out of 48676 MiB total
epoch 1 loss: 0.0013761145046373713 val loss: 0.0015918856515781954
7207 MiB free out of 48676 MiB total
epoch 2 loss: 0.0013261182903079316 val loss: 0.0015956808638293296
7207 MiB free out of 48676 MiB total
epoch 3 loss: 0.001290357044126722 val loss: 0.0015992831395124085
7207 MiB free out of 48676 MiB total
epoch 4 loss: 0.0012626327343241428 val loss: 0.0016023021526052617
7207 MiB free out of 48676 MiB total
finished adding batches
subset {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
12 mlp.down_proj
Pruning ...
realigning
initial loss 3.492598056793213
final loss 3.488799810409546
quantized
not here
quantized in 51.73985004425049 seconds
34861 MiB free out of 48676 MiB total
finished: {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0011297598366581951 val loss: 0.0008314992956002243
6181 MiB free out of 48676 MiB total
epoch 1 loss: 0.0011184595023223665 val loss: 0.0008415348420385271
6181 MiB free out of 48676 MiB total
epoch 2 loss: 0.001113492100557778 val loss: 0.0008498073875671253
6181 MiB free out of 48676 MiB total
epoch 3 loss: 0.0011109308707091259 val loss: 0.000855440906889271
6181 MiB free out of 48676 MiB total
epoch 4 loss: 0.0011093917810285348 val loss: 0.0008592551203037146
6181 MiB free out of 48676 MiB total
34861 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
6181 MiB free out of 48676 MiB total
after cast to cpu
37553 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj'], ['self_attn.o_proj'], ['mlp.up_proj', 'mlp.gate_proj'], ['mlp.down_proj']]
finished adding batches
subset {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
13 self_attn.k_proj
Pruning ...
realigning
initial loss 377.2494812011719
final loss 329.22100830078125
quantized
not here
quantized in 21.106443643569946 seconds
37551 MiB free out of 48676 MiB total
13 self_attn.v_proj
Pruning ...
realigning
initial loss 104.75308990478516
final loss 104.05255889892578
quantized
not here
quantized in 18.902541637420654 seconds
37551 MiB free out of 48676 MiB total
13 self_attn.q_proj
Pruning ...
realigning
initial loss 345.24700927734375
final loss 312.35430908203125
quantized
not here
quantized in 20.075342893600464 seconds
37551 MiB free out of 48676 MiB total
finished: {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 27648
optimizing the following parameters: dict_keys(['self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 152051712
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0009452867329855508 val loss: 0.003313699984573759
7207 MiB free out of 48676 MiB total
epoch 1 loss: 0.0007966922867126414 val loss: 0.003296326380223036
7207 MiB free out of 48676 MiB total
epoch 2 loss: 0.000734343933800119 val loss: 0.003282602207036689
7207 MiB free out of 48676 MiB total
epoch 3 loss: 0.0006929815858711663 val loss: 0.003269223467214033
7207 MiB free out of 48676 MiB total
epoch 4 loss: 0.0006622099913329293 val loss: 0.0032580561673967168
7207 MiB free out of 48676 MiB total
finished adding batches
subset {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
13 self_attn.o_proj
Pruning ...
realigning
initial loss 8.69823169708252
final loss 8.21562385559082
quantized
not here
quantized in 22.751715421676636 seconds
37349 MiB free out of 48676 MiB total
finished: {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 36864
optimizing the following parameters: dict_keys(['mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135274496
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0006911820773893851 val loss: 0.0024954516120487824
7239 MiB free out of 48676 MiB total
epoch 1 loss: 0.0005723853378185595 val loss: 0.00248850166099146
7239 MiB free out of 48676 MiB total
epoch 2 loss: 0.0005222340598720621 val loss: 0.002483574309735559
7239 MiB free out of 48676 MiB total
epoch 3 loss: 0.0004895132378806011 val loss: 0.002479790011420846
7239 MiB free out of 48676 MiB total
epoch 4 loss: 0.00046552966568924603 val loss: 0.002477098794770427
7239 MiB free out of 48676 MiB total
finished adding batches
subset {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
13 mlp.up_proj
Pruning ...
realigning
initial loss 173.17984008789062
final loss 171.67138671875
quantized
not here
quantized in 53.63155245780945 seconds
37177 MiB free out of 48676 MiB total
13 mlp.gate_proj
Pruning ...
realigning
initial loss 211.44091796875
final loss 201.015625
quantized
not here
quantized in 52.79591941833496 seconds
36833 MiB free out of 48676 MiB total
finished: {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 69120
optimizing the following parameters: dict_keys(['mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 45096960
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0017562769207870588 val loss: 0.001970507211808581
6863 MiB free out of 48676 MiB total
epoch 1 loss: 0.0016406900504080113 val loss: 0.0019677861273521557
6863 MiB free out of 48676 MiB total
epoch 2 loss: 0.0015737057128717424 val loss: 0.001967691168829333
6863 MiB free out of 48676 MiB total
epoch 3 loss: 0.0015264056655723834 val loss: 0.0019694913644343615
6863 MiB free out of 48676 MiB total
epoch 4 loss: 0.0014903643823345192 val loss: 0.001971908764971886
6863 MiB free out of 48676 MiB total
finished adding batches
subset {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
13 mlp.down_proj
Pruning ...
realigning
initial loss 4.336377143859863
final loss 4.32719612121582
quantized
not here
quantized in 56.46517992019653 seconds
35809 MiB free out of 48676 MiB total
finished: {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.001398216431880428 val loss: 0.0010775448754429817
6119 MiB free out of 48676 MiB total
epoch 1 loss: 0.0013798451745969942 val loss: 0.0010868155004573055
6119 MiB free out of 48676 MiB total
epoch 2 loss: 0.0013706026829822804 val loss: 0.0010965406254399568
6119 MiB free out of 48676 MiB total
epoch 3 loss: 0.0013650398013851373 val loss: 0.0011042752157663926
6119 MiB free out of 48676 MiB total
epoch 4 loss: 0.001361291929242725 val loss: 0.001109797609387897
6119 MiB free out of 48676 MiB total
35809 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
6119 MiB free out of 48676 MiB total
after cast to cpu
37725 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj'], ['self_attn.o_proj'], ['mlp.up_proj', 'mlp.gate_proj'], ['mlp.down_proj']]
finished adding batches
subset {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
14 self_attn.k_proj
Pruning ...
realigning
initial loss 401.8953552246094
final loss 349.8493347167969
quantized
not here
quantized in 21.031076431274414 seconds
37723 MiB free out of 48676 MiB total
14 self_attn.v_proj
Pruning ...
realigning
initial loss 104.755126953125
final loss 104.05221557617188
quantized
not here
quantized in 18.894714832305908 seconds
37659 MiB free out of 48676 MiB total
14 self_attn.q_proj
Pruning ...
realigning
initial loss 367.85626220703125
final loss 326.7926940917969
quantized
not here
quantized in 19.97570538520813 seconds
37595 MiB free out of 48676 MiB total
finished: {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 27648
optimizing the following parameters: dict_keys(['self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 152051712
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.001209413093420153 val loss: 0.00401494865946006
8227 MiB free out of 48676 MiB total
epoch 1 loss: 0.00100868614981664 val loss: 0.003992687517893501
6179 MiB free out of 48676 MiB total
epoch 2 loss: 0.0009210867569890979 val loss: 0.0039684326911810786
6179 MiB free out of 48676 MiB total
epoch 3 loss: 0.0008631313294245047 val loss: 0.0039460753032471985
6179 MiB free out of 48676 MiB total
epoch 4 loss: 0.000820512254449568 val loss: 0.003929037047782913
6179 MiB free out of 48676 MiB total
finished adding batches
subset {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
14 self_attn.o_proj
Pruning ...
realigning
initial loss 11.172539710998535
final loss 11.09360122680664
quantized
not here
quantized in 21.442872762680054 seconds
37479 MiB free out of 48676 MiB total
finished: {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 36864
optimizing the following parameters: dict_keys(['mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135274496
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.00090921068795069 val loss: 0.0031989759008865803
7273 MiB free out of 48676 MiB total
epoch 1 loss: 0.0007377466863545123 val loss: 0.00319379975553602
7273 MiB free out of 48676 MiB total
epoch 2 loss: 0.0006668425749012385 val loss: 0.0031904407805996016
7273 MiB free out of 48676 MiB total
epoch 3 loss: 0.0006214055433702015 val loss: 0.0031882765179034323
7273 MiB free out of 48676 MiB total
epoch 4 loss: 0.0005884573438379448 val loss: 0.003186568239470944
7273 MiB free out of 48676 MiB total
finished adding batches
subset {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
14 mlp.up_proj
Pruning ...
realigning
initial loss 188.817626953125
final loss 186.85752868652344
quantized
not here
quantized in 53.192028284072876 seconds
36455 MiB free out of 48676 MiB total
14 mlp.gate_proj
Pruning ...
realigning
initial loss 236.68775939941406
final loss 220.6995391845703
quantized
not here
quantized in 52.526628255844116 seconds
36111 MiB free out of 48676 MiB total
finished: {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 69120
optimizing the following parameters: dict_keys(['mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 45096960
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.002039171451542643 val loss: 0.002117772091878578
8457 MiB free out of 48676 MiB total
epoch 1 loss: 0.0019060168633586727 val loss: 0.00212163120158948
6409 MiB free out of 48676 MiB total
epoch 2 loss: 0.0018287204266016488 val loss: 0.0021275877224979922
6409 MiB free out of 48676 MiB total
epoch 3 loss: 0.0017736907620928832 val loss: 0.0021343582193367183
6409 MiB free out of 48676 MiB total
epoch 4 loss: 0.0017313492026005406 val loss: 0.002140444194083102
6409 MiB free out of 48676 MiB total
finished adding batches
subset {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
14 mlp.down_proj
Pruning ...
realigning
initial loss 4.995160102844238
final loss 4.991045951843262
quantized
not here
quantized in 51.86409902572632 seconds
36197 MiB free out of 48676 MiB total
finished: {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0016198647736018756 val loss: 0.0011928483727388084
6255 MiB free out of 48676 MiB total
epoch 1 loss: 0.0016031409249990247 val loss: 0.001203142834128812
6255 MiB free out of 48676 MiB total
epoch 2 loss: 0.0015945997120070388 val loss: 0.0012146944063715637
6255 MiB free out of 48676 MiB total
epoch 3 loss: 0.0015899659065325977 val loss: 0.001223760424181819
6255 MiB free out of 48676 MiB total
epoch 4 loss: 0.0015872800449869828 val loss: 0.0012294990592636168
6255 MiB free out of 48676 MiB total
36197 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
6255 MiB free out of 48676 MiB total
after cast to cpu
37725 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj'], ['self_attn.o_proj'], ['mlp.up_proj', 'mlp.gate_proj'], ['mlp.down_proj']]
finished adding batches
subset {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
15 self_attn.k_proj
Pruning ...
realigning
initial loss 387.02978515625
final loss 329.43365478515625
quantized
not here
quantized in 21.13388156890869 seconds
37723 MiB free out of 48676 MiB total
15 self_attn.v_proj
Pruning ...
realigning
initial loss 108.19496154785156
final loss 107.42464447021484
quantized
not here
quantized in 18.838234186172485 seconds
37723 MiB free out of 48676 MiB total
15 self_attn.q_proj
Pruning ...
realigning
initial loss 342.063232421875
final loss 298.8235778808594
quantized
not here
quantized in 20.35914945602417 seconds
37659 MiB free out of 48676 MiB total
finished: {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 27648
optimizing the following parameters: dict_keys(['self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 152051712
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0012272176118131028 val loss: 0.004583174682920799
7187 MiB free out of 48676 MiB total
epoch 1 loss: 0.0010263574145028542 val loss: 0.004527030076133087
5139 MiB free out of 48676 MiB total
epoch 2 loss: 0.0009399273949384224 val loss: 0.004495161032536998
5139 MiB free out of 48676 MiB total
epoch 3 loss: 0.0008824940809972759 val loss: 0.0044606770388782024
5139 MiB free out of 48676 MiB total
epoch 4 loss: 0.0008402129669775604 val loss: 0.004450905718840659
5139 MiB free out of 48676 MiB total
finished adding batches
subset {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
15 self_attn.o_proj
Pruning ...
realigning
initial loss 10.231508255004883
final loss 9.950057983398438
quantized
not here
quantized in 22.487858772277832 seconds
37457 MiB free out of 48676 MiB total
finished: {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 36864
optimizing the following parameters: dict_keys(['mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135274496
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0008998626394713938 val loss: 0.0038758075243094936
8243 MiB free out of 48676 MiB total
epoch 1 loss: 0.0007524680427195563 val loss: 0.0038693896494805813
8243 MiB free out of 48676 MiB total
epoch 2 loss: 0.000686107568526495 val loss: 0.0038650562637485564
8243 MiB free out of 48676 MiB total
epoch 3 loss: 0.0006426381596611463 val loss: 0.003861561810481362
8243 MiB free out of 48676 MiB total
epoch 4 loss: 0.0006108359475547331 val loss: 0.003858751355437562
8243 MiB free out of 48676 MiB total
finished adding batches
subset {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
15 mlp.up_proj
Pruning ...
realigning
initial loss 208.89242553710938
final loss 206.7833251953125
quantized
not here
quantized in 54.27486324310303 seconds
36347 MiB free out of 48676 MiB total
15 mlp.gate_proj
Pruning ...
realigning
initial loss 259.3468017578125
final loss 243.01605224609375
quantized
not here
quantized in 52.74479079246521 seconds
36089 MiB free out of 48676 MiB total
finished: {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 69120
optimizing the following parameters: dict_keys(['mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 45096960
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.002489899459760636 val loss: 0.002677388081792742
8435 MiB free out of 48676 MiB total
epoch 1 loss: 0.0023135281444410793 val loss: 0.0026852742885239422
6387 MiB free out of 48676 MiB total
epoch 2 loss: 0.0022159101063152775 val loss: 0.002694146314752288
6387 MiB free out of 48676 MiB total
epoch 3 loss: 0.002147843570128316 val loss: 0.002703259960981086
6387 MiB free out of 48676 MiB total
epoch 4 loss: 0.002095911400829209 val loss: 0.002711327324504964
6387 MiB free out of 48676 MiB total
finished adding batches
subset {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
15 mlp.down_proj
Pruning ...
realigning
initial loss 6.2652587890625
final loss 6.256242752075195
quantized
not here
quantized in 52.942293882369995 seconds
35151 MiB free out of 48676 MiB total
finished: {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.002028834714110417 val loss: 0.0014511386034428142
6471 MiB free out of 48676 MiB total
epoch 1 loss: 0.0020069607153345714 val loss: 0.0014724734137416817
6471 MiB free out of 48676 MiB total
epoch 2 loss: 0.0019957471358793555 val loss: 0.0014920330286258832
6471 MiB free out of 48676 MiB total
epoch 3 loss: 0.0019893134776793886 val loss: 0.0015069349465193227
6471 MiB free out of 48676 MiB total
epoch 4 loss: 0.0019853262447213638 val loss: 0.0015166487864917144
6471 MiB free out of 48676 MiB total
35151 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
6471 MiB free out of 48676 MiB total
after cast to cpu
37725 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj'], ['self_attn.o_proj'], ['mlp.up_proj', 'mlp.gate_proj'], ['mlp.down_proj']]
finished adding batches
subset {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
16 self_attn.k_proj
Pruning ...
realigning
initial loss 410.1724853515625
final loss 344.843505859375
quantized
not here
quantized in 21.25681734085083 seconds
37723 MiB free out of 48676 MiB total
16 self_attn.v_proj
Pruning ...
realigning
initial loss 124.30447387695312
final loss 123.8741226196289
quantized
not here
quantized in 18.929206132888794 seconds
37659 MiB free out of 48676 MiB total
16 self_attn.q_proj
Pruning ...
realigning
initial loss 376.6242980957031
final loss 324.3662109375
quantized
not here
quantized in 20.19574546813965 seconds
37703 MiB free out of 48676 MiB total
finished: {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 27648
optimizing the following parameters: dict_keys(['self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 152051712
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0016541617796974606 val loss: 0.0065358629217371345
7219 MiB free out of 48676 MiB total
epoch 1 loss: 0.001374148424474697 val loss: 0.0064415357483085245
7219 MiB free out of 48676 MiB total
epoch 2 loss: 0.0012536066660686629 val loss: 0.006359768012771383
7219 MiB free out of 48676 MiB total
epoch 3 loss: 0.001174115112007712 val loss: 0.0062819410231895745
7219 MiB free out of 48676 MiB total
epoch 4 loss: 0.0011160055942127656 val loss: 0.006217490357812494
7219 MiB free out of 48676 MiB total
finished adding batches
subset {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
16 self_attn.o_proj
Pruning ...
realigning
initial loss 14.1956787109375
final loss 14.09385871887207
quantized
not here
quantized in 21.43687677383423 seconds
37361 MiB free out of 48676 MiB total
finished: {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 36864
optimizing the following parameters: dict_keys(['mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135274496
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0012687703256233362 val loss: 0.004565089708194137
8275 MiB free out of 48676 MiB total
epoch 1 loss: 0.0010077258607452677 val loss: 0.004556602070806548
6227 MiB free out of 48676 MiB total
epoch 2 loss: 0.0008993160067802819 val loss: 0.004550827055936679
6227 MiB free out of 48676 MiB total
epoch 3 loss: 0.0008312441100315482 val loss: 0.004545986303128302
6227 MiB free out of 48676 MiB total
epoch 4 loss: 0.0007828560151210695 val loss: 0.0045413466868922114
6227 MiB free out of 48676 MiB total
finished adding batches
subset {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
16 mlp.up_proj
Pruning ...
realigning
initial loss 240.14474487304688
final loss 236.9434051513672
quantized
not here
quantized in 53.34651589393616 seconds
36337 MiB free out of 48676 MiB total
16 mlp.gate_proj
Pruning ...
realigning
initial loss 313.15252685546875
final loss 286.06353759765625
quantized
not here
quantized in 52.43724822998047 seconds
35993 MiB free out of 48676 MiB total
finished: {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 69120
optimizing the following parameters: dict_keys(['mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 45096960
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.003304596513771685 val loss: 0.00351473105547484
8339 MiB free out of 48676 MiB total
epoch 1 loss: 0.003053821650610189 val loss: 0.003511896255076863
6291 MiB free out of 48676 MiB total
epoch 2 loss: 0.0029169213685236173 val loss: 0.0035117373045068234
6291 MiB free out of 48676 MiB total
epoch 3 loss: 0.0028218876432219986 val loss: 0.003513773888698779
6291 MiB free out of 48676 MiB total
epoch 4 loss: 0.0027491844612086425 val loss: 0.003516667740768753
6291 MiB free out of 48676 MiB total
finished adding batches
subset {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
16 mlp.down_proj
Pruning ...
realigning
initial loss 8.57327938079834
final loss 8.553215980529785
quantized
not here
quantized in 54.087390422821045 seconds
36079 MiB free out of 48676 MiB total
finished: {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.002777658235572744 val loss: 0.0020698547959909774
6203 MiB free out of 48676 MiB total
epoch 1 loss: 0.002747522394201951 val loss: 0.0020933437917847186
6203 MiB free out of 48676 MiB total
epoch 2 loss: 0.0027298555396555457 val loss: 0.002118816686561331
6203 MiB free out of 48676 MiB total
epoch 3 loss: 0.0027186459919903427 val loss: 0.002141742588719353
6203 MiB free out of 48676 MiB total
epoch 4 loss: 0.0027113310225104215 val loss: 0.0021604335925076157
6203 MiB free out of 48676 MiB total
36079 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
6203 MiB free out of 48676 MiB total
after cast to cpu
37725 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj'], ['self_attn.o_proj'], ['mlp.up_proj', 'mlp.gate_proj'], ['mlp.down_proj']]
finished adding batches
subset {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
17 self_attn.k_proj
Pruning ...
realigning
initial loss 444.4179382324219
final loss 362.08355712890625
quantized
not here
quantized in 20.997573852539062 seconds
37723 MiB free out of 48676 MiB total
17 self_attn.v_proj
Pruning ...
realigning
initial loss 133.13656616210938
final loss 132.49038696289062
quantized
not here
quantized in 18.716540575027466 seconds
37659 MiB free out of 48676 MiB total
17 self_attn.q_proj
Pruning ...
realigning
initial loss 392.1114807128906
final loss 333.1948547363281
quantized
not here
quantized in 20.01810622215271 seconds
37595 MiB free out of 48676 MiB total
finished: {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 27648
optimizing the following parameters: dict_keys(['self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 152051712
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0015275416071744985 val loss: 0.006240297574549913
7347 MiB free out of 48676 MiB total
epoch 1 loss: 0.0012431248251232319 val loss: 0.006210649444255978
7347 MiB free out of 48676 MiB total
epoch 2 loss: 0.0011209405138288275 val loss: 0.006160242308396846
7347 MiB free out of 48676 MiB total
epoch 3 loss: 0.0010438938584229618 val loss: 0.006188579049194232
7347 MiB free out of 48676 MiB total
epoch 4 loss: 0.0009860689015113167 val loss: 0.006140562996733934
7347 MiB free out of 48676 MiB total
finished adding batches
subset {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
17 self_attn.o_proj
Pruning ...
realigning
initial loss 10.550954818725586
final loss 10.285628318786621
quantized
not here
quantized in 22.58539056777954 seconds
37339 MiB free out of 48676 MiB total
finished: {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 36864
optimizing the following parameters: dict_keys(['mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135274496
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0008546493018002366 val loss: 0.005697049346053973
8253 MiB free out of 48676 MiB total
epoch 1 loss: 0.0006774524363208911 val loss: 0.005686226446414366
6205 MiB free out of 48676 MiB total
epoch 2 loss: 0.0006096729689488711 val loss: 0.005676293367287144
6205 MiB free out of 48676 MiB total
epoch 3 loss: 0.0005673029734225565 val loss: 0.005667024670401588
6205 MiB free out of 48676 MiB total
epoch 4 loss: 0.0005372520686250937 val loss: 0.005661540839355439
6205 MiB free out of 48676 MiB total
finished adding batches
subset {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
17 mlp.up_proj
Pruning ...
realigning
initial loss 264.2071838378906
final loss 261.68017578125
quantized
not here
quantized in 57.59114146232605 seconds
36315 MiB free out of 48676 MiB total
17 mlp.gate_proj
Pruning ...
realigning
initial loss 358.3164367675781
final loss 328.23406982421875
quantized
not here
quantized in 56.57089710235596 seconds
35971 MiB free out of 48676 MiB total
finished: {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 69120
optimizing the following parameters: dict_keys(['mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 45096960
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0036569892999978038 val loss: 0.0045337851624935865
8317 MiB free out of 48676 MiB total
epoch 1 loss: 0.0034033494575851364 val loss: 0.004528167919488624
6269 MiB free out of 48676 MiB total
epoch 2 loss: 0.003259473745856667 val loss: 0.004526986333075911
6269 MiB free out of 48676 MiB total
epoch 3 loss: 0.0031576154397043865 val loss: 0.004530081583652645
6269 MiB free out of 48676 MiB total
epoch 4 loss: 0.0030789838147029513 val loss: 0.004538372624665499
6269 MiB free out of 48676 MiB total
finished adding batches
subset {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
17 mlp.down_proj
Pruning ...
realigning
initial loss 9.570024490356445
final loss 9.549007415771484
quantized
not here
quantized in 53.97928190231323 seconds
36057 MiB free out of 48676 MiB total
finished: {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0030838627862976864 val loss: 0.0026661086303647608
6181 MiB free out of 48676 MiB total
epoch 1 loss: 0.003039478924620198 val loss: 0.002726922379224561
6181 MiB free out of 48676 MiB total
epoch 2 loss: 0.003015472539118491 val loss: 0.0027764426195062697
6181 MiB free out of 48676 MiB total
epoch 3 loss: 0.0029998084482940612 val loss: 0.0028210332238813862
6181 MiB free out of 48676 MiB total
epoch 4 loss: 0.0029893538576288847 val loss: 0.0028589341964107007
6181 MiB free out of 48676 MiB total
36057 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
6181 MiB free out of 48676 MiB total
after cast to cpu
37725 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj'], ['self_attn.o_proj'], ['mlp.up_proj', 'mlp.gate_proj'], ['mlp.down_proj']]
finished adding batches
subset {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
18 self_attn.k_proj
Pruning ...
realigning
initial loss 463.1064758300781
final loss 374.8900146484375
quantized
not here
quantized in 21.213633060455322 seconds
37723 MiB free out of 48676 MiB total
18 self_attn.v_proj
Pruning ...
realigning
initial loss 161.95529174804688
final loss 161.40371704101562
quantized
not here
quantized in 19.068968772888184 seconds
37659 MiB free out of 48676 MiB total
18 self_attn.q_proj
Pruning ...
realigning
initial loss 415.4652099609375
final loss 343.18853759765625
quantized
not here
quantized in 20.091875791549683 seconds
37703 MiB free out of 48676 MiB total
finished: {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 27648
optimizing the following parameters: dict_keys(['self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 152051712
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0017221019797943882 val loss: 0.006903543049702421
7199 MiB free out of 48676 MiB total
epoch 1 loss: 0.0014094697708060266 val loss: 0.0068412613472901285
7199 MiB free out of 48676 MiB total
epoch 2 loss: 0.001279240267649584 val loss: 0.0068087304825894535
7199 MiB free out of 48676 MiB total
epoch 3 loss: 0.0011956345833823434 val loss: 0.00679599805152975
7199 MiB free out of 48676 MiB total
epoch 4 loss: 0.0011353830545886012 val loss: 0.006782592769013718
7199 MiB free out of 48676 MiB total
finished adding batches
subset {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
18 self_attn.o_proj
Pruning ...
realigning
initial loss 12.50228214263916
final loss 12.287908554077148
quantized
not here
quantized in 21.392414093017578 seconds
37673 MiB free out of 48676 MiB total
finished: {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 36864
optimizing the following parameters: dict_keys(['mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135274496
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0010708050972425553 val loss: 0.006100137776229531
7275 MiB free out of 48676 MiB total
epoch 1 loss: 0.0008554362948416383 val loss: 0.006087817542720586
7275 MiB free out of 48676 MiB total
epoch 2 loss: 0.0007711072926213092 val loss: 0.0060795336903538555
7275 MiB free out of 48676 MiB total
epoch 3 loss: 0.0007186375842138659 val loss: 0.006073872791603208
7275 MiB free out of 48676 MiB total
epoch 4 loss: 0.0006816696072746709 val loss: 0.006069626164389774
7275 MiB free out of 48676 MiB total
finished adding batches
subset {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
18 mlp.up_proj
Pruning ...
realigning
initial loss 291.21826171875
final loss 289.1470642089844
quantized
not here
quantized in 55.12849140167236 seconds
36521 MiB free out of 48676 MiB total
18 mlp.gate_proj
Pruning ...
realigning
initial loss 398.16461181640625
final loss 368.20538330078125
quantized
not here
quantized in 52.47901964187622 seconds
36241 MiB free out of 48676 MiB total
finished: {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 69120
optimizing the following parameters: dict_keys(['mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 45096960
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.004371476763481041 val loss: 0.0050137337821070105
8587 MiB free out of 48676 MiB total
epoch 1 loss: 0.004069608999998309 val loss: 0.00502772445906885
6539 MiB free out of 48676 MiB total
epoch 2 loss: 0.0038969583674770547 val loss: 0.00504242567694746
6539 MiB free out of 48676 MiB total
epoch 3 loss: 0.0037745337594969897 val loss: 0.005057654227130115
6539 MiB free out of 48676 MiB total
epoch 4 loss: 0.0036799804965994554 val loss: 0.005071785883046687
6539 MiB free out of 48676 MiB total
finished adding batches
subset {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
18 mlp.down_proj
Pruning ...
realigning
initial loss 11.456635475158691
final loss 11.38608455657959
quantized
not here
quantized in 56.070882081985474 seconds
35281 MiB free out of 48676 MiB total
finished: {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.003643049356469419 val loss: 0.002775905275484547
6193 MiB free out of 48676 MiB total
epoch 1 loss: 0.00356431945147051 val loss: 0.002878630912164226
6193 MiB free out of 48676 MiB total
epoch 2 loss: 0.003519156223774189 val loss: 0.002971148060169071
4145 MiB free out of 48676 MiB total
epoch 3 loss: 0.00348892369584064 val loss: 0.0030535660189343616
4145 MiB free out of 48676 MiB total
epoch 4 loss: 0.0034680246189964237 val loss: 0.0031219291413435712
4145 MiB free out of 48676 MiB total
35281 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
4145 MiB free out of 48676 MiB total
after cast to cpu
37897 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj'], ['self_attn.o_proj'], ['mlp.up_proj', 'mlp.gate_proj'], ['mlp.down_proj']]
finished adding batches
subset {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
19 self_attn.k_proj
Pruning ...
realigning
initial loss 439.12908935546875
final loss 359.85333251953125
quantized
not here
quantized in 21.086594104766846 seconds
37809 MiB free out of 48676 MiB total
19 self_attn.v_proj
Pruning ...
realigning
initial loss 163.34803771972656
final loss 162.82638549804688
quantized
not here
quantized in 18.999138593673706 seconds
37745 MiB free out of 48676 MiB total
19 self_attn.q_proj
Pruning ...
realigning
initial loss 388.6387634277344
final loss 332.9989013671875
quantized
not here
quantized in 20.167015314102173 seconds
37681 MiB free out of 48676 MiB total
finished: {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 27648
optimizing the following parameters: dict_keys(['self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 152051712
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0017292414950134116 val loss: 0.006671244802419096
7165 MiB free out of 48676 MiB total
epoch 1 loss: 0.0014081701647228329 val loss: 0.006661769381025806
5117 MiB free out of 48676 MiB total
epoch 2 loss: 0.0012682422493526246 val loss: 0.006662145955488086
5117 MiB free out of 48676 MiB total
epoch 3 loss: 0.0011775693128583953 val loss: 0.006658332538791001
5117 MiB free out of 48676 MiB total
epoch 4 loss: 0.0011115273882751353 val loss: 0.00666436628671363
5117 MiB free out of 48676 MiB total
finished adding batches
subset {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
19 self_attn.o_proj
Pruning ...
realigning
initial loss 10.704086303710938
final loss 10.56015396118164
quantized
not here
quantized in 22.67545199394226 seconds
37425 MiB free out of 48676 MiB total
finished: {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 36864
optimizing the following parameters: dict_keys(['mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135274496
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0010553562024142593 val loss: 0.00710852729389444
7219 MiB free out of 48676 MiB total
epoch 1 loss: 0.0008623248559160857 val loss: 0.007073994580423459
7219 MiB free out of 48676 MiB total
epoch 2 loss: 0.0007801499923516531 val loss: 0.007051298773149028
7219 MiB free out of 48676 MiB total
epoch 3 loss: 0.0007282729679900513 val loss: 0.007033793342998251
7219 MiB free out of 48676 MiB total
epoch 4 loss: 0.0006915314625075553 val loss: 0.00702046585502103
7219 MiB free out of 48676 MiB total
finished adding batches
subset {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
19 mlp.up_proj
Pruning ...
realigning
initial loss 313.7678527832031
final loss 311.00543212890625
quantized
not here
quantized in 52.88494324684143 seconds
36315 MiB free out of 48676 MiB total
19 mlp.gate_proj
Pruning ...
realigning
initial loss 422.10015869140625
final loss 394.65142822265625
quantized
not here
quantized in 52.26560640335083 seconds
36057 MiB free out of 48676 MiB total
finished: {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 69120
optimizing the following parameters: dict_keys(['mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 45096960
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0050512690540927 val loss: 0.005441839486593381
8403 MiB free out of 48676 MiB total
epoch 1 loss: 0.004718539010355016 val loss: 0.005471798038342968
6355 MiB free out of 48676 MiB total
epoch 2 loss: 0.004528196735918755 val loss: 0.005499326071003452
6355 MiB free out of 48676 MiB total
epoch 3 loss: 0.004391796228446765 val loss: 0.005522752413526177
6355 MiB free out of 48676 MiB total
epoch 4 loss: 0.004285585917386925 val loss: 0.005542346218135208
6355 MiB free out of 48676 MiB total
finished adding batches
subset {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
19 mlp.down_proj
Pruning ...
realigning
initial loss 11.796810150146484
final loss 11.769637107849121
quantized
not here
quantized in 54.774906635284424 seconds
36143 MiB free out of 48676 MiB total
finished: {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.003813499421084998 val loss: 0.0030141846364131197
6153 MiB free out of 48676 MiB total
epoch 1 loss: 0.003772519325139001 val loss: 0.003065249969949946
6153 MiB free out of 48676 MiB total
epoch 2 loss: 0.0037494842945307028 val loss: 0.003106500400463119
6153 MiB free out of 48676 MiB total
epoch 3 loss: 0.003734623096534051 val loss: 0.003140396744129248
6153 MiB free out of 48676 MiB total
epoch 4 loss: 0.003724713718838757 val loss: 0.0031658715015510097
6153 MiB free out of 48676 MiB total
36143 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
6153 MiB free out of 48676 MiB total
after cast to cpu
37725 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj'], ['self_attn.o_proj'], ['mlp.up_proj', 'mlp.gate_proj'], ['mlp.down_proj']]
finished adding batches
subset {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
20 self_attn.k_proj
Pruning ...
realigning
initial loss 443.82989501953125
final loss 370.9226989746094
quantized
not here
quantized in 21.086580514907837 seconds
37723 MiB free out of 48676 MiB total
20 self_attn.v_proj
Pruning ...
realigning
initial loss 168.20855712890625
final loss 167.84947204589844
quantized
not here
quantized in 18.78524160385132 seconds
37659 MiB free out of 48676 MiB total
20 self_attn.q_proj
Pruning ...
realigning
initial loss 423.1576232910156
final loss 353.97216796875
quantized
not here
quantized in 20.0349440574646 seconds
37531 MiB free out of 48676 MiB total
finished: {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 27648
optimizing the following parameters: dict_keys(['self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 152051712
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.002016948736127233 val loss: 0.008351055555976927
7187 MiB free out of 48676 MiB total
epoch 1 loss: 0.001633499495255819 val loss: 0.008400715596508235
7187 MiB free out of 48676 MiB total
epoch 2 loss: 0.0014721673423991888 val loss: 0.00838593952357769
7187 MiB free out of 48676 MiB total
epoch 3 loss: 0.0013686355596291833 val loss: 0.008361335203517228
7187 MiB free out of 48676 MiB total
epoch 4 loss: 0.0012938255349581596 val loss: 0.008303409471409395
7187 MiB free out of 48676 MiB total
finished adding batches
subset {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
20 self_attn.o_proj
Pruning ...
realigning
initial loss 23.794292449951172
final loss 23.104740142822266
quantized
not here
quantized in 21.73551869392395 seconds
37329 MiB free out of 48676 MiB total
finished: {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 36864
optimizing the following parameters: dict_keys(['mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135274496
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0017075658361136448 val loss: 0.00769925766508095
7219 MiB free out of 48676 MiB total
epoch 1 loss: 0.0012141747911300627 val loss: 0.0076480132702272385
7219 MiB free out of 48676 MiB total
epoch 2 loss: 0.0010454352341184858 val loss: 0.007613334397319704
7219 MiB free out of 48676 MiB total
epoch 3 loss: 0.0009480648636781552 val loss: 0.00758889177814126
7219 MiB free out of 48676 MiB total
epoch 4 loss: 0.0008833653910187422 val loss: 0.007570324931293726
7219 MiB free out of 48676 MiB total
finished adding batches
subset {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
20 mlp.up_proj
Pruning ...
realigning
initial loss 339.876953125
final loss 336.5275573730469
quantized
not here
quantized in 54.14692497253418 seconds
36305 MiB free out of 48676 MiB total
20 mlp.gate_proj
Pruning ...
realigning
initial loss 460.1741943359375
final loss 430.68316650390625
quantized
not here
quantized in 51.68582272529602 seconds
35961 MiB free out of 48676 MiB total
finished: {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 69120
optimizing the following parameters: dict_keys(['mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 45096960
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.00623369132517837 val loss: 0.006746002356521785
8307 MiB free out of 48676 MiB total
epoch 1 loss: 0.005776043257355923 val loss: 0.006759772106306627
6259 MiB free out of 48676 MiB total
epoch 2 loss: 0.005505462206201628 val loss: 0.0067735762859229
6259 MiB free out of 48676 MiB total
epoch 3 loss: 0.005312364490237087 val loss: 0.006788414408219978
6259 MiB free out of 48676 MiB total
epoch 4 loss: 0.005163414276466938 val loss: 0.006802434014389291
6259 MiB free out of 48676 MiB total
finished adding batches
subset {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
20 mlp.down_proj
Pruning ...
realigning
initial loss 15.077155113220215
final loss 14.93879508972168
quantized
not here
quantized in 59.43803548812866 seconds
35961 MiB free out of 48676 MiB total
finished: {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.004805846092494903 val loss: 0.0037864027108298615
6131 MiB free out of 48676 MiB total
epoch 1 loss: 0.004740398948342772 val loss: 0.0038968518347246572
6131 MiB free out of 48676 MiB total
epoch 2 loss: 0.004705607792857336 val loss: 0.0039640604227315634
6131 MiB free out of 48676 MiB total
epoch 3 loss: 0.004681328879087232 val loss: 0.004005345850600861
6131 MiB free out of 48676 MiB total
epoch 4 loss: 0.00466353179217549 val loss: 0.00402243435382843
6131 MiB free out of 48676 MiB total
35961 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
6131 MiB free out of 48676 MiB total
after cast to cpu
37897 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj'], ['self_attn.o_proj'], ['mlp.up_proj', 'mlp.gate_proj'], ['mlp.down_proj']]
finished adding batches
subset {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
21 self_attn.k_proj
Pruning ...
realigning
initial loss 450.7417297363281
final loss 386.83966064453125
quantized
not here
quantized in 21.067301034927368 seconds
37809 MiB free out of 48676 MiB total
21 self_attn.v_proj
Pruning ...
realigning
initial loss 202.38583374023438
final loss 201.96908569335938
quantized
not here
quantized in 19.520188808441162 seconds
37745 MiB free out of 48676 MiB total
21 self_attn.q_proj
Pruning ...
realigning
initial loss 428.03070068359375
final loss 371.8785705566406
quantized
not here
quantized in 20.187724113464355 seconds
37681 MiB free out of 48676 MiB total
finished: {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 27648
optimizing the following parameters: dict_keys(['self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 152051712
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.001875593153272348 val loss: 0.011196782055776566
7293 MiB free out of 48676 MiB total
epoch 1 loss: 0.001540161367302062 val loss: 0.011148272664286196
5245 MiB free out of 48676 MiB total
epoch 2 loss: 0.0013908885348428157 val loss: 0.011106398305855691
5245 MiB free out of 48676 MiB total
epoch 3 loss: 0.0012927041461807676 val loss: 0.01108186412602663
5245 MiB free out of 48676 MiB total
epoch 4 loss: 0.0012213938316563144 val loss: 0.011054478527512401
5245 MiB free out of 48676 MiB total
finished adding batches
subset {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
21 self_attn.o_proj
Pruning ...
realigning
initial loss 13.63022232055664
final loss 13.059173583984375
quantized
not here
quantized in 24.149173974990845 seconds
37339 MiB free out of 48676 MiB total
finished: {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 36864
optimizing the following parameters: dict_keys(['mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135274496
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0011178984796060831 val loss: 0.010457910015247762
7229 MiB free out of 48676 MiB total
epoch 1 loss: 0.0008756621291468036 val loss: 0.010458474571350962
7229 MiB free out of 48676 MiB total
epoch 2 loss: 0.0007901272897470335 val loss: 0.0104603924555704
7229 MiB free out of 48676 MiB total
epoch 3 loss: 0.0007385986159533786 val loss: 0.01046598027460277
7229 MiB free out of 48676 MiB total
epoch 4 loss: 0.0007030071305962338 val loss: 0.010468720574863255
7229 MiB free out of 48676 MiB total
finished adding batches
subset {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
21 mlp.up_proj
Pruning ...
realigning
initial loss 356.91656494140625
final loss 353.9314880371094
quantized
not here
quantized in 52.810906171798706 seconds
36315 MiB free out of 48676 MiB total
21 mlp.gate_proj
Pruning ...
realigning
initial loss 482.25640869140625
final loss 456.2606201171875
quantized
not here
quantized in 51.3097198009491 seconds
35971 MiB free out of 48676 MiB total
finished: {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 69120
optimizing the following parameters: dict_keys(['mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 45096960
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.006675929533230374 val loss: 0.00837914747535251
8317 MiB free out of 48676 MiB total
epoch 1 loss: 0.006244297324883519 val loss: 0.008413080417085439
6269 MiB free out of 48676 MiB total
epoch 2 loss: 0.005979401237709681 val loss: 0.008447470725513995
6269 MiB free out of 48676 MiB total
epoch 3 loss: 0.005786723089840962 val loss: 0.008480959862936288
6269 MiB free out of 48676 MiB total
epoch 4 loss: 0.005635888745018747 val loss: 0.00851180386962369
6269 MiB free out of 48676 MiB total
finished adding batches
subset {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
21 mlp.down_proj
Pruning ...
realigning
initial loss 16.183712005615234
final loss 16.097900390625
quantized
not here
quantized in 54.62170195579529 seconds
35971 MiB free out of 48676 MiB total
finished: {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.005157244882866507 val loss: 0.004637856298359111
6141 MiB free out of 48676 MiB total
epoch 1 loss: 0.005035387523093959 val loss: 0.004760106443427503
6141 MiB free out of 48676 MiB total
epoch 2 loss: 0.004975089443178149 val loss: 0.0048254009743686765
6141 MiB free out of 48676 MiB total
epoch 3 loss: 0.004933155349135632 val loss: 0.004877395258517936
6141 MiB free out of 48676 MiB total
epoch 4 loss: 0.004902017793938285 val loss: 0.004925335291773081
6141 MiB free out of 48676 MiB total
35971 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
6141 MiB free out of 48676 MiB total
after cast to cpu
37553 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj'], ['self_attn.o_proj'], ['mlp.up_proj', 'mlp.gate_proj'], ['mlp.down_proj']]
finished adding batches
subset {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
22 self_attn.k_proj
Pruning ...
realigning
initial loss 488.4015808105469
final loss 411.4453125
quantized
not here
quantized in 21.204506635665894 seconds
37551 MiB free out of 48676 MiB total
22 self_attn.v_proj
Pruning ...
realigning
initial loss 209.29208374023438
final loss 208.7274169921875
quantized
not here
quantized in 19.223981142044067 seconds
37487 MiB free out of 48676 MiB total
22 self_attn.q_proj
Pruning ...
realigning
initial loss 453.19873046875
final loss 397.630615234375
quantized
not here
quantized in 20.361156940460205 seconds
37659 MiB free out of 48676 MiB total
finished: {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 27648
optimizing the following parameters: dict_keys(['self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 152051712
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.003198783762854873 val loss: 0.011251967225689441
7219 MiB free out of 48676 MiB total
epoch 1 loss: 0.0025084317585424287 val loss: 0.011192857171408832
7219 MiB free out of 48676 MiB total
epoch 2 loss: 0.0022418914013542235 val loss: 0.011158987530507147
7219 MiB free out of 48676 MiB total
epoch 3 loss: 0.0020708885558633483 val loss: 0.011136436543893069
7219 MiB free out of 48676 MiB total
epoch 4 loss: 0.001948214933690906 val loss: 0.011133605672512203
7219 MiB free out of 48676 MiB total
finished adding batches
subset {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
22 self_attn.o_proj
Pruning ...
realigning
initial loss 50.51085662841797
final loss 49.182830810546875
quantized
not here
quantized in 22.073453426361084 seconds
37629 MiB free out of 48676 MiB total
finished: {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 36864
optimizing the following parameters: dict_keys(['mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135274496
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.004032434893815662 val loss: 0.011216462415177375
7295 MiB free out of 48676 MiB total
epoch 1 loss: 0.002786404582366231 val loss: 0.011208333657123148
7295 MiB free out of 48676 MiB total
epoch 2 loss: 0.0023113154211387155 val loss: 0.011214144411496818
7295 MiB free out of 48676 MiB total
epoch 3 loss: 0.002024994493694976 val loss: 0.011231257871259004
7295 MiB free out of 48676 MiB total
epoch 4 loss: 0.0018340348651690874 val loss: 0.0112565424060449
7295 MiB free out of 48676 MiB total
finished adding batches
subset {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
22 mlp.up_proj
Pruning ...
realigning
initial loss 384.044189453125
final loss 380.4131164550781
quantized
not here
quantized in 53.330601930618286 seconds
36541 MiB free out of 48676 MiB total
22 mlp.gate_proj
Pruning ...
realigning
initial loss 518.6851196289062
final loss 492.4537658691406
quantized
not here
quantized in 51.88929867744446 seconds
36197 MiB free out of 48676 MiB total
finished: {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 69120
optimizing the following parameters: dict_keys(['mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 45096960
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.007940384151879698 val loss: 0.009494507510680705
8543 MiB free out of 48676 MiB total
epoch 1 loss: 0.007356969079410192 val loss: 0.009534270269796252
6495 MiB free out of 48676 MiB total
epoch 2 loss: 0.006998353717790451 val loss: 0.009572605777066201
6495 MiB free out of 48676 MiB total
epoch 3 loss: 0.006741526351106586 val loss: 0.009608366759493947
6495 MiB free out of 48676 MiB total
epoch 4 loss: 0.006547866567416349 val loss: 0.009639471885748208
6495 MiB free out of 48676 MiB total
finished adding batches
subset {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
22 mlp.down_proj
Pruning ...
realigning
initial loss 20.147869110107422
final loss 19.449630737304688
quantized
not here
quantized in 59.577104568481445 seconds
35237 MiB free out of 48676 MiB total
finished: {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.006193359899043571 val loss: 0.0051014210330322385
6557 MiB free out of 48676 MiB total
epoch 1 loss: 0.0060387963567336556 val loss: 0.005242875951807946
6557 MiB free out of 48676 MiB total
epoch 2 loss: 0.0059534493448154535 val loss: 0.005331308173481375
4509 MiB free out of 48676 MiB total
epoch 3 loss: 0.005890062158869114 val loss: 0.005403910268796608
4509 MiB free out of 48676 MiB total
epoch 4 loss: 0.005840951551363105 val loss: 0.00546624677372165
4509 MiB free out of 48676 MiB total
35237 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
4509 MiB free out of 48676 MiB total
after cast to cpu
37897 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj'], ['self_attn.o_proj'], ['mlp.up_proj', 'mlp.gate_proj'], ['mlp.down_proj']]
finished adding batches
subset {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
23 self_attn.k_proj
Pruning ...
realigning
initial loss 501.7159118652344
final loss 433.26251220703125
quantized
not here
quantized in 20.963388919830322 seconds
37895 MiB free out of 48676 MiB total
23 self_attn.v_proj
Pruning ...
realigning
initial loss 255.75814819335938
final loss 255.2596435546875
quantized
not here
quantized in 18.94411277770996 seconds
37895 MiB free out of 48676 MiB total
23 self_attn.q_proj
Pruning ...
realigning
initial loss 459.60546875
final loss 416.1667175292969
quantized
not here
quantized in 20.729684114456177 seconds
37895 MiB free out of 48676 MiB total
finished: {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 27648
optimizing the following parameters: dict_keys(['self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 152051712
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0027116618748550536 val loss: 0.013839976396411657
8231 MiB free out of 48676 MiB total
epoch 1 loss: 0.0022271955631367746 val loss: 0.013788532058242708
8231 MiB free out of 48676 MiB total
epoch 2 loss: 0.0020089406907572993 val loss: 0.013755137042608112
8231 MiB free out of 48676 MiB total
epoch 3 loss: 0.0018664898816496134 val loss: 0.013726563134696335
8231 MiB free out of 48676 MiB total
epoch 4 loss: 0.00176399220526946 val loss: 0.013706110476050526
8231 MiB free out of 48676 MiB total
finished adding batches
subset {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
23 self_attn.o_proj
Pruning ...
realigning
initial loss 16.48623275756836
final loss 16.297950744628906
quantized
not here
quantized in 22.898367643356323 seconds
37435 MiB free out of 48676 MiB total
finished: {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 36864
optimizing the following parameters: dict_keys(['mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135274496
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0015870479501245427 val loss: 0.012000480492133647
8285 MiB free out of 48676 MiB total
epoch 1 loss: 0.0012748643503073254 val loss: 0.01198467897484079
6237 MiB free out of 48676 MiB total
epoch 2 loss: 0.0011545652737368073 val loss: 0.01197779190260917
6237 MiB free out of 48676 MiB total
epoch 3 loss: 0.0010821936361935514 val loss: 0.011976122739724815
6237 MiB free out of 48676 MiB total
epoch 4 loss: 0.0010328555617888924 val loss: 0.011976266745477915
6237 MiB free out of 48676 MiB total
finished adding batches
subset {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
23 mlp.up_proj
Pruning ...
realigning
initial loss 398.6807861328125
final loss 396.4687805175781
quantized
not here
quantized in 55.396113872528076 seconds
36153 MiB free out of 48676 MiB total
23 mlp.gate_proj
Pruning ...
realigning
initial loss 518.415771484375
final loss 499.6127014160156
quantized
not here
quantized in 54.79395842552185 seconds
36067 MiB free out of 48676 MiB total
finished: {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 69120
optimizing the following parameters: dict_keys(['mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 45096960
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.008591214500484057 val loss: 0.009803509165067226
8413 MiB free out of 48676 MiB total
epoch 1 loss: 0.008025900839129463 val loss: 0.009842704108450562
6365 MiB free out of 48676 MiB total
epoch 2 loss: 0.007671739665966015 val loss: 0.009881591598968953
6365 MiB free out of 48676 MiB total
epoch 3 loss: 0.007414053463435266 val loss: 0.009918698109686375
6365 MiB free out of 48676 MiB total
epoch 4 loss: 0.007213988585135667 val loss: 0.009953189466614276
6365 MiB free out of 48676 MiB total
finished adding batches
subset {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
23 mlp.down_proj
Pruning ...
realigning
initial loss 17.774986267089844
final loss 17.753238677978516
quantized
not here
quantized in 54.606916666030884 seconds
35129 MiB free out of 48676 MiB total
finished: {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.00578720759585849 val loss: 0.004879326006630436
6449 MiB free out of 48676 MiB total
epoch 1 loss: 0.005743552599597024 val loss: 0.004930327646434307
6449 MiB free out of 48676 MiB total
epoch 2 loss: 0.005716157145798206 val loss: 0.0049702747783157974
6449 MiB free out of 48676 MiB total
epoch 3 loss: 0.005696579202776775 val loss: 0.0050059214408975095
6449 MiB free out of 48676 MiB total
epoch 4 loss: 0.00568224106245907 val loss: 0.005038385250372812
6449 MiB free out of 48676 MiB total
35129 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
6449 MiB free out of 48676 MiB total
after cast to cpu
37553 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj'], ['self_attn.o_proj'], ['mlp.up_proj', 'mlp.gate_proj'], ['mlp.down_proj']]
finished adding batches
subset {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
24 self_attn.k_proj
Pruning ...
realigning
initial loss 465.8616943359375
final loss 410.983642578125
quantized
not here
quantized in 21.196687698364258 seconds
37551 MiB free out of 48676 MiB total
24 self_attn.v_proj
Pruning ...
realigning
initial loss 242.1951141357422
final loss 241.72885131835938
quantized
not here
quantized in 19.917946338653564 seconds
37659 MiB free out of 48676 MiB total
24 self_attn.q_proj
Pruning ...
realigning
initial loss 444.38714599609375
final loss 400.46044921875
quantized
not here
quantized in 20.439245462417603 seconds
37659 MiB free out of 48676 MiB total
finished: {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 27648
optimizing the following parameters: dict_keys(['self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 152051712
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0034661046465771506 val loss: 0.016942843794822693
7379 MiB free out of 48676 MiB total
epoch 1 loss: 0.002820768086166936 val loss: 0.016863435623236
7379 MiB free out of 48676 MiB total
epoch 2 loss: 0.0025292660375271225 val loss: 0.016797636984847486
7379 MiB free out of 48676 MiB total
epoch 3 loss: 0.002334904084818845 val loss: 0.016755463206209242
7379 MiB free out of 48676 MiB total
epoch 4 loss: 0.002192310057580471 val loss: 0.016716390498913825
7379 MiB free out of 48676 MiB total
finished adding batches
subset {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
24 self_attn.o_proj
Pruning ...
realigning
initial loss 43.101959228515625
final loss 37.03864288330078
quantized
not here
quantized in 23.16991376876831 seconds
37403 MiB free out of 48676 MiB total
finished: {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 36864
optimizing the following parameters: dict_keys(['mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135274496
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0031119276463869028 val loss: 0.017377137090079486
8285 MiB free out of 48676 MiB total
epoch 1 loss: 0.002182993521273602 val loss: 0.017303011380136013
6237 MiB free out of 48676 MiB total
epoch 2 loss: 0.0018745577608569874 val loss: 0.017254862817935646
6237 MiB free out of 48676 MiB total
epoch 3 loss: 0.0016938875187406666 val loss: 0.017226265743374825
6237 MiB free out of 48676 MiB total
epoch 4 loss: 0.0015727582322142553 val loss: 0.01721361675299704
6237 MiB free out of 48676 MiB total
finished adding batches
subset {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
24 mlp.up_proj
Pruning ...
realigning
initial loss 422.5673828125
final loss 420.26263427734375
quantized
not here
quantized in 53.23062062263489 seconds
36207 MiB free out of 48676 MiB total
24 mlp.gate_proj
Pruning ...
realigning
initial loss 553.551513671875
final loss 530.30712890625
quantized
not here
quantized in 52.50322246551514 seconds
35863 MiB free out of 48676 MiB total
finished: {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 69120
optimizing the following parameters: dict_keys(['mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 45096960
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.009366099664475769 val loss: 0.01312461047200486
7185 MiB free out of 48676 MiB total
epoch 1 loss: 0.008750957444135565 val loss: 0.013158490881323814
7185 MiB free out of 48676 MiB total
epoch 2 loss: 0.008374285684112692 val loss: 0.01319508854066953
7185 MiB free out of 48676 MiB total
epoch 3 loss: 0.008097820547845913 val loss: 0.013231997902039438
7185 MiB free out of 48676 MiB total
epoch 4 loss: 0.007883620946813608 val loss: 0.013267777452711016
7185 MiB free out of 48676 MiB total
finished adding batches
subset {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
24 mlp.down_proj
Pruning ...
realigning
initial loss 21.674888610839844
final loss 21.357749938964844
quantized
not here
quantized in 54.86173152923584 seconds
35949 MiB free out of 48676 MiB total
finished: {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0066751881095115095 val loss: 0.006751214852556586
6179 MiB free out of 48676 MiB total
epoch 1 loss: 0.006401464095688425 val loss: 0.006998082913924009
6179 MiB free out of 48676 MiB total
epoch 2 loss: 0.006302267796854721 val loss: 0.0070626846281811595
6179 MiB free out of 48676 MiB total
epoch 3 loss: 0.006235638062207727 val loss: 0.0070748121361248195
6179 MiB free out of 48676 MiB total
epoch 4 loss: 0.00618579940055497 val loss: 0.0070761335955467075
6179 MiB free out of 48676 MiB total
35949 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
6179 MiB free out of 48676 MiB total
after cast to cpu
37725 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj'], ['self_attn.o_proj'], ['mlp.up_proj', 'mlp.gate_proj'], ['mlp.down_proj']]
finished adding batches
subset {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
25 self_attn.k_proj
Pruning ...
realigning
initial loss 517.0880126953125
final loss 457.7782287597656
quantized
not here
quantized in 21.074731826782227 seconds
37723 MiB free out of 48676 MiB total
25 self_attn.v_proj
Pruning ...
realigning
initial loss 302.2027282714844
final loss 301.64697265625
quantized
not here
quantized in 19.30811309814453 seconds
37659 MiB free out of 48676 MiB total
25 self_attn.q_proj
Pruning ...
realigning
initial loss 487.6271667480469
final loss 449.2542419433594
quantized
not here
quantized in 20.055508136749268 seconds
37595 MiB free out of 48676 MiB total
finished: {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 27648
optimizing the following parameters: dict_keys(['self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 152051712
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.003316885196909425 val loss: 0.01930703571997583
7293 MiB free out of 48676 MiB total
epoch 1 loss: 0.002644515678184689 val loss: 0.019344489090144634
7293 MiB free out of 48676 MiB total
epoch 2 loss: 0.0023865807052061427 val loss: 0.019401739118620753
7293 MiB free out of 48676 MiB total
epoch 3 loss: 0.0022208488153410144 val loss: 0.019451307482086122
7293 MiB free out of 48676 MiB total
epoch 4 loss: 0.0021019172090745997 val loss: 0.019490551785565913
7293 MiB free out of 48676 MiB total
finished adding batches
subset {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
25 self_attn.o_proj
Pruning ...
realigning
initial loss 19.524307250976562
final loss 18.88616943359375
quantized
not here
quantized in 21.875970602035522 seconds
37253 MiB free out of 48676 MiB total
finished: {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 36864
optimizing the following parameters: dict_keys(['mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135274496
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0017136950255007832 val loss: 0.018070378922857344
8231 MiB free out of 48676 MiB total
epoch 1 loss: 0.0013141462577550556 val loss: 0.018000278621912003
6183 MiB free out of 48676 MiB total
epoch 2 loss: 0.0011746144468816055 val loss: 0.017953235539607704
6183 MiB free out of 48676 MiB total
epoch 3 loss: 0.001091065344553499 val loss: 0.017924945103004575
6183 MiB free out of 48676 MiB total
epoch 4 loss: 0.0010345865425733791 val loss: 0.017904577078297734
6183 MiB free out of 48676 MiB total
finished adding batches
subset {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
25 mlp.up_proj
Pruning ...
realigning
initial loss 448.2227783203125
final loss 445.8166809082031
quantized
not here
quantized in 52.82635760307312 seconds
37167 MiB free out of 48676 MiB total
25 mlp.gate_proj
Pruning ...
realigning
initial loss 597.008056640625
final loss 569.8843383789062
quantized
not here
quantized in 53.09662127494812 seconds
36823 MiB free out of 48676 MiB total
finished: {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 69120
optimizing the following parameters: dict_keys(['mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 45096960
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.010807997867232189 val loss: 0.014553198241628706
6885 MiB free out of 48676 MiB total
epoch 1 loss: 0.010090109339216724 val loss: 0.01452908432111144
6885 MiB free out of 48676 MiB total
epoch 2 loss: 0.009659962946898304 val loss: 0.014543622732162476
6885 MiB free out of 48676 MiB total
epoch 3 loss: 0.009343348603579216 val loss: 0.014574724074918777
6885 MiB free out of 48676 MiB total
epoch 4 loss: 0.009091485349927098 val loss: 0.014616906351875514
6885 MiB free out of 48676 MiB total
finished adding batches
subset {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
25 mlp.down_proj
Pruning ...
realigning
initial loss 18.73137092590332
final loss 18.714584350585938
quantized
not here
quantized in 54.98682236671448 seconds
35799 MiB free out of 48676 MiB total
finished: {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.006108527879405301 val loss: 0.007126165117369965
6163 MiB free out of 48676 MiB total
epoch 1 loss: 0.006075739413063275 val loss: 0.007148883421905339
6163 MiB free out of 48676 MiB total
epoch 2 loss: 0.00605582552816486 val loss: 0.0071721989079378545
6163 MiB free out of 48676 MiB total
epoch 3 loss: 0.006042106484528631 val loss: 0.007197047147201374
6163 MiB free out of 48676 MiB total
epoch 4 loss: 0.006032485263858689 val loss: 0.007220999366836622
6163 MiB free out of 48676 MiB total
35799 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
6163 MiB free out of 48676 MiB total
after cast to cpu
37553 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj'], ['self_attn.o_proj'], ['mlp.up_proj', 'mlp.gate_proj'], ['mlp.down_proj']]
finished adding batches
subset {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
26 self_attn.k_proj
Pruning ...
realigning
initial loss 518.9476928710938
final loss 448.45245361328125
quantized
not here
quantized in 21.440329551696777 seconds
37551 MiB free out of 48676 MiB total
26 self_attn.v_proj
Pruning ...
realigning
initial loss 299.5340881347656
final loss 299.1121826171875
quantized
not here
quantized in 18.953900575637817 seconds
37551 MiB free out of 48676 MiB total
26 self_attn.q_proj
Pruning ...
realigning
initial loss 476.4132080078125
final loss 425.9782409667969
quantized
not here
quantized in 20.291273593902588 seconds
37487 MiB free out of 48676 MiB total
finished: {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 27648
optimizing the following parameters: dict_keys(['self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 152051712
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.004168392963038059 val loss: 0.022627711528912187
7379 MiB free out of 48676 MiB total
epoch 1 loss: 0.0034305467415833846 val loss: 0.022671131999231875
7379 MiB free out of 48676 MiB total
epoch 2 loss: 0.003089891970375902 val loss: 0.02270154911093414
7379 MiB free out of 48676 MiB total
epoch 3 loss: 0.0028652043038164265 val loss: 0.022718613035976887
7379 MiB free out of 48676 MiB total
epoch 4 loss: 0.0027021572623198153 val loss: 0.022727451752871275
7379 MiB free out of 48676 MiB total
finished adding batches
subset {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
26 self_attn.o_proj
Pruning ...
realigning
initial loss 39.056182861328125
final loss 27.1827335357666
quantized
not here
quantized in 22.938585996627808 seconds
37403 MiB free out of 48676 MiB total
finished: {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 36864
optimizing the following parameters: dict_keys(['mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135274496
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0025819214733928675 val loss: 0.022060701856389642
8285 MiB free out of 48676 MiB total
epoch 1 loss: 0.0020377808823468513 val loss: 0.022029109997674823
6237 MiB free out of 48676 MiB total
epoch 2 loss: 0.0018181588184233988 val loss: 0.02201515797059983
6237 MiB free out of 48676 MiB total
epoch 3 loss: 0.0016805948107503355 val loss: 0.022010774817317724
6237 MiB free out of 48676 MiB total
epoch 4 loss: 0.001584304479365528 val loss: 0.022006023325957358
6237 MiB free out of 48676 MiB total
finished adding batches
subset {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
26 mlp.up_proj
Pruning ...
realigning
initial loss 468.09967041015625
final loss 464.7267761230469
quantized
not here
quantized in 53.29400324821472 seconds
36293 MiB free out of 48676 MiB total
26 mlp.gate_proj
Pruning ...
realigning
initial loss 645.904296875
final loss 606.2734375
quantized
not here
quantized in 53.08528685569763 seconds
35949 MiB free out of 48676 MiB total
finished: {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 69120
optimizing the following parameters: dict_keys(['mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 45096960
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.010725616266427096 val loss: 0.017808295786380768
7271 MiB free out of 48676 MiB total
epoch 1 loss: 0.01010154665709706 val loss: 0.017819935572333634
7271 MiB free out of 48676 MiB total
epoch 2 loss: 0.009730828409374226 val loss: 0.01783522474579513
7271 MiB free out of 48676 MiB total
epoch 3 loss: 0.009457618252781685 val loss: 0.01785198866855353
7271 MiB free out of 48676 MiB total
epoch 4 loss: 0.009240241241059266 val loss: 0.01787061768118292
7271 MiB free out of 48676 MiB total
finished adding batches
subset {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
26 mlp.down_proj
Pruning ...
realigning
initial loss 21.0716552734375
final loss 20.988357543945312
quantized
not here
quantized in 55.824763774871826 seconds
34925 MiB free out of 48676 MiB total
finished: {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.006625526468269527 val loss: 0.008580494322814047
6245 MiB free out of 48676 MiB total
epoch 1 loss: 0.006512024116091197 val loss: 0.008632389071863145
6245 MiB free out of 48676 MiB total
epoch 2 loss: 0.006469093314080965 val loss: 0.00868038582848385
6245 MiB free out of 48676 MiB total
epoch 3 loss: 0.006441294630349148 val loss: 0.008728315006010234
6245 MiB free out of 48676 MiB total
epoch 4 loss: 0.006422006477805553 val loss: 0.008774166053626686
6245 MiB free out of 48676 MiB total
34925 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
6245 MiB free out of 48676 MiB total
after cast to cpu
37725 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj'], ['self_attn.o_proj'], ['mlp.up_proj', 'mlp.gate_proj'], ['mlp.down_proj']]
finished adding batches
subset {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
27 self_attn.k_proj
Pruning ...
realigning
initial loss 645.7902221679688
final loss 480.50164794921875
quantized
not here
quantized in 20.83991050720215 seconds
37723 MiB free out of 48676 MiB total
27 self_attn.v_proj
Pruning ...
realigning
initial loss 304.04638671875
final loss 302.6439208984375
quantized
not here
quantized in 19.643441438674927 seconds
37659 MiB free out of 48676 MiB total
27 self_attn.q_proj
Pruning ...
realigning
initial loss 529.8629760742188
final loss 443.6387634277344
quantized
not here
quantized in 20.135482788085938 seconds
37595 MiB free out of 48676 MiB total
finished: {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 27648
optimizing the following parameters: dict_keys(['self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 152051712
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0030890161124261795 val loss: 0.030400664662010968
8199 MiB free out of 48676 MiB total
epoch 1 loss: 0.0025517262911307625 val loss: 0.030365146114490926
6151 MiB free out of 48676 MiB total
epoch 2 loss: 0.0023186698845165665 val loss: 0.030331222340464592
6151 MiB free out of 48676 MiB total
epoch 3 loss: 0.0021675586413039127 val loss: 0.03028907091356814
6151 MiB free out of 48676 MiB total
epoch 4 loss: 0.0020591751472238684 val loss: 0.030267128138802946
6151 MiB free out of 48676 MiB total
finished adding batches
subset {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
27 self_attn.o_proj
Pruning ...
realigning
initial loss 26.697601318359375
final loss 26.013790130615234
quantized
not here
quantized in 21.661314010620117 seconds
37307 MiB free out of 48676 MiB total
finished: {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 36864
optimizing the following parameters: dict_keys(['mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135274496
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0020091908945687464 val loss: 0.0253645348129794
8253 MiB free out of 48676 MiB total
epoch 1 loss: 0.0014722535306646023 val loss: 0.02527425903826952
6205 MiB free out of 48676 MiB total
epoch 2 loss: 0.0013092044846416684 val loss: 0.02521531644742936
6205 MiB free out of 48676 MiB total
epoch 3 loss: 0.0012154979895058204 val loss: 0.02518543496262282
6205 MiB free out of 48676 MiB total
epoch 4 loss: 0.0011533962565408729 val loss: 0.025160479126498103
6205 MiB free out of 48676 MiB total
finished adding batches
subset {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
27 mlp.up_proj
Pruning ...
realigning
initial loss 498.94232177734375
final loss 494.0234375
quantized
not here
quantized in 54.405619859695435 seconds
36197 MiB free out of 48676 MiB total
27 mlp.gate_proj
Pruning ...
realigning
initial loss 708.8076171875
final loss 654.4136962890625
quantized
not here
quantized in 59.279311180114746 seconds
35939 MiB free out of 48676 MiB total
finished: {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 69120
optimizing the following parameters: dict_keys(['mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 45096960
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.012004322474240325 val loss: 0.02372511604335159
8285 MiB free out of 48676 MiB total
epoch 1 loss: 0.01130920965806581 val loss: 0.02371767384465784
6237 MiB free out of 48676 MiB total
epoch 2 loss: 0.010901572015427519 val loss: 0.023712646099738777
6237 MiB free out of 48676 MiB total
epoch 3 loss: 0.010601096051686909 val loss: 0.02371125086210668
6237 MiB free out of 48676 MiB total
epoch 4 loss: 0.010362376939156093 val loss: 0.0237146484432742
6237 MiB free out of 48676 MiB total
finished adding batches
subset {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
27 mlp.down_proj
Pruning ...
realigning
initial loss 23.03701400756836
final loss 22.989913940429688
quantized
not here
quantized in 55.323633909225464 seconds
36025 MiB free out of 48676 MiB total
finished: {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.007467143703252077 val loss: 0.0109032325563021
6255 MiB free out of 48676 MiB total
epoch 1 loss: 0.007380058104899945 val loss: 0.01101745938649401
6255 MiB free out of 48676 MiB total
epoch 2 loss: 0.007320994340261677 val loss: 0.011113185959402472
6255 MiB free out of 48676 MiB total
epoch 3 loss: 0.007275935226061847 val loss: 0.011216725164558738
6255 MiB free out of 48676 MiB total
epoch 4 loss: 0.007241259081638418 val loss: 0.011320426827296615
6255 MiB free out of 48676 MiB total
36025 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
6255 MiB free out of 48676 MiB total
after cast to cpu
37725 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj'], ['self_attn.o_proj'], ['mlp.up_proj', 'mlp.gate_proj'], ['mlp.down_proj']]
finished adding batches
subset {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
28 self_attn.k_proj
Pruning ...
realigning
initial loss 631.0696411132812
final loss 480.34490966796875
quantized
not here
quantized in 20.941964626312256 seconds
37723 MiB free out of 48676 MiB total
28 self_attn.v_proj
Pruning ...
realigning
initial loss 338.07373046875
final loss 337.23431396484375
quantized
not here
quantized in 19.4905002117157 seconds
37831 MiB free out of 48676 MiB total
28 self_attn.q_proj
Pruning ...
realigning
initial loss 532.474853515625
final loss 453.40753173828125
quantized
not here
quantized in 20.073572635650635 seconds
37767 MiB free out of 48676 MiB total
finished: {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 27648
optimizing the following parameters: dict_keys(['self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 152051712
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.005226511015280266 val loss: 0.03583425679244101
7379 MiB free out of 48676 MiB total
epoch 1 loss: 0.004208792013741913 val loss: 0.03565565124154091
7379 MiB free out of 48676 MiB total
epoch 2 loss: 0.0037940852034807904 val loss: 0.03554747742600739
5331 MiB free out of 48676 MiB total
epoch 3 loss: 0.0035232378795626573 val loss: 0.035468666115775704
5331 MiB free out of 48676 MiB total
epoch 4 loss: 0.00332594462815905 val loss: 0.035424258559942245
5331 MiB free out of 48676 MiB total
finished adding batches
subset {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
28 self_attn.o_proj
Pruning ...
realigning
initial loss 41.899658203125
final loss 39.94215393066406
quantized
not here
quantized in 22.25385069847107 seconds
37339 MiB free out of 48676 MiB total
finished: {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 36864
optimizing the following parameters: dict_keys(['mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135274496
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0032380122338508954 val loss: 0.030649308813735843
8317 MiB free out of 48676 MiB total
epoch 1 loss: 0.00249411735785543 val loss: 0.030591435846872628
6269 MiB free out of 48676 MiB total
epoch 2 loss: 0.0022210662900761236 val loss: 0.030552237411029637
6269 MiB free out of 48676 MiB total
epoch 3 loss: 0.002049099093710538 val loss: 0.030523169552907348
6269 MiB free out of 48676 MiB total
epoch 4 loss: 0.0019274259102530777 val loss: 0.030506145441904664
6269 MiB free out of 48676 MiB total
finished adding batches
subset {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
28 mlp.up_proj
Pruning ...
realigning
initial loss 564.9763793945312
final loss 553.3594360351562
quantized
not here
quantized in 53.64404892921448 seconds
37339 MiB free out of 48676 MiB total
28 mlp.gate_proj
Pruning ...
realigning
initial loss 753.2792358398438
final loss 694.9226684570312
quantized
not here
quantized in 53.357478857040405 seconds
36995 MiB free out of 48676 MiB total
finished: {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 69120
optimizing the following parameters: dict_keys(['mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 45096960
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.014474667543254327 val loss: 0.025020134169608355
6929 MiB free out of 48676 MiB total
epoch 1 loss: 0.013546611189667601 val loss: 0.025031318771652877
6929 MiB free out of 48676 MiB total
epoch 2 loss: 0.013029308945988305 val loss: 0.0250467344885692
6929 MiB free out of 48676 MiB total
epoch 3 loss: 0.012653232704906259 val loss: 0.025068406015634537
6929 MiB free out of 48676 MiB total
epoch 4 loss: 0.012356016181001905 val loss: 0.02509823499713093
6929 MiB free out of 48676 MiB total
finished adding batches
subset {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
28 mlp.down_proj
Pruning ...
realigning
initial loss 27.896343231201172
final loss 27.686376571655273
quantized
not here
quantized in 58.19943881034851 seconds
35885 MiB free out of 48676 MiB total
finished: {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.00892355107498588 val loss: 0.01299688423750922
6163 MiB free out of 48676 MiB total
epoch 1 loss: 0.00878990491037257 val loss: 0.01315524202072993
6163 MiB free out of 48676 MiB total
epoch 2 loss: 0.00870871288861963 val loss: 0.013279890059493482
6163 MiB free out of 48676 MiB total
epoch 3 loss: 0.008648115981486626 val loss: 0.013396571099292487
6163 MiB free out of 48676 MiB total
epoch 4 loss: 0.008601923276728485 val loss: 0.013503136986400932
6163 MiB free out of 48676 MiB total
35885 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
6163 MiB free out of 48676 MiB total
after cast to cpu
37553 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj'], ['self_attn.o_proj'], ['mlp.up_proj', 'mlp.gate_proj'], ['mlp.down_proj']]
finished adding batches
subset {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
29 self_attn.k_proj
Pruning ...
realigning
initial loss 581.1336669921875
final loss 434.73895263671875
quantized
not here
quantized in 20.912031412124634 seconds
37551 MiB free out of 48676 MiB total
29 self_attn.v_proj
Pruning ...
realigning
initial loss 315.8170166015625
final loss 315.0210266113281
quantized
not here
quantized in 19.222910404205322 seconds
37487 MiB free out of 48676 MiB total
29 self_attn.q_proj
Pruning ...
realigning
initial loss 503.704345703125
final loss 410.4604187011719
quantized
not here
quantized in 20.109505653381348 seconds
37595 MiB free out of 48676 MiB total
finished: {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 27648
optimizing the following parameters: dict_keys(['self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 152051712
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0050627984546736116 val loss: 0.09555921447463334
7203 MiB free out of 48676 MiB total
epoch 1 loss: 0.0039200278333737515 val loss: 0.09378411085344851
7203 MiB free out of 48676 MiB total
epoch 2 loss: 0.0034935717940243194 val loss: 0.09235291136428714
7203 MiB free out of 48676 MiB total
epoch 3 loss: 0.0032249552677967586 val loss: 0.09092544624581933
7203 MiB free out of 48676 MiB total
epoch 4 loss: 0.0030337941334437346 val loss: 0.08982713427394629
7203 MiB free out of 48676 MiB total
finished adding batches
subset {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
29 self_attn.o_proj
Pruning ...
realigning
initial loss 78.21246337890625
final loss 62.91063690185547
quantized
not here
quantized in 22.786551475524902 seconds
37479 MiB free out of 48676 MiB total
finished: {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 36864
optimizing the following parameters: dict_keys(['mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135274496
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.004090917855137377 val loss: 0.05037387739866972
7273 MiB free out of 48676 MiB total
epoch 1 loss: 0.002843598731487873 val loss: 0.0502569533418864
7273 MiB free out of 48676 MiB total
epoch 2 loss: 0.0024894487414712785 val loss: 0.050132753094658256
7273 MiB free out of 48676 MiB total
epoch 3 loss: 0.0022761043364880607 val loss: 0.050006025936454535
7273 MiB free out of 48676 MiB total
epoch 4 loss: 0.0021289195719873533 val loss: 0.049908909713849425
7273 MiB free out of 48676 MiB total
finished adding batches
subset {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
29 mlp.up_proj
Pruning ...
realigning
initial loss 676.763916015625
final loss 632.8826904296875
quantized
not here
quantized in 54.060545921325684 seconds
36455 MiB free out of 48676 MiB total
29 mlp.gate_proj
Pruning ...
realigning
initial loss 868.0123291015625
final loss 769.5999145507812
quantized
not here
quantized in 53.52945899963379 seconds
36111 MiB free out of 48676 MiB total
finished: {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 69120
optimizing the following parameters: dict_keys(['mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 45096960
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.019631669420050457 val loss: 0.14686470059677958
8457 MiB free out of 48676 MiB total
epoch 1 loss: 0.018329440390516538 val loss: 0.14558168081566691
8457 MiB free out of 48676 MiB total
epoch 2 loss: 0.017616537465073634 val loss: 0.14442586665973067
8457 MiB free out of 48676 MiB total
epoch 3 loss: 0.01709746899723541 val loss: 0.14320298237726092
8457 MiB free out of 48676 MiB total
epoch 4 loss: 0.01668516104837181 val loss: 0.14204572676680982
8457 MiB free out of 48676 MiB total
finished adding batches
subset {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
29 mlp.down_proj
Pruning ...
realigning
initial loss 32.8584098815918
final loss 32.15422821044922
quantized
not here
quantized in 59.87855887413025 seconds
35087 MiB free out of 48676 MiB total
finished: {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.010346532544645015 val loss: 0.019921916420571506
7259 MiB free out of 48676 MiB total
epoch 1 loss: 0.010170128341997042 val loss: 0.020059905597008765
7259 MiB free out of 48676 MiB total
epoch 2 loss: 0.010059334221296012 val loss: 0.020141320070251822
5211 MiB free out of 48676 MiB total
epoch 3 loss: 0.009972344923880883 val loss: 0.0202391721541062
5211 MiB free out of 48676 MiB total
epoch 4 loss: 0.009903462079819292 val loss: 0.020358078414574265
5211 MiB free out of 48676 MiB total
35087 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
5211 MiB free out of 48676 MiB total
after cast to cpu
37897 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj'], ['self_attn.o_proj'], ['mlp.up_proj', 'mlp.gate_proj'], ['mlp.down_proj']]
finished adding batches
subset {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
30 self_attn.k_proj
Pruning ...
realigning
initial loss 677.09375
final loss 469.13726806640625
quantized
not here
quantized in 22.118491649627686 seconds
37809 MiB free out of 48676 MiB total
30 self_attn.v_proj
Pruning ...
realigning
initial loss 353.2793884277344
final loss 350.83978271484375
quantized
not here
quantized in 19.88473892211914 seconds
37745 MiB free out of 48676 MiB total
30 self_attn.q_proj
Pruning ...
realigning
initial loss 550.3450317382812
final loss 429.6777648925781
quantized
not here
quantized in 21.01029944419861 seconds
37681 MiB free out of 48676 MiB total
finished: {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 27648
optimizing the following parameters: dict_keys(['self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 152051712
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0045537789665104356 val loss: 8.993702288717031
7177 MiB free out of 48676 MiB total
epoch 1 loss: 0.003274783723099972 val loss: 8.68965857475996
7177 MiB free out of 48676 MiB total
epoch 2 loss: 0.0029183633259890485 val loss: 8.763684533536434
7177 MiB free out of 48676 MiB total
epoch 3 loss: 0.002705088632865227 val loss: 8.944309741258621
7177 MiB free out of 48676 MiB total
epoch 4 loss: 0.002557194184191758 val loss: 8.35095327720046
7177 MiB free out of 48676 MiB total
finished adding batches
subset {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
30 self_attn.o_proj
Pruning ...
realigning
initial loss 57.317108154296875
final loss 46.862918853759766
quantized
not here
quantized in 22.766653299331665 seconds
37673 MiB free out of 48676 MiB total
finished: {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 36864
optimizing the following parameters: dict_keys(['mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135274496
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0035055105181527324 val loss: 3.6346164038404822
7275 MiB free out of 48676 MiB total
epoch 1 loss: 0.002567242947407067 val loss: 3.681896829046309
7275 MiB free out of 48676 MiB total
epoch 2 loss: 0.0022600046931984252 val loss: 3.725030269473791
7275 MiB free out of 48676 MiB total
epoch 3 loss: 0.0020750741387018934 val loss: 3.748927848879248
7275 MiB free out of 48676 MiB total
epoch 4 loss: 0.001951003217982361 val loss: 3.7684221295639873
7275 MiB free out of 48676 MiB total
finished adding batches
subset {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
30 mlp.up_proj
Pruning ...
realigning
initial loss 1243.5400390625
final loss 705.9345703125
quantized
not here
quantized in 57.48194146156311 seconds
36649 MiB free out of 48676 MiB total
30 mlp.gate_proj
Pruning ...
realigning
initial loss 1513.775390625
final loss 856.2279052734375
quantized
not here
quantized in 56.124064207077026 seconds
36305 MiB free out of 48676 MiB total
finished: {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 69120
optimizing the following parameters: dict_keys(['mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 45096960
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.04973596124909818 val loss: 1.1756918791215867
8651 MiB free out of 48676 MiB total
epoch 1 loss: 0.04502504240372218 val loss: 1.1449549719691277
6603 MiB free out of 48676 MiB total
epoch 2 loss: 0.041678303212393075 val loss: 1.1173699665814638
6603 MiB free out of 48676 MiB total
epoch 3 loss: 0.03880601155105978 val loss: 1.091642213286832
6603 MiB free out of 48676 MiB total
epoch 4 loss: 0.03624465534812771 val loss: 1.0623322078026831
6603 MiB free out of 48676 MiB total
finished adding batches
subset {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
30 mlp.down_proj
Pruning ...
realigning
initial loss 47.89433670043945
final loss 46.75719451904297
quantized
not here
quantized in 61.85500168800354 seconds
35345 MiB free out of 48676 MiB total
finished: {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.015028613430331461 val loss: 0.038988376269117
6665 MiB free out of 48676 MiB total
epoch 1 loss: 0.014785355509957299 val loss: 0.05250321782659739
6665 MiB free out of 48676 MiB total
epoch 2 loss: 0.01463092997437343 val loss: 0.05300659313797951
6665 MiB free out of 48676 MiB total
epoch 3 loss: 0.01450540035875747 val loss: 0.052625079639256
6665 MiB free out of 48676 MiB total
epoch 4 loss: 0.01440245606499957 val loss: 0.05095148098189384
6665 MiB free out of 48676 MiB total
35345 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
6665 MiB free out of 48676 MiB total
after cast to cpu
37897 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj'], ['self_attn.o_proj'], ['mlp.up_proj', 'mlp.gate_proj'], ['mlp.down_proj']]
finished adding batches
subset {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
31 self_attn.k_proj
Pruning ...
realigning
initial loss 673.2779541015625
final loss 403.7685546875
quantized
not here
quantized in 23.2819082736969 seconds
37895 MiB free out of 48676 MiB total
31 self_attn.v_proj
Pruning ...
realigning
initial loss 201.6826171875
final loss 200.9287872314453
quantized
not here
quantized in 21.164287328720093 seconds
37767 MiB free out of 48676 MiB total
31 self_attn.q_proj
Pruning ...
realigning
initial loss 441.87384033203125
final loss 330.6483459472656
quantized
not here
quantized in 22.076592683792114 seconds
37703 MiB free out of 48676 MiB total
finished: {'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 27648
optimizing the following parameters: dict_keys(['self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 152051712
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.05579733068589121 val loss: 2.537671759724617
8211 MiB free out of 48676 MiB total
epoch 1 loss: 0.03960705605277326 val loss: 2.859046459197998
6163 MiB free out of 48676 MiB total
epoch 2 loss: 0.03487890723044984 val loss: 3.1622840613126755
6163 MiB free out of 48676 MiB total
epoch 3 loss: 0.03228055285580922 val loss: 3.3321637958288193
6163 MiB free out of 48676 MiB total
epoch 4 loss: 0.030392092929105274 val loss: 3.4776285588741302
6163 MiB free out of 48676 MiB total
finished adding batches
subset {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
31 self_attn.o_proj
Pruning ...
realigning
initial loss 145.45556640625
final loss 82.04521179199219
quantized
not here
quantized in 22.877967834472656 seconds
37329 MiB free out of 48676 MiB total
finished: {'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 36864
optimizing the following parameters: dict_keys(['mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135274496
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.008949041581217898 val loss: 0.1299131433479488
8243 MiB free out of 48676 MiB total
epoch 1 loss: 0.006795901332225185 val loss: 0.14571348344907165
6195 MiB free out of 48676 MiB total
epoch 2 loss: 0.006040199114067946 val loss: 0.1576362424530089
6195 MiB free out of 48676 MiB total
epoch 3 loss: 0.005587227871728828 val loss: 0.1699640853330493
6195 MiB free out of 48676 MiB total
epoch 4 loss: 0.005252692033536732 val loss: 0.17770376754924655
6195 MiB free out of 48676 MiB total
finished adding batches
subset {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
31 mlp.up_proj
Pruning ...
realigning
initial loss 1464.7041015625
final loss 662.513427734375
quantized
not here
quantized in 54.634026527404785 seconds
36305 MiB free out of 48676 MiB total
31 mlp.gate_proj
Pruning ...
realigning
initial loss 1114.3741455078125
final loss 749.8784790039062
quantized
not here
quantized in 52.47256588935852 seconds
35961 MiB free out of 48676 MiB total
finished: {'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False)}
the following parameters will not be optimized: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms'])
number of parameters to not optimize: 69120
optimizing the following parameters: dict_keys(['mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 45096960
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.13437954743858427 val loss: 2.767452597618103
8307 MiB free out of 48676 MiB total
epoch 1 loss: 0.10418767691589892 val loss: 2.749414771795273
6259 MiB free out of 48676 MiB total
epoch 2 loss: 0.08607090421719477 val loss: 2.7101065814495087
6259 MiB free out of 48676 MiB total
epoch 3 loss: 0.07287067163269967 val loss: 2.66830937564373
6259 MiB free out of 48676 MiB total
epoch 4 loss: 0.06321640854002908 val loss: 2.626835972070694
6259 MiB free out of 48676 MiB total
finished adding batches
subset {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
31 mlp.down_proj
Pruning ...
realigning
initial loss 129.38560485839844
final loss 105.62286376953125
quantized
not here
quantized in 59.893834352493286 seconds
35961 MiB free out of 48676 MiB total
finished: {'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.032659683463862166 val loss: 0.17288472317159176
6613 MiB free out of 48676 MiB total
epoch 1 loss: 0.03152953337121289 val loss: 0.17165570333600044
6613 MiB free out of 48676 MiB total
epoch 2 loss: 0.031053561353473924 val loss: 0.17134421411901712
6613 MiB free out of 48676 MiB total
epoch 3 loss: 0.030697924084961414 val loss: 0.17128169536590576
6613 MiB free out of 48676 MiB total
epoch 4 loss: 0.03040175932983402 val loss: 0.17251300811767578
6613 MiB free out of 48676 MiB total
35961 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
6613 MiB free out of 48676 MiB total
after cast to cpu
37725 MiB free out of 48676 MiB total
Total bits: 12995657728 Total params: 6476005376
average bits per value: 2.0067397992227978
total time taken: 29046.045211076736
Loading tokenizer for meta-llama/Llama-2-7b-hf
/home/lliu/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Dataset: wikitext2
Evaluating ...
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
Perplexity: 7.593173
