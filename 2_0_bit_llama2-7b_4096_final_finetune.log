/home/lliu/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.30s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.22it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.12it/s]
Model loaded. LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
4096
Loading tokenizer for meta-llama/Llama-2-7b-hf
Starting...
/home/lliu/anaconda3/lib/python3.11/site-packages/torch/cuda/__init__.py:611: UserWarning: Can't initialize NVML
  warnings.warn("Can't initialize NVML")
39011 MiB free out of 48676 MiB total
attention_mask torch.Size([1, 1, 4096, 4096]) cuda:7 torch.float16
position_ids torch.Size([1, 4096]) cuda:7 torch.int64
position_ids torch.Size([1, 4096])
attention_mask torch.Size([1, 1, 4096, 4096])
Ready.
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
0 self_attn.q_proj
Pruning ...
realigning
initial loss 1.56924569606781
final loss 1.1642513275146484
quantized
not here
quantized in 21.757851362228394 seconds
36577 MiB free out of 48676 MiB total
0 self_attn.k_proj
Pruning ...
realigning
initial loss 1.8915133476257324
final loss 1.5419642925262451
quantized
not here
quantized in 19.39346218109131 seconds
36641 MiB free out of 48676 MiB total
0 self_attn.v_proj
Pruning ...
realigning
initial loss 0.11391407996416092
final loss 0.09255972504615784
quantized
not here
quantized in 18.590361833572388 seconds
36705 MiB free out of 48676 MiB total
0 self_attn.o_proj
Pruning ...
realigning
initial loss 0.016694191843271255
final loss 0.01346983015537262
quantized
not here
quantized in 18.136688947677612 seconds
36705 MiB free out of 48676 MiB total
0 mlp.gate_proj
Pruning ...
realigning
initial loss 5.378171443939209
final loss 4.615377426147461
quantized
not here
quantized in 51.91096091270447 seconds
36425 MiB free out of 48676 MiB total
0 mlp.up_proj
Pruning ...
realigning
initial loss 3.686795473098755
final loss 3.4293406009674072
quantized
not here
quantized in 49.3807692527771 seconds
36145 MiB free out of 48676 MiB total
0 mlp.down_proj
Pruning ...
realigning
initial loss 0.020199565216898918
final loss 0.016655195504426956
quantized
not here
quantized in 47.619935274124146 seconds
36037 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 6.669872373521457e-06 val loss: 5.8921224592722865e-06
8385 MiB free out of 48676 MiB total
epoch 1 loss: 5.848224905236066e-06 val loss: 5.668995640917274e-06
8385 MiB free out of 48676 MiB total
epoch 2 loss: 5.698609911064523e-06 val loss: 5.570438219137941e-06
8385 MiB free out of 48676 MiB total
epoch 3 loss: 5.621043815295934e-06 val loss: 5.516496827340234e-06
8385 MiB free out of 48676 MiB total
epoch 4 loss: 5.570623926587359e-06 val loss: 5.4723235223264055e-06
8385 MiB free out of 48676 MiB total
36037 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8385 MiB free out of 48676 MiB total
after cast to cpu
38579 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
1 self_attn.q_proj
Pruning ...
realigning
initial loss 22.881473541259766
final loss 17.634428024291992
quantized
not here
quantized in 23.883543968200684 seconds
36543 MiB free out of 48676 MiB total
1 self_attn.k_proj
Pruning ...
realigning
initial loss 24.776357650756836
final loss 19.73410987854004
quantized
not here
quantized in 22.7837872505188 seconds
36629 MiB free out of 48676 MiB total
1 self_attn.v_proj
Pruning ...
realigning
initial loss 1.0469536781311035
final loss 0.9162658452987671
quantized
not here
quantized in 21.630560636520386 seconds
36629 MiB free out of 48676 MiB total
1 self_attn.o_proj
Pruning ...
realigning
initial loss 0.09110896289348602
final loss 0.08487710356712341
quantized
not here
quantized in 20.531388521194458 seconds
36693 MiB free out of 48676 MiB total
1 mlp.gate_proj
Pruning ...
realigning
initial loss 25.727439880371094
final loss 18.336498260498047
quantized
not here
quantized in 57.256354331970215 seconds
36413 MiB free out of 48676 MiB total
1 mlp.up_proj
Pruning ...
realigning
initial loss 14.21088981628418
final loss 13.374269485473633
quantized
not here
quantized in 57.35410952568054 seconds
36133 MiB free out of 48676 MiB total
1 mlp.down_proj
Pruning ...
realigning
initial loss 0.11344698816537857
final loss 0.11344698816537857
quantized
not here
quantized in 51.822575092315674 seconds
36025 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0209124241301879 val loss: 0.0007788730872562155
8373 MiB free out of 48676 MiB total
epoch 1 loss: 0.0006607987642155422 val loss: 0.0007964836295286659
8373 MiB free out of 48676 MiB total
epoch 2 loss: 0.0002913008350446944 val loss: 0.0007786433634464629
8373 MiB free out of 48676 MiB total
epoch 3 loss: 0.00017718016846401952 val loss: 0.0007464403061021585
8373 MiB free out of 48676 MiB total
epoch 4 loss: 0.00014727440941442183 val loss: 0.000717244045517873
8373 MiB free out of 48676 MiB total
36025 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8373 MiB free out of 48676 MiB total
after cast to cpu
38579 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
2 self_attn.q_proj
Pruning ...
realigning
initial loss 74.07418823242188
final loss 62.9006462097168
quantized
not here
quantized in 24.344809532165527 seconds
36575 MiB free out of 48676 MiB total
2 self_attn.k_proj
Pruning ...
realigning
initial loss 89.68663024902344
final loss 71.64221954345703
quantized
not here
quantized in 24.219921827316284 seconds
36661 MiB free out of 48676 MiB total
2 self_attn.v_proj
Pruning ...
realigning
initial loss 14.106127738952637
final loss 13.536956787109375
quantized
not here
quantized in 21.112492084503174 seconds
36661 MiB free out of 48676 MiB total
2 self_attn.o_proj
Pruning ...
realigning
initial loss 0.18468061089515686
final loss 0.17025628685951233
quantized
not here
quantized in 20.984810829162598 seconds
36725 MiB free out of 48676 MiB total
2 mlp.gate_proj
Pruning ...
realigning
initial loss 37.81641387939453
final loss 35.618980407714844
quantized
not here
quantized in 57.39228415489197 seconds
36445 MiB free out of 48676 MiB total
2 mlp.up_proj
Pruning ...
realigning
initial loss 27.870197296142578
final loss 27.498802185058594
quantized
not here
quantized in 55.094176292419434 seconds
36165 MiB free out of 48676 MiB total
2 mlp.down_proj
Pruning ...
realigning
initial loss 0.1599484384059906
final loss 0.1589311957359314
quantized
not here
quantized in 52.48266077041626 seconds
36057 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.00014186751087663652 val loss: 0.0007644804936717264
8405 MiB free out of 48676 MiB total
epoch 1 loss: 0.00013672533663111608 val loss: 0.0007637943926965818
8405 MiB free out of 48676 MiB total
epoch 2 loss: 0.00013523279295668544 val loss: 0.0007636698792339303
8405 MiB free out of 48676 MiB total
epoch 3 loss: 0.00013416667411547678 val loss: 0.0007638610804860946
8405 MiB free out of 48676 MiB total
epoch 4 loss: 0.00013332626406281634 val loss: 0.00076395804353524
8405 MiB free out of 48676 MiB total
36057 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8405 MiB free out of 48676 MiB total
after cast to cpu
38579 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
3 self_attn.q_proj
Pruning ...
realigning
initial loss 176.3816375732422
final loss 155.53640747070312
quantized
not here
quantized in 24.155646085739136 seconds
36575 MiB free out of 48676 MiB total
3 self_attn.k_proj
Pruning ...
realigning
initial loss 204.26559448242188
final loss 168.15240478515625
quantized
not here
quantized in 23.885161638259888 seconds
36661 MiB free out of 48676 MiB total
3 self_attn.v_proj
Pruning ...
realigning
initial loss 35.791831970214844
final loss 35.1504020690918
quantized
not here
quantized in 21.38390350341797 seconds
36747 MiB free out of 48676 MiB total
3 self_attn.o_proj
Pruning ...
realigning
initial loss 0.2792697548866272
final loss 0.21047313511371613
quantized
not here
quantized in 22.14905619621277 seconds
36747 MiB free out of 48676 MiB total
3 mlp.gate_proj
Pruning ...
realigning
initial loss 57.688743591308594
final loss 55.966922760009766
quantized
not here
quantized in 57.48922657966614 seconds
36531 MiB free out of 48676 MiB total
3 mlp.up_proj
Pruning ...
realigning
initial loss 44.73365783691406
final loss 44.49824523925781
quantized
not here
quantized in 54.76717185974121 seconds
36251 MiB free out of 48676 MiB total
3 mlp.down_proj
Pruning ...
realigning
initial loss 0.32410091161727905
final loss 0.3218863308429718
quantized
not here
quantized in 52.664360761642456 seconds
35971 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0002808160970744211 val loss: 0.0020501978433458135
8319 MiB free out of 48676 MiB total
epoch 1 loss: 0.00026248488234159595 val loss: 0.0020317293383413926
8319 MiB free out of 48676 MiB total
epoch 2 loss: 0.00025970889043946954 val loss: 0.002026677888352424
8319 MiB free out of 48676 MiB total
epoch 3 loss: 0.00025805887923979753 val loss: 0.002025199675699696
8319 MiB free out of 48676 MiB total
epoch 4 loss: 0.0002569003254393465 val loss: 0.002025212816079147
8319 MiB free out of 48676 MiB total
35971 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8319 MiB free out of 48676 MiB total
after cast to cpu
38579 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
4 self_attn.q_proj
Pruning ...
realigning
initial loss 146.31146240234375
final loss 135.64852905273438
quantized
not here
quantized in 23.975602865219116 seconds
36575 MiB free out of 48676 MiB total
4 self_attn.k_proj
Pruning ...
realigning
initial loss 167.02859497070312
final loss 146.1425018310547
quantized
not here
quantized in 23.32628083229065 seconds
36661 MiB free out of 48676 MiB total
4 self_attn.v_proj
Pruning ...
realigning
initial loss 33.75697326660156
final loss 33.12540817260742
quantized
not here
quantized in 21.21965479850769 seconds
36661 MiB free out of 48676 MiB total
4 self_attn.o_proj
Pruning ...
realigning
initial loss 0.6623339653015137
final loss 0.5091476440429688
quantized
not here
quantized in 22.17108702659607 seconds
36725 MiB free out of 48676 MiB total
4 mlp.gate_proj
Pruning ...
realigning
initial loss 82.57435607910156
final loss 78.72354888916016
quantized
not here
quantized in 57.70038676261902 seconds
36445 MiB free out of 48676 MiB total
4 mlp.up_proj
Pruning ...
realigning
initial loss 58.444862365722656
final loss 57.92974853515625
quantized
not here
quantized in 56.5857994556427 seconds
36165 MiB free out of 48676 MiB total
4 mlp.down_proj
Pruning ...
realigning
initial loss 0.616568922996521
final loss 0.6033287048339844
quantized
not here
quantized in 54.77545785903931 seconds
36057 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.000505941221035755 val loss: 0.0026955982320941985
8405 MiB free out of 48676 MiB total
epoch 1 loss: 0.0004763616659602121 val loss: 0.0026916337810689583
8405 MiB free out of 48676 MiB total
epoch 2 loss: 0.00047016204712235776 val loss: 0.002680644698557444
8405 MiB free out of 48676 MiB total
epoch 3 loss: 0.00046737193656554155 val loss: 0.0026789550756802782
8405 MiB free out of 48676 MiB total
epoch 4 loss: 0.00046560715691157384 val loss: 0.002680079429410398
8405 MiB free out of 48676 MiB total
36057 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8405 MiB free out of 48676 MiB total
after cast to cpu
38579 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
5 self_attn.q_proj
Pruning ...
realigning
initial loss 167.948486328125
final loss 150.48489379882812
quantized
not here
quantized in 23.879727363586426 seconds
36575 MiB free out of 48676 MiB total
5 self_attn.k_proj
Pruning ...
realigning
initial loss 208.58389282226562
final loss 172.8115692138672
quantized
not here
quantized in 23.037651777267456 seconds
36661 MiB free out of 48676 MiB total
5 self_attn.v_proj
Pruning ...
realigning
initial loss 39.01858901977539
final loss 38.51028060913086
quantized
not here
quantized in 21.36639952659607 seconds
36661 MiB free out of 48676 MiB total
5 self_attn.o_proj
Pruning ...
realigning
initial loss 1.108183741569519
final loss 0.9423757791519165
quantized
not here
quantized in 22.266778469085693 seconds
36725 MiB free out of 48676 MiB total
5 mlp.gate_proj
Pruning ...
realigning
initial loss 101.97362518310547
final loss 97.80992126464844
quantized
not here
quantized in 56.25183153152466 seconds
36445 MiB free out of 48676 MiB total
5 mlp.up_proj
Pruning ...
realigning
initial loss 72.44287872314453
final loss 71.95344543457031
quantized
not here
quantized in 54.75691866874695 seconds
36165 MiB free out of 48676 MiB total
5 mlp.down_proj
Pruning ...
realigning
initial loss 1.0338584184646606
final loss 1.0186105966567993
quantized
not here
quantized in 53.502971172332764 seconds
36057 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0007940731629787479 val loss: 0.003190950257703662
9429 MiB free out of 48676 MiB total
epoch 1 loss: 0.0007587241807414102 val loss: 0.003351072606164962
9429 MiB free out of 48676 MiB total
epoch 2 loss: 0.0007516674331782269 val loss: 0.0034307911701034755
9429 MiB free out of 48676 MiB total
epoch 3 loss: 0.0007475599645658804 val loss: 0.00348991209466476
9429 MiB free out of 48676 MiB total
epoch 4 loss: 0.00074482335776338 val loss: 0.0035413982113823295
9429 MiB free out of 48676 MiB total
36057 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
9429 MiB free out of 48676 MiB total
after cast to cpu
38579 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
6 self_attn.q_proj
Pruning ...
realigning
initial loss 276.86016845703125
final loss 242.53570556640625
quantized
not here
quantized in 23.999919414520264 seconds
36575 MiB free out of 48676 MiB total
6 self_attn.k_proj
Pruning ...
realigning
initial loss 302.6595153808594
final loss 246.82496643066406
quantized
not here
quantized in 23.045988082885742 seconds
36661 MiB free out of 48676 MiB total
6 self_attn.v_proj
Pruning ...
realigning
initial loss 57.40948486328125
final loss 56.4884147644043
quantized
not here
quantized in 21.634392976760864 seconds
36747 MiB free out of 48676 MiB total
6 self_attn.o_proj
Pruning ...
realigning
initial loss 1.5893898010253906
final loss 1.4096088409423828
quantized
not here
quantized in 22.41769814491272 seconds
36747 MiB free out of 48676 MiB total
6 mlp.gate_proj
Pruning ...
realigning
initial loss 132.49900817871094
final loss 125.48329162597656
quantized
not here
quantized in 57.319727659225464 seconds
36531 MiB free out of 48676 MiB total
6 mlp.up_proj
Pruning ...
realigning
initial loss 88.23812866210938
final loss 87.4565658569336
quantized
not here
quantized in 56.25983715057373 seconds
36251 MiB free out of 48676 MiB total
6 mlp.down_proj
Pruning ...
realigning
initial loss 1.5766115188598633
final loss 1.550673007965088
quantized
not here
quantized in 54.946966886520386 seconds
35971 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0012339365948719205 val loss: 0.0033006406447384506
8319 MiB free out of 48676 MiB total
epoch 1 loss: 0.0011714242864400148 val loss: 0.0034988363622687757
8319 MiB free out of 48676 MiB total
epoch 2 loss: 0.0011580654017961933 val loss: 0.0036386251013027504
8319 MiB free out of 48676 MiB total
epoch 3 loss: 0.001149745355178311 val loss: 0.0037484481144929305
8319 MiB free out of 48676 MiB total
epoch 4 loss: 0.0011438731944508618 val loss: 0.003842547463136725
8319 MiB free out of 48676 MiB total
35971 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8319 MiB free out of 48676 MiB total
after cast to cpu
38579 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
7 self_attn.q_proj
Pruning ...
realigning
initial loss 312.5882263183594
final loss 269.8381652832031
quantized
not here
quantized in 23.890459537506104 seconds
36575 MiB free out of 48676 MiB total
7 self_attn.k_proj
Pruning ...
realigning
initial loss 319.33392333984375
final loss 263.7790222167969
quantized
not here
quantized in 22.90129852294922 seconds
36661 MiB free out of 48676 MiB total
7 self_attn.v_proj
Pruning ...
realigning
initial loss 64.88159942626953
final loss 64.00713348388672
quantized
not here
quantized in 21.3265540599823 seconds
36661 MiB free out of 48676 MiB total
7 self_attn.o_proj
Pruning ...
realigning
initial loss 2.3176536560058594
final loss 2.0388123989105225
quantized
not here
quantized in 22.365227222442627 seconds
36725 MiB free out of 48676 MiB total
7 mlp.gate_proj
Pruning ...
realigning
initial loss 152.83128356933594
final loss 145.73672485351562
quantized
not here
quantized in 57.486976623535156 seconds
36445 MiB free out of 48676 MiB total
7 mlp.up_proj
Pruning ...
realigning
initial loss 103.84725952148438
final loss 103.05098724365234
quantized
not here
quantized in 56.10757303237915 seconds
36165 MiB free out of 48676 MiB total
7 mlp.down_proj
Pruning ...
realigning
initial loss 2.198960542678833
final loss 2.174010992050171
quantized
not here
quantized in 56.426034927368164 seconds
36057 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.001731159996779752 val loss: 0.004863455629674718
9429 MiB free out of 48676 MiB total
epoch 1 loss: 0.0016575464833294973 val loss: 0.005048708728281781
9429 MiB free out of 48676 MiB total
epoch 2 loss: 0.0016384829659727984 val loss: 0.005158141430001706
9429 MiB free out of 48676 MiB total
epoch 3 loss: 0.0016263389743471635 val loss: 0.005240320781012997
9429 MiB free out of 48676 MiB total
epoch 4 loss: 0.0016178020005099825 val loss: 0.005314234440447763
9429 MiB free out of 48676 MiB total
36057 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
9429 MiB free out of 48676 MiB total
after cast to cpu
38579 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
8 self_attn.q_proj
Pruning ...
realigning
initial loss 302.7135009765625
final loss 272.9486389160156
quantized
not here
quantized in 23.886003732681274 seconds
36575 MiB free out of 48676 MiB total
8 self_attn.k_proj
Pruning ...
realigning
initial loss 298.4496154785156
final loss 260.3521728515625
quantized
not here
quantized in 23.2606303691864 seconds
36661 MiB free out of 48676 MiB total
8 self_attn.v_proj
Pruning ...
realigning
initial loss 66.28659057617188
final loss 65.24946594238281
quantized
not here
quantized in 21.549318075180054 seconds
36747 MiB free out of 48676 MiB total
8 self_attn.o_proj
Pruning ...
realigning
initial loss 3.9635815620422363
final loss 3.806328773498535
quantized
not here
quantized in 22.484044790267944 seconds
36833 MiB free out of 48676 MiB total
8 mlp.gate_proj
Pruning ...
realigning
initial loss 162.60093688964844
final loss 154.07774353027344
quantized
not here
quantized in 57.99538612365723 seconds
36553 MiB free out of 48676 MiB total
8 mlp.up_proj
Pruning ...
realigning
initial loss 115.8498764038086
final loss 114.70964050292969
quantized
not here
quantized in 56.58834385871887 seconds
36273 MiB free out of 48676 MiB total
8 mlp.down_proj
Pruning ...
realigning
initial loss 2.7692480087280273
final loss 2.7347846031188965
quantized
not here
quantized in 54.542532444000244 seconds
35993 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.002302414464793401 val loss: 0.004749139683553949
8341 MiB free out of 48676 MiB total
epoch 1 loss: 0.0022004164220561506 val loss: 0.0048906739393714815
8341 MiB free out of 48676 MiB total
epoch 2 loss: 0.0021757009799330262 val loss: 0.0049890664231497794
8341 MiB free out of 48676 MiB total
epoch 3 loss: 0.002160374278901145 val loss: 0.005069693928817287
8341 MiB free out of 48676 MiB total
epoch 4 loss: 0.0021496845492947614 val loss: 0.005140826309798285
8341 MiB free out of 48676 MiB total
35993 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8341 MiB free out of 48676 MiB total
after cast to cpu
38579 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
9 self_attn.q_proj
Pruning ...
realigning
initial loss 283.2371826171875
final loss 257.5692138671875
quantized
not here
quantized in 23.920386791229248 seconds
36575 MiB free out of 48676 MiB total
9 self_attn.k_proj
Pruning ...
realigning
initial loss 316.38287353515625
final loss 277.8140563964844
quantized
not here
quantized in 23.202265739440918 seconds
36661 MiB free out of 48676 MiB total
9 self_attn.v_proj
Pruning ...
realigning
initial loss 72.5474853515625
final loss 71.59335327148438
quantized
not here
quantized in 21.76210904121399 seconds
36747 MiB free out of 48676 MiB total
9 self_attn.o_proj
Pruning ...
realigning
initial loss 6.548647880554199
final loss 6.468360424041748
quantized
not here
quantized in 21.39536690711975 seconds
36811 MiB free out of 48676 MiB total
9 mlp.gate_proj
Pruning ...
realigning
initial loss 171.9461669921875
final loss 162.8884735107422
quantized
not here
quantized in 57.849445104599 seconds
36531 MiB free out of 48676 MiB total
9 mlp.up_proj
Pruning ...
realigning
initial loss 126.64402770996094
final loss 125.245361328125
quantized
not here
quantized in 55.70701241493225 seconds
36251 MiB free out of 48676 MiB total
9 mlp.down_proj
Pruning ...
realigning
initial loss 3.106813907623291
final loss 3.0836684703826904
quantized
not here
quantized in 55.75035381317139 seconds
35971 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0027609012668108335 val loss: 0.005912436550715938
9343 MiB free out of 48676 MiB total
epoch 1 loss: 0.0026671411342249485 val loss: 0.006035958940628916
9343 MiB free out of 48676 MiB total
epoch 2 loss: 0.0026446062274771975 val loss: 0.006113881972851232
9343 MiB free out of 48676 MiB total
epoch 3 loss: 0.0026303877275495324 val loss: 0.0061900371802039444
9343 MiB free out of 48676 MiB total
epoch 4 loss: 0.00262030143312586 val loss: 0.00626710697542876
9343 MiB free out of 48676 MiB total
35971 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
9343 MiB free out of 48676 MiB total
after cast to cpu
38579 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
10 self_attn.q_proj
Pruning ...
realigning
initial loss 297.83636474609375
final loss 268.16278076171875
quantized
not here
quantized in 23.809611320495605 seconds
36575 MiB free out of 48676 MiB total
10 self_attn.k_proj
Pruning ...
realigning
initial loss 331.8840637207031
final loss 292.8531494140625
quantized
not here
quantized in 23.01590609550476 seconds
36661 MiB free out of 48676 MiB total
10 self_attn.v_proj
Pruning ...
realigning
initial loss 73.16075134277344
final loss 72.02549743652344
quantized
not here
quantized in 21.390267610549927 seconds
36747 MiB free out of 48676 MiB total
10 self_attn.o_proj
Pruning ...
realigning
initial loss 9.986391067504883
final loss 9.640910148620605
quantized
not here
quantized in 22.307602167129517 seconds
36747 MiB free out of 48676 MiB total
10 mlp.gate_proj
Pruning ...
realigning
initial loss 184.8038787841797
final loss 174.37673950195312
quantized
not here
quantized in 57.95173978805542 seconds
36531 MiB free out of 48676 MiB total
10 mlp.up_proj
Pruning ...
realigning
initial loss 135.58944702148438
final loss 133.87948608398438
quantized
not here
quantized in 56.10957384109497 seconds
36251 MiB free out of 48676 MiB total
10 mlp.down_proj
Pruning ...
realigning
initial loss 3.9411263465881348
final loss 3.8535797595977783
quantized
not here
quantized in 57.90087389945984 seconds
35971 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0033420063664379995 val loss: 0.008683453197591007
8319 MiB free out of 48676 MiB total
epoch 1 loss: 0.003236056065361481 val loss: 0.008889168675523251
8319 MiB free out of 48676 MiB total
epoch 2 loss: 0.0032066992589534493 val loss: 0.009028937434777617
8319 MiB free out of 48676 MiB total
epoch 3 loss: 0.0031879322177701397 val loss: 0.009150703670457006
8319 MiB free out of 48676 MiB total
epoch 4 loss: 0.003174777439198806 val loss: 0.009261305385734886
8319 MiB free out of 48676 MiB total
35971 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8319 MiB free out of 48676 MiB total
after cast to cpu
38579 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
11 self_attn.q_proj
Pruning ...
realigning
initial loss 372.19549560546875
final loss 333.31097412109375
quantized
not here
quantized in 24.301048755645752 seconds
36575 MiB free out of 48676 MiB total
11 self_attn.k_proj
Pruning ...
realigning
initial loss 361.22454833984375
final loss 318.365966796875
quantized
not here
quantized in 23.20935034751892 seconds
36661 MiB free out of 48676 MiB total
11 self_attn.v_proj
Pruning ...
realigning
initial loss 99.782958984375
final loss 98.53823852539062
quantized
not here
quantized in 22.034559726715088 seconds
36661 MiB free out of 48676 MiB total
11 self_attn.o_proj
Pruning ...
realigning
initial loss 8.224029541015625
final loss 7.686996936798096
quantized
not here
quantized in 22.371373176574707 seconds
36725 MiB free out of 48676 MiB total
11 mlp.gate_proj
Pruning ...
realigning
initial loss 199.63864135742188
final loss 187.0761260986328
quantized
not here
quantized in 58.27657985687256 seconds
36445 MiB free out of 48676 MiB total
11 mlp.up_proj
Pruning ...
realigning
initial loss 150.60751342773438
final loss 149.04400634765625
quantized
not here
quantized in 55.69398880004883 seconds
36165 MiB free out of 48676 MiB total
11 mlp.down_proj
Pruning ...
realigning
initial loss 4.299873352050781
final loss 4.2555742263793945
quantized
not here
quantized in 55.60961031913757 seconds
36057 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.003795630360400537 val loss: 0.007263299426995218
8405 MiB free out of 48676 MiB total
epoch 1 loss: 0.0036532829308271175 val loss: 0.00743472104659304
8405 MiB free out of 48676 MiB total
epoch 2 loss: 0.003614679826569045 val loss: 0.007506992114940658
8405 MiB free out of 48676 MiB total
epoch 3 loss: 0.003589969244785607 val loss: 0.007557140721473843
8405 MiB free out of 48676 MiB total
epoch 4 loss: 0.003572463923774194 val loss: 0.007598132535349578
8405 MiB free out of 48676 MiB total
36057 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8405 MiB free out of 48676 MiB total
after cast to cpu
38579 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
12 self_attn.q_proj
Pruning ...
realigning
initial loss 357.38885498046875
final loss 322.498291015625
quantized
not here
quantized in 23.817975759506226 seconds
36575 MiB free out of 48676 MiB total
12 self_attn.k_proj
Pruning ...
realigning
initial loss 393.4090270996094
final loss 344.48846435546875
quantized
not here
quantized in 23.075140714645386 seconds
36661 MiB free out of 48676 MiB total
12 self_attn.v_proj
Pruning ...
realigning
initial loss 97.6458511352539
final loss 96.65319061279297
quantized
not here
quantized in 21.665693044662476 seconds
36747 MiB free out of 48676 MiB total
12 self_attn.o_proj
Pruning ...
realigning
initial loss 10.547369956970215
final loss 10.222344398498535
quantized
not here
quantized in 22.599251747131348 seconds
36833 MiB free out of 48676 MiB total
12 mlp.gate_proj
Pruning ...
realigning
initial loss 208.58370971679688
final loss 197.03318786621094
quantized
not here
quantized in 58.27704167366028 seconds
36553 MiB free out of 48676 MiB total
12 mlp.up_proj
Pruning ...
realigning
initial loss 164.59339904785156
final loss 162.75228881835938
quantized
not here
quantized in 56.34103870391846 seconds
36273 MiB free out of 48676 MiB total
12 mlp.down_proj
Pruning ...
realigning
initial loss 4.62619686126709
final loss 4.592390060424805
quantized
not here
quantized in 56.891823291778564 seconds
35993 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.004423815131303854 val loss: 0.007846005668397993
8341 MiB free out of 48676 MiB total
epoch 1 loss: 0.004291307857783977 val loss: 0.008029102493310347
8341 MiB free out of 48676 MiB total
epoch 2 loss: 0.004253735900419997 val loss: 0.008123726060148329
8341 MiB free out of 48676 MiB total
epoch 3 loss: 0.004229086511259084 val loss: 0.008199498406611383
8341 MiB free out of 48676 MiB total
epoch 4 loss: 0.0042114411498914706 val loss: 0.008265526674222201
8341 MiB free out of 48676 MiB total
35993 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8341 MiB free out of 48676 MiB total
after cast to cpu
38579 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
13 self_attn.q_proj
Pruning ...
realigning
initial loss 360.63897705078125
final loss 321.5132141113281
quantized
not here
quantized in 23.799646615982056 seconds
36575 MiB free out of 48676 MiB total
13 self_attn.k_proj
Pruning ...
realigning
initial loss 394.8692626953125
final loss 339.9315185546875
quantized
not here
quantized in 22.819014310836792 seconds
36661 MiB free out of 48676 MiB total
13 self_attn.v_proj
Pruning ...
realigning
initial loss 108.09858703613281
final loss 107.36998748779297
quantized
not here
quantized in 21.62239718437195 seconds
36747 MiB free out of 48676 MiB total
13 self_attn.o_proj
Pruning ...
realigning
initial loss 10.572565078735352
final loss 10.305952072143555
quantized
not here
quantized in 22.264371871948242 seconds
36833 MiB free out of 48676 MiB total
13 mlp.gate_proj
Pruning ...
realigning
initial loss 221.37319946289062
final loss 208.13623046875
quantized
not here
quantized in 58.24857544898987 seconds
36617 MiB free out of 48676 MiB total
13 mlp.up_proj
Pruning ...
realigning
initial loss 179.579833984375
final loss 177.771240234375
quantized
not here
quantized in 56.67312788963318 seconds
36337 MiB free out of 48676 MiB total
13 mlp.down_proj
Pruning ...
realigning
initial loss 5.149576187133789
final loss 5.131896018981934
quantized
not here
quantized in 56.45493531227112 seconds
36229 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.004977509132004343 val loss: 0.009038703923579305
8233 MiB free out of 48676 MiB total
epoch 1 loss: 0.004809144349565031 val loss: 0.009467503987252712
8233 MiB free out of 48676 MiB total
epoch 2 loss: 0.004764563149365131 val loss: 0.009734473191201687
8233 MiB free out of 48676 MiB total
epoch 3 loss: 0.004735763082862832 val loss: 0.009961815143469721
8233 MiB free out of 48676 MiB total
epoch 4 loss: 0.0047148154699243605 val loss: 0.010160896985325962
8233 MiB free out of 48676 MiB total
36229 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8233 MiB free out of 48676 MiB total
after cast to cpu
38579 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
14 self_attn.q_proj
Pruning ...
realigning
initial loss 388.9443359375
final loss 339.3280029296875
quantized
not here
quantized in 23.816489934921265 seconds
36575 MiB free out of 48676 MiB total
14 self_attn.k_proj
Pruning ...
realigning
initial loss 422.67449951171875
final loss 361.9632568359375
quantized
not here
quantized in 22.954412698745728 seconds
36661 MiB free out of 48676 MiB total
14 self_attn.v_proj
Pruning ...
realigning
initial loss 108.9345932006836
final loss 108.1956558227539
quantized
not here
quantized in 21.524760723114014 seconds
36747 MiB free out of 48676 MiB total
14 self_attn.o_proj
Pruning ...
realigning
initial loss 13.800034523010254
final loss 13.704773902893066
quantized
not here
quantized in 19.12209701538086 seconds
36747 MiB free out of 48676 MiB total
14 mlp.gate_proj
Pruning ...
realigning
initial loss 248.48171997070312
final loss 229.71217346191406
quantized
not here
quantized in 52.58418393135071 seconds
36531 MiB free out of 48676 MiB total
14 mlp.up_proj
Pruning ...
realigning
initial loss 196.11526489257812
final loss 194.03994750976562
quantized
not here
quantized in 50.75562572479248 seconds
36251 MiB free out of 48676 MiB total
14 mlp.down_proj
Pruning ...
realigning
initial loss 5.841971397399902
final loss 5.831549644470215
quantized
not here
quantized in 50.04304575920105 seconds
35971 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.006037404793460155 val loss: 0.010175070550758392
8319 MiB free out of 48676 MiB total
epoch 1 loss: 0.005849337961990386 val loss: 0.010482024226803333
8319 MiB free out of 48676 MiB total
epoch 2 loss: 0.005795364199002506 val loss: 0.010672570730093867
8319 MiB free out of 48676 MiB total
epoch 3 loss: 0.005759686311648693 val loss: 0.010838339396286756
8319 MiB free out of 48676 MiB total
epoch 4 loss: 0.0057334655393788125 val loss: 0.01098847232060507
8319 MiB free out of 48676 MiB total
35971 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8319 MiB free out of 48676 MiB total
after cast to cpu
38579 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
15 self_attn.q_proj
Pruning ...
realigning
initial loss 363.72021484375
final loss 309.7206115722656
quantized
not here
quantized in 21.496938228607178 seconds
36575 MiB free out of 48676 MiB total
15 self_attn.k_proj
Pruning ...
realigning
initial loss 409.1344909667969
final loss 342.4910888671875
quantized
not here
quantized in 20.404631853103638 seconds
36661 MiB free out of 48676 MiB total
15 self_attn.v_proj
Pruning ...
realigning
initial loss 112.4442138671875
final loss 111.61743927001953
quantized
not here
quantized in 19.15552806854248 seconds
36661 MiB free out of 48676 MiB total
15 self_attn.o_proj
Pruning ...
realigning
initial loss 12.723081588745117
final loss 12.558889389038086
quantized
not here
quantized in 19.93192768096924 seconds
36725 MiB free out of 48676 MiB total
15 mlp.gate_proj
Pruning ...
realigning
initial loss 272.93359375
final loss 253.4476318359375
quantized
not here
quantized in 52.29401898384094 seconds
36445 MiB free out of 48676 MiB total
15 mlp.up_proj
Pruning ...
realigning
initial loss 217.59341430664062
final loss 215.39759826660156
quantized
not here
quantized in 50.68388533592224 seconds
36165 MiB free out of 48676 MiB total
15 mlp.down_proj
Pruning ...
realigning
initial loss 7.136415004730225
final loss 7.123391628265381
quantized
not here
quantized in 51.08339190483093 seconds
36057 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.007022301189863356 val loss: 0.012389568437356502
9429 MiB free out of 48676 MiB total
epoch 1 loss: 0.006816993201937294 val loss: 0.012774150585755706
9429 MiB free out of 48676 MiB total
epoch 2 loss: 0.006755371334293159 val loss: 0.013003614032641053
9429 MiB free out of 48676 MiB total
epoch 3 loss: 0.006715413837810047 val loss: 0.013204388378653675
9429 MiB free out of 48676 MiB total
epoch 4 loss: 0.006686657750833547 val loss: 0.01338676130399108
9429 MiB free out of 48676 MiB total
36057 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
9429 MiB free out of 48676 MiB total
after cast to cpu
38579 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
16 self_attn.q_proj
Pruning ...
realigning
initial loss 399.982421875
final loss 338.2333984375
quantized
not here
quantized in 21.32738947868347 seconds
36575 MiB free out of 48676 MiB total
16 self_attn.k_proj
Pruning ...
realigning
initial loss 434.7511901855469
final loss 358.4559631347656
quantized
not here
quantized in 20.560529708862305 seconds
36661 MiB free out of 48676 MiB total
16 self_attn.v_proj
Pruning ...
realigning
initial loss 129.1230926513672
final loss 128.69390869140625
quantized
not here
quantized in 19.249188661575317 seconds
36661 MiB free out of 48676 MiB total
16 self_attn.o_proj
Pruning ...
realigning
initial loss 17.150747299194336
final loss 17.049171447753906
quantized
not here
quantized in 18.87112832069397 seconds
36725 MiB free out of 48676 MiB total
16 mlp.gate_proj
Pruning ...
realigning
initial loss 329.891845703125
final loss 299.1701354980469
quantized
not here
quantized in 52.120240449905396 seconds
36445 MiB free out of 48676 MiB total
16 mlp.up_proj
Pruning ...
realigning
initial loss 249.64471435546875
final loss 246.18653869628906
quantized
not here
quantized in 50.744789600372314 seconds
36165 MiB free out of 48676 MiB total
16 mlp.down_proj
Pruning ...
realigning
initial loss 9.43591022491455
final loss 9.418465614318848
quantized
not here
quantized in 50.50891852378845 seconds
35885 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.009499795014562551 val loss: 0.01805442781187594
9257 MiB free out of 48676 MiB total
epoch 1 loss: 0.009169004864816088 val loss: 0.019151915446855128
9257 MiB free out of 48676 MiB total
epoch 2 loss: 0.00906993471289752 val loss: 0.019817220862023532
9257 MiB free out of 48676 MiB total
epoch 3 loss: 0.009007159176690038 val loss: 0.02041805547196418
9257 MiB free out of 48676 MiB total
epoch 4 loss: 0.008961033599916846 val loss: 0.020988749340176582
9257 MiB free out of 48676 MiB total
35885 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
9257 MiB free out of 48676 MiB total
after cast to cpu
38579 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
17 self_attn.q_proj
Pruning ...
realigning
initial loss 419.6286315917969
final loss 347.3739318847656
quantized
not here
quantized in 21.403204679489136 seconds
36575 MiB free out of 48676 MiB total
17 self_attn.k_proj
Pruning ...
realigning
initial loss 468.9991760253906
final loss 376.2625732421875
quantized
not here
quantized in 20.479790210723877 seconds
36661 MiB free out of 48676 MiB total
17 self_attn.v_proj
Pruning ...
realigning
initial loss 138.4224090576172
final loss 137.79428100585938
quantized
not here
quantized in 19.467566967010498 seconds
36661 MiB free out of 48676 MiB total
17 self_attn.o_proj
Pruning ...
realigning
initial loss 13.895206451416016
final loss 13.491827011108398
quantized
not here
quantized in 20.086231470108032 seconds
36725 MiB free out of 48676 MiB total
17 mlp.gate_proj
Pruning ...
realigning
initial loss 379.0932312011719
final loss 345.6644287109375
quantized
not here
quantized in 52.542534828186035 seconds
36445 MiB free out of 48676 MiB total
17 mlp.up_proj
Pruning ...
realigning
initial loss 274.78338623046875
final loss 272.16876220703125
quantized
not here
quantized in 51.085832834243774 seconds
36165 MiB free out of 48676 MiB total
17 mlp.down_proj
Pruning ...
realigning
initial loss 10.432684898376465
final loss 10.425413131713867
quantized
not here
quantized in 50.5359833240509 seconds
36057 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.009634128276957199 val loss: 0.01800944039132446
9429 MiB free out of 48676 MiB total
epoch 1 loss: 0.009318343931226991 val loss: 0.01873784710187465
9429 MiB free out of 48676 MiB total
epoch 2 loss: 0.009227160655427724 val loss: 0.01913082937244326
9429 MiB free out of 48676 MiB total
epoch 3 loss: 0.009167849668301642 val loss: 0.01946758560370654
9429 MiB free out of 48676 MiB total
epoch 4 loss: 0.009123152543907054 val loss: 0.019787603872828186
9429 MiB free out of 48676 MiB total
36057 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
9429 MiB free out of 48676 MiB total
after cast to cpu
38579 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
18 self_attn.q_proj
Pruning ...
realigning
initial loss 443.2532958984375
final loss 358.2608642578125
quantized
not here
quantized in 21.275527954101562 seconds
36575 MiB free out of 48676 MiB total
18 self_attn.k_proj
Pruning ...
realigning
initial loss 494.4909973144531
final loss 389.09368896484375
quantized
not here
quantized in 20.518057107925415 seconds
36661 MiB free out of 48676 MiB total
18 self_attn.v_proj
Pruning ...
realigning
initial loss 169.1518096923828
final loss 168.5968475341797
quantized
not here
quantized in 19.17773175239563 seconds
36747 MiB free out of 48676 MiB total
18 self_attn.o_proj
Pruning ...
realigning
initial loss 13.374175071716309
final loss 13.1803560256958
quantized
not here
quantized in 19.255085706710815 seconds
36747 MiB free out of 48676 MiB total
18 mlp.gate_proj
Pruning ...
realigning
initial loss 424.1799621582031
final loss 389.42364501953125
quantized
not here
quantized in 53.01460909843445 seconds
36531 MiB free out of 48676 MiB total
18 mlp.up_proj
Pruning ...
realigning
initial loss 304.7025451660156
final loss 302.52679443359375
quantized
not here
quantized in 51.653263092041016 seconds
36251 MiB free out of 48676 MiB total
18 mlp.down_proj
Pruning ...
realigning
initial loss 12.301656723022461
final loss 12.287490844726562
quantized
not here
quantized in 52.160709381103516 seconds
35971 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.010908906580880284 val loss: 0.022336493711918592
8319 MiB free out of 48676 MiB total
epoch 1 loss: 0.010549050646659452 val loss: 0.024278823635540903
8319 MiB free out of 48676 MiB total
epoch 2 loss: 0.01044127067871159 val loss: 0.025319977896288037
8319 MiB free out of 48676 MiB total
epoch 3 loss: 0.010372969562013168 val loss: 0.02611533785238862
8319 MiB free out of 48676 MiB total
epoch 4 loss: 0.010322355396056082 val loss: 0.02680163679178804
8319 MiB free out of 48676 MiB total
35971 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8319 MiB free out of 48676 MiB total
after cast to cpu
38579 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
19 self_attn.q_proj
Pruning ...
realigning
initial loss 420.1928405761719
final loss 350.343994140625
quantized
not here
quantized in 21.388225078582764 seconds
36575 MiB free out of 48676 MiB total
19 self_attn.k_proj
Pruning ...
realigning
initial loss 473.89111328125
final loss 377.4512939453125
quantized
not here
quantized in 20.48741102218628 seconds
36661 MiB free out of 48676 MiB total
19 self_attn.v_proj
Pruning ...
realigning
initial loss 171.9422607421875
final loss 171.45819091796875
quantized
not here
quantized in 19.20752263069153 seconds
36661 MiB free out of 48676 MiB total
19 self_attn.o_proj
Pruning ...
realigning
initial loss 13.794926643371582
final loss 13.278624534606934
quantized
not here
quantized in 20.071659326553345 seconds
36725 MiB free out of 48676 MiB total
19 mlp.gate_proj
Pruning ...
realigning
initial loss 445.58935546875
final loss 417.1772155761719
quantized
not here
quantized in 52.25544595718384 seconds
36445 MiB free out of 48676 MiB total
19 mlp.up_proj
Pruning ...
realigning
initial loss 328.6205749511719
final loss 326.0129699707031
quantized
not here
quantized in 50.207639932632446 seconds
36165 MiB free out of 48676 MiB total
19 mlp.down_proj
Pruning ...
realigning
initial loss 13.14200496673584
final loss 13.1288423538208
quantized
not here
quantized in 51.61777687072754 seconds
36057 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.01210696273483336 val loss: 0.02085019275546074
8405 MiB free out of 48676 MiB total
epoch 1 loss: 0.01176191630656831 val loss: 0.02131934033241123
8405 MiB free out of 48676 MiB total
epoch 2 loss: 0.011649839354504365 val loss: 0.02149148192256689
8405 MiB free out of 48676 MiB total
epoch 3 loss: 0.011575572170841042 val loss: 0.021621856722049415
8405 MiB free out of 48676 MiB total
epoch 4 loss: 0.011520260166435037 val loss: 0.021740608499385417
8405 MiB free out of 48676 MiB total
36057 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8405 MiB free out of 48676 MiB total
after cast to cpu
38579 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
20 self_attn.q_proj
Pruning ...
realigning
initial loss 448.947509765625
final loss 368.27069091796875
quantized
not here
quantized in 21.374227046966553 seconds
36575 MiB free out of 48676 MiB total
20 self_attn.k_proj
Pruning ...
realigning
initial loss 468.8568420410156
final loss 386.3447265625
quantized
not here
quantized in 20.483535766601562 seconds
36661 MiB free out of 48676 MiB total
20 self_attn.v_proj
Pruning ...
realigning
initial loss 175.6117401123047
final loss 175.26165771484375
quantized
not here
quantized in 19.3085834980011 seconds
36661 MiB free out of 48676 MiB total
20 self_attn.o_proj
Pruning ...
realigning
initial loss 27.506893157958984
final loss 26.65947151184082
quantized
not here
quantized in 19.162769556045532 seconds
36725 MiB free out of 48676 MiB total
20 mlp.gate_proj
Pruning ...
realigning
initial loss 476.3368225097656
final loss 445.70538330078125
quantized
not here
quantized in 51.98888540267944 seconds
36445 MiB free out of 48676 MiB total
20 mlp.up_proj
Pruning ...
realigning
initial loss 349.25787353515625
final loss 346.072509765625
quantized
not here
quantized in 51.8734815120697 seconds
36165 MiB free out of 48676 MiB total
20 mlp.down_proj
Pruning ...
realigning
initial loss 16.834653854370117
final loss 16.780771255493164
quantized
not here
quantized in 52.89432430267334 seconds
36057 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.016177038552996237 val loss: 0.029660740168765187
8405 MiB free out of 48676 MiB total
epoch 1 loss: 0.015290936149540357 val loss: 0.03228145791217685
8405 MiB free out of 48676 MiB total
epoch 2 loss: 0.015009550799732096 val loss: 0.033264414640143514
8405 MiB free out of 48676 MiB total
epoch 3 loss: 0.014850387909973506 val loss: 0.03364970698021352
8405 MiB free out of 48676 MiB total
epoch 4 loss: 0.014736975019332021 val loss: 0.0337878679856658
8405 MiB free out of 48676 MiB total
36057 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8405 MiB free out of 48676 MiB total
after cast to cpu
38579 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
21 self_attn.q_proj
Pruning ...
realigning
initial loss 448.1623840332031
final loss 388.01336669921875
quantized
not here
quantized in 21.42926788330078 seconds
36575 MiB free out of 48676 MiB total
21 self_attn.k_proj
Pruning ...
realigning
initial loss 476.2904052734375
final loss 406.54608154296875
quantized
not here
quantized in 20.498185634613037 seconds
36661 MiB free out of 48676 MiB total
21 self_attn.v_proj
Pruning ...
realigning
initial loss 212.3695831298828
final loss 211.9474639892578
quantized
not here
quantized in 19.952016830444336 seconds
36661 MiB free out of 48676 MiB total
21 self_attn.o_proj
Pruning ...
realigning
initial loss 17.266427993774414
final loss 15.686455726623535
quantized
not here
quantized in 20.081753492355347 seconds
36725 MiB free out of 48676 MiB total
21 mlp.gate_proj
Pruning ...
realigning
initial loss 505.79779052734375
final loss 478.763427734375
quantized
not here
quantized in 52.27241635322571 seconds
36445 MiB free out of 48676 MiB total
21 mlp.up_proj
Pruning ...
realigning
initial loss 372.11761474609375
final loss 369.21917724609375
quantized
not here
quantized in 51.170509815216064 seconds
36165 MiB free out of 48676 MiB total
21 mlp.down_proj
Pruning ...
realigning
initial loss 17.324283599853516
final loss 17.308673858642578
quantized
not here
quantized in 51.911811113357544 seconds
36057 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.015372387271781918 val loss: 0.03413908299989998
9429 MiB free out of 48676 MiB total
epoch 1 loss: 0.014810627500992268 val loss: 0.03526297817006707
9429 MiB free out of 48676 MiB total
epoch 2 loss: 0.014650624441856053 val loss: 0.03585070068947971
9429 MiB free out of 48676 MiB total
epoch 3 loss: 0.014551820735505316 val loss: 0.03625972243025899
9429 MiB free out of 48676 MiB total
epoch 4 loss: 0.014477542485110462 val loss: 0.03658995544537902
9429 MiB free out of 48676 MiB total
36057 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
9429 MiB free out of 48676 MiB total
after cast to cpu
38579 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
22 self_attn.q_proj
Pruning ...
realigning
initial loss 484.3896179199219
final loss 418.3414001464844
quantized
not here
quantized in 21.444554567337036 seconds
36575 MiB free out of 48676 MiB total
22 self_attn.k_proj
Pruning ...
realigning
initial loss 514.3775024414062
final loss 430.56396484375
quantized
not here
quantized in 20.599602460861206 seconds
36661 MiB free out of 48676 MiB total
22 self_attn.v_proj
Pruning ...
realigning
initial loss 219.57135009765625
final loss 219.10470581054688
quantized
not here
quantized in 19.29718041419983 seconds
36747 MiB free out of 48676 MiB total
22 self_attn.o_proj
Pruning ...
realigning
initial loss 74.54473876953125
final loss 72.22346496582031
quantized
not here
quantized in 19.56468939781189 seconds
36833 MiB free out of 48676 MiB total
22 mlp.gate_proj
Pruning ...
realigning
initial loss 531.0028686523438
final loss 506.2188415527344
quantized
not here
quantized in 51.88452863693237 seconds
36617 MiB free out of 48676 MiB total
22 mlp.up_proj
Pruning ...
realigning
initial loss 390.64459228515625
final loss 387.81854248046875
quantized
not here
quantized in 51.972758054733276 seconds
36337 MiB free out of 48676 MiB total
22 mlp.down_proj
Pruning ...
realigning
initial loss 19.284992218017578
final loss 19.270788192749023
quantized
not here
quantized in 51.87729001045227 seconds
36229 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.024665876029757783 val loss: 0.036718318704515696
9601 MiB free out of 48676 MiB total
epoch 1 loss: 0.022496919438708574 val loss: 0.037998625775799155
9601 MiB free out of 48676 MiB total
epoch 2 loss: 0.021451521824928932 val loss: 0.038545730989426374
9601 MiB free out of 48676 MiB total
epoch 3 loss: 0.020919533373671584 val loss: 0.03875575540587306
9601 MiB free out of 48676 MiB total
epoch 4 loss: 0.0206362731551053 val loss: 0.03879807214252651
9601 MiB free out of 48676 MiB total
36229 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
9601 MiB free out of 48676 MiB total
after cast to cpu
38579 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
23 self_attn.q_proj
Pruning ...
realigning
initial loss 494.8587951660156
final loss 445.8977355957031
quantized
not here
quantized in 21.510180234909058 seconds
36575 MiB free out of 48676 MiB total
23 self_attn.k_proj
Pruning ...
realigning
initial loss 534.4002075195312
final loss 460.1258544921875
quantized
not here
quantized in 20.50951623916626 seconds
36661 MiB free out of 48676 MiB total
23 self_attn.v_proj
Pruning ...
realigning
initial loss 274.05841064453125
final loss 273.56488037109375
quantized
not here
quantized in 19.347997426986694 seconds
36747 MiB free out of 48676 MiB total
23 self_attn.o_proj
Pruning ...
realigning
initial loss 22.221759796142578
final loss 21.950960159301758
quantized
not here
quantized in 18.902254581451416 seconds
36747 MiB free out of 48676 MiB total
23 mlp.gate_proj
Pruning ...
realigning
initial loss 551.9521484375
final loss 532.7447509765625
quantized
not here
quantized in 52.04293131828308 seconds
36531 MiB free out of 48676 MiB total
23 mlp.up_proj
Pruning ...
realigning
initial loss 423.88983154296875
final loss 421.8487548828125
quantized
not here
quantized in 51.37131595611572 seconds
36251 MiB free out of 48676 MiB total
23 mlp.down_proj
Pruning ...
realigning
initial loss 21.040180206298828
final loss 21.031877517700195
quantized
not here
quantized in 51.5834424495697 seconds
35971 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.02023234631633386 val loss: 0.036586437141522765
8319 MiB free out of 48676 MiB total
epoch 1 loss: 0.019612271586083807 val loss: 0.03703894722275436
8319 MiB free out of 48676 MiB total
epoch 2 loss: 0.01938542354037054 val loss: 0.03720588027499616
8319 MiB free out of 48676 MiB total
epoch 3 loss: 0.019250747543992475 val loss: 0.037312019150704145
8319 MiB free out of 48676 MiB total
epoch 4 loss: 0.019156337817548774 val loss: 0.037401056149974465
8319 MiB free out of 48676 MiB total
35971 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8319 MiB free out of 48676 MiB total
after cast to cpu
38579 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
24 self_attn.q_proj
Pruning ...
realigning
initial loss 475.860595703125
final loss 427.4408874511719
quantized
not here
quantized in 21.71877956390381 seconds
36575 MiB free out of 48676 MiB total
24 self_attn.k_proj
Pruning ...
realigning
initial loss 497.3932189941406
final loss 437.6707458496094
quantized
not here
quantized in 20.773393869400024 seconds
36661 MiB free out of 48676 MiB total
24 self_attn.v_proj
Pruning ...
realigning
initial loss 259.20703125
final loss 258.7231750488281
quantized
not here
quantized in 19.889060258865356 seconds
36747 MiB free out of 48676 MiB total
24 self_attn.o_proj
Pruning ...
realigning
initial loss 46.08650207519531
final loss 37.14484786987305
quantized
not here
quantized in 20.17911124229431 seconds
36747 MiB free out of 48676 MiB total
24 mlp.gate_proj
Pruning ...
realigning
initial loss 584.0975952148438
final loss 561.3875122070312
quantized
not here
quantized in 51.936328172683716 seconds
36531 MiB free out of 48676 MiB total
24 mlp.up_proj
Pruning ...
realigning
initial loss 443.74530029296875
final loss 441.7779235839844
quantized
not here
quantized in 51.34930944442749 seconds
36251 MiB free out of 48676 MiB total
24 mlp.down_proj
Pruning ...
realigning
initial loss 21.945758819580078
final loss 21.938077926635742
quantized
not here
quantized in 51.001009464263916 seconds
35971 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.022653268548310734 val loss: 0.04586351593025029
8319 MiB free out of 48676 MiB total
epoch 1 loss: 0.021263327289489098 val loss: 0.045378947630524635
8319 MiB free out of 48676 MiB total
epoch 2 loss: 0.020858294243225828 val loss: 0.045034124283120036
8319 MiB free out of 48676 MiB total
epoch 3 loss: 0.020640487520722672 val loss: 0.044844773365184665
8319 MiB free out of 48676 MiB total
epoch 4 loss: 0.020498123107245192 val loss: 0.04475509375333786
8319 MiB free out of 48676 MiB total
35971 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8319 MiB free out of 48676 MiB total
after cast to cpu
38579 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
25 self_attn.q_proj
Pruning ...
realigning
initial loss 523.0193481445312
final loss 478.0876770019531
quantized
not here
quantized in 21.47772240638733 seconds
36575 MiB free out of 48676 MiB total
25 self_attn.k_proj
Pruning ...
realigning
initial loss 553.6185913085938
final loss 487.65924072265625
quantized
not here
quantized in 20.428791761398315 seconds
36661 MiB free out of 48676 MiB total
25 self_attn.v_proj
Pruning ...
realigning
initial loss 322.0347900390625
final loss 321.5082702636719
quantized
not here
quantized in 19.46579098701477 seconds
36747 MiB free out of 48676 MiB total
25 self_attn.o_proj
Pruning ...
realigning
initial loss 20.927566528320312
final loss 20.266138076782227
quantized
not here
quantized in 19.150190591812134 seconds
36811 MiB free out of 48676 MiB total
25 mlp.gate_proj
Pruning ...
realigning
initial loss 637.1512451171875
final loss 607.908447265625
quantized
not here
quantized in 53.02982544898987 seconds
36531 MiB free out of 48676 MiB total
25 mlp.up_proj
Pruning ...
realigning
initial loss 476.8128662109375
final loss 474.58428955078125
quantized
not here
quantized in 51.37515068054199 seconds
36251 MiB free out of 48676 MiB total
25 mlp.down_proj
Pruning ...
realigning
initial loss 22.649545669555664
final loss 22.63184356689453
quantized
not here
quantized in 51.57773303985596 seconds
35971 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.02280633665213827 val loss: 0.052381135523319244
9343 MiB free out of 48676 MiB total
epoch 1 loss: 0.021941522558336146 val loss: 0.05231847590766847
9343 MiB free out of 48676 MiB total
epoch 2 loss: 0.021649716902174987 val loss: 0.052203085040673614
9343 MiB free out of 48676 MiB total
epoch 3 loss: 0.02146550762699917 val loss: 0.05210778792388737
9343 MiB free out of 48676 MiB total
epoch 4 loss: 0.021327954455045983 val loss: 0.052057577995583415
9343 MiB free out of 48676 MiB total
35971 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
9343 MiB free out of 48676 MiB total
after cast to cpu
38579 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
26 self_attn.q_proj
Pruning ...
realigning
initial loss 516.9679565429688
final loss 454.7384033203125
quantized
not here
quantized in 21.852949857711792 seconds
36575 MiB free out of 48676 MiB total
26 self_attn.k_proj
Pruning ...
realigning
initial loss 566.5780029296875
final loss 479.0538635253906
quantized
not here
quantized in 20.809720993041992 seconds
36661 MiB free out of 48676 MiB total
26 self_attn.v_proj
Pruning ...
realigning
initial loss 319.86676025390625
final loss 319.4077453613281
quantized
not here
quantized in 19.693854808807373 seconds
36747 MiB free out of 48676 MiB total
26 self_attn.o_proj
Pruning ...
realigning
initial loss 40.84283447265625
final loss 27.63515853881836
quantized
not here
quantized in 20.197282075881958 seconds
36747 MiB free out of 48676 MiB total
26 mlp.gate_proj
Pruning ...
realigning
initial loss 697.5660400390625
final loss 653.217041015625
quantized
not here
quantized in 52.70173382759094 seconds
36531 MiB free out of 48676 MiB total
26 mlp.up_proj
Pruning ...
realigning
initial loss 498.2857971191406
final loss 495.34466552734375
quantized
not here
quantized in 51.59634852409363 seconds
36251 MiB free out of 48676 MiB total
26 mlp.down_proj
Pruning ...
realigning
initial loss 23.90854835510254
final loss 23.896778106689453
quantized
not here
quantized in 51.24317789077759 seconds
35971 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.024821491038892418 val loss: 0.0592707775067538
8319 MiB free out of 48676 MiB total
epoch 1 loss: 0.023938184167491272 val loss: 0.05975862615741789
8319 MiB free out of 48676 MiB total
epoch 2 loss: 0.02365498481958639 val loss: 0.0600126136559993
8319 MiB free out of 48676 MiB total
epoch 3 loss: 0.023481454758439213 val loss: 0.06016112631186843
8319 MiB free out of 48676 MiB total
epoch 4 loss: 0.02335316910466645 val loss: 0.06025884603150189
8319 MiB free out of 48676 MiB total
35971 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8319 MiB free out of 48676 MiB total
after cast to cpu
38579 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
27 self_attn.q_proj
Pruning ...
realigning
initial loss 580.5172729492188
final loss 478.1295471191406
quantized
not here
quantized in 21.375548839569092 seconds
36575 MiB free out of 48676 MiB total
27 self_attn.k_proj
Pruning ...
realigning
initial loss 710.6474609375
final loss 515.7634887695312
quantized
not here
quantized in 20.44012975692749 seconds
36661 MiB free out of 48676 MiB total
27 self_attn.v_proj
Pruning ...
realigning
initial loss 328.8819885253906
final loss 327.7792663574219
quantized
not here
quantized in 20.205862998962402 seconds
36747 MiB free out of 48676 MiB total
27 self_attn.o_proj
Pruning ...
realigning
initial loss 32.50895309448242
final loss 31.627147674560547
quantized
not here
quantized in 19.14766502380371 seconds
36833 MiB free out of 48676 MiB total
27 mlp.gate_proj
Pruning ...
realigning
initial loss 767.7279052734375
final loss 706.95703125
quantized
not here
quantized in 52.58465933799744 seconds
36617 MiB free out of 48676 MiB total
27 mlp.up_proj
Pruning ...
realigning
initial loss 534.6515502929688
final loss 529.4884033203125
quantized
not here
quantized in 51.23453140258789 seconds
36337 MiB free out of 48676 MiB total
27 mlp.down_proj
Pruning ...
realigning
initial loss 27.140453338623047
final loss 27.105985641479492
quantized
not here
quantized in 52.03777766227722 seconds
36229 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.026873404771322384 val loss: 0.07766067003831267
9601 MiB free out of 48676 MiB total
epoch 1 loss: 0.02577932488929946 val loss: 0.07884974451735616
9601 MiB free out of 48676 MiB total
epoch 2 loss: 0.025362472704728134 val loss: 0.07917333068326116
9601 MiB free out of 48676 MiB total
epoch 3 loss: 0.025128471403149888 val loss: 0.07925150915980339
9601 MiB free out of 48676 MiB total
epoch 4 loss: 0.024970691054477356 val loss: 0.07928957091644406
9601 MiB free out of 48676 MiB total
36229 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
9601 MiB free out of 48676 MiB total
after cast to cpu
38579 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
28 self_attn.q_proj
Pruning ...
realigning
initial loss 581.3734130859375
final loss 488.120849609375
quantized
not here
quantized in 21.469871044158936 seconds
36575 MiB free out of 48676 MiB total
28 self_attn.k_proj
Pruning ...
realigning
initial loss 703.7068481445312
final loss 523.1502075195312
quantized
not here
quantized in 20.529465675354004 seconds
36661 MiB free out of 48676 MiB total
28 self_attn.v_proj
Pruning ...
realigning
initial loss 366.3170471191406
final loss 365.51605224609375
quantized
not here
quantized in 19.81041383743286 seconds
36661 MiB free out of 48676 MiB total
28 self_attn.o_proj
Pruning ...
realigning
initial loss 52.54256057739258
final loss 49.582420349121094
quantized
not here
quantized in 19.82252860069275 seconds
36725 MiB free out of 48676 MiB total
28 mlp.gate_proj
Pruning ...
realigning
initial loss 815.3575439453125
final loss 748.1343994140625
quantized
not here
quantized in 52.84698414802551 seconds
36445 MiB free out of 48676 MiB total
28 mlp.up_proj
Pruning ...
realigning
initial loss 605.0634765625
final loss 593.4361572265625
quantized
not here
quantized in 51.856250286102295 seconds
36165 MiB free out of 48676 MiB total
28 mlp.down_proj
Pruning ...
realigning
initial loss 32.126609802246094
final loss 32.06535339355469
quantized
not here
quantized in 53.91753029823303 seconds
35885 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.03533370945660863 val loss: 0.08832191349938512
9257 MiB free out of 48676 MiB total
epoch 1 loss: 0.03321253905596677 val loss: 0.09463388379663229
9257 MiB free out of 48676 MiB total
epoch 2 loss: 0.032443450021673925 val loss: 0.09763043886050582
9257 MiB free out of 48676 MiB total
epoch 3 loss: 0.03205424857151229 val loss: 0.09862945508211851
9257 MiB free out of 48676 MiB total
epoch 4 loss: 0.031801996621652506 val loss: 0.09886341774836183
9257 MiB free out of 48676 MiB total
35885 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
9257 MiB free out of 48676 MiB total
after cast to cpu
38579 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
29 self_attn.q_proj
Pruning ...
realigning
initial loss 548.4703369140625
final loss 445.4741516113281
quantized
not here
quantized in 21.562730312347412 seconds
36575 MiB free out of 48676 MiB total
29 self_attn.k_proj
Pruning ...
realigning
initial loss 629.306640625
final loss 468.45294189453125
quantized
not here
quantized in 20.542248249053955 seconds
36661 MiB free out of 48676 MiB total
29 self_attn.v_proj
Pruning ...
realigning
initial loss 340.6217346191406
final loss 339.93133544921875
quantized
not here
quantized in 19.515347003936768 seconds
36661 MiB free out of 48676 MiB total
29 self_attn.o_proj
Pruning ...
realigning
initial loss 80.385498046875
final loss 57.712379455566406
quantized
not here
quantized in 20.2390353679657 seconds
36725 MiB free out of 48676 MiB total
29 mlp.gate_proj
Pruning ...
realigning
initial loss 918.4134521484375
final loss 817.5814208984375
quantized
not here
quantized in 53.03963851928711 seconds
36445 MiB free out of 48676 MiB total
29 mlp.up_proj
Pruning ...
realigning
initial loss 697.8933715820312
final loss 666.0599975585938
quantized
not here
quantized in 52.968382596969604 seconds
36165 MiB free out of 48676 MiB total
29 mlp.down_proj
Pruning ...
realigning
initial loss 37.45335006713867
final loss 37.313419342041016
quantized
not here
quantized in 53.69127154350281 seconds
36057 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.04159606448956765 val loss: 0.385776381008327
8405 MiB free out of 48676 MiB total
epoch 1 loss: 0.038532345643034205 val loss: 0.4020704897120595
8405 MiB free out of 48676 MiB total
epoch 2 loss: 0.0375111865287181 val loss: 0.4093893524259329
8405 MiB free out of 48676 MiB total
epoch 3 loss: 0.03702586310100742 val loss: 0.41291672363877296
8405 MiB free out of 48676 MiB total
epoch 4 loss: 0.03671766011393629 val loss: 0.4147300971671939
8405 MiB free out of 48676 MiB total
36057 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8405 MiB free out of 48676 MiB total
after cast to cpu
38579 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
30 self_attn.q_proj
Pruning ...
realigning
initial loss 598.991943359375
final loss 467.9399108886719
quantized
not here
quantized in 21.465614080429077 seconds
36575 MiB free out of 48676 MiB total
30 self_attn.k_proj
Pruning ...
realigning
initial loss 749.9430541992188
final loss 512.0807495117188
quantized
not here
quantized in 20.41289186477661 seconds
36661 MiB free out of 48676 MiB total
30 self_attn.v_proj
Pruning ...
realigning
initial loss 384.67303466796875
final loss 382.32391357421875
quantized
not here
quantized in 19.731212615966797 seconds
36747 MiB free out of 48676 MiB total
30 self_attn.o_proj
Pruning ...
realigning
initial loss 71.18325805664062
final loss 58.617881774902344
quantized
not here
quantized in 20.270219087600708 seconds
36811 MiB free out of 48676 MiB total
30 mlp.gate_proj
Pruning ...
realigning
initial loss 1150.14453125
final loss 883.817138671875
quantized
not here
quantized in 53.05373191833496 seconds
36531 MiB free out of 48676 MiB total
30 mlp.up_proj
Pruning ...
realigning
initial loss 886.0562744140625
final loss 724.4716796875
quantized
not here
quantized in 52.61496591567993 seconds
36251 MiB free out of 48676 MiB total
30 mlp.down_proj
Pruning ...
realigning
initial loss 54.98240661621094
final loss 53.479225158691406
quantized
not here
quantized in 57.390711069107056 seconds
35971 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0770503459207248 val loss: 9.290130823850632
9343 MiB free out of 48676 MiB total
epoch 1 loss: 0.0586086914408952 val loss: 32.26540958881378
9343 MiB free out of 48676 MiB total
epoch 2 loss: 0.05212873895652592 val loss: 13.352196000516415
9343 MiB free out of 48676 MiB total
epoch 3 loss: 0.05037563617224805 val loss: 9.520601108670235
9343 MiB free out of 48676 MiB total
epoch 4 loss: 0.04954360981355421 val loss: 7.802836686372757
9343 MiB free out of 48676 MiB total
35971 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
9343 MiB free out of 48676 MiB total
after cast to cpu
38579 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
31 self_attn.q_proj
Pruning ...
realigning
initial loss 490.1017761230469
final loss 359.54229736328125
quantized
not here
quantized in 21.313206672668457 seconds
36575 MiB free out of 48676 MiB total
31 self_attn.k_proj
Pruning ...
realigning
initial loss 749.478759765625
final loss 436.7936096191406
quantized
not here
quantized in 20.62186336517334 seconds
36661 MiB free out of 48676 MiB total
31 self_attn.v_proj
Pruning ...
realigning
initial loss 218.58575439453125
final loss 217.78823852539062
quantized
not here
quantized in 19.361937522888184 seconds
36747 MiB free out of 48676 MiB total
31 self_attn.o_proj
Pruning ...
realigning
initial loss 135.02603149414062
final loss 73.55130767822266
quantized
not here
quantized in 20.300923824310303 seconds
36747 MiB free out of 48676 MiB total
31 mlp.gate_proj
Pruning ...
realigning
initial loss 1133.971435546875
final loss 778.1339721679688
quantized
not here
quantized in 52.96462059020996 seconds
36531 MiB free out of 48676 MiB total
31 mlp.up_proj
Pruning ...
realigning
initial loss 1567.967529296875
final loss 711.29345703125
quantized
not here
quantized in 52.73735570907593 seconds
36251 MiB free out of 48676 MiB total
31 mlp.down_proj
Pruning ...
realigning
initial loss 127.12669372558594
final loss 104.38290405273438
quantized
not here
quantized in 57.79292702674866 seconds
35971 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.centriods', 'self_attn.q_proj.quantizer.rowwise_norms', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.centriods', 'self_attn.k_proj.quantizer.rowwise_norms', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.centriods', 'self_attn.v_proj.quantizer.rowwise_norms', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.centriods', 'self_attn.o_proj.quantizer.rowwise_norms', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.centriods', 'mlp.gate_proj.quantizer.rowwise_norms', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.centriods', 'mlp.up_proj.quantizer.rowwise_norms', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.centriods', 'mlp.down_proj.quantizer.rowwise_norms', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 93440
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.16919869103003293 val loss: 4.969424933195114
8319 MiB free out of 48676 MiB total
epoch 1 loss: 0.13810629973886535 val loss: 4.840781956911087
8319 MiB free out of 48676 MiB total
epoch 2 loss: 0.12748273205943406 val loss: 4.784380957484245
8319 MiB free out of 48676 MiB total
epoch 3 loss: 0.12140766222728416 val loss: 4.745956242084503
8319 MiB free out of 48676 MiB total
epoch 4 loss: 0.11759346537292004 val loss: 4.707433149218559
8319 MiB free out of 48676 MiB total
35971 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8319 MiB free out of 48676 MiB total
after cast to cpu
38579 MiB free out of 48676 MiB total
Total bits: 12995657728 Total params: 6476005376
average bits per value: 2.0067397992227978
total time taken: 15738.9172270298
Loading tokenizer for meta-llama/Llama-2-7b-hf
/home/lliu/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Dataset: wikitext2
Evaluating ...
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
Perplexity: 7.098966
