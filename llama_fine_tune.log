wandb: Currently logged in as: m6481. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in /data/lliu/huffman/wandb/run-20241114_190846-9dz7zvuk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glowing-hill-1
wandb: ‚≠êÔ∏è View project at https://wandb.ai/m6481/post_training_quantization
wandb: üöÄ View run at https://wandb.ai/m6481/post_training_quantization/runs/9dz7zvuk
/home/lliu/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:01<00:01,  1.22s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:01<00:00,  1.29it/s]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:01<00:00,  1.18it/s]
Model loaded. LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00,  1.01it/s]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:01<00:00,  1.48it/s]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:01<00:00,  1.38it/s]
Model loaded. LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (rotary_emb): LlamaRotaryEmbedding()
          (k_proj): QuantizedLinear(
            (quantizer): VectorQuantizer_SampleReassign()
          )
          (v_proj): QuantizedLinear(
            (quantizer): VectorQuantizer_SampleReassign()
          )
          (q_proj): QuantizedLinear(
            (quantizer): VectorQuantizer_SampleReassign()
          )
          (o_proj): QuantizedLinear(
            (quantizer): VectorQuantizer_SampleReassign()
          )
        )
        (mlp): LlamaMLP(
          (act_fn): SiLUActivation()
          (up_proj): QuantizedLinear(
            (quantizer): VectorQuantizer_SampleReassign()
          )
          (gate_proj): QuantizedLinear(
            (quantizer): VectorQuantizer_SampleReassign()
          )
          (down_proj): QuantizedLinear(
            (quantizer): VectorQuantizer_SampleReassign()
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
original dtype torch.float16
LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (rotary_emb): LlamaRotaryEmbedding()
          (k_proj): QuantizedLinear(
            (quantizer): VectorQuantizer_SampleReassign()
          )
          (v_proj): QuantizedLinear(
            (quantizer): VectorQuantizer_SampleReassign()
          )
          (q_proj): QuantizedLinear(
            (quantizer): VectorQuantizer_SampleReassign()
          )
          (o_proj): QuantizedLinear(
            (quantizer): VectorQuantizer_SampleReassign()
          )
        )
        (mlp): LlamaMLP(
          (act_fn): SiLUActivation()
          (up_proj): QuantizedLinear(
            (quantizer): VectorQuantizer_SampleReassign()
          )
          (gate_proj): QuantizedLinear(
            (quantizer): VectorQuantizer_SampleReassign()
          )
          (down_proj): QuantizedLinear(
            (quantizer): VectorQuantizer_SampleReassign()
          )
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
1
Loading tokenizer for meta-llama/Llama-2-7b-hf
the following parameters will not be optimized: dict_keys(['model.embed_tokens.weight', 'lm_head.weight'])
number of parameters to not optimize: 262144000
optimizing the following parameters: dict_keys(['model.layers.0.self_attn.k_proj.bias', 'model.layers.0.self_attn.k_proj.quantizer.codebook', 'model.layers.0.self_attn.v_proj.bias', 'model.layers.0.self_attn.v_proj.quantizer.codebook', 'model.layers.0.self_attn.q_proj.bias', 'model.layers.0.self_attn.q_proj.quantizer.codebook', 'model.layers.0.self_attn.o_proj.bias', 'model.layers.0.self_attn.o_proj.quantizer.codebook', 'model.layers.0.mlp.up_proj.bias', 'model.layers.0.mlp.up_proj.quantizer.codebook', 'model.layers.0.mlp.gate_proj.bias', 'model.layers.0.mlp.gate_proj.quantizer.codebook', 'model.layers.0.mlp.down_proj.bias', 'model.layers.0.mlp.down_proj.quantizer.codebook', 'model.layers.0.input_layernorm.weight', 'model.layers.0.post_attention_layernorm.weight', 'model.layers.1.self_attn.k_proj.bias', 'model.layers.1.self_attn.k_proj.quantizer.codebook', 'model.layers.1.self_attn.v_proj.bias', 'model.layers.1.self_attn.v_proj.quantizer.codebook', 'model.layers.1.self_attn.q_proj.bias', 'model.layers.1.self_attn.q_proj.quantizer.codebook', 'model.layers.1.self_attn.o_proj.bias', 'model.layers.1.self_attn.o_proj.quantizer.codebook', 'model.layers.1.mlp.up_proj.bias', 'model.layers.1.mlp.up_proj.quantizer.codebook', 'model.layers.1.mlp.gate_proj.bias', 'model.layers.1.mlp.gate_proj.quantizer.codebook', 'model.layers.1.mlp.down_proj.bias', 'model.layers.1.mlp.down_proj.quantizer.codebook', 'model.layers.1.input_layernorm.weight', 'model.layers.1.post_attention_layernorm.weight', 'model.layers.2.self_attn.k_proj.bias', 'model.layers.2.self_attn.k_proj.quantizer.codebook', 'model.layers.2.self_attn.v_proj.bias', 'model.layers.2.self_attn.v_proj.quantizer.codebook', 'model.layers.2.self_attn.q_proj.bias', 'model.layers.2.self_attn.q_proj.quantizer.codebook', 'model.layers.2.self_attn.o_proj.bias', 'model.layers.2.self_attn.o_proj.quantizer.codebook', 'model.layers.2.mlp.up_proj.bias', 'model.layers.2.mlp.up_proj.quantizer.codebook', 'model.layers.2.mlp.gate_proj.bias', 'model.layers.2.mlp.gate_proj.quantizer.codebook', 'model.layers.2.mlp.down_proj.bias', 'model.layers.2.mlp.down_proj.quantizer.codebook', 'model.layers.2.input_layernorm.weight', 'model.layers.2.post_attention_layernorm.weight', 'model.layers.3.self_attn.k_proj.bias', 'model.layers.3.self_attn.k_proj.quantizer.codebook', 'model.layers.3.self_attn.v_proj.bias', 'model.layers.3.self_attn.v_proj.quantizer.codebook', 'model.layers.3.self_attn.q_proj.bias', 'model.layers.3.self_attn.q_proj.quantizer.codebook', 'model.layers.3.self_attn.o_proj.bias', 'model.layers.3.self_attn.o_proj.quantizer.codebook', 'model.layers.3.mlp.up_proj.bias', 'model.layers.3.mlp.up_proj.quantizer.codebook', 'model.layers.3.mlp.gate_proj.bias', 'model.layers.3.mlp.gate_proj.quantizer.codebook', 'model.layers.3.mlp.down_proj.bias', 'model.layers.3.mlp.down_proj.quantizer.codebook', 'model.layers.3.input_layernorm.weight', 'model.layers.3.post_attention_layernorm.weight', 'model.layers.4.self_attn.k_proj.bias', 'model.layers.4.self_attn.k_proj.quantizer.codebook', 'model.layers.4.self_attn.v_proj.bias', 'model.layers.4.self_attn.v_proj.quantizer.codebook', 'model.layers.4.self_attn.q_proj.bias', 'model.layers.4.self_attn.q_proj.quantizer.codebook', 'model.layers.4.self_attn.o_proj.bias', 'model.layers.4.self_attn.o_proj.quantizer.codebook', 'model.layers.4.mlp.up_proj.bias', 'model.layers.4.mlp.up_proj.quantizer.codebook', 'model.layers.4.mlp.gate_proj.bias', 'model.layers.4.mlp.gate_proj.quantizer.codebook', 'model.layers.4.mlp.down_proj.bias', 'model.layers.4.mlp.down_proj.quantizer.codebook', 'model.layers.4.input_layernorm.weight', 'model.layers.4.post_attention_layernorm.weight', 'model.layers.5.self_attn.k_proj.bias', 'model.layers.5.self_attn.k_proj.quantizer.codebook', 'model.layers.5.self_attn.v_proj.bias', 'model.layers.5.self_attn.v_proj.quantizer.codebook', 'model.layers.5.self_attn.q_proj.bias', 'model.layers.5.self_attn.q_proj.quantizer.codebook', 'model.layers.5.self_attn.o_proj.bias', 'model.layers.5.self_attn.o_proj.quantizer.codebook', 'model.layers.5.mlp.up_proj.bias', 'model.layers.5.mlp.up_proj.quantizer.codebook', 'model.layers.5.mlp.gate_proj.bias', 'model.layers.5.mlp.gate_proj.quantizer.codebook', 'model.layers.5.mlp.down_proj.bias', 'model.layers.5.mlp.down_proj.quantizer.codebook', 'model.layers.5.input_layernorm.weight', 'model.layers.5.post_attention_layernorm.weight', 'model.layers.6.self_attn.k_proj.bias', 'model.layers.6.self_attn.k_proj.quantizer.codebook', 'model.layers.6.self_attn.v_proj.bias', 'model.layers.6.self_attn.v_proj.quantizer.codebook', 'model.layers.6.self_attn.q_proj.bias', 'model.layers.6.self_attn.q_proj.quantizer.codebook', 'model.layers.6.self_attn.o_proj.bias', 'model.layers.6.self_attn.o_proj.quantizer.codebook', 'model.layers.6.mlp.up_proj.bias', 'model.layers.6.mlp.up_proj.quantizer.codebook', 'model.layers.6.mlp.gate_proj.bias', 'model.layers.6.mlp.gate_proj.quantizer.codebook', 'model.layers.6.mlp.down_proj.bias', 'model.layers.6.mlp.down_proj.quantizer.codebook', 'model.layers.6.input_layernorm.weight', 'model.layers.6.post_attention_layernorm.weight', 'model.layers.7.self_attn.k_proj.bias', 'model.layers.7.self_attn.k_proj.quantizer.codebook', 'model.layers.7.self_attn.v_proj.bias', 'model.layers.7.self_attn.v_proj.quantizer.codebook', 'model.layers.7.self_attn.q_proj.bias', 'model.layers.7.self_attn.q_proj.quantizer.codebook', 'model.layers.7.self_attn.o_proj.bias', 'model.layers.7.self_attn.o_proj.quantizer.codebook', 'model.layers.7.mlp.up_proj.bias', 'model.layers.7.mlp.up_proj.quantizer.codebook', 'model.layers.7.mlp.gate_proj.bias', 'model.layers.7.mlp.gate_proj.quantizer.codebook', 'model.layers.7.mlp.down_proj.bias', 'model.layers.7.mlp.down_proj.quantizer.codebook', 'model.layers.7.input_layernorm.weight', 'model.layers.7.post_attention_layernorm.weight', 'model.layers.8.self_attn.k_proj.bias', 'model.layers.8.self_attn.k_proj.quantizer.codebook', 'model.layers.8.self_attn.v_proj.bias', 'model.layers.8.self_attn.v_proj.quantizer.codebook', 'model.layers.8.self_attn.q_proj.bias', 'model.layers.8.self_attn.q_proj.quantizer.codebook', 'model.layers.8.self_attn.o_proj.bias', 'model.layers.8.self_attn.o_proj.quantizer.codebook', 'model.layers.8.mlp.up_proj.bias', 'model.layers.8.mlp.up_proj.quantizer.codebook', 'model.layers.8.mlp.gate_proj.bias', 'model.layers.8.mlp.gate_proj.quantizer.codebook', 'model.layers.8.mlp.down_proj.bias', 'model.layers.8.mlp.down_proj.quantizer.codebook', 'model.layers.8.input_layernorm.weight', 'model.layers.8.post_attention_layernorm.weight', 'model.layers.9.self_attn.k_proj.bias', 'model.layers.9.self_attn.k_proj.quantizer.codebook', 'model.layers.9.self_attn.v_proj.bias', 'model.layers.9.self_attn.v_proj.quantizer.codebook', 'model.layers.9.self_attn.q_proj.bias', 'model.layers.9.self_attn.q_proj.quantizer.codebook', 'model.layers.9.self_attn.o_proj.bias', 'model.layers.9.self_attn.o_proj.quantizer.codebook', 'model.layers.9.mlp.up_proj.bias', 'model.layers.9.mlp.up_proj.quantizer.codebook', 'model.layers.9.mlp.gate_proj.bias', 'model.layers.9.mlp.gate_proj.quantizer.codebook', 'model.layers.9.mlp.down_proj.bias', 'model.layers.9.mlp.down_proj.quantizer.codebook', 'model.layers.9.input_layernorm.weight', 'model.layers.9.post_attention_layernorm.weight', 'model.layers.10.self_attn.k_proj.bias', 'model.layers.10.self_attn.k_proj.quantizer.codebook', 'model.layers.10.self_attn.v_proj.bias', 'model.layers.10.self_attn.v_proj.quantizer.codebook', 'model.layers.10.self_attn.q_proj.bias', 'model.layers.10.self_attn.q_proj.quantizer.codebook', 'model.layers.10.self_attn.o_proj.bias', 'model.layers.10.self_attn.o_proj.quantizer.codebook', 'model.layers.10.mlp.up_proj.bias', 'model.layers.10.mlp.up_proj.quantizer.codebook', 'model.layers.10.mlp.gate_proj.bias', 'model.layers.10.mlp.gate_proj.quantizer.codebook', 'model.layers.10.mlp.down_proj.bias', 'model.layers.10.mlp.down_proj.quantizer.codebook', 'model.layers.10.input_layernorm.weight', 'model.layers.10.post_attention_layernorm.weight', 'model.layers.11.self_attn.k_proj.bias', 'model.layers.11.self_attn.k_proj.quantizer.codebook', 'model.layers.11.self_attn.v_proj.bias', 'model.layers.11.self_attn.v_proj.quantizer.codebook', 'model.layers.11.self_attn.q_proj.bias', 'model.layers.11.self_attn.q_proj.quantizer.codebook', 'model.layers.11.self_attn.o_proj.bias', 'model.layers.11.self_attn.o_proj.quantizer.codebook', 'model.layers.11.mlp.up_proj.bias', 'model.layers.11.mlp.up_proj.quantizer.codebook', 'model.layers.11.mlp.gate_proj.bias', 'model.layers.11.mlp.gate_proj.quantizer.codebook', 'model.layers.11.mlp.down_proj.bias', 'model.layers.11.mlp.down_proj.quantizer.codebook', 'model.layers.11.input_layernorm.weight', 'model.layers.11.post_attention_layernorm.weight', 'model.layers.12.self_attn.k_proj.bias', 'model.layers.12.self_attn.k_proj.quantizer.codebook', 'model.layers.12.self_attn.v_proj.bias', 'model.layers.12.self_attn.v_proj.quantizer.codebook', 'model.layers.12.self_attn.q_proj.bias', 'model.layers.12.self_attn.q_proj.quantizer.codebook', 'model.layers.12.self_attn.o_proj.bias', 'model.layers.12.self_attn.o_proj.quantizer.codebook', 'model.layers.12.mlp.up_proj.bias', 'model.layers.12.mlp.up_proj.quantizer.codebook', 'model.layers.12.mlp.gate_proj.bias', 'model.layers.12.mlp.gate_proj.quantizer.codebook', 'model.layers.12.mlp.down_proj.bias', 'model.layers.12.mlp.down_proj.quantizer.codebook', 'model.layers.12.input_layernorm.weight', 'model.layers.12.post_attention_layernorm.weight', 'model.layers.13.self_attn.k_proj.bias', 'model.layers.13.self_attn.k_proj.quantizer.codebook', 'model.layers.13.self_attn.v_proj.bias', 'model.layers.13.self_attn.v_proj.quantizer.codebook', 'model.layers.13.self_attn.q_proj.bias', 'model.layers.13.self_attn.q_proj.quantizer.codebook', 'model.layers.13.self_attn.o_proj.bias', 'model.layers.13.self_attn.o_proj.quantizer.codebook', 'model.layers.13.mlp.up_proj.bias', 'model.layers.13.mlp.up_proj.quantizer.codebook', 'model.layers.13.mlp.gate_proj.bias', 'model.layers.13.mlp.gate_proj.quantizer.codebook', 'model.layers.13.mlp.down_proj.bias', 'model.layers.13.mlp.down_proj.quantizer.codebook', 'model.layers.13.input_layernorm.weight', 'model.layers.13.post_attention_layernorm.weight', 'model.layers.14.self_attn.k_proj.bias', 'model.layers.14.self_attn.k_proj.quantizer.codebook', 'model.layers.14.self_attn.v_proj.bias', 'model.layers.14.self_attn.v_proj.quantizer.codebook', 'model.layers.14.self_attn.q_proj.bias', 'model.layers.14.self_attn.q_proj.quantizer.codebook', 'model.layers.14.self_attn.o_proj.bias', 'model.layers.14.self_attn.o_proj.quantizer.codebook', 'model.layers.14.mlp.up_proj.bias', 'model.layers.14.mlp.up_proj.quantizer.codebook', 'model.layers.14.mlp.gate_proj.bias', 'model.layers.14.mlp.gate_proj.quantizer.codebook', 'model.layers.14.mlp.down_proj.bias', 'model.layers.14.mlp.down_proj.quantizer.codebook', 'model.layers.14.input_layernorm.weight', 'model.layers.14.post_attention_layernorm.weight', 'model.layers.15.self_attn.k_proj.bias', 'model.layers.15.self_attn.k_proj.quantizer.codebook', 'model.layers.15.self_attn.v_proj.bias', 'model.layers.15.self_attn.v_proj.quantizer.codebook', 'model.layers.15.self_attn.q_proj.bias', 'model.layers.15.self_attn.q_proj.quantizer.codebook', 'model.layers.15.self_attn.o_proj.bias', 'model.layers.15.self_attn.o_proj.quantizer.codebook', 'model.layers.15.mlp.up_proj.bias', 'model.layers.15.mlp.up_proj.quantizer.codebook', 'model.layers.15.mlp.gate_proj.bias', 'model.layers.15.mlp.gate_proj.quantizer.codebook', 'model.layers.15.mlp.down_proj.bias', 'model.layers.15.mlp.down_proj.quantizer.codebook', 'model.layers.15.input_layernorm.weight', 'model.layers.15.post_attention_layernorm.weight', 'model.layers.16.self_attn.k_proj.bias', 'model.layers.16.self_attn.k_proj.quantizer.codebook', 'model.layers.16.self_attn.v_proj.bias', 'model.layers.16.self_attn.v_proj.quantizer.codebook', 'model.layers.16.self_attn.q_proj.bias', 'model.layers.16.self_attn.q_proj.quantizer.codebook', 'model.layers.16.self_attn.o_proj.bias', 'model.layers.16.self_attn.o_proj.quantizer.codebook', 'model.layers.16.mlp.up_proj.bias', 'model.layers.16.mlp.up_proj.quantizer.codebook', 'model.layers.16.mlp.gate_proj.bias', 'model.layers.16.mlp.gate_proj.quantizer.codebook', 'model.layers.16.mlp.down_proj.bias', 'model.layers.16.mlp.down_proj.quantizer.codebook', 'model.layers.16.input_layernorm.weight', 'model.layers.16.post_attention_layernorm.weight', 'model.layers.17.self_attn.k_proj.bias', 'model.layers.17.self_attn.k_proj.quantizer.codebook', 'model.layers.17.self_attn.v_proj.bias', 'model.layers.17.self_attn.v_proj.quantizer.codebook', 'model.layers.17.self_attn.q_proj.bias', 'model.layers.17.self_attn.q_proj.quantizer.codebook', 'model.layers.17.self_attn.o_proj.bias', 'model.layers.17.self_attn.o_proj.quantizer.codebook', 'model.layers.17.mlp.up_proj.bias', 'model.layers.17.mlp.up_proj.quantizer.codebook', 'model.layers.17.mlp.gate_proj.bias', 'model.layers.17.mlp.gate_proj.quantizer.codebook', 'model.layers.17.mlp.down_proj.bias', 'model.layers.17.mlp.down_proj.quantizer.codebook', 'model.layers.17.input_layernorm.weight', 'model.layers.17.post_attention_layernorm.weight', 'model.layers.18.self_attn.k_proj.bias', 'model.layers.18.self_attn.k_proj.quantizer.codebook', 'model.layers.18.self_attn.v_proj.bias', 'model.layers.18.self_attn.v_proj.quantizer.codebook', 'model.layers.18.self_attn.q_proj.bias', 'model.layers.18.self_attn.q_proj.quantizer.codebook', 'model.layers.18.self_attn.o_proj.bias', 'model.layers.18.self_attn.o_proj.quantizer.codebook', 'model.layers.18.mlp.up_proj.bias', 'model.layers.18.mlp.up_proj.quantizer.codebook', 'model.layers.18.mlp.gate_proj.bias', 'model.layers.18.mlp.gate_proj.quantizer.codebook', 'model.layers.18.mlp.down_proj.bias', 'model.layers.18.mlp.down_proj.quantizer.codebook', 'model.layers.18.input_layernorm.weight', 'model.layers.18.post_attention_layernorm.weight', 'model.layers.19.self_attn.k_proj.bias', 'model.layers.19.self_attn.k_proj.quantizer.codebook', 'model.layers.19.self_attn.v_proj.bias', 'model.layers.19.self_attn.v_proj.quantizer.codebook', 'model.layers.19.self_attn.q_proj.bias', 'model.layers.19.self_attn.q_proj.quantizer.codebook', 'model.layers.19.self_attn.o_proj.bias', 'model.layers.19.self_attn.o_proj.quantizer.codebook', 'model.layers.19.mlp.up_proj.bias', 'model.layers.19.mlp.up_proj.quantizer.codebook', 'model.layers.19.mlp.gate_proj.bias', 'model.layers.19.mlp.gate_proj.quantizer.codebook', 'model.layers.19.mlp.down_proj.bias', 'model.layers.19.mlp.down_proj.quantizer.codebook', 'model.layers.19.input_layernorm.weight', 'model.layers.19.post_attention_layernorm.weight', 'model.layers.20.self_attn.k_proj.bias', 'model.layers.20.self_attn.k_proj.quantizer.codebook', 'model.layers.20.self_attn.v_proj.bias', 'model.layers.20.self_attn.v_proj.quantizer.codebook', 'model.layers.20.self_attn.q_proj.bias', 'model.layers.20.self_attn.q_proj.quantizer.codebook', 'model.layers.20.self_attn.o_proj.bias', 'model.layers.20.self_attn.o_proj.quantizer.codebook', 'model.layers.20.mlp.up_proj.bias', 'model.layers.20.mlp.up_proj.quantizer.codebook', 'model.layers.20.mlp.gate_proj.bias', 'model.layers.20.mlp.gate_proj.quantizer.codebook', 'model.layers.20.mlp.down_proj.bias', 'model.layers.20.mlp.down_proj.quantizer.codebook', 'model.layers.20.input_layernorm.weight', 'model.layers.20.post_attention_layernorm.weight', 'model.layers.21.self_attn.k_proj.bias', 'model.layers.21.self_attn.k_proj.quantizer.codebook', 'model.layers.21.self_attn.v_proj.bias', 'model.layers.21.self_attn.v_proj.quantizer.codebook', 'model.layers.21.self_attn.q_proj.bias', 'model.layers.21.self_attn.q_proj.quantizer.codebook', 'model.layers.21.self_attn.o_proj.bias', 'model.layers.21.self_attn.o_proj.quantizer.codebook', 'model.layers.21.mlp.up_proj.bias', 'model.layers.21.mlp.up_proj.quantizer.codebook', 'model.layers.21.mlp.gate_proj.bias', 'model.layers.21.mlp.gate_proj.quantizer.codebook', 'model.layers.21.mlp.down_proj.bias', 'model.layers.21.mlp.down_proj.quantizer.codebook', 'model.layers.21.input_layernorm.weight', 'model.layers.21.post_attention_layernorm.weight', 'model.layers.22.self_attn.k_proj.bias', 'model.layers.22.self_attn.k_proj.quantizer.codebook', 'model.layers.22.self_attn.v_proj.bias', 'model.layers.22.self_attn.v_proj.quantizer.codebook', 'model.layers.22.self_attn.q_proj.bias', 'model.layers.22.self_attn.q_proj.quantizer.codebook', 'model.layers.22.self_attn.o_proj.bias', 'model.layers.22.self_attn.o_proj.quantizer.codebook', 'model.layers.22.mlp.up_proj.bias', 'model.layers.22.mlp.up_proj.quantizer.codebook', 'model.layers.22.mlp.gate_proj.bias', 'model.layers.22.mlp.gate_proj.quantizer.codebook', 'model.layers.22.mlp.down_proj.bias', 'model.layers.22.mlp.down_proj.quantizer.codebook', 'model.layers.22.input_layernorm.weight', 'model.layers.22.post_attention_layernorm.weight', 'model.layers.23.self_attn.k_proj.bias', 'model.layers.23.self_attn.k_proj.quantizer.codebook', 'model.layers.23.self_attn.v_proj.bias', 'model.layers.23.self_attn.v_proj.quantizer.codebook', 'model.layers.23.self_attn.q_proj.bias', 'model.layers.23.self_attn.q_proj.quantizer.codebook', 'model.layers.23.self_attn.o_proj.bias', 'model.layers.23.self_attn.o_proj.quantizer.codebook', 'model.layers.23.mlp.up_proj.bias', 'model.layers.23.mlp.up_proj.quantizer.codebook', 'model.layers.23.mlp.gate_proj.bias', 'model.layers.23.mlp.gate_proj.quantizer.codebook', 'model.layers.23.mlp.down_proj.bias', 'model.layers.23.mlp.down_proj.quantizer.codebook', 'model.layers.23.input_layernorm.weight', 'model.layers.23.post_attention_layernorm.weight', 'model.layers.24.self_attn.k_proj.bias', 'model.layers.24.self_attn.k_proj.quantizer.codebook', 'model.layers.24.self_attn.v_proj.bias', 'model.layers.24.self_attn.v_proj.quantizer.codebook', 'model.layers.24.self_attn.q_proj.bias', 'model.layers.24.self_attn.q_proj.quantizer.codebook', 'model.layers.24.self_attn.o_proj.bias', 'model.layers.24.self_attn.o_proj.quantizer.codebook', 'model.layers.24.mlp.up_proj.bias', 'model.layers.24.mlp.up_proj.quantizer.codebook', 'model.layers.24.mlp.gate_proj.bias', 'model.layers.24.mlp.gate_proj.quantizer.codebook', 'model.layers.24.mlp.down_proj.bias', 'model.layers.24.mlp.down_proj.quantizer.codebook', 'model.layers.24.input_layernorm.weight', 'model.layers.24.post_attention_layernorm.weight', 'model.layers.25.self_attn.k_proj.bias', 'model.layers.25.self_attn.k_proj.quantizer.codebook', 'model.layers.25.self_attn.v_proj.bias', 'model.layers.25.self_attn.v_proj.quantizer.codebook', 'model.layers.25.self_attn.q_proj.bias', 'model.layers.25.self_attn.q_proj.quantizer.codebook', 'model.layers.25.self_attn.o_proj.bias', 'model.layers.25.self_attn.o_proj.quantizer.codebook', 'model.layers.25.mlp.up_proj.bias', 'model.layers.25.mlp.up_proj.quantizer.codebook', 'model.layers.25.mlp.gate_proj.bias', 'model.layers.25.mlp.gate_proj.quantizer.codebook', 'model.layers.25.mlp.down_proj.bias', 'model.layers.25.mlp.down_proj.quantizer.codebook', 'model.layers.25.input_layernorm.weight', 'model.layers.25.post_attention_layernorm.weight', 'model.layers.26.self_attn.k_proj.bias', 'model.layers.26.self_attn.k_proj.quantizer.codebook', 'model.layers.26.self_attn.v_proj.bias', 'model.layers.26.self_attn.v_proj.quantizer.codebook', 'model.layers.26.self_attn.q_proj.bias', 'model.layers.26.self_attn.q_proj.quantizer.codebook', 'model.layers.26.self_attn.o_proj.bias', 'model.layers.26.self_attn.o_proj.quantizer.codebook', 'model.layers.26.mlp.up_proj.bias', 'model.layers.26.mlp.up_proj.quantizer.codebook', 'model.layers.26.mlp.gate_proj.bias', 'model.layers.26.mlp.gate_proj.quantizer.codebook', 'model.layers.26.mlp.down_proj.bias', 'model.layers.26.mlp.down_proj.quantizer.codebook', 'model.layers.26.input_layernorm.weight', 'model.layers.26.post_attention_layernorm.weight', 'model.layers.27.self_attn.k_proj.bias', 'model.layers.27.self_attn.k_proj.quantizer.codebook', 'model.layers.27.self_attn.v_proj.bias', 'model.layers.27.self_attn.v_proj.quantizer.codebook', 'model.layers.27.self_attn.q_proj.bias', 'model.layers.27.self_attn.q_proj.quantizer.codebook', 'model.layers.27.self_attn.o_proj.bias', 'model.layers.27.self_attn.o_proj.quantizer.codebook', 'model.layers.27.mlp.up_proj.bias', 'model.layers.27.mlp.up_proj.quantizer.codebook', 'model.layers.27.mlp.gate_proj.bias', 'model.layers.27.mlp.gate_proj.quantizer.codebook', 'model.layers.27.mlp.down_proj.bias', 'model.layers.27.mlp.down_proj.quantizer.codebook', 'model.layers.27.input_layernorm.weight', 'model.layers.27.post_attention_layernorm.weight', 'model.layers.28.self_attn.k_proj.bias', 'model.layers.28.self_attn.k_proj.quantizer.codebook', 'model.layers.28.self_attn.v_proj.bias', 'model.layers.28.self_attn.v_proj.quantizer.codebook', 'model.layers.28.self_attn.q_proj.bias', 'model.layers.28.self_attn.q_proj.quantizer.codebook', 'model.layers.28.self_attn.o_proj.bias', 'model.layers.28.self_attn.o_proj.quantizer.codebook', 'model.layers.28.mlp.up_proj.bias', 'model.layers.28.mlp.up_proj.quantizer.codebook', 'model.layers.28.mlp.gate_proj.bias', 'model.layers.28.mlp.gate_proj.quantizer.codebook', 'model.layers.28.mlp.down_proj.bias', 'model.layers.28.mlp.down_proj.quantizer.codebook', 'model.layers.28.input_layernorm.weight', 'model.layers.28.post_attention_layernorm.weight', 'model.layers.29.self_attn.k_proj.bias', 'model.layers.29.self_attn.k_proj.quantizer.codebook', 'model.layers.29.self_attn.v_proj.bias', 'model.layers.29.self_attn.v_proj.quantizer.codebook', 'model.layers.29.self_attn.q_proj.bias', 'model.layers.29.self_attn.q_proj.quantizer.codebook', 'model.layers.29.self_attn.o_proj.bias', 'model.layers.29.self_attn.o_proj.quantizer.codebook', 'model.layers.29.mlp.up_proj.bias', 'model.layers.29.mlp.up_proj.quantizer.codebook', 'model.layers.29.mlp.gate_proj.bias', 'model.layers.29.mlp.gate_proj.quantizer.codebook', 'model.layers.29.mlp.down_proj.bias', 'model.layers.29.mlp.down_proj.quantizer.codebook', 'model.layers.29.input_layernorm.weight', 'model.layers.29.post_attention_layernorm.weight', 'model.layers.30.self_attn.k_proj.bias', 'model.layers.30.self_attn.k_proj.quantizer.codebook', 'model.layers.30.self_attn.v_proj.bias', 'model.layers.30.self_attn.v_proj.quantizer.codebook', 'model.layers.30.self_attn.q_proj.bias', 'model.layers.30.self_attn.q_proj.quantizer.codebook', 'model.layers.30.self_attn.o_proj.bias', 'model.layers.30.self_attn.o_proj.quantizer.codebook', 'model.layers.30.mlp.up_proj.bias', 'model.layers.30.mlp.up_proj.quantizer.codebook', 'model.layers.30.mlp.gate_proj.bias', 'model.layers.30.mlp.gate_proj.quantizer.codebook', 'model.layers.30.mlp.down_proj.bias', 'model.layers.30.mlp.down_proj.quantizer.codebook', 'model.layers.30.input_layernorm.weight', 'model.layers.30.post_attention_layernorm.weight', 'model.layers.31.self_attn.k_proj.bias', 'model.layers.31.self_attn.k_proj.quantizer.codebook', 'model.layers.31.self_attn.v_proj.bias', 'model.layers.31.self_attn.v_proj.quantizer.codebook', 'model.layers.31.self_attn.q_proj.bias', 'model.layers.31.self_attn.q_proj.quantizer.codebook', 'model.layers.31.self_attn.o_proj.bias', 'model.layers.31.self_attn.o_proj.quantizer.codebook', 'model.layers.31.mlp.up_proj.bias', 'model.layers.31.mlp.up_proj.quantizer.codebook', 'model.layers.31.mlp.gate_proj.bias', 'model.layers.31.mlp.gate_proj.quantizer.codebook', 'model.layers.31.mlp.down_proj.bias', 'model.layers.31.mlp.down_proj.quantizer.codebook', 'model.layers.31.input_layernorm.weight', 'model.layers.31.post_attention_layernorm.weight', 'model.norm.weight'])
model.layers.0.self_attn.k_proj.bias 4,096
model.layers.0.self_attn.k_proj.quantizer.codebook 1,024
model.layers.0.self_attn.v_proj.bias 4,096
model.layers.0.self_attn.v_proj.quantizer.codebook 1,024
model.layers.0.self_attn.q_proj.bias 4,096
model.layers.0.self_attn.q_proj.quantizer.codebook 1,024
model.layers.0.self_attn.o_proj.bias 4,096
model.layers.0.self_attn.o_proj.quantizer.codebook 1,024
model.layers.0.mlp.up_proj.bias 11,008
model.layers.0.mlp.up_proj.quantizer.codebook 1,024
model.layers.0.mlp.gate_proj.bias 11,008
model.layers.0.mlp.gate_proj.quantizer.codebook 1,024
model.layers.0.mlp.down_proj.bias 4,096
model.layers.0.mlp.down_proj.quantizer.codebook 1,024
model.layers.0.input_layernorm.weight 4,096
model.layers.0.post_attention_layernorm.weight 4,096
model.layers.1.self_attn.k_proj.bias 4,096
model.layers.1.self_attn.k_proj.quantizer.codebook 1,024
model.layers.1.self_attn.v_proj.bias 4,096
model.layers.1.self_attn.v_proj.quantizer.codebook 1,024
model.layers.1.self_attn.q_proj.bias 4,096
model.layers.1.self_attn.q_proj.quantizer.codebook 1,024
model.layers.1.self_attn.o_proj.bias 4,096
model.layers.1.self_attn.o_proj.quantizer.codebook 1,024
model.layers.1.mlp.up_proj.bias 11,008
model.layers.1.mlp.up_proj.quantizer.codebook 1,024
model.layers.1.mlp.gate_proj.bias 11,008
model.layers.1.mlp.gate_proj.quantizer.codebook 1,024
model.layers.1.mlp.down_proj.bias 4,096
model.layers.1.mlp.down_proj.quantizer.codebook 1,024
model.layers.1.input_layernorm.weight 4,096
model.layers.1.post_attention_layernorm.weight 4,096
model.layers.2.self_attn.k_proj.bias 4,096
model.layers.2.self_attn.k_proj.quantizer.codebook 1,024
model.layers.2.self_attn.v_proj.bias 4,096
model.layers.2.self_attn.v_proj.quantizer.codebook 1,024
model.layers.2.self_attn.q_proj.bias 4,096
model.layers.2.self_attn.q_proj.quantizer.codebook 1,024
model.layers.2.self_attn.o_proj.bias 4,096
model.layers.2.self_attn.o_proj.quantizer.codebook 1,024
model.layers.2.mlp.up_proj.bias 11,008
model.layers.2.mlp.up_proj.quantizer.codebook 1,024
model.layers.2.mlp.gate_proj.bias 11,008
model.layers.2.mlp.gate_proj.quantizer.codebook 1,024
model.layers.2.mlp.down_proj.bias 4,096
model.layers.2.mlp.down_proj.quantizer.codebook 1,024
model.layers.2.input_layernorm.weight 4,096
model.layers.2.post_attention_layernorm.weight 4,096
model.layers.3.self_attn.k_proj.bias 4,096
model.layers.3.self_attn.k_proj.quantizer.codebook 1,024
model.layers.3.self_attn.v_proj.bias 4,096
model.layers.3.self_attn.v_proj.quantizer.codebook 1,024
model.layers.3.self_attn.q_proj.bias 4,096
model.layers.3.self_attn.q_proj.quantizer.codebook 1,024
model.layers.3.self_attn.o_proj.bias 4,096
model.layers.3.self_attn.o_proj.quantizer.codebook 1,024
model.layers.3.mlp.up_proj.bias 11,008
model.layers.3.mlp.up_proj.quantizer.codebook 1,024
model.layers.3.mlp.gate_proj.bias 11,008
model.layers.3.mlp.gate_proj.quantizer.codebook 1,024
model.layers.3.mlp.down_proj.bias 4,096
model.layers.3.mlp.down_proj.quantizer.codebook 1,024
model.layers.3.input_layernorm.weight 4,096
model.layers.3.post_attention_layernorm.weight 4,096
model.layers.4.self_attn.k_proj.bias 4,096
model.layers.4.self_attn.k_proj.quantizer.codebook 1,024
model.layers.4.self_attn.v_proj.bias 4,096
model.layers.4.self_attn.v_proj.quantizer.codebook 1,024
model.layers.4.self_attn.q_proj.bias 4,096
model.layers.4.self_attn.q_proj.quantizer.codebook 1,024
model.layers.4.self_attn.o_proj.bias 4,096
model.layers.4.self_attn.o_proj.quantizer.codebook 1,024
model.layers.4.mlp.up_proj.bias 11,008
model.layers.4.mlp.up_proj.quantizer.codebook 1,024
model.layers.4.mlp.gate_proj.bias 11,008
model.layers.4.mlp.gate_proj.quantizer.codebook 1,024
model.layers.4.mlp.down_proj.bias 4,096
model.layers.4.mlp.down_proj.quantizer.codebook 1,024
model.layers.4.input_layernorm.weight 4,096
model.layers.4.post_attention_layernorm.weight 4,096
model.layers.5.self_attn.k_proj.bias 4,096
model.layers.5.self_attn.k_proj.quantizer.codebook 1,024
model.layers.5.self_attn.v_proj.bias 4,096
model.layers.5.self_attn.v_proj.quantizer.codebook 1,024
model.layers.5.self_attn.q_proj.bias 4,096
model.layers.5.self_attn.q_proj.quantizer.codebook 1,024
model.layers.5.self_attn.o_proj.bias 4,096
model.layers.5.self_attn.o_proj.quantizer.codebook 1,024
model.layers.5.mlp.up_proj.bias 11,008
model.layers.5.mlp.up_proj.quantizer.codebook 1,024
model.layers.5.mlp.gate_proj.bias 11,008
model.layers.5.mlp.gate_proj.quantizer.codebook 1,024
model.layers.5.mlp.down_proj.bias 4,096
model.layers.5.mlp.down_proj.quantizer.codebook 1,024
model.layers.5.input_layernorm.weight 4,096
model.layers.5.post_attention_layernorm.weight 4,096
model.layers.6.self_attn.k_proj.bias 4,096
model.layers.6.self_attn.k_proj.quantizer.codebook 1,024
model.layers.6.self_attn.v_proj.bias 4,096
model.layers.6.self_attn.v_proj.quantizer.codebook 1,024
model.layers.6.self_attn.q_proj.bias 4,096
model.layers.6.self_attn.q_proj.quantizer.codebook 1,024
model.layers.6.self_attn.o_proj.bias 4,096
model.layers.6.self_attn.o_proj.quantizer.codebook 1,024
model.layers.6.mlp.up_proj.bias 11,008
model.layers.6.mlp.up_proj.quantizer.codebook 1,024
model.layers.6.mlp.gate_proj.bias 11,008
model.layers.6.mlp.gate_proj.quantizer.codebook 1,024
model.layers.6.mlp.down_proj.bias 4,096
model.layers.6.mlp.down_proj.quantizer.codebook 1,024
model.layers.6.input_layernorm.weight 4,096
model.layers.6.post_attention_layernorm.weight 4,096
model.layers.7.self_attn.k_proj.bias 4,096
model.layers.7.self_attn.k_proj.quantizer.codebook 1,024
model.layers.7.self_attn.v_proj.bias 4,096
model.layers.7.self_attn.v_proj.quantizer.codebook 1,024
model.layers.7.self_attn.q_proj.bias 4,096
model.layers.7.self_attn.q_proj.quantizer.codebook 1,024
model.layers.7.self_attn.o_proj.bias 4,096
model.layers.7.self_attn.o_proj.quantizer.codebook 1,024
model.layers.7.mlp.up_proj.bias 11,008
model.layers.7.mlp.up_proj.quantizer.codebook 1,024
model.layers.7.mlp.gate_proj.bias 11,008
model.layers.7.mlp.gate_proj.quantizer.codebook 1,024
model.layers.7.mlp.down_proj.bias 4,096
model.layers.7.mlp.down_proj.quantizer.codebook 1,024
model.layers.7.input_layernorm.weight 4,096
model.layers.7.post_attention_layernorm.weight 4,096
model.layers.8.self_attn.k_proj.bias 4,096
model.layers.8.self_attn.k_proj.quantizer.codebook 1,024
model.layers.8.self_attn.v_proj.bias 4,096
model.layers.8.self_attn.v_proj.quantizer.codebook 1,024
model.layers.8.self_attn.q_proj.bias 4,096
model.layers.8.self_attn.q_proj.quantizer.codebook 1,024
model.layers.8.self_attn.o_proj.bias 4,096
model.layers.8.self_attn.o_proj.quantizer.codebook 1,024
model.layers.8.mlp.up_proj.bias 11,008
model.layers.8.mlp.up_proj.quantizer.codebook 1,024
model.layers.8.mlp.gate_proj.bias 11,008
model.layers.8.mlp.gate_proj.quantizer.codebook 1,024
model.layers.8.mlp.down_proj.bias 4,096
model.layers.8.mlp.down_proj.quantizer.codebook 1,024
model.layers.8.input_layernorm.weight 4,096
model.layers.8.post_attention_layernorm.weight 4,096
model.layers.9.self_attn.k_proj.bias 4,096
model.layers.9.self_attn.k_proj.quantizer.codebook 1,024
model.layers.9.self_attn.v_proj.bias 4,096
model.layers.9.self_attn.v_proj.quantizer.codebook 1,024
model.layers.9.self_attn.q_proj.bias 4,096
model.layers.9.self_attn.q_proj.quantizer.codebook 1,024
model.layers.9.self_attn.o_proj.bias 4,096
model.layers.9.self_attn.o_proj.quantizer.codebook 1,024
model.layers.9.mlp.up_proj.bias 11,008
model.layers.9.mlp.up_proj.quantizer.codebook 1,024
model.layers.9.mlp.gate_proj.bias 11,008
model.layers.9.mlp.gate_proj.quantizer.codebook 1,024
model.layers.9.mlp.down_proj.bias 4,096
model.layers.9.mlp.down_proj.quantizer.codebook 1,024
model.layers.9.input_layernorm.weight 4,096
model.layers.9.post_attention_layernorm.weight 4,096
model.layers.10.self_attn.k_proj.bias 4,096
model.layers.10.self_attn.k_proj.quantizer.codebook 1,024
model.layers.10.self_attn.v_proj.bias 4,096
model.layers.10.self_attn.v_proj.quantizer.codebook 1,024
model.layers.10.self_attn.q_proj.bias 4,096
model.layers.10.self_attn.q_proj.quantizer.codebook 1,024
model.layers.10.self_attn.o_proj.bias 4,096
model.layers.10.self_attn.o_proj.quantizer.codebook 1,024
model.layers.10.mlp.up_proj.bias 11,008
model.layers.10.mlp.up_proj.quantizer.codebook 1,024
model.layers.10.mlp.gate_proj.bias 11,008
model.layers.10.mlp.gate_proj.quantizer.codebook 1,024
model.layers.10.mlp.down_proj.bias 4,096
model.layers.10.mlp.down_proj.quantizer.codebook 1,024
model.layers.10.input_layernorm.weight 4,096
model.layers.10.post_attention_layernorm.weight 4,096
model.layers.11.self_attn.k_proj.bias 4,096
model.layers.11.self_attn.k_proj.quantizer.codebook 1,024
model.layers.11.self_attn.v_proj.bias 4,096
model.layers.11.self_attn.v_proj.quantizer.codebook 1,024
model.layers.11.self_attn.q_proj.bias 4,096
model.layers.11.self_attn.q_proj.quantizer.codebook 1,024
model.layers.11.self_attn.o_proj.bias 4,096
model.layers.11.self_attn.o_proj.quantizer.codebook 1,024
model.layers.11.mlp.up_proj.bias 11,008
model.layers.11.mlp.up_proj.quantizer.codebook 1,024
model.layers.11.mlp.gate_proj.bias 11,008
model.layers.11.mlp.gate_proj.quantizer.codebook 1,024
model.layers.11.mlp.down_proj.bias 4,096
model.layers.11.mlp.down_proj.quantizer.codebook 1,024
model.layers.11.input_layernorm.weight 4,096
model.layers.11.post_attention_layernorm.weight 4,096
model.layers.12.self_attn.k_proj.bias 4,096
model.layers.12.self_attn.k_proj.quantizer.codebook 1,024
model.layers.12.self_attn.v_proj.bias 4,096
model.layers.12.self_attn.v_proj.quantizer.codebook 1,024
model.layers.12.self_attn.q_proj.bias 4,096
model.layers.12.self_attn.q_proj.quantizer.codebook 1,024
model.layers.12.self_attn.o_proj.bias 4,096
model.layers.12.self_attn.o_proj.quantizer.codebook 1,024
model.layers.12.mlp.up_proj.bias 11,008
model.layers.12.mlp.up_proj.quantizer.codebook 1,024
model.layers.12.mlp.gate_proj.bias 11,008
model.layers.12.mlp.gate_proj.quantizer.codebook 1,024
model.layers.12.mlp.down_proj.bias 4,096
model.layers.12.mlp.down_proj.quantizer.codebook 1,024
model.layers.12.input_layernorm.weight 4,096
model.layers.12.post_attention_layernorm.weight 4,096
model.layers.13.self_attn.k_proj.bias 4,096
model.layers.13.self_attn.k_proj.quantizer.codebook 1,024
model.layers.13.self_attn.v_proj.bias 4,096
model.layers.13.self_attn.v_proj.quantizer.codebook 1,024
model.layers.13.self_attn.q_proj.bias 4,096
model.layers.13.self_attn.q_proj.quantizer.codebook 1,024
model.layers.13.self_attn.o_proj.bias 4,096
model.layers.13.self_attn.o_proj.quantizer.codebook 1,024
model.layers.13.mlp.up_proj.bias 11,008
model.layers.13.mlp.up_proj.quantizer.codebook 1,024
model.layers.13.mlp.gate_proj.bias 11,008
model.layers.13.mlp.gate_proj.quantizer.codebook 1,024
model.layers.13.mlp.down_proj.bias 4,096
model.layers.13.mlp.down_proj.quantizer.codebook 1,024
model.layers.13.input_layernorm.weight 4,096
model.layers.13.post_attention_layernorm.weight 4,096
model.layers.14.self_attn.k_proj.bias 4,096
model.layers.14.self_attn.k_proj.quantizer.codebook 1,024
model.layers.14.self_attn.v_proj.bias 4,096
model.layers.14.self_attn.v_proj.quantizer.codebook 1,024
model.layers.14.self_attn.q_proj.bias 4,096
model.layers.14.self_attn.q_proj.quantizer.codebook 1,024
model.layers.14.self_attn.o_proj.bias 4,096
model.layers.14.self_attn.o_proj.quantizer.codebook 1,024
model.layers.14.mlp.up_proj.bias 11,008
model.layers.14.mlp.up_proj.quantizer.codebook 1,024
model.layers.14.mlp.gate_proj.bias 11,008
model.layers.14.mlp.gate_proj.quantizer.codebook 1,024
model.layers.14.mlp.down_proj.bias 4,096
model.layers.14.mlp.down_proj.quantizer.codebook 1,024
model.layers.14.input_layernorm.weight 4,096
model.layers.14.post_attention_layernorm.weight 4,096
model.layers.15.self_attn.k_proj.bias 4,096
model.layers.15.self_attn.k_proj.quantizer.codebook 1,024
model.layers.15.self_attn.v_proj.bias 4,096
model.layers.15.self_attn.v_proj.quantizer.codebook 1,024
model.layers.15.self_attn.q_proj.bias 4,096
model.layers.15.self_attn.q_proj.quantizer.codebook 1,024
model.layers.15.self_attn.o_proj.bias 4,096
model.layers.15.self_attn.o_proj.quantizer.codebook 1,024
model.layers.15.mlp.up_proj.bias 11,008
model.layers.15.mlp.up_proj.quantizer.codebook 1,024
model.layers.15.mlp.gate_proj.bias 11,008
model.layers.15.mlp.gate_proj.quantizer.codebook 1,024
model.layers.15.mlp.down_proj.bias 4,096
model.layers.15.mlp.down_proj.quantizer.codebook 1,024
model.layers.15.input_layernorm.weight 4,096
model.layers.15.post_attention_layernorm.weight 4,096
model.layers.16.self_attn.k_proj.bias 4,096
model.layers.16.self_attn.k_proj.quantizer.codebook 1,024
model.layers.16.self_attn.v_proj.bias 4,096
model.layers.16.self_attn.v_proj.quantizer.codebook 1,024
model.layers.16.self_attn.q_proj.bias 4,096
model.layers.16.self_attn.q_proj.quantizer.codebook 1,024
model.layers.16.self_attn.o_proj.bias 4,096
model.layers.16.self_attn.o_proj.quantizer.codebook 1,024
model.layers.16.mlp.up_proj.bias 11,008
model.layers.16.mlp.up_proj.quantizer.codebook 1,024
model.layers.16.mlp.gate_proj.bias 11,008
model.layers.16.mlp.gate_proj.quantizer.codebook 1,024
model.layers.16.mlp.down_proj.bias 4,096
model.layers.16.mlp.down_proj.quantizer.codebook 1,024
model.layers.16.input_layernorm.weight 4,096
model.layers.16.post_attention_layernorm.weight 4,096
model.layers.17.self_attn.k_proj.bias 4,096
model.layers.17.self_attn.k_proj.quantizer.codebook 1,024
model.layers.17.self_attn.v_proj.bias 4,096
model.layers.17.self_attn.v_proj.quantizer.codebook 1,024
model.layers.17.self_attn.q_proj.bias 4,096
model.layers.17.self_attn.q_proj.quantizer.codebook 1,024
model.layers.17.self_attn.o_proj.bias 4,096
model.layers.17.self_attn.o_proj.quantizer.codebook 1,024
model.layers.17.mlp.up_proj.bias 11,008
model.layers.17.mlp.up_proj.quantizer.codebook 1,024
model.layers.17.mlp.gate_proj.bias 11,008
model.layers.17.mlp.gate_proj.quantizer.codebook 1,024
model.layers.17.mlp.down_proj.bias 4,096
model.layers.17.mlp.down_proj.quantizer.codebook 1,024
model.layers.17.input_layernorm.weight 4,096
model.layers.17.post_attention_layernorm.weight 4,096
model.layers.18.self_attn.k_proj.bias 4,096
model.layers.18.self_attn.k_proj.quantizer.codebook 1,024
model.layers.18.self_attn.v_proj.bias 4,096
model.layers.18.self_attn.v_proj.quantizer.codebook 1,024
model.layers.18.self_attn.q_proj.bias 4,096
model.layers.18.self_attn.q_proj.quantizer.codebook 1,024
model.layers.18.self_attn.o_proj.bias 4,096
model.layers.18.self_attn.o_proj.quantizer.codebook 1,024
model.layers.18.mlp.up_proj.bias 11,008
model.layers.18.mlp.up_proj.quantizer.codebook 1,024
model.layers.18.mlp.gate_proj.bias 11,008
model.layers.18.mlp.gate_proj.quantizer.codebook 1,024
model.layers.18.mlp.down_proj.bias 4,096
model.layers.18.mlp.down_proj.quantizer.codebook 1,024
model.layers.18.input_layernorm.weight 4,096
model.layers.18.post_attention_layernorm.weight 4,096
model.layers.19.self_attn.k_proj.bias 4,096
model.layers.19.self_attn.k_proj.quantizer.codebook 1,024
model.layers.19.self_attn.v_proj.bias 4,096
model.layers.19.self_attn.v_proj.quantizer.codebook 1,024
model.layers.19.self_attn.q_proj.bias 4,096
model.layers.19.self_attn.q_proj.quantizer.codebook 1,024
model.layers.19.self_attn.o_proj.bias 4,096
model.layers.19.self_attn.o_proj.quantizer.codebook 1,024
model.layers.19.mlp.up_proj.bias 11,008
model.layers.19.mlp.up_proj.quantizer.codebook 1,024
model.layers.19.mlp.gate_proj.bias 11,008
model.layers.19.mlp.gate_proj.quantizer.codebook 1,024
model.layers.19.mlp.down_proj.bias 4,096
model.layers.19.mlp.down_proj.quantizer.codebook 1,024
model.layers.19.input_layernorm.weight 4,096
model.layers.19.post_attention_layernorm.weight 4,096
model.layers.20.self_attn.k_proj.bias 4,096
model.layers.20.self_attn.k_proj.quantizer.codebook 1,024
model.layers.20.self_attn.v_proj.bias 4,096
model.layers.20.self_attn.v_proj.quantizer.codebook 1,024
model.layers.20.self_attn.q_proj.bias 4,096
model.layers.20.self_attn.q_proj.quantizer.codebook 1,024
model.layers.20.self_attn.o_proj.bias 4,096
model.layers.20.self_attn.o_proj.quantizer.codebook 1,024
model.layers.20.mlp.up_proj.bias 11,008
model.layers.20.mlp.up_proj.quantizer.codebook 1,024
model.layers.20.mlp.gate_proj.bias 11,008
model.layers.20.mlp.gate_proj.quantizer.codebook 1,024
model.layers.20.mlp.down_proj.bias 4,096
model.layers.20.mlp.down_proj.quantizer.codebook 1,024
model.layers.20.input_layernorm.weight 4,096
model.layers.20.post_attention_layernorm.weight 4,096
model.layers.21.self_attn.k_proj.bias 4,096
model.layers.21.self_attn.k_proj.quantizer.codebook 1,024
model.layers.21.self_attn.v_proj.bias 4,096
model.layers.21.self_attn.v_proj.quantizer.codebook 1,024
model.layers.21.self_attn.q_proj.bias 4,096
model.layers.21.self_attn.q_proj.quantizer.codebook 1,024
model.layers.21.self_attn.o_proj.bias 4,096
model.layers.21.self_attn.o_proj.quantizer.codebook 1,024
model.layers.21.mlp.up_proj.bias 11,008
model.layers.21.mlp.up_proj.quantizer.codebook 1,024
model.layers.21.mlp.gate_proj.bias 11,008
model.layers.21.mlp.gate_proj.quantizer.codebook 1,024
model.layers.21.mlp.down_proj.bias 4,096
model.layers.21.mlp.down_proj.quantizer.codebook 1,024
model.layers.21.input_layernorm.weight 4,096
model.layers.21.post_attention_layernorm.weight 4,096
model.layers.22.self_attn.k_proj.bias 4,096
model.layers.22.self_attn.k_proj.quantizer.codebook 1,024
model.layers.22.self_attn.v_proj.bias 4,096
model.layers.22.self_attn.v_proj.quantizer.codebook 1,024
model.layers.22.self_attn.q_proj.bias 4,096
model.layers.22.self_attn.q_proj.quantizer.codebook 1,024
model.layers.22.self_attn.o_proj.bias 4,096
model.layers.22.self_attn.o_proj.quantizer.codebook 1,024
model.layers.22.mlp.up_proj.bias 11,008
model.layers.22.mlp.up_proj.quantizer.codebook 1,024
model.layers.22.mlp.gate_proj.bias 11,008
model.layers.22.mlp.gate_proj.quantizer.codebook 1,024
model.layers.22.mlp.down_proj.bias 4,096
model.layers.22.mlp.down_proj.quantizer.codebook 1,024
model.layers.22.input_layernorm.weight 4,096
model.layers.22.post_attention_layernorm.weight 4,096
model.layers.23.self_attn.k_proj.bias 4,096
model.layers.23.self_attn.k_proj.quantizer.codebook 1,024
model.layers.23.self_attn.v_proj.bias 4,096
model.layers.23.self_attn.v_proj.quantizer.codebook 1,024
model.layers.23.self_attn.q_proj.bias 4,096
model.layers.23.self_attn.q_proj.quantizer.codebook 1,024
model.layers.23.self_attn.o_proj.bias 4,096
model.layers.23.self_attn.o_proj.quantizer.codebook 1,024
model.layers.23.mlp.up_proj.bias 11,008
model.layers.23.mlp.up_proj.quantizer.codebook 1,024
model.layers.23.mlp.gate_proj.bias 11,008
model.layers.23.mlp.gate_proj.quantizer.codebook 1,024
model.layers.23.mlp.down_proj.bias 4,096
model.layers.23.mlp.down_proj.quantizer.codebook 1,024
model.layers.23.input_layernorm.weight 4,096
model.layers.23.post_attention_layernorm.weight 4,096
model.layers.24.self_attn.k_proj.bias 4,096
model.layers.24.self_attn.k_proj.quantizer.codebook 1,024
model.layers.24.self_attn.v_proj.bias 4,096
model.layers.24.self_attn.v_proj.quantizer.codebook 1,024
model.layers.24.self_attn.q_proj.bias 4,096
model.layers.24.self_attn.q_proj.quantizer.codebook 1,024
model.layers.24.self_attn.o_proj.bias 4,096
model.layers.24.self_attn.o_proj.quantizer.codebook 1,024
model.layers.24.mlp.up_proj.bias 11,008
model.layers.24.mlp.up_proj.quantizer.codebook 1,024
model.layers.24.mlp.gate_proj.bias 11,008
model.layers.24.mlp.gate_proj.quantizer.codebook 1,024
model.layers.24.mlp.down_proj.bias 4,096
model.layers.24.mlp.down_proj.quantizer.codebook 1,024
model.layers.24.input_layernorm.weight 4,096
model.layers.24.post_attention_layernorm.weight 4,096
model.layers.25.self_attn.k_proj.bias 4,096
model.layers.25.self_attn.k_proj.quantizer.codebook 1,024
model.layers.25.self_attn.v_proj.bias 4,096
model.layers.25.self_attn.v_proj.quantizer.codebook 1,024
model.layers.25.self_attn.q_proj.bias 4,096
model.layers.25.self_attn.q_proj.quantizer.codebook 1,024
model.layers.25.self_attn.o_proj.bias 4,096
model.layers.25.self_attn.o_proj.quantizer.codebook 1,024
model.layers.25.mlp.up_proj.bias 11,008
model.layers.25.mlp.up_proj.quantizer.codebook 1,024
model.layers.25.mlp.gate_proj.bias 11,008
model.layers.25.mlp.gate_proj.quantizer.codebook 1,024
model.layers.25.mlp.down_proj.bias 4,096
model.layers.25.mlp.down_proj.quantizer.codebook 1,024
model.layers.25.input_layernorm.weight 4,096
model.layers.25.post_attention_layernorm.weight 4,096
model.layers.26.self_attn.k_proj.bias 4,096
model.layers.26.self_attn.k_proj.quantizer.codebook 1,024
model.layers.26.self_attn.v_proj.bias 4,096
model.layers.26.self_attn.v_proj.quantizer.codebook 1,024
model.layers.26.self_attn.q_proj.bias 4,096
model.layers.26.self_attn.q_proj.quantizer.codebook 1,024
model.layers.26.self_attn.o_proj.bias 4,096
model.layers.26.self_attn.o_proj.quantizer.codebook 1,024
model.layers.26.mlp.up_proj.bias 11,008
model.layers.26.mlp.up_proj.quantizer.codebook 1,024
model.layers.26.mlp.gate_proj.bias 11,008
model.layers.26.mlp.gate_proj.quantizer.codebook 1,024
model.layers.26.mlp.down_proj.bias 4,096
model.layers.26.mlp.down_proj.quantizer.codebook 1,024
model.layers.26.input_layernorm.weight 4,096
model.layers.26.post_attention_layernorm.weight 4,096
model.layers.27.self_attn.k_proj.bias 4,096
model.layers.27.self_attn.k_proj.quantizer.codebook 1,024
model.layers.27.self_attn.v_proj.bias 4,096
model.layers.27.self_attn.v_proj.quantizer.codebook 1,024
model.layers.27.self_attn.q_proj.bias 4,096
model.layers.27.self_attn.q_proj.quantizer.codebook 1,024
model.layers.27.self_attn.o_proj.bias 4,096
model.layers.27.self_attn.o_proj.quantizer.codebook 1,024
model.layers.27.mlp.up_proj.bias 11,008
model.layers.27.mlp.up_proj.quantizer.codebook 1,024
model.layers.27.mlp.gate_proj.bias 11,008
model.layers.27.mlp.gate_proj.quantizer.codebook 1,024
model.layers.27.mlp.down_proj.bias 4,096
model.layers.27.mlp.down_proj.quantizer.codebook 1,024
model.layers.27.input_layernorm.weight 4,096
model.layers.27.post_attention_layernorm.weight 4,096
model.layers.28.self_attn.k_proj.bias 4,096
model.layers.28.self_attn.k_proj.quantizer.codebook 1,024
model.layers.28.self_attn.v_proj.bias 4,096
model.layers.28.self_attn.v_proj.quantizer.codebook 1,024
model.layers.28.self_attn.q_proj.bias 4,096
model.layers.28.self_attn.q_proj.quantizer.codebook 1,024
model.layers.28.self_attn.o_proj.bias 4,096
model.layers.28.self_attn.o_proj.quantizer.codebook 1,024
model.layers.28.mlp.up_proj.bias 11,008
model.layers.28.mlp.up_proj.quantizer.codebook 1,024
model.layers.28.mlp.gate_proj.bias 11,008
model.layers.28.mlp.gate_proj.quantizer.codebook 1,024
model.layers.28.mlp.down_proj.bias 4,096
model.layers.28.mlp.down_proj.quantizer.codebook 1,024
model.layers.28.input_layernorm.weight 4,096
model.layers.28.post_attention_layernorm.weight 4,096
model.layers.29.self_attn.k_proj.bias 4,096
model.layers.29.self_attn.k_proj.quantizer.codebook 1,024
model.layers.29.self_attn.v_proj.bias 4,096
model.layers.29.self_attn.v_proj.quantizer.codebook 1,024
model.layers.29.self_attn.q_proj.bias 4,096
model.layers.29.self_attn.q_proj.quantizer.codebook 1,024
model.layers.29.self_attn.o_proj.bias 4,096
model.layers.29.self_attn.o_proj.quantizer.codebook 1,024
model.layers.29.mlp.up_proj.bias 11,008
model.layers.29.mlp.up_proj.quantizer.codebook 1,024
model.layers.29.mlp.gate_proj.bias 11,008
model.layers.29.mlp.gate_proj.quantizer.codebook 1,024
model.layers.29.mlp.down_proj.bias 4,096
model.layers.29.mlp.down_proj.quantizer.codebook 1,024
model.layers.29.input_layernorm.weight 4,096
model.layers.29.post_attention_layernorm.weight 4,096
model.layers.30.self_attn.k_proj.bias 4,096
model.layers.30.self_attn.k_proj.quantizer.codebook 1,024
model.layers.30.self_attn.v_proj.bias 4,096
model.layers.30.self_attn.v_proj.quantizer.codebook 1,024
model.layers.30.self_attn.q_proj.bias 4,096
model.layers.30.self_attn.q_proj.quantizer.codebook 1,024
model.layers.30.self_attn.o_proj.bias 4,096
model.layers.30.self_attn.o_proj.quantizer.codebook 1,024
model.layers.30.mlp.up_proj.bias 11,008
model.layers.30.mlp.up_proj.quantizer.codebook 1,024
model.layers.30.mlp.gate_proj.bias 11,008
model.layers.30.mlp.gate_proj.quantizer.codebook 1,024
model.layers.30.mlp.down_proj.bias 4,096
model.layers.30.mlp.down_proj.quantizer.codebook 1,024
model.layers.30.input_layernorm.weight 4,096
model.layers.30.post_attention_layernorm.weight 4,096
model.layers.31.self_attn.k_proj.bias 4,096
model.layers.31.self_attn.k_proj.quantizer.codebook 1,024
model.layers.31.self_attn.v_proj.bias 4,096
model.layers.31.self_attn.v_proj.quantizer.codebook 1,024
model.layers.31.self_attn.q_proj.bias 4,096
model.layers.31.self_attn.q_proj.quantizer.codebook 1,024
model.layers.31.self_attn.o_proj.bias 4,096
model.layers.31.self_attn.o_proj.quantizer.codebook 1,024
model.layers.31.mlp.up_proj.bias 11,008
model.layers.31.mlp.up_proj.quantizer.codebook 1,024
model.layers.31.mlp.gate_proj.bias 11,008
model.layers.31.mlp.gate_proj.quantizer.codebook 1,024
model.layers.31.mlp.down_proj.bias 4,096
model.layers.31.mlp.down_proj.quantizer.codebook 1,024
model.layers.31.input_layernorm.weight 4,096
model.layers.31.post_attention_layernorm.weight 4,096
model.norm.weight 4,096
total number of parameters to optimize: 1,855,488
n accumulation steps: 1
n batches: 128
54694 MiB free out of 81050 MiB total
  0%|          | 0/128 [00:00<?, ?it/s]  1%|          | 1/128 [00:10<21:13, 10.03s/it]  2%|‚ñè         | 2/128 [00:19<20:28,  9.75s/it]  2%|‚ñè         | 3/128 [00:29<20:08,  9.67s/it]  3%|‚ñé         | 4/128 [00:38<19:52,  9.62s/it]  4%|‚ñç         | 5/128 [00:48<19:51,  9.69s/it]  5%|‚ñç         | 6/128 [00:58<19:36,  9.64s/it]  5%|‚ñå         | 7/128 [01:07<19:23,  9.62s/it]  6%|‚ñã         | 8/128 [01:17<19:11,  9.60s/it]  7%|‚ñã         | 9/128 [01:26<19:00,  9.58s/it]  8%|‚ñä         | 10/128 [01:36<18:50,  9.58s/it]  9%|‚ñä         | 11/128 [01:45<18:39,  9.57s/it]  9%|‚ñâ         | 12/128 [01:55<18:29,  9.56s/it] 10%|‚ñà         | 13/128 [02:04<18:19,  9.56s/it] 11%|‚ñà         | 14/128 [02:14<18:11,  9.57s/it] 12%|‚ñà‚ñè        | 15/128 [02:24<18:00,  9.56s/it] 12%|‚ñà‚ñé        | 16/128 [02:33<17:54,  9.60s/it] 13%|‚ñà‚ñé        | 17/128 [02:43<17:44,  9.59s/it] 14%|‚ñà‚ñç        | 18/128 [02:52<17:32,  9.57s/it] 15%|‚ñà‚ñç        | 19/128 [03:02<17:22,  9.57s/it] 16%|‚ñà‚ñå        | 20/128 [03:11<17:12,  9.56s/it] 16%|‚ñà‚ñã        | 21/128 [03:21<17:02,  9.56s/it] 17%|‚ñà‚ñã        | 22/128 [03:31<16:53,  9.56s/it] 18%|‚ñà‚ñä        | 23/128 [03:40<16:43,  9.56s/it] 19%|‚ñà‚ñâ        | 24/128 [03:50<16:33,  9.55s/it] 20%|‚ñà‚ñâ        | 25/128 [03:59<16:23,  9.55s/it] 20%|‚ñà‚ñà        | 26/128 [04:09<16:13,  9.55s/it] 21%|‚ñà‚ñà        | 27/128 [04:18<16:09,  9.60s/it] 22%|‚ñà‚ñà‚ñè       | 28/128 [04:28<15:59,  9.59s/it] 23%|‚ñà‚ñà‚ñé       | 29/128 [04:38<15:49,  9.59s/it] 23%|‚ñà‚ñà‚ñé       | 30/128 [04:47<15:39,  9.58s/it] 24%|‚ñà‚ñà‚ñç       | 31/128 [04:57<15:28,  9.57s/it] 25%|‚ñà‚ñà‚ñå       | 32/128 [05:06<15:19,  9.58s/it] 26%|‚ñà‚ñà‚ñå       | 33/128 [05:16<15:07,  9.55s/it] 27%|‚ñà‚ñà‚ñã       | 34/128 [05:25<14:58,  9.56s/it] 27%|‚ñà‚ñà‚ñã       | 35/128 [05:35<14:48,  9.56s/it] 28%|‚ñà‚ñà‚ñä       | 36/128 [05:45<14:39,  9.56s/it] 29%|‚ñà‚ñà‚ñâ       | 37/128 [05:54<14:30,  9.56s/it] 30%|‚ñà‚ñà‚ñâ       | 38/128 [06:04<14:21,  9.57s/it] 30%|‚ñà‚ñà‚ñà       | 39/128 [06:13<14:14,  9.60s/it] 31%|‚ñà‚ñà‚ñà‚ñè      | 40/128 [06:23<14:03,  9.59s/it] 32%|‚ñà‚ñà‚ñà‚ñè      | 41/128 [06:32<13:53,  9.58s/it] 33%|‚ñà‚ñà‚ñà‚ñé      | 42/128 [06:42<13:42,  9.56s/it] 34%|‚ñà‚ñà‚ñà‚ñé      | 43/128 [06:52<13:32,  9.56s/it] 34%|‚ñà‚ñà‚ñà‚ñç      | 44/128 [07:01<13:23,  9.56s/it] 35%|‚ñà‚ñà‚ñà‚ñå      | 45/128 [07:11<13:14,  9.57s/it] 36%|‚ñà‚ñà‚ñà‚ñå      | 46/128 [07:20<13:04,  9.56s/it] 37%|‚ñà‚ñà‚ñà‚ñã      | 47/128 [07:30<12:54,  9.56s/it] 38%|‚ñà‚ñà‚ñà‚ñä      | 48/128 [07:39<12:44,  9.56s/it] 38%|‚ñà‚ñà‚ñà‚ñä      | 49/128 [07:49<12:34,  9.55s/it] 39%|‚ñà‚ñà‚ñà‚ñâ      | 50/128 [07:59<12:26,  9.57s/it] 40%|‚ñà‚ñà‚ñà‚ñâ      | 51/128 [08:08<12:16,  9.57s/it] 41%|‚ñà‚ñà‚ñà‚ñà      | 52/128 [08:18<12:07,  9.57s/it] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 53/128 [08:27<11:56,  9.56s/it] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 54/128 [08:37<11:47,  9.56s/it] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 55/128 [08:46<11:37,  9.55s/it] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 56/128 [08:56<11:27,  9.55s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 57/128 [09:05<11:18,  9.56s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 58/128 [09:15<11:08,  9.55s/it] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 59/128 [09:24<10:59,  9.56s/it] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 60/128 [09:34<10:49,  9.55s/it] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 61/128 [09:44<10:42,  9.59s/it] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 62/128 [09:53<10:32,  9.58s/it] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 63/128 [10:03<10:22,  9.57s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 64/128 [10:12<10:12,  9.57s/it] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 65/128 [10:22<10:02,  9.57s/it] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 66/128 [10:32<09:52,  9.56s/it] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 67/128 [10:41<09:43,  9.56s/it] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 68/128 [10:51<09:32,  9.55s/it] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 69/128 [11:00<09:23,  9.55s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 70/128 [11:10<09:13,  9.55s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 71/128 [11:19<09:04,  9.56s/it] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 72/128 [11:29<08:57,  9.59s/it] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 73/128 [11:38<08:46,  9.58s/it] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 74/128 [11:48<08:36,  9.57s/it] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 75/128 [11:58<08:26,  9.56s/it] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 76/128 [12:07<08:16,  9.55s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 77/128 [12:17<08:07,  9.55s/it] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 78/128 [12:26<07:57,  9.55s/it] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 79/128 [12:36<07:47,  9.54s/it] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 80/128 [12:45<07:38,  9.55s/it] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 81/128 [12:55<07:28,  9.54s/it] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 82/128 [13:04<07:18,  9.54s/it] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 83/128 [13:14<07:11,  9.58s/it] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 84/128 [13:24<07:01,  9.57s/it] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 85/128 [13:33<06:51,  9.56s/it] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 86/128 [13:43<06:41,  9.55s/it] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 87/128 [13:52<06:31,  9.55s/it] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 88/128 [14:02<06:21,  9.55s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 89/128 [14:11<06:12,  9.55s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 90/128 [14:21<06:02,  9.55s/it] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 91/128 [14:30<05:53,  9.55s/it] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 92/128 [14:40<05:43,  9.54s/it] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 93/128 [14:49<05:33,  9.54s/it] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 94/128 [14:59<05:24,  9.55s/it] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 95/128 [15:09<05:16,  9.59s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 96/128 [15:18<05:06,  9.58s/it] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 97/128 [15:28<04:56,  9.57s/it] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 98/128 [15:37<04:47,  9.57s/it] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 99/128 [15:47<04:37,  9.57s/it] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 100/128 [15:56<04:27,  9.56s/it] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 101/128 [16:06<04:18,  9.57s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 102/128 [16:16<04:08,  9.57s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 103/128 [16:25<03:59,  9.57s/it] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 104/128 [16:35<03:49,  9.56s/it] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 105/128 [16:44<03:39,  9.56s/it] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 106/128 [16:54<03:31,  9.61s/it] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 107/128 [17:04<03:21,  9.60s/it] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 108/128 [17:13<03:11,  9.59s/it] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 109/128 [17:23<03:02,  9.59s/it] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 110/128 [17:32<02:52,  9.57s/it] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 111/128 [17:42<02:42,  9.56s/it] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 112/128 [17:51<02:33,  9.57s/it] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 113/128 [18:01<02:23,  9.57s/it] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 114/128 [18:11<02:13,  9.56s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 115/128 [18:20<02:04,  9.55s/it] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 116/128 [18:30<01:54,  9.56s/it] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 117/128 [18:39<01:45,  9.60s/it] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 118/128 [18:49<01:35,  9.59s/it] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 119/128 [18:58<01:26,  9.57s/it] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 120/128 [19:08<01:16,  9.56s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 121/128 [19:18<01:07,  9.57s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 122/128 [19:27<00:57,  9.57s/it] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 123/128 [19:37<00:47,  9.57s/it] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 124/128 [19:46<00:38,  9.56s/it] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 125/128 [19:56<00:28,  9.57s/it] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 126/128 [20:05<00:19,  9.57s/it] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 127/128 [20:15<00:09,  9.57s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 128/128 [20:25<00:00,  9.59s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 128/128 [20:25<00:00,  9.57s/it]
epoch 0 loss: 8.421408113092184
27458 MiB free out of 81050 MiB total
  0%|          | 0/128 [00:00<?, ?it/s]  1%|          | 1/128 [00:09<20:16,  9.58s/it]  2%|‚ñè         | 2/128 [00:19<20:03,  9.55s/it]  2%|‚ñè         | 3/128 [00:28<19:53,  9.55s/it]  3%|‚ñé         | 4/128 [00:38<19:45,  9.56s/it]  4%|‚ñç         | 5/128 [00:47<19:35,  9.56s/it]  5%|‚ñç         | 6/128 [00:57<19:26,  9.56s/it]  5%|‚ñå         | 7/128 [01:06<19:16,  9.56s/it]  6%|‚ñã         | 8/128 [01:16<19:05,  9.55s/it]  7%|‚ñã         | 9/128 [01:25<18:55,  9.54s/it]  8%|‚ñä         | 10/128 [01:35<18:46,  9.55s/it]  9%|‚ñä         | 11/128 [01:45<18:36,  9.54s/it]  9%|‚ñâ         | 12/128 [01:54<18:32,  9.59s/it] 10%|‚ñà         | 13/128 [02:04<18:21,  9.58s/it] 11%|‚ñà         | 14/128 [02:13<18:10,  9.56s/it] 12%|‚ñà‚ñè        | 15/128 [02:23<18:01,  9.57s/it] 12%|‚ñà‚ñé        | 16/128 [02:32<17:50,  9.56s/it] 13%|‚ñà‚ñé        | 17/128 [02:42<17:40,  9.56s/it] 14%|‚ñà‚ñç        | 18/128 [02:52<17:31,  9.56s/it] 15%|‚ñà‚ñç        | 19/128 [03:01<17:20,  9.55s/it] 16%|‚ñà‚ñå        | 20/128 [03:11<17:12,  9.56s/it] 16%|‚ñà‚ñã        | 21/128 [03:20<17:03,  9.57s/it] 17%|‚ñà‚ñã        | 22/128 [03:30<16:54,  9.57s/it] 18%|‚ñà‚ñä        | 23/128 [03:40<16:48,  9.60s/it] 19%|‚ñà‚ñâ        | 24/128 [03:49<16:37,  9.59s/it] 20%|‚ñà‚ñâ        | 25/128 [03:59<16:27,  9.59s/it] 20%|‚ñà‚ñà        | 26/128 [04:08<16:16,  9.58s/it] 21%|‚ñà‚ñà        | 27/128 [04:18<16:06,  9.57s/it] 22%|‚ñà‚ñà‚ñè       | 28/128 [04:27<15:55,  9.56s/it] 23%|‚ñà‚ñà‚ñé       | 29/128 [04:37<15:44,  9.54s/it] 23%|‚ñà‚ñà‚ñé       | 30/128 [04:46<15:36,  9.56s/it] 24%|‚ñà‚ñà‚ñç       | 31/128 [04:56<15:26,  9.55s/it] 25%|‚ñà‚ñà‚ñå       | 32/128 [05:06<15:17,  9.56s/it] 26%|‚ñà‚ñà‚ñå       | 33/128 [05:15<15:07,  9.56s/it] 27%|‚ñà‚ñà‚ñã       | 34/128 [05:25<15:01,  9.59s/it] 27%|‚ñà‚ñà‚ñã       | 35/128 [05:34<14:50,  9.57s/it] 28%|‚ñà‚ñà‚ñä       | 36/128 [05:44<14:40,  9.57s/it] 29%|‚ñà‚ñà‚ñâ       | 37/128 [05:53<14:31,  9.57s/it] 30%|‚ñà‚ñà‚ñâ       | 38/128 [06:03<14:20,  9.56s/it] 30%|‚ñà‚ñà‚ñà       | 39/128 [06:12<14:10,  9.55s/it] 31%|‚ñà‚ñà‚ñà‚ñè      | 40/128 [06:22<14:00,  9.55s/it] 32%|‚ñà‚ñà‚ñà‚ñè      | 41/128 [06:32<13:50,  9.54s/it] 33%|‚ñà‚ñà‚ñà‚ñé      | 42/128 [06:41<13:40,  9.54s/it] 34%|‚ñà‚ñà‚ñà‚ñé      | 43/128 [06:51<13:30,  9.54s/it] 34%|‚ñà‚ñà‚ñà‚ñç      | 44/128 [07:00<13:21,  9.54s/it] 35%|‚ñà‚ñà‚ñà‚ñå      | 45/128 [07:10<13:14,  9.57s/it] 36%|‚ñà‚ñà‚ñà‚ñå      | 46/128 [07:19<13:04,  9.57s/it] 37%|‚ñà‚ñà‚ñà‚ñã      | 47/128 [07:29<12:55,  9.57s/it] 38%|‚ñà‚ñà‚ñà‚ñä      | 48/128 [07:39<12:46,  9.58s/it] 38%|‚ñà‚ñà‚ñà‚ñä      | 49/128 [07:48<12:36,  9.57s/it] 39%|‚ñà‚ñà‚ñà‚ñâ      | 50/128 [07:58<12:27,  9.58s/it] 40%|‚ñà‚ñà‚ñà‚ñâ      | 51/128 [08:07<12:16,  9.56s/it] 41%|‚ñà‚ñà‚ñà‚ñà      | 52/128 [08:17<12:07,  9.57s/it] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 53/128 [08:26<11:56,  9.55s/it] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 54/128 [08:36<11:46,  9.54s/it] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 55/128 [08:45<11:36,  9.54s/it] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 56/128 [08:55<11:29,  9.58s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 57/128 [09:05<11:19,  9.57s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 58/128 [09:14<11:09,  9.56s/it] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 59/128 [09:24<10:59,  9.56s/it] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 60/128 [09:33<10:50,  9.57s/it] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 61/128 [09:43<10:40,  9.56s/it] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 62/128 [09:52<10:30,  9.55s/it] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 63/128 [10:02<10:20,  9.55s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 64/128 [10:11<10:10,  9.54s/it] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 65/128 [10:21<10:01,  9.55s/it] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 66/128 [10:31<09:51,  9.55s/it] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 67/128 [10:40<09:42,  9.54s/it] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 68/128 [10:50<09:34,  9.58s/it] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 69/128 [10:59<09:24,  9.57s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 70/128 [11:09<09:14,  9.56s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 71/128 [11:18<09:04,  9.56s/it] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 72/128 [11:28<08:55,  9.55s/it] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 73/128 [11:37<08:45,  9.55s/it] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 74/128 [11:47<08:36,  9.56s/it] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 75/128 [11:57<08:26,  9.56s/it] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 76/128 [12:06<08:17,  9.57s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 77/128 [12:16<08:07,  9.56s/it] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 78/128 [12:25<07:57,  9.56s/it] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 79/128 [12:35<07:49,  9.59s/it] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 80/128 [12:45<07:40,  9.59s/it] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 81/128 [12:54<07:30,  9.58s/it] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 82/128 [13:04<07:20,  9.57s/it] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 83/128 [13:13<07:10,  9.56s/it] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 84/128 [13:23<07:00,  9.56s/it] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 85/128 [13:32<06:51,  9.56s/it] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 86/128 [13:42<06:41,  9.56s/it] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 87/128 [13:51<06:32,  9.56s/it] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 88/128 [14:01<06:22,  9.56s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 89/128 [14:11<06:13,  9.56s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 90/128 [14:20<06:04,  9.60s/it] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 91/128 [14:30<05:54,  9.59s/it] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 92/128 [14:39<05:44,  9.58s/it] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 93/128 [14:49<05:34,  9.57s/it] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 94/128 [14:58<05:25,  9.56s/it] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 95/128 [15:08<05:15,  9.55s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 96/128 [15:17<05:05,  9.54s/it] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 97/128 [15:27<04:55,  9.54s/it] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 98/128 [15:37<04:46,  9.55s/it] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 99/128 [15:46<04:37,  9.55s/it] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 100/128 [15:56<04:27,  9.55s/it] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 101/128 [16:05<04:18,  9.58s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 102/128 [16:15<04:08,  9.58s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 103/128 [16:24<03:59,  9.57s/it] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 104/128 [16:34<03:49,  9.56s/it] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 105/128 [16:44<03:39,  9.55s/it] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 106/128 [16:53<03:30,  9.55s/it] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 107/128 [17:03<03:20,  9.56s/it] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 108/128 [17:12<03:11,  9.56s/it] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 109/128 [17:22<03:01,  9.56s/it] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 110/128 [17:31<02:51,  9.55s/it] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 111/128 [17:41<02:42,  9.55s/it] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 112/128 [17:51<02:33,  9.59s/it] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 113/128 [18:00<02:23,  9.58s/it] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 114/128 [18:10<02:14,  9.58s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 115/128 [18:19<02:04,  9.57s/it] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 116/128 [18:29<01:54,  9.56s/it] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 117/128 [18:38<01:45,  9.55s/it] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 118/128 [18:48<01:35,  9.55s/it] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 119/128 [18:57<01:25,  9.55s/it] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 120/128 [19:07<01:16,  9.55s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 121/128 [19:16<01:06,  9.55s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 122/128 [19:26<00:57,  9.55s/it] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 123/128 [19:36<00:47,  9.55s/it] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 124/128 [19:45<00:38,  9.58s/it] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 125/128 [19:55<00:28,  9.56s/it] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 126/128 [20:04<00:19,  9.56s/it] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 127/128 [20:14<00:09,  9.55s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 128/128 [20:23<00:00,  9.55s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 128/128 [20:23<00:00,  9.56s/it]
epoch 1 loss: 7.717934291809797
27458 MiB free out of 81050 MiB total
  0%|          | 0/128 [00:00<?, ?it/s]  1%|          | 1/128 [00:09<20:12,  9.55s/it]  2%|‚ñè         | 2/128 [00:19<20:05,  9.56s/it]  2%|‚ñè         | 3/128 [00:28<19:54,  9.56s/it]  3%|‚ñé         | 4/128 [00:38<19:45,  9.56s/it]  4%|‚ñç         | 5/128 [00:47<19:34,  9.55s/it]  5%|‚ñç         | 6/128 [00:57<19:24,  9.55s/it]  5%|‚ñå         | 7/128 [01:06<19:19,  9.59s/it]  6%|‚ñã         | 8/128 [01:16<19:10,  9.58s/it]  7%|‚ñã         | 9/128 [01:26<18:59,  9.57s/it]  8%|‚ñä         | 10/128 [01:35<18:48,  9.56s/it]  9%|‚ñä         | 11/128 [01:45<18:39,  9.57s/it]  9%|‚ñâ         | 12/128 [01:54<18:28,  9.56s/it] 10%|‚ñà         | 13/128 [02:04<18:19,  9.56s/it] 11%|‚ñà         | 14/128 [02:13<18:09,  9.56s/it] 12%|‚ñà‚ñè        | 15/128 [02:23<17:59,  9.56s/it] 12%|‚ñà‚ñé        | 16/128 [02:32<17:49,  9.55s/it] 13%|‚ñà‚ñé        | 17/128 [02:42<17:39,  9.54s/it] 14%|‚ñà‚ñç        | 18/128 [02:52<17:32,  9.57s/it] 15%|‚ñà‚ñç        | 19/128 [03:01<17:22,  9.57s/it] 16%|‚ñà‚ñå        | 20/128 [03:11<17:13,  9.57s/it] 16%|‚ñà‚ñã        | 21/128 [03:20<17:03,  9.56s/it] 17%|‚ñà‚ñã        | 22/128 [03:30<16:53,  9.56s/it] 18%|‚ñà‚ñä        | 23/128 [03:39<16:42,  9.55s/it] 19%|‚ñà‚ñâ        | 24/128 [03:49<16:33,  9.56s/it] 20%|‚ñà‚ñâ        | 25/128 [03:59<16:24,  9.56s/it] 20%|‚ñà‚ñà        | 26/128 [04:08<16:13,  9.55s/it] 21%|‚ñà‚ñà        | 27/128 [04:18<16:05,  9.56s/it] 22%|‚ñà‚ñà‚ñè       | 28/128 [04:27<15:55,  9.56s/it] 23%|‚ñà‚ñà‚ñé       | 29/128 [04:37<15:49,  9.59s/it] 23%|‚ñà‚ñà‚ñé       | 30/128 [04:46<15:39,  9.59s/it] 24%|‚ñà‚ñà‚ñç       | 31/128 [04:56<15:28,  9.57s/it] 25%|‚ñà‚ñà‚ñå       | 32/128 [05:06<15:18,  9.56s/it] 26%|‚ñà‚ñà‚ñå       | 33/128 [05:15<15:08,  9.57s/it] 27%|‚ñà‚ñà‚ñã       | 34/128 [05:25<14:58,  9.56s/it] 27%|‚ñà‚ñà‚ñã       | 35/128 [05:34<14:48,  9.55s/it] 28%|‚ñà‚ñà‚ñä       | 36/128 [05:44<14:39,  9.56s/it] 29%|‚ñà‚ñà‚ñâ       | 37/128 [05:53<14:29,  9.56s/it] 30%|‚ñà‚ñà‚ñâ       | 38/128 [06:03<14:20,  9.56s/it] 30%|‚ñà‚ñà‚ñà       | 39/128 [06:12<14:10,  9.55s/it] 31%|‚ñà‚ñà‚ñà‚ñè      | 40/128 [06:22<14:00,  9.55s/it] 32%|‚ñà‚ñà‚ñà‚ñè      | 41/128 [06:32<13:54,  9.59s/it] 33%|‚ñà‚ñà‚ñà‚ñé      | 42/128 [06:41<13:44,  9.58s/it] 34%|‚ñà‚ñà‚ñà‚ñé      | 43/128 [06:51<13:34,  9.58s/it] 34%|‚ñà‚ñà‚ñà‚ñç      | 44/128 [07:00<13:23,  9.57s/it] 35%|‚ñà‚ñà‚ñà‚ñå      | 45/128 [07:10<13:13,  9.57s/it] 36%|‚ñà‚ñà‚ñà‚ñå      | 46/128 [07:19<13:04,  9.56s/it] 37%|‚ñà‚ñà‚ñà‚ñã      | 47/128 [07:29<12:53,  9.55s/it] 38%|‚ñà‚ñà‚ñà‚ñä      | 48/128 [07:38<12:44,  9.55s/it] 38%|‚ñà‚ñà‚ñà‚ñä      | 49/128 [07:48<12:34,  9.55s/it] 39%|‚ñà‚ñà‚ñà‚ñâ      | 50/128 [07:58<12:24,  9.55s/it] 40%|‚ñà‚ñà‚ñà‚ñâ      | 51/128 [08:07<12:15,  9.55s/it] 41%|‚ñà‚ñà‚ñà‚ñà      | 52/128 [08:17<12:08,  9.58s/it] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 53/128 [08:26<11:58,  9.58s/it] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 54/128 [08:36<11:48,  9.57s/it] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 55/128 [08:45<11:37,  9.56s/it] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 56/128 [08:55<11:28,  9.56s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 57/128 [09:05<11:18,  9.56s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 58/128 [09:14<11:09,  9.56s/it] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 59/128 [09:24<11:00,  9.58s/it] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 60/128 [09:33<10:50,  9.57s/it] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 61/128 [09:43<10:41,  9.57s/it] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 62/128 [09:52<10:31,  9.57s/it] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 63/128 [10:02<10:23,  9.59s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 64/128 [10:12<10:13,  9.58s/it] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 65/128 [10:21<10:03,  9.58s/it] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 66/128 [10:31<09:52,  9.56s/it] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 67/128 [10:40<09:42,  9.55s/it] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 68/128 [10:50<09:33,  9.55s/it] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 69/128 [10:59<09:23,  9.56s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 70/128 [11:09<09:14,  9.56s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 71/128 [11:19<09:05,  9.57s/it] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 72/128 [11:28<08:55,  9.57s/it] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 73/128 [11:38<08:45,  9.55s/it] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 74/128 [11:47<08:37,  9.59s/it] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 75/128 [11:57<08:28,  9.59s/it] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 76/128 [12:06<08:17,  9.57s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 77/128 [12:16<08:07,  9.56s/it] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 78/128 [12:26<07:58,  9.56s/it] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 79/128 [12:35<07:48,  9.56s/it] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 80/128 [12:45<07:38,  9.55s/it] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 81/128 [12:54<07:28,  9.55s/it] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 82/128 [13:04<07:19,  9.55s/it] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 83/128 [13:13<07:10,  9.56s/it] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 84/128 [13:23<07:00,  9.55s/it] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 85/128 [13:32<06:51,  9.58s/it] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 86/128 [13:42<06:41,  9.56s/it] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 87/128 [13:51<06:31,  9.55s/it] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 88/128 [14:01<06:21,  9.54s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 89/128 [14:11<06:11,  9.54s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 90/128 [14:20<06:02,  9.53s/it] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 91/128 [14:30<05:52,  9.54s/it] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 92/128 [14:39<05:43,  9.55s/it] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 93/128 [14:49<05:34,  9.55s/it] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 94/128 [14:58<05:24,  9.55s/it] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 95/128 [15:08<05:15,  9.55s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 96/128 [15:17<05:05,  9.55s/it] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 97/128 [15:27<04:57,  9.59s/it] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 98/128 [15:37<04:47,  9.57s/it] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 99/128 [15:46<04:37,  9.56s/it] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 100/128 [15:56<04:27,  9.57s/it] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 101/128 [16:05<04:18,  9.56s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 102/128 [16:15<04:08,  9.56s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 103/128 [16:24<03:58,  9.55s/it] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 104/128 [16:34<03:49,  9.55s/it] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 105/128 [16:43<03:39,  9.55s/it] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 106/128 [16:53<03:30,  9.55s/it] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 107/128 [17:03<03:20,  9.56s/it] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 108/128 [17:12<03:11,  9.58s/it] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 109/128 [17:22<03:01,  9.56s/it] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 110/128 [17:31<02:52,  9.56s/it] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 111/128 [17:41<02:42,  9.56s/it] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 112/128 [17:50<02:33,  9.57s/it] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 113/128 [18:00<02:23,  9.57s/it] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 114/128 [18:09<02:13,  9.55s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 115/128 [18:19<02:04,  9.55s/it] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 116/128 [18:29<01:54,  9.56s/it] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 117/128 [18:38<01:45,  9.55s/it] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 118/128 [18:48<01:35,  9.55s/it] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 119/128 [18:57<01:26,  9.57s/it] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 120/128 [19:07<01:16,  9.57s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 121/128 [19:16<01:06,  9.56s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 122/128 [19:26<00:57,  9.55s/it] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 123/128 [19:35<00:47,  9.54s/it] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 124/128 [19:45<00:38,  9.54s/it] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 125/128 [19:55<00:28,  9.54s/it] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 126/128 [20:04<00:19,  9.54s/it] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 127/128 [20:14<00:09,  9.54s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 128/128 [20:23<00:00,  9.54s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 128/128 [20:23<00:00,  9.56s/it]
epoch 2 loss: 7.560352731496096
27458 MiB free out of 81050 MiB total
  0%|          | 0/128 [00:00<?, ?it/s]  1%|          | 1/128 [00:09<20:12,  9.54s/it]  2%|‚ñè         | 2/128 [00:19<20:10,  9.60s/it]  2%|‚ñè         | 3/128 [00:28<19:57,  9.58s/it]  3%|‚ñé         | 4/128 [00:38<19:46,  9.57s/it]  4%|‚ñç         | 5/128 [00:47<19:35,  9.55s/it]  5%|‚ñç         | 6/128 [00:57<19:24,  9.55s/it]  5%|‚ñå         | 7/128 [01:06<19:14,  9.54s/it]  6%|‚ñã         | 8/128 [01:16<19:06,  9.55s/it]  7%|‚ñã         | 9/128 [01:26<18:56,  9.55s/it]  8%|‚ñä         | 10/128 [01:35<18:47,  9.56s/it]  9%|‚ñä         | 11/128 [01:45<18:36,  9.54s/it]  9%|‚ñâ         | 12/128 [01:54<18:27,  9.55s/it] 10%|‚ñà         | 13/128 [02:04<18:22,  9.59s/it] 11%|‚ñà         | 14/128 [02:13<18:12,  9.58s/it] 12%|‚ñà‚ñè        | 15/128 [02:23<18:01,  9.57s/it] 12%|‚ñà‚ñé        | 16/128 [02:33<17:52,  9.57s/it] 13%|‚ñà‚ñé        | 17/128 [02:42<17:42,  9.57s/it] 14%|‚ñà‚ñç        | 18/128 [02:52<17:32,  9.56s/it] 15%|‚ñà‚ñç        | 19/128 [03:01<17:22,  9.56s/it] 16%|‚ñà‚ñå        | 20/128 [03:11<17:12,  9.56s/it] 16%|‚ñà‚ñã        | 21/128 [03:20<17:03,  9.57s/it] 17%|‚ñà‚ñã        | 22/128 [03:30<16:52,  9.55s/it] 18%|‚ñà‚ñä        | 23/128 [03:39<16:43,  9.56s/it] 19%|‚ñà‚ñâ        | 24/128 [03:49<16:32,  9.54s/it] 20%|‚ñà‚ñâ        | 25/128 [03:59<16:27,  9.58s/it] 20%|‚ñà‚ñà        | 26/128 [04:08<16:15,  9.57s/it] 21%|‚ñà‚ñà        | 27/128 [04:18<16:05,  9.56s/it] 22%|‚ñà‚ñà‚ñè       | 28/128 [04:27<15:55,  9.56s/it] 23%|‚ñà‚ñà‚ñé       | 29/128 [04:37<15:45,  9.55s/it] 23%|‚ñà‚ñà‚ñé       | 30/128 [04:46<15:36,  9.56s/it] 24%|‚ñà‚ñà‚ñç       | 31/128 [04:56<15:25,  9.54s/it] 25%|‚ñà‚ñà‚ñå       | 32/128 [05:05<15:17,  9.56s/it] 26%|‚ñà‚ñà‚ñå       | 33/128 [05:15<15:08,  9.56s/it] 27%|‚ñà‚ñà‚ñã       | 34/128 [05:25<14:58,  9.55s/it] 27%|‚ñà‚ñà‚ñã       | 35/128 [05:34<14:49,  9.56s/it] 28%|‚ñà‚ñà‚ñä       | 36/128 [05:44<14:42,  9.59s/it] 29%|‚ñà‚ñà‚ñâ       | 37/128 [05:53<14:32,  9.58s/it] 30%|‚ñà‚ñà‚ñâ       | 38/128 [06:03<14:21,  9.58s/it] 30%|‚ñà‚ñà‚ñà       | 39/128 [06:12<14:11,  9.57s/it] 31%|‚ñà‚ñà‚ñà‚ñè      | 40/128 [06:22<14:01,  9.56s/it] 32%|‚ñà‚ñà‚ñà‚ñè      | 41/128 [06:32<13:51,  9.56s/it] 33%|‚ñà‚ñà‚ñà‚ñé      | 42/128 [06:41<13:41,  9.56s/it] 34%|‚ñà‚ñà‚ñà‚ñé      | 43/128 [06:51<13:32,  9.56s/it] 34%|‚ñà‚ñà‚ñà‚ñç      | 44/128 [07:00<13:21,  9.55s/it] 35%|‚ñà‚ñà‚ñà‚ñå      | 45/128 [07:10<13:12,  9.55s/it] 36%|‚ñà‚ñà‚ñà‚ñå      | 46/128 [07:19<13:02,  9.55s/it] 37%|‚ñà‚ñà‚ñà‚ñã      | 47/128 [07:29<12:55,  9.57s/it] 38%|‚ñà‚ñà‚ñà‚ñä      | 48/128 [07:38<12:44,  9.56s/it] 38%|‚ñà‚ñà‚ñà‚ñä      | 49/128 [07:48<12:34,  9.55s/it] 39%|‚ñà‚ñà‚ñà‚ñâ      | 50/128 [07:57<12:23,  9.54s/it] 40%|‚ñà‚ñà‚ñà‚ñâ      | 51/128 [08:07<12:15,  9.55s/it] 41%|‚ñà‚ñà‚ñà‚ñà      | 52/128 [08:17<12:05,  9.55s/it] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 53/128 [08:26<11:56,  9.55s/it] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 54/128 [08:36<11:47,  9.56s/it] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 55/128 [08:45<11:37,  9.55s/it] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 56/128 [08:55<11:27,  9.55s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 57/128 [09:04<11:18,  9.56s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 58/128 [09:14<11:11,  9.59s/it] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 59/128 [09:24<11:00,  9.58s/it] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 60/128 [09:33<10:50,  9.56s/it] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 61/128 [09:43<10:40,  9.57s/it] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 62/128 [09:52<10:31,  9.56s/it] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 63/128 [10:02<10:21,  9.56s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 64/128 [10:11<10:11,  9.55s/it] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 65/128 [10:21<10:02,  9.56s/it] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 66/128 [10:30<09:52,  9.56s/it] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 67/128 [10:40<09:42,  9.55s/it] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 68/128 [10:50<09:32,  9.54s/it] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 69/128 [10:59<09:23,  9.54s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 70/128 [11:09<09:15,  9.58s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 71/128 [11:18<09:05,  9.58s/it] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 72/128 [11:28<08:54,  9.55s/it] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 73/128 [11:37<08:45,  9.55s/it] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 74/128 [11:47<08:35,  9.54s/it] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 75/128 [11:56<08:25,  9.54s/it] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 76/128 [12:06<08:15,  9.54s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 77/128 [12:15<08:06,  9.54s/it] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 78/128 [12:25<07:56,  9.53s/it] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 79/128 [12:35<07:47,  9.54s/it] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 80/128 [12:44<07:37,  9.53s/it] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 81/128 [12:54<07:29,  9.57s/it] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 82/128 [13:03<07:20,  9.57s/it] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 83/128 [13:13<07:10,  9.57s/it] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 84/128 [13:22<07:00,  9.57s/it] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 85/128 [13:32<06:51,  9.56s/it] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 86/128 [13:42<06:41,  9.56s/it] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 87/128 [13:51<06:31,  9.56s/it] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 88/128 [14:01<06:22,  9.55s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 89/128 [14:10<06:12,  9.55s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 90/128 [14:20<06:03,  9.56s/it] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 91/128 [14:29<05:53,  9.56s/it] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 92/128 [14:39<05:44,  9.58s/it] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 93/128 [14:48<05:34,  9.57s/it] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 94/128 [14:58<05:25,  9.56s/it] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 95/128 [15:08<05:15,  9.56s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 96/128 [15:17<05:05,  9.56s/it] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 97/128 [15:27<04:56,  9.55s/it] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 98/128 [15:36<04:46,  9.55s/it] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 99/128 [15:46<04:37,  9.55s/it] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 100/128 [15:55<04:27,  9.56s/it] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 101/128 [16:05<04:18,  9.56s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 102/128 [16:14<04:08,  9.56s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 103/128 [16:24<03:59,  9.59s/it] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 104/128 [16:34<03:49,  9.57s/it] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 105/128 [16:43<03:39,  9.56s/it] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 106/128 [16:53<03:30,  9.55s/it] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 107/128 [17:02<03:20,  9.54s/it] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 108/128 [17:12<03:10,  9.54s/it] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 109/128 [17:21<03:01,  9.54s/it] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 110/128 [17:31<02:51,  9.54s/it] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 111/128 [17:40<02:42,  9.54s/it] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 112/128 [17:50<02:32,  9.54s/it] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 113/128 [18:00<02:23,  9.55s/it] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 114/128 [18:09<02:14,  9.57s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 115/128 [18:19<02:04,  9.56s/it] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 116/128 [18:28<01:54,  9.56s/it] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 117/128 [18:38<01:45,  9.56s/it] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 118/128 [18:47<01:35,  9.56s/it] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 119/128 [18:57<01:25,  9.55s/it] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 120/128 [19:06<01:16,  9.55s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 121/128 [19:16<01:06,  9.55s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 122/128 [19:26<00:57,  9.55s/it] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 123/128 [19:35<00:47,  9.55s/it] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 124/128 [19:45<00:38,  9.55s/it] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 125/128 [19:54<00:28,  9.54s/it] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 126/128 [20:04<00:19,  9.58s/it] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 127/128 [20:13<00:09,  9.58s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 128/128 [20:23<00:00,  9.56s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 128/128 [20:23<00:00,  9.56s/it]
epoch 3 loss: 7.393157761543989
27458 MiB free out of 81050 MiB total
  0%|          | 0/128 [00:00<?, ?it/s]  1%|          | 1/128 [00:09<20:12,  9.55s/it]  2%|‚ñè         | 2/128 [00:19<19:59,  9.52s/it]  2%|‚ñè         | 3/128 [00:28<19:51,  9.53s/it]  3%|‚ñé         | 4/128 [00:38<19:41,  9.53s/it]  4%|‚ñç         | 5/128 [00:47<19:32,  9.53s/it]  5%|‚ñç         | 6/128 [00:57<19:23,  9.54s/it]  5%|‚ñå         | 7/128 [01:06<19:14,  9.54s/it]  6%|‚ñã         | 8/128 [01:16<19:06,  9.56s/it]  7%|‚ñã         | 9/128 [01:25<19:00,  9.59s/it]  8%|‚ñä         | 10/128 [01:35<18:49,  9.57s/it]  9%|‚ñä         | 11/128 [01:45<18:39,  9.57s/it]  9%|‚ñâ         | 12/128 [01:54<18:29,  9.56s/it] 10%|‚ñà         | 13/128 [02:04<18:19,  9.57s/it] 11%|‚ñà         | 14/128 [02:13<18:10,  9.56s/it] 12%|‚ñà‚ñè        | 15/128 [02:23<18:00,  9.56s/it] 12%|‚ñà‚ñé        | 16/128 [02:32<17:49,  9.55s/it] 13%|‚ñà‚ñé        | 17/128 [02:42<17:40,  9.56s/it] 14%|‚ñà‚ñç        | 18/128 [02:51<17:30,  9.55s/it] 15%|‚ñà‚ñç        | 19/128 [03:01<17:21,  9.55s/it] 16%|‚ñà‚ñå        | 20/128 [03:11<17:14,  9.58s/it] 16%|‚ñà‚ñã        | 21/128 [03:20<17:03,  9.57s/it] 17%|‚ñà‚ñã        | 22/128 [03:30<16:54,  9.57s/it] 18%|‚ñà‚ñä        | 23/128 [03:39<16:42,  9.55s/it] 19%|‚ñà‚ñâ        | 24/128 [03:49<16:32,  9.54s/it] 20%|‚ñà‚ñâ        | 25/128 [03:58<16:24,  9.55s/it] 20%|‚ñà‚ñà        | 26/128 [04:08<16:14,  9.55s/it] 21%|‚ñà‚ñà        | 27/128 [04:17<16:04,  9.55s/it] 22%|‚ñà‚ñà‚ñè       | 28/128 [04:27<15:55,  9.55s/it] 23%|‚ñà‚ñà‚ñé       | 29/128 [04:37<15:45,  9.55s/it] 23%|‚ñà‚ñà‚ñé       | 30/128 [04:46<15:36,  9.56s/it] 24%|‚ñà‚ñà‚ñç       | 31/128 [04:56<15:29,  9.58s/it] 25%|‚ñà‚ñà‚ñå       | 32/128 [05:05<15:18,  9.57s/it] 26%|‚ñà‚ñà‚ñå       | 33/128 [05:15<15:08,  9.56s/it] 27%|‚ñà‚ñà‚ñã       | 34/128 [05:24<14:58,  9.56s/it] 27%|‚ñà‚ñà‚ñã       | 35/128 [05:34<14:48,  9.56s/it] 28%|‚ñà‚ñà‚ñä       | 36/128 [05:44<14:39,  9.56s/it] 29%|‚ñà‚ñà‚ñâ       | 37/128 [05:53<14:29,  9.55s/it] 30%|‚ñà‚ñà‚ñâ       | 38/128 [06:03<14:18,  9.54s/it] 30%|‚ñà‚ñà‚ñà       | 39/128 [06:12<14:09,  9.55s/it] 31%|‚ñà‚ñà‚ñà‚ñè      | 40/128 [06:22<13:59,  9.54s/it] 32%|‚ñà‚ñà‚ñà‚ñè      | 41/128 [06:31<13:50,  9.54s/it] 33%|‚ñà‚ñà‚ñà‚ñé      | 42/128 [06:41<13:44,  9.58s/it] 34%|‚ñà‚ñà‚ñà‚ñé      | 43/128 [06:50<13:33,  9.58s/it] 34%|‚ñà‚ñà‚ñà‚ñç      | 44/128 [07:00<13:23,  9.57s/it] 35%|‚ñà‚ñà‚ñà‚ñå      | 45/128 [07:10<13:13,  9.56s/it] 36%|‚ñà‚ñà‚ñà‚ñå      | 46/128 [07:19<13:04,  9.56s/it] 37%|‚ñà‚ñà‚ñà‚ñã      | 47/128 [07:29<12:53,  9.55s/it] 38%|‚ñà‚ñà‚ñà‚ñä      | 48/128 [07:38<12:44,  9.55s/it] 38%|‚ñà‚ñà‚ñà‚ñä      | 49/128 [07:48<12:34,  9.55s/it] 39%|‚ñà‚ñà‚ñà‚ñâ      | 50/128 [07:57<12:25,  9.55s/it] 40%|‚ñà‚ñà‚ñà‚ñâ      | 51/128 [08:07<12:15,  9.55s/it] 41%|‚ñà‚ñà‚ñà‚ñà      | 52/128 [08:16<12:06,  9.55s/it] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 53/128 [08:26<11:56,  9.56s/it] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 54/128 [08:36<11:48,  9.58s/it] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 55/128 [08:45<11:38,  9.57s/it] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 56/128 [08:55<11:29,  9.57s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 57/128 [09:04<11:19,  9.57s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 58/128 [09:14<11:08,  9.55s/it] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 59/128 [09:23<10:58,  9.54s/it] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 60/128 [09:33<10:49,  9.55s/it] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 61/128 [09:42<10:39,  9.55s/it] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 62/128 [09:52<10:30,  9.55s/it] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 63/128 [10:02<10:20,  9.54s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 64/128 [10:11<10:11,  9.55s/it] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 65/128 [10:21<10:03,  9.58s/it] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 66/128 [10:30<09:53,  9.58s/it] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 67/128 [10:40<09:43,  9.57s/it] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 68/128 [10:49<09:33,  9.56s/it] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 69/128 [10:59<09:24,  9.57s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 70/128 [11:09<09:14,  9.57s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 71/128 [11:18<09:04,  9.56s/it] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 72/128 [11:28<08:54,  9.55s/it] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 73/128 [11:37<08:45,  9.56s/it] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 74/128 [11:47<08:36,  9.56s/it] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 75/128 [11:56<08:26,  9.56s/it] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 76/128 [12:06<08:18,  9.58s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 77/128 [12:16<08:08,  9.58s/it] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 78/128 [12:25<07:58,  9.57s/it] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 79/128 [12:35<07:48,  9.56s/it] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 80/128 [12:44<07:39,  9.57s/it] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 81/128 [12:54<07:29,  9.56s/it] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 82/128 [13:03<07:19,  9.56s/it] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 83/128 [13:13<07:10,  9.56s/it] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 84/128 [13:22<07:00,  9.55s/it] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 85/128 [13:32<06:51,  9.56s/it] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 86/128 [13:42<06:41,  9.56s/it] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 87/128 [13:51<06:33,  9.60s/it] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 88/128 [14:01<06:23,  9.59s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 89/128 [14:10<06:13,  9.58s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 90/128 [14:20<06:03,  9.57s/it] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 91/128 [14:29<05:53,  9.56s/it] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 92/128 [14:39<05:44,  9.56s/it] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 93/128 [14:49<05:34,  9.55s/it] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 94/128 [14:58<05:24,  9.55s/it] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 95/128 [15:08<05:15,  9.55s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 96/128 [15:17<05:05,  9.55s/it] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 97/128 [15:27<04:55,  9.54s/it] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 98/128 [15:36<04:46,  9.56s/it] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 99/128 [15:46<04:38,  9.60s/it] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 100/128 [15:56<04:28,  9.59s/it] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 101/128 [16:05<04:18,  9.59s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 102/128 [16:15<04:08,  9.58s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 103/128 [16:24<03:59,  9.56s/it] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 104/128 [16:34<03:49,  9.56s/it] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 105/128 [16:43<03:39,  9.56s/it] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 106/128 [16:53<03:30,  9.55s/it] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 107/128 [17:02<03:20,  9.55s/it] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 108/128 [17:12<03:11,  9.56s/it] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 109/128 [17:22<03:01,  9.56s/it] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 110/128 [17:31<02:52,  9.60s/it] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 111/128 [17:41<02:42,  9.58s/it] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 112/128 [17:50<02:33,  9.57s/it] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 113/128 [18:00<02:23,  9.57s/it] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 114/128 [18:09<02:13,  9.57s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 115/128 [18:19<02:04,  9.57s/it] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 116/128 [18:29<01:54,  9.57s/it] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 117/128 [18:38<01:45,  9.56s/it] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 118/128 [18:48<01:35,  9.55s/it] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 119/128 [18:57<01:25,  9.55s/it] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 120/128 [19:07<01:16,  9.56s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 121/128 [19:16<01:07,  9.58s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 122/128 [19:26<00:57,  9.57s/it] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 123/128 [19:36<00:47,  9.57s/it] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 124/128 [19:45<00:38,  9.56s/it] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 125/128 [19:55<00:28,  9.56s/it] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 126/128 [20:04<00:19,  9.56s/it] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 127/128 [20:14<00:09,  9.55s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 128/128 [20:23<00:00,  9.55s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 128/128 [20:23<00:00,  9.56s/it]
epoch 4 loss: 7.296654127538204
27458 MiB free out of 81050 MiB total
Loading tokenizer for meta-llama/Llama-2-7b-hf
/home/lliu/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Dataset: wikitext2
Evaluating ...
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
Perplexity: 1545.481934
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb: wikitext2/perplexity ‚ñÅ
wandb: 
wandb: Run summary:
wandb: wikitext2/perplexity 1545.48193
wandb: 
wandb: üöÄ View run glowing-hill-1 at: https://wandb.ai/m6481/post_training_quantization/runs/9dz7zvuk
wandb: Ô∏è‚ö° View job at https://wandb.ai/m6481/post_training_quantization/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjQ5NjA2NTU1Nw==/version_details/v0
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241114_190846-9dz7zvuk/logs
