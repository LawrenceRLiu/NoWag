{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import numpy as np\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class low_rank_and_sparse(nn.Module):\n",
    "    def __init__(self, \n",
    "                 k: int,\n",
    "                 H: torch.Tensor,\n",
    "                 Weights: torch.Tensor,\n",
    "                 ):\n",
    "        \"\"\"Vector Quantizer\n",
    "\n",
    "        Args:\n",
    "            initial_codebook (torch.Tensor): the initial codebook (n,k)\n",
    "            H (torch.Tensor): the Hessian of shape (n,n)\n",
    "            Weights (torch.Tensor): weights of shape (n,n)\n",
    "        \"\"\"\n",
    "        super(low_rank_and_sparse, self).__init__()\n",
    "\n",
    "        # self.codebook = nn.Parameter(initial_codebook, requires_grad=True)\n",
    "        self.H = H\n",
    "        self.Weights = Weights\n",
    "        n = Weights.shape[0]\n",
    "\n",
    "        self.A = nn.Parameter(torch.empty(n,k,device = Weights.device,\n",
    "                                                            dtype = Weights.dtype\n",
    "                                                            ).normal_(0,1), requires_grad=True)\n",
    "        self.B = nn.Parameter(torch.empty(k,n,device = Weights.device,\n",
    "                                                            dtype = Weights.dtype\n",
    "                                                            ).uniform_(-1,1), requires_grad=True)\n",
    "        \n",
    "        x = torch.empty(n,n,device = Weights.device,\n",
    "                                                            dtype = Weights.dtype\n",
    "                                                            ).uniform_(0,1)\n",
    "        \n",
    "\n",
    "        self.sparse = nn.Parameter(\n",
    "            torch.log(x/(1-x)), requires_grad=True)\n",
    "        #randomly initialize the initial assignments through xavier uniform initialization\n",
    "        #a matrix of shape (n,k)\n",
    "        # self.initial_assignments = nn.Parameter(torch.empty(n,k,device = initial_codebook.device,\n",
    "        #                                                     dtype = initial_codebook.dtype\n",
    "        #                                                     ).uniform_(-1,1), requires_grad=True)\n",
    "\n",
    "\n",
    "    def binary_penalty(self, sparse, beta):\n",
    "        \"\"\"Penalty function for the sparse assignments\n",
    "\n",
    "        Args:\n",
    "            sparse (torch.Tensor): the sparse assignments of shape (n,n)\n",
    "                assumed to be past through \n",
    "            beta (float): a hyperparameter for the penalty, this penalty\n",
    "            is 1- |2*a_ij - 1|**(beta)\n",
    "\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: the penalty\n",
    "        \"\"\"\n",
    "        penalty = torch.sum(1 - torch.abs(2*sparse - 1)**beta)\n",
    "        return penalty\n",
    "    \n",
    "    def forward(self,beta,penalty_1_weight,penalty_2_weight):\n",
    "        \"\"\"Forward pass of the quantizer\n",
    "\n",
    "        Args:\n",
    "            beta (float): a hyperparameter for the first penalty, this penalty\n",
    "            is 1- |2*a_ij - 1|**(beta)\n",
    "\n",
    "            penalty_1_weight (float): the weight of the first penalty\n",
    "            penalty_2_weight (float): the weight of the second penalty\n",
    "            this penalty is the sum of the assignments, therefore enforceing \n",
    "            sparsity\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: the quantized codebook\n",
    "        \"\"\"\n",
    "        #get the assignments\n",
    "        sparse_assignments = torch.sigmoid(self.sparse)*1.2 - 0.1\n",
    "        sparse_assignments = torch.clip(sparse_assignments,0,1)\n",
    "\n",
    "        low_rank_product = self.A @ self.B\n",
    "\n",
    "        #add the sparse locations\n",
    "        W_hat = low_rank_product + sparse_assignments * (self.Weights - low_rank_product)\n",
    "\n",
    "        #get the difference between the quantized weights and the original weights\n",
    "        diff = W_hat - self.Weights\n",
    "\n",
    "        #get the loss\n",
    "        loss = torch.einsum('ik,kl,il->', diff, self.H, diff)\n",
    "\n",
    "        #get the penalty\n",
    "        binary_penalty = self.binary_penalty(sparse_assignments,beta)\n",
    "\n",
    "        #we have another penalty which enforce sparsity\n",
    "        sparse_penalty = torch.sum(sparse_assignments)\n",
    "        penalty = penalty_1_weight * binary_penalty + penalty_2_weight * sparse_penalty\n",
    "        return loss + penalty, loss, penalty, binary_penalty, sparse_penalty\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.load(\"/home/lliu/huffman/test/original_weights.pt\")\n",
    "weights = data['weights'].float()\n",
    "Hessian = data['H'].float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "quantizer = low_rank_and_sparse(128, Hessian, weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_penalty_scheduler(current_penalty, current_weight,multiple_up = 1.001, multiple_down = 0.999, target_sparsity = 0.005, n = Hessian.shape[0]):\n",
    "    \"\"\"A scheduler for the sparse penalty\"\"\"\n",
    "    current_sparsity = current_penalty / (n**2)\n",
    "    if current_sparsity > target_sparsity:\n",
    "        return current_weight * multiple_up\n",
    "    else:\n",
    "        return current_weight * multiple_down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(quantizer.parameters(), lr=1e-3)\n",
    "#add a learning rate scheduler\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1000, gamma=0.5)\n",
    "\n",
    "def cosine_annealing(i, T, scale):\n",
    "    return scale * (1 + np.cos(i/T * 3.1415))\n",
    "\n",
    "#train the quantizer\n",
    "beta_init = 10\n",
    "T = 1000\n",
    "\n",
    "penalty_1_weight = 0.01\n",
    "penalty_2_weight = 0.01\n",
    "\n",
    "losses = []\n",
    "errors = []\n",
    "penalties = []\n",
    "sparse_penalties = []\n",
    "binary_penalties = []\n",
    "\n",
    "for i in tqdm.tqdm(range(T)):\n",
    "    optimizer.zero_grad()\n",
    "    beta = (1-i/T) * (beta_init-1) + 1\n",
    "    loss,error,penalty, binary_penalty, sparse_penalty = quantizer(beta, penalty_1_weight, penalty_2_weight)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    losses.append(loss.item())\n",
    "    errors.append(error.item())\n",
    "    penalties.append(penalty.item())\n",
    "    \n",
    "    penalty_2_weight = sparse_penalty_scheduler(sparse_penalty.item(), penalty_2_weight)\n",
    "    sparse_penalties.append(sparse_penalty.item())\n",
    "    binary_penalties.append(binary_penalty.item())\n",
    "\n",
    "    if i % 100 == 99:\n",
    "        tqdm.tqdm.write(f\"Loss: {loss.item()}, Error: {error.item()/Hessian.shape[0]}, Penalty: {penalty.item()} Beta: {beta}, \\n\" +\\\n",
    "                        f\"sparse penalty: {sparse_penalty.item()/(Hessian.shape[0]**2)}, binary penalty: {binary_penalty.item()} penalty_2_weight: {penalty_2_weight}\")\n",
    "\n",
    "    # scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(losses)\n",
    "# plt.plot(errors)\n",
    "plt.plot(penalties)\n",
    "# plt.xscale('log')\n",
    "plt.yscale('log')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(errors[10000:])\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(errors)/weights.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the assignments\n",
    "#get the assignments\n",
    "sparse_assignments = torch.sigmoid(quantizer.sparse)\n",
    "\n",
    "low_rank_product = quantizer.A @ quantizer.B\n",
    "\n",
    "sparse_assignments = torch.round(sparse_assignments)\n",
    "#add the sparse locations\n",
    "W_hat = low_rank_product + sparse_assignments * (quantizer.Weights - low_rank_product)\n",
    "print(W_hat)\n",
    "#get the difference between the quantized weights and the original weights\n",
    "print(quantizer.Weights)\n",
    "diff = W_hat - quantizer.Weights\n",
    "print(diff)\n",
    "print(torch.mean(torch.abs(diff))/torch.mean(torch.abs(quantizer.Weights)))\n",
    "# print(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(sparse_assignments.detach().cpu().numpy(),interpolation='nearest', aspect='auto')\n",
    "#scale it to be a square image\n",
    "#add a colorbar\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sum(sparse_assignments)/(H.shape[0]**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(sparse_assignments.detach().cpu().numpy().flatten(),bins=100)\n",
    "# plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(quantizer.codebook.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(quantizer.Weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = weights.clone()\n",
    "H = Hessian.clone()\n",
    "dead = torch.diag(H) == 0\n",
    "\n",
    "columns = W.shape[1]\n",
    "rows = W.shape[0]\n",
    "H[dead, dead] = 1\n",
    "W[:, dead] = 0\n",
    "blocksize = 128\n",
    "prunen = 0\n",
    "prunem = 0\n",
    "percdamp = 0.01\n",
    "sparsity = 0.5\n",
    "dev = W.device\n",
    "\n",
    "Losses = torch.zeros(rows, device=dev)\n",
    "\n",
    "damp = percdamp * torch.mean(torch.diag(H))\n",
    "diag = torch.arange(columns, device=dev)\n",
    "H[diag, diag] += damp\n",
    "H = torch.linalg.cholesky(H)\n",
    "H = torch.cholesky_inverse(H)\n",
    "H = torch.linalg.cholesky(H, upper=True)\n",
    "Hinv = H\n",
    "\n",
    "mask = None\n",
    "\n",
    "for i1 in range(0, columns, blocksize):\n",
    "    i2 = min(i1 + blocksize, columns)\n",
    "    count = i2 - i1\n",
    "\n",
    "    W1 = W[:, i1:i2].clone()\n",
    "    Q1 = torch.zeros_like(W1)\n",
    "    Err1 = torch.zeros_like(W1)\n",
    "    Losses1 = torch.zeros_like(W1)\n",
    "    Hinv1 = Hinv[i1:i2, i1:i2]\n",
    "\n",
    "    if prunen == 0: \n",
    "        if mask is not None:\n",
    "            mask1 = mask[:, i1:i2]\n",
    "        else:\n",
    "            tmp = W1 ** 2 / (torch.diag(Hinv1).reshape((1, -1))) ** 2\n",
    "            thresh = torch.sort(tmp.flatten())[0][int(tmp.numel() * sparsity)]\n",
    "            mask1 = tmp <= thresh\n",
    "    else:\n",
    "        mask1 = torch.zeros_like(W1) == 1\n",
    "\n",
    "    for i in range(count):\n",
    "        w = W1[:, i]\n",
    "        d = Hinv1[i, i]\n",
    "\n",
    "        if prunen != 0 and i % prunem == 0:\n",
    "            tmp = W1[:, i:(i + prunem)] ** 2 / (torch.diag(Hinv1)[i:(i + prunem)].reshape((1, -1))) ** 2\n",
    "            mask1.scatter_(1, i + torch.topk(tmp, prunen, dim=1, largest=False)[1], True)\n",
    "\n",
    "        q = w.clone()\n",
    "        q[mask1[:, i]] = 0\n",
    "\n",
    "        # if hasattr(self, 'quantizer'):\n",
    "        #     q = quantize(\n",
    "        #         q.unsqueeze(1), self.quantizer.scale, self.quantizer.zero, self.quantizer.maxq\n",
    "        #     ).flatten()\n",
    "\n",
    "        Q1[:, i] = q\n",
    "        Losses1[:, i] = (w - q) ** 2 / d ** 2\n",
    "\n",
    "        err1 = (w - q) / d\n",
    "        W1[:, i:] -= err1.unsqueeze(1).matmul(Hinv1[i, i:].unsqueeze(0))\n",
    "        Err1[:, i] = err1\n",
    "\n",
    "    W[:, i1:i2] = Q1\n",
    "    Losses += torch.sum(Losses1, 1) / 2\n",
    "\n",
    "    W[:, i2:] -= Err1.matmul(Hinv[i1:i2, i2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = weights - W\n",
    "print(torch.mean(torch.abs(diff))/torch.mean(torch.abs(weights)))\n",
    "torch.einsum('ik,kl,il->', diff, Hessian, diff)/H.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
