/home/lliu/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.01s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.54it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.42it/s]
Model loaded. LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
4096
Loading tokenizer for meta-llama/Llama-2-7b-hf
Starting...
position_ids tensor([[   0,    1,    2,  ..., 4093, 4094, 4095]], device='cuda:5')
Ready.
layer original dtype torch.float16
0 self_attn.q_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
20.669 s H_error tensor(1.4851, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3512, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38153 MiB free out of 48676 MiB total
0 self_attn.k_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
reducing lr to  0.00020589113209464906
reducing lr to  0.00018530201888518417
reducing lr to  0.00016677181699666576
reducing lr to  0.0001500946352969992
reducing lr to  0.0001350851717672993
reducing lr to  0.00012157665459056936
reducing lr to  0.00010941898913151243
reducing lr to  9.847709021836118e-05
reducing lr to  8.862938119652506e-05
stopped after 49 iterations
18.505 s H_error tensor(1.1192, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3484, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38185 MiB free out of 48676 MiB total
0 self_attn.v_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
18.444 s H_error tensor(0.0689, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3811, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38217 MiB free out of 48676 MiB total
0 self_attn.o_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
18.347 s H_error tensor(0.0105, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3608, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38249 MiB free out of 48676 MiB total
0 mlp.gate_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
47.224 s H_error tensor(3.6410, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3265, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37937 MiB free out of 48676 MiB total
0 mlp.up_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
47.348 s H_error tensor(3.3259, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3260, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37797 MiB free out of 48676 MiB total
0 mlp.down_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
49.67 s H_error tensor(0.0101, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3393, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37857 MiB free out of 48676 MiB total
Fine tuning ...
LlamaDecoderLayer(
  (self_attn): LlamaAttention(
    (q_proj): QuantizedLinear()
    (k_proj): QuantizedLinear()
    (v_proj): QuantizedLinear()
    (o_proj): QuantizedLinear()
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (mlp): LlamaMLP(
    (gate_proj): QuantizedLinear()
    (up_proj): QuantizedLinear()
    (down_proj): QuantizedLinear()
    (act_fn): SiLUActivation()
  )
  (input_layernorm): LlamaRMSNorm()
  (post_attention_layernorm): LlamaRMSNorm()
)
attempting to cast to float32
attention_mask torch.Size([1, 1, 4096, 4096]) cuda:5 torch.float32
position_ids torch.Size([1, 4096]) cuda:5 torch.int64
Found 2 differentiable parameters
differentiable parameters: dict_keys(['input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 8192 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
----------
epoch=0
train loss=8.01e-06	
----------
epoch=1
train loss=6.72e-06	
----------
epoch=2
train loss=6.20e-06	
----------
epoch=3
train loss=6.02e-06	
----------
epoch=4
train loss=5.94e-06	
trying to convert back to original dtype
Fine tuned
layer original dtype torch.float16
1 self_attn.q_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 9 iterations
18.731 s H_error tensor(-7.1538, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3804, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38693 MiB free out of 48676 MiB total
1 self_attn.k_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 11 iterations
17.358 s H_error tensor(-7.9310, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3769, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38597 MiB free out of 48676 MiB total
1 self_attn.v_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
18.495 s H_error tensor(0.7373, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3906, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38565 MiB free out of 48676 MiB total
1 self_attn.o_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
18.477 s H_error tensor(0.0820, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3327, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38501 MiB free out of 48676 MiB total
1 mlp.gate_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
stopped after 49 iterations
47.39 s H_error tensor(14.9839, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3216, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38253 MiB free out of 48676 MiB total
1 mlp.up_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
47.369 s H_error tensor(12.8348, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3194, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38167 MiB free out of 48676 MiB total
1 mlp.down_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 3 iterations
43.266 s H_error tensor(-0.1345, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3302, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37995 MiB free out of 48676 MiB total
Fine tuning ...
LlamaDecoderLayer(
  (self_attn): LlamaAttention(
    (q_proj): QuantizedLinear()
    (k_proj): QuantizedLinear()
    (v_proj): QuantizedLinear()
    (o_proj): QuantizedLinear()
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (mlp): LlamaMLP(
    (gate_proj): QuantizedLinear()
    (up_proj): QuantizedLinear()
    (down_proj): QuantizedLinear()
    (act_fn): SiLUActivation()
  )
  (input_layernorm): LlamaRMSNorm()
  (post_attention_layernorm): LlamaRMSNorm()
)
attempting to cast to float32
attention_mask torch.Size([1, 1, 4096, 4096]) cuda:5 torch.float32
position_ids torch.Size([1, 4096]) cuda:5 torch.int64
Found 2 differentiable parameters
differentiable parameters: dict_keys(['input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 8192 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
----------
epoch=0
train loss=4.66e-03	
----------
epoch=1
train loss=2.55e-03	
----------
epoch=2
train loss=1.31e-03	
----------
epoch=3
train loss=7.82e-04	
----------
epoch=4
train loss=5.98e-04	
trying to convert back to original dtype
Fine tuned
layer original dtype torch.float16
2 self_attn.q_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 18 iterations
18.989 s H_error tensor(-6.1625, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3456, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
2 self_attn.k_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 23 iterations
17.767 s H_error tensor(-4.3554, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3474, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
2 self_attn.v_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
18.518 s H_error tensor(13.4544, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3302, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
2 self_attn.o_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
18.43 s H_error tensor(0.2151, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3248, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
2 mlp.gate_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
stopped after 49 iterations
47.644 s H_error tensor(32.6173, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3189, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38553 MiB free out of 48676 MiB total
2 mlp.up_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
stopped after 49 iterations
47.356 s H_error tensor(27.4650, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3152, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38381 MiB free out of 48676 MiB total
2 mlp.down_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
49.874 s H_error tensor(0.1717, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3179, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38209 MiB free out of 48676 MiB total
Fine tuning ...
LlamaDecoderLayer(
  (self_attn): LlamaAttention(
    (q_proj): QuantizedLinear()
    (k_proj): QuantizedLinear()
    (v_proj): QuantizedLinear()
    (o_proj): QuantizedLinear()
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (mlp): LlamaMLP(
    (gate_proj): QuantizedLinear()
    (up_proj): QuantizedLinear()
    (down_proj): QuantizedLinear()
    (act_fn): SiLUActivation()
  )
  (input_layernorm): LlamaRMSNorm()
  (post_attention_layernorm): LlamaRMSNorm()
)
attempting to cast to float32
attention_mask torch.Size([1, 1, 4096, 4096]) cuda:5 torch.float32
position_ids torch.Size([1, 4096]) cuda:5 torch.int64
Found 2 differentiable parameters
differentiable parameters: dict_keys(['input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 8192 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
----------
epoch=0
train loss=2.02e-04	
----------
epoch=1
train loss=1.96e-04	
----------
epoch=2
train loss=1.91e-04	
----------
epoch=3
train loss=1.88e-04	
----------
epoch=4
train loss=1.86e-04	
trying to convert back to original dtype
Fine tuned
layer original dtype torch.float16
3 self_attn.q_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
19.84 s H_error tensor(92.7298, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3635, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
3 self_attn.k_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
18.619 s H_error tensor(154.4878, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3482, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
3 self_attn.v_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
18.584 s H_error tensor(37.3105, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3264, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
3 self_attn.o_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
18.522 s H_error tensor(0.2174, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3217, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
3 mlp.gate_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
47.575 s H_error tensor(55.5041, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3160, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38553 MiB free out of 48676 MiB total
3 mlp.up_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
47.505 s H_error tensor(46.4543, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3145, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38381 MiB free out of 48676 MiB total
3 mlp.down_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
50.082 s H_error tensor(0.3193, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3184, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38209 MiB free out of 48676 MiB total
Fine tuning ...
LlamaDecoderLayer(
  (self_attn): LlamaAttention(
    (q_proj): QuantizedLinear()
    (k_proj): QuantizedLinear()
    (v_proj): QuantizedLinear()
    (o_proj): QuantizedLinear()
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (mlp): LlamaMLP(
    (gate_proj): QuantizedLinear()
    (up_proj): QuantizedLinear()
    (down_proj): QuantizedLinear()
    (act_fn): SiLUActivation()
  )
  (input_layernorm): LlamaRMSNorm()
  (post_attention_layernorm): LlamaRMSNorm()
)
attempting to cast to float32
attention_mask torch.Size([1, 1, 4096, 4096]) cuda:5 torch.float32
position_ids torch.Size([1, 4096]) cuda:5 torch.int64
Found 2 differentiable parameters
differentiable parameters: dict_keys(['input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 8192 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
----------
epoch=0
train loss=3.96e-04	
----------
epoch=1
train loss=3.87e-04	
----------
epoch=2
train loss=3.79e-04	
----------
epoch=3
train loss=3.73e-04	
----------
epoch=4
train loss=3.68e-04	
trying to convert back to original dtype
Fine tuned
layer original dtype torch.float16
4 self_attn.q_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
19.891 s H_error tensor(150.1101, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3330, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
4 self_attn.k_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 42 iterations
18.255 s H_error tensor(-4.9603, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3851, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
4 self_attn.v_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
18.513 s H_error tensor(36.1274, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3243, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
4 self_attn.o_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
18.507 s H_error tensor(0.4790, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3228, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
4 mlp.gate_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
stopped after 49 iterations
47.728 s H_error tensor(79.5524, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3174, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38553 MiB free out of 48676 MiB total
4 mlp.up_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
47.499 s H_error tensor(63.5417, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3160, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38381 MiB free out of 48676 MiB total
4 mlp.down_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
49.891 s H_error tensor(0.5978, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3206, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38209 MiB free out of 48676 MiB total
Fine tuning ...
LlamaDecoderLayer(
  (self_attn): LlamaAttention(
    (q_proj): QuantizedLinear()
    (k_proj): QuantizedLinear()
    (v_proj): QuantizedLinear()
    (o_proj): QuantizedLinear()
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (mlp): LlamaMLP(
    (gate_proj): QuantizedLinear()
    (up_proj): QuantizedLinear()
    (down_proj): QuantizedLinear()
    (act_fn): SiLUActivation()
  )
  (input_layernorm): LlamaRMSNorm()
  (post_attention_layernorm): LlamaRMSNorm()
)
attempting to cast to float32
attention_mask torch.Size([1, 1, 4096, 4096]) cuda:5 torch.float32
position_ids torch.Size([1, 4096]) cuda:5 torch.int64
Found 2 differentiable parameters
differentiable parameters: dict_keys(['input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 8192 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
----------
epoch=0
train loss=7.38e-04	
----------
epoch=1
train loss=7.18e-04	
----------
epoch=2
train loss=7.01e-04	
----------
epoch=3
train loss=6.87e-04	
----------
epoch=4
train loss=6.75e-04	
trying to convert back to original dtype
Fine tuned
layer original dtype torch.float16
5 self_attn.q_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
19.86 s H_error tensor(124.5889, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3518, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
5 self_attn.k_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 46 iterations
18.381 s H_error tensor(-6.1970, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3814, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
5 self_attn.v_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
18.453 s H_error tensor(42.8330, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3240, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
5 self_attn.o_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
18.467 s H_error tensor(0.8648, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3214, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
5 mlp.gate_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
reducing lr to  0.0009000000000000001
stopped after 49 iterations
47.47 s H_error tensor(101.5521, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3166, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38553 MiB free out of 48676 MiB total
5 mlp.up_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
47.422 s H_error tensor(79.7061, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3155, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38381 MiB free out of 48676 MiB total
5 mlp.down_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
50.005 s H_error tensor(0.8879, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3184, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38209 MiB free out of 48676 MiB total
Fine tuning ...
LlamaDecoderLayer(
  (self_attn): LlamaAttention(
    (q_proj): QuantizedLinear()
    (k_proj): QuantizedLinear()
    (v_proj): QuantizedLinear()
    (o_proj): QuantizedLinear()
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (mlp): LlamaMLP(
    (gate_proj): QuantizedLinear()
    (up_proj): QuantizedLinear()
    (down_proj): QuantizedLinear()
    (act_fn): SiLUActivation()
  )
  (input_layernorm): LlamaRMSNorm()
  (post_attention_layernorm): LlamaRMSNorm()
)
attempting to cast to float32
attention_mask torch.Size([1, 1, 4096, 4096]) cuda:5 torch.float32
position_ids torch.Size([1, 4096]) cuda:5 torch.int64
Found 2 differentiable parameters
differentiable parameters: dict_keys(['input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 8192 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
----------
epoch=0
train loss=1.06e-03	
----------
epoch=1
train loss=1.04e-03	
----------
epoch=2
train loss=1.02e-03	
----------
epoch=3
train loss=1.00e-03	
----------
epoch=4
train loss=9.91e-04	
trying to convert back to original dtype
Fine tuned
layer original dtype torch.float16
6 self_attn.q_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
19.929 s H_error tensor(224.1033, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3548, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
6 self_attn.k_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
18.496 s H_error tensor(145.0129, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3741, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
6 self_attn.v_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
18.495 s H_error tensor(62.5168, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3273, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
6 self_attn.o_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
18.44 s H_error tensor(1.2974, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3196, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
6 mlp.gate_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
reducing lr to  0.00020589113209464906
stopped after 49 iterations
47.488 s H_error tensor(125.9423, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3169, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38553 MiB free out of 48676 MiB total
6 mlp.up_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
47.411 s H_error tensor(95.2219, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3159, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38381 MiB free out of 48676 MiB total
6 mlp.down_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
50.01 s H_error tensor(1.3121, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3194, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38209 MiB free out of 48676 MiB total
Fine tuning ...
LlamaDecoderLayer(
  (self_attn): LlamaAttention(
    (q_proj): QuantizedLinear()
    (k_proj): QuantizedLinear()
    (v_proj): QuantizedLinear()
    (o_proj): QuantizedLinear()
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (mlp): LlamaMLP(
    (gate_proj): QuantizedLinear()
    (up_proj): QuantizedLinear()
    (down_proj): QuantizedLinear()
    (act_fn): SiLUActivation()
  )
  (input_layernorm): LlamaRMSNorm()
  (post_attention_layernorm): LlamaRMSNorm()
)
attempting to cast to float32
attention_mask torch.Size([1, 1, 4096, 4096]) cuda:5 torch.float32
position_ids torch.Size([1, 4096]) cuda:5 torch.int64
Found 2 differentiable parameters
differentiable parameters: dict_keys(['input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 8192 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
----------
epoch=0
train loss=1.52e-03	
----------
epoch=1
train loss=1.49e-03	
----------
epoch=2
train loss=1.47e-03	
----------
epoch=3
train loss=1.45e-03	
----------
epoch=4
train loss=1.44e-03	
trying to convert back to original dtype
Fine tuned
layer original dtype torch.float16
7 self_attn.q_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
stopped after 49 iterations
19.888 s H_error tensor(258.8577, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3316, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
7 self_attn.k_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
stopped after 49 iterations
18.442 s H_error tensor(273.0923, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3325, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
7 self_attn.v_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
18.476 s H_error tensor(68.2163, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3267, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
7 self_attn.o_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
18.456 s H_error tensor(1.9885, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3195, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
7 mlp.gate_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
reducing lr to  0.00020589113209464906
reducing lr to  0.00018530201888518417
reducing lr to  0.00016677181699666576
stopped after 49 iterations
47.352 s H_error tensor(142.6862, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3171, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38553 MiB free out of 48676 MiB total
7 mlp.up_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
47.39 s H_error tensor(109.0215, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3163, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38381 MiB free out of 48676 MiB total
7 mlp.down_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
49.93 s H_error tensor(1.7522, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3191, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38209 MiB free out of 48676 MiB total
Fine tuning ...
LlamaDecoderLayer(
  (self_attn): LlamaAttention(
    (q_proj): QuantizedLinear()
    (k_proj): QuantizedLinear()
    (v_proj): QuantizedLinear()
    (o_proj): QuantizedLinear()
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (mlp): LlamaMLP(
    (gate_proj): QuantizedLinear()
    (up_proj): QuantizedLinear()
    (down_proj): QuantizedLinear()
    (act_fn): SiLUActivation()
  )
  (input_layernorm): LlamaRMSNorm()
  (post_attention_layernorm): LlamaRMSNorm()
)
attempting to cast to float32
attention_mask torch.Size([1, 1, 4096, 4096]) cuda:5 torch.float32
position_ids torch.Size([1, 4096]) cuda:5 torch.int64
Found 2 differentiable parameters
differentiable parameters: dict_keys(['input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 8192 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
----------
epoch=0
train loss=1.90e-03	
----------
epoch=1
train loss=1.88e-03	
----------
epoch=2
train loss=1.86e-03	
----------
epoch=3
train loss=1.85e-03	
----------
epoch=4
train loss=1.84e-03	
trying to convert back to original dtype
Fine tuned
layer original dtype torch.float16
8 self_attn.q_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
19.892 s H_error tensor(248.7095, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3360, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
8 self_attn.k_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
18.498 s H_error tensor(168.9848, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3730, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
8 self_attn.v_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
18.447 s H_error tensor(70.5406, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3255, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
8 self_attn.o_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
18.401 s H_error tensor(3.4680, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3190, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
8 mlp.gate_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
reducing lr to  0.00020589113209464906
reducing lr to  0.00018530201888518417
stopped after 49 iterations
47.491 s H_error tensor(148.2823, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3171, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38553 MiB free out of 48676 MiB total
8 mlp.up_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
47.669 s H_error tensor(119.9116, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3169, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38381 MiB free out of 48676 MiB total
8 mlp.down_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
49.958 s H_error tensor(2.2084, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3184, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38209 MiB free out of 48676 MiB total
Fine tuning ...
LlamaDecoderLayer(
  (self_attn): LlamaAttention(
    (q_proj): QuantizedLinear()
    (k_proj): QuantizedLinear()
    (v_proj): QuantizedLinear()
    (o_proj): QuantizedLinear()
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (mlp): LlamaMLP(
    (gate_proj): QuantizedLinear()
    (up_proj): QuantizedLinear()
    (down_proj): QuantizedLinear()
    (act_fn): SiLUActivation()
  )
  (input_layernorm): LlamaRMSNorm()
  (post_attention_layernorm): LlamaRMSNorm()
)
attempting to cast to float32
attention_mask torch.Size([1, 1, 4096, 4096]) cuda:5 torch.float32
position_ids torch.Size([1, 4096]) cuda:5 torch.int64
Found 2 differentiable parameters
differentiable parameters: dict_keys(['input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 8192 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
----------
epoch=0
train loss=2.43e-03	
----------
epoch=1
train loss=2.41e-03	
----------
epoch=2
train loss=2.40e-03	
----------
epoch=3
train loss=2.39e-03	
----------
epoch=4
train loss=2.38e-03	
trying to convert back to original dtype
Fine tuned
layer original dtype torch.float16
9 self_attn.q_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
19.881 s H_error tensor(263.3060, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3300, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
9 self_attn.k_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
stopped after 49 iterations
18.542 s H_error tensor(294.3362, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3302, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
9 self_attn.v_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
stopped after 49 iterations
18.514 s H_error tensor(76.4582, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3238, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
9 self_attn.o_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
18.438 s H_error tensor(4.9593, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3180, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
9 mlp.gate_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
stopped after 49 iterations
47.544 s H_error tensor(154.5228, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3171, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38553 MiB free out of 48676 MiB total
9 mlp.up_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
stopped after 49 iterations
47.44 s H_error tensor(129.3007, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3170, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38381 MiB free out of 48676 MiB total
9 mlp.down_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
49.965 s H_error tensor(2.6903, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3183, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38209 MiB free out of 48676 MiB total
Fine tuning ...
LlamaDecoderLayer(
  (self_attn): LlamaAttention(
    (q_proj): QuantizedLinear()
    (k_proj): QuantizedLinear()
    (v_proj): QuantizedLinear()
    (o_proj): QuantizedLinear()
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (mlp): LlamaMLP(
    (gate_proj): QuantizedLinear()
    (up_proj): QuantizedLinear()
    (down_proj): QuantizedLinear()
    (act_fn): SiLUActivation()
  )
  (input_layernorm): LlamaRMSNorm()
  (post_attention_layernorm): LlamaRMSNorm()
)
attempting to cast to float32
attention_mask torch.Size([1, 1, 4096, 4096]) cuda:5 torch.float32
position_ids torch.Size([1, 4096]) cuda:5 torch.int64
Found 2 differentiable parameters
differentiable parameters: dict_keys(['input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 8192 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
----------
epoch=0
train loss=3.03e-03	
----------
epoch=1
train loss=3.00e-03	
----------
epoch=2
train loss=2.97e-03	
----------
epoch=3
train loss=2.95e-03	
----------
epoch=4
train loss=2.93e-03	
trying to convert back to original dtype
Fine tuned
layer original dtype torch.float16
10 self_attn.q_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
19.911 s H_error tensor(256.5481, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3417, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
10 self_attn.k_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
18.507 s H_error tensor(280.7764, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3379, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
10 self_attn.v_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
stopped after 49 iterations
18.52 s H_error tensor(75.9576, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3219, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
10 self_attn.o_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
18.429 s H_error tensor(7.1962, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3167, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
10 mlp.gate_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
reducing lr to  0.00020589113209464906
reducing lr to  0.00018530201888518417
reducing lr to  0.00016677181699666576
reducing lr to  0.0001500946352969992
stopped after 49 iterations
47.61 s H_error tensor(162.9332, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3174, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38553 MiB free out of 48676 MiB total
10 mlp.up_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
47.625 s H_error tensor(140.2461, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3171, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38381 MiB free out of 48676 MiB total
10 mlp.down_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
49.953 s H_error tensor(3.1731, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3188, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38209 MiB free out of 48676 MiB total
Fine tuning ...
LlamaDecoderLayer(
  (self_attn): LlamaAttention(
    (q_proj): QuantizedLinear()
    (k_proj): QuantizedLinear()
    (v_proj): QuantizedLinear()
    (o_proj): QuantizedLinear()
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (mlp): LlamaMLP(
    (gate_proj): QuantizedLinear()
    (up_proj): QuantizedLinear()
    (down_proj): QuantizedLinear()
    (act_fn): SiLUActivation()
  )
  (input_layernorm): LlamaRMSNorm()
  (post_attention_layernorm): LlamaRMSNorm()
)
attempting to cast to float32
attention_mask torch.Size([1, 1, 4096, 4096]) cuda:5 torch.float32
position_ids torch.Size([1, 4096]) cuda:5 torch.int64
Found 2 differentiable parameters
differentiable parameters: dict_keys(['input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 8192 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
----------
epoch=0
train loss=3.66e-03	
----------
epoch=1
train loss=3.63e-03	
----------
epoch=2
train loss=3.61e-03	
----------
epoch=3
train loss=3.60e-03	
----------
epoch=4
train loss=3.59e-03	
trying to convert back to original dtype
Fine tuned
layer original dtype torch.float16
11 self_attn.q_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
19.844 s H_error tensor(258.5831, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3612, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
11 self_attn.k_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
18.488 s H_error tensor(225.1849, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3646, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
11 self_attn.v_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
stopped after 49 iterations
18.424 s H_error tensor(102.3936, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3223, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
11 self_attn.o_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
18.448 s H_error tensor(7.9485, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3156, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
11 mlp.gate_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
stopped after 49 iterations
47.468 s H_error tensor(175.7542, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3167, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38553 MiB free out of 48676 MiB total
11 mlp.up_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
47.415 s H_error tensor(155.2932, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3167, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38381 MiB free out of 48676 MiB total
11 mlp.down_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
49.876 s H_error tensor(3.5199, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3178, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38209 MiB free out of 48676 MiB total
Fine tuning ...
LlamaDecoderLayer(
  (self_attn): LlamaAttention(
    (q_proj): QuantizedLinear()
    (k_proj): QuantizedLinear()
    (v_proj): QuantizedLinear()
    (o_proj): QuantizedLinear()
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (mlp): LlamaMLP(
    (gate_proj): QuantizedLinear()
    (up_proj): QuantizedLinear()
    (down_proj): QuantizedLinear()
    (act_fn): SiLUActivation()
  )
  (input_layernorm): LlamaRMSNorm()
  (post_attention_layernorm): LlamaRMSNorm()
)
attempting to cast to float32
attention_mask torch.Size([1, 1, 4096, 4096]) cuda:5 torch.float32
position_ids torch.Size([1, 4096]) cuda:5 torch.int64
Found 2 differentiable parameters
differentiable parameters: dict_keys(['input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 8192 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
----------
epoch=0
train loss=4.31e-03	
----------
epoch=1
train loss=4.28e-03	
----------
epoch=2
train loss=4.25e-03	
----------
epoch=3
train loss=4.23e-03	
----------
epoch=4
train loss=4.22e-03	
trying to convert back to original dtype
Fine tuned
layer original dtype torch.float16
12 self_attn.q_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
19.855 s H_error tensor(278.6316, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3533, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
12 self_attn.k_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
18.484 s H_error tensor(311.2230, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3500, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
12 self_attn.v_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
18.429 s H_error tensor(100.1480, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3212, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
12 self_attn.o_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
18.498 s H_error tensor(8.6379, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3147, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
12 mlp.gate_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
reducing lr to  0.00020589113209464906
reducing lr to  0.00018530201888518417
stopped after 49 iterations
47.343 s H_error tensor(184.2418, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3161, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38553 MiB free out of 48676 MiB total
12 mlp.up_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
47.371 s H_error tensor(168.1084, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3156, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38381 MiB free out of 48676 MiB total
12 mlp.down_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
stopped after 49 iterations
49.819 s H_error tensor(3.8151, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3174, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38209 MiB free out of 48676 MiB total
Fine tuning ...
LlamaDecoderLayer(
  (self_attn): LlamaAttention(
    (q_proj): QuantizedLinear()
    (k_proj): QuantizedLinear()
    (v_proj): QuantizedLinear()
    (o_proj): QuantizedLinear()
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (mlp): LlamaMLP(
    (gate_proj): QuantizedLinear()
    (up_proj): QuantizedLinear()
    (down_proj): QuantizedLinear()
    (act_fn): SiLUActivation()
  )
  (input_layernorm): LlamaRMSNorm()
  (post_attention_layernorm): LlamaRMSNorm()
)
attempting to cast to float32
attention_mask torch.Size([1, 1, 4096, 4096]) cuda:5 torch.float32
position_ids torch.Size([1, 4096]) cuda:5 torch.int64
Found 2 differentiable parameters
differentiable parameters: dict_keys(['input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 8192 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
----------
epoch=0
train loss=4.84e-03	
----------
epoch=1
train loss=4.79e-03	
----------
epoch=2
train loss=4.75e-03	
----------
epoch=3
train loss=4.72e-03	
----------
epoch=4
train loss=4.69e-03	
trying to convert back to original dtype
Fine tuned
layer original dtype torch.float16
13 self_attn.q_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
19.869 s H_error tensor(254.4139, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3702, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
13 self_attn.k_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
18.437 s H_error tensor(146.7396, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3927, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
13 self_attn.v_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
reducing lr to  0.00020589113209464906
reducing lr to  0.00018530201888518417
reducing lr to  0.00016677181699666576
reducing lr to  0.0001500946352969992
stopped after 49 iterations
18.424 s H_error tensor(111.9676, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3210, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
13 self_attn.o_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
18.465 s H_error tensor(8.8143, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3176, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
13 mlp.gate_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
stopped after 49 iterations
47.345 s H_error tensor(193.9770, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3164, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38553 MiB free out of 48676 MiB total
13 mlp.up_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
stopped after 49 iterations
47.406 s H_error tensor(181.1755, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3156, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38381 MiB free out of 48676 MiB total
13 mlp.down_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
49.996 s H_error tensor(4.4421, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3187, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38209 MiB free out of 48676 MiB total
Fine tuning ...
LlamaDecoderLayer(
  (self_attn): LlamaAttention(
    (q_proj): QuantizedLinear()
    (k_proj): QuantizedLinear()
    (v_proj): QuantizedLinear()
    (o_proj): QuantizedLinear()
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (mlp): LlamaMLP(
    (gate_proj): QuantizedLinear()
    (up_proj): QuantizedLinear()
    (down_proj): QuantizedLinear()
    (act_fn): SiLUActivation()
  )
  (input_layernorm): LlamaRMSNorm()
  (post_attention_layernorm): LlamaRMSNorm()
)
attempting to cast to float32
attention_mask torch.Size([1, 1, 4096, 4096]) cuda:5 torch.float32
position_ids torch.Size([1, 4096]) cuda:5 torch.int64
Found 2 differentiable parameters
differentiable parameters: dict_keys(['input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 8192 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
----------
epoch=0
train loss=5.34e-03	
----------
epoch=1
train loss=5.28e-03	
----------
epoch=2
train loss=5.23e-03	
----------
epoch=3
train loss=5.19e-03	
----------
epoch=4
train loss=5.16e-03	
trying to convert back to original dtype
Fine tuned
layer original dtype torch.float16
14 self_attn.q_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
reducing lr to  0.0009000000000000001
stopped after 49 iterations
19.833 s H_error tensor(327.3241, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3271, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
14 self_attn.k_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
18.507 s H_error tensor(303.5071, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3599, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
14 self_attn.v_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
18.479 s H_error tensor(112.3383, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3208, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
14 self_attn.o_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
18.506 s H_error tensor(11.5237, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3146, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
14 mlp.gate_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
stopped after 49 iterations
47.579 s H_error tensor(209.5502, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3164, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38553 MiB free out of 48676 MiB total
14 mlp.up_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
47.333 s H_error tensor(196.9974, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3152, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38381 MiB free out of 48676 MiB total
14 mlp.down_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
reducing lr to  0.0009000000000000001
stopped after 49 iterations
49.883 s H_error tensor(5.1715, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3182, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38209 MiB free out of 48676 MiB total
Fine tuning ...
LlamaDecoderLayer(
  (self_attn): LlamaAttention(
    (q_proj): QuantizedLinear()
    (k_proj): QuantizedLinear()
    (v_proj): QuantizedLinear()
    (o_proj): QuantizedLinear()
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (mlp): LlamaMLP(
    (gate_proj): QuantizedLinear()
    (up_proj): QuantizedLinear()
    (down_proj): QuantizedLinear()
    (act_fn): SiLUActivation()
  )
  (input_layernorm): LlamaRMSNorm()
  (post_attention_layernorm): LlamaRMSNorm()
)
attempting to cast to float32
attention_mask torch.Size([1, 1, 4096, 4096]) cuda:5 torch.float32
position_ids torch.Size([1, 4096]) cuda:5 torch.int64
Found 2 differentiable parameters
differentiable parameters: dict_keys(['input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 8192 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
----------
epoch=0
train loss=6.27e-03	
----------
epoch=1
train loss=6.23e-03	
----------
epoch=2
train loss=6.19e-03	
----------
epoch=3
train loss=6.16e-03	
----------
epoch=4
train loss=6.14e-03	
trying to convert back to original dtype
Fine tuned
layer original dtype torch.float16
15 self_attn.q_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
19.841 s H_error tensor(297.2228, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3387, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
15 self_attn.k_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
18.51 s H_error tensor(323.8176, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3504, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
15 self_attn.v_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
reducing lr to  0.00020589113209464906
reducing lr to  0.00018530201888518417
reducing lr to  0.00016677181699666576
reducing lr to  0.0001500946352969992
reducing lr to  0.0001350851717672993
stopped after 49 iterations
18.449 s H_error tensor(115.7148, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3212, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
15 self_attn.o_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
18.446 s H_error tensor(11.1007, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3147, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
15 mlp.gate_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
stopped after 49 iterations
47.472 s H_error tensor(228.6530, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3168, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38553 MiB free out of 48676 MiB total
15 mlp.up_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
47.277 s H_error tensor(215.2531, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3163, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38381 MiB free out of 48676 MiB total
15 mlp.down_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
49.912 s H_error tensor(6.3189, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3196, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38209 MiB free out of 48676 MiB total
Fine tuning ...
LlamaDecoderLayer(
  (self_attn): LlamaAttention(
    (q_proj): QuantizedLinear()
    (k_proj): QuantizedLinear()
    (v_proj): QuantizedLinear()
    (o_proj): QuantizedLinear()
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (mlp): LlamaMLP(
    (gate_proj): QuantizedLinear()
    (up_proj): QuantizedLinear()
    (down_proj): QuantizedLinear()
    (act_fn): SiLUActivation()
  )
  (input_layernorm): LlamaRMSNorm()
  (post_attention_layernorm): LlamaRMSNorm()
)
attempting to cast to float32
attention_mask torch.Size([1, 1, 4096, 4096]) cuda:5 torch.float32
position_ids torch.Size([1, 4096]) cuda:5 torch.int64
Found 2 differentiable parameters
differentiable parameters: dict_keys(['input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 8192 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
----------
epoch=0
train loss=7.42e-03	
----------
epoch=1
train loss=7.35e-03	
----------
epoch=2
train loss=7.28e-03	
----------
epoch=3
train loss=7.22e-03	
----------
epoch=4
train loss=7.16e-03	
trying to convert back to original dtype
Fine tuned
layer original dtype torch.float16
16 self_attn.q_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
19.841 s H_error tensor(312.5777, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3266, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
16 self_attn.k_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
18.437 s H_error tensor(251.1157, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3692, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
16 self_attn.v_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
stopped after 49 iterations
18.424 s H_error tensor(131.1548, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3189, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
16 self_attn.o_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
18.415 s H_error tensor(12.6869, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3146, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
16 mlp.gate_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
stopped after 49 iterations
47.521 s H_error tensor(264.4098, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3164, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38553 MiB free out of 48676 MiB total
16 mlp.up_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
47.468 s H_error tensor(245.8314, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3150, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38381 MiB free out of 48676 MiB total
16 mlp.down_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
49.891 s H_error tensor(8.3299, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3210, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38209 MiB free out of 48676 MiB total
Fine tuning ...
LlamaDecoderLayer(
  (self_attn): LlamaAttention(
    (q_proj): QuantizedLinear()
    (k_proj): QuantizedLinear()
    (v_proj): QuantizedLinear()
    (o_proj): QuantizedLinear()
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (mlp): LlamaMLP(
    (gate_proj): QuantizedLinear()
    (up_proj): QuantizedLinear()
    (down_proj): QuantizedLinear()
    (act_fn): SiLUActivation()
  )
  (input_layernorm): LlamaRMSNorm()
  (post_attention_layernorm): LlamaRMSNorm()
)
attempting to cast to float32
attention_mask torch.Size([1, 1, 4096, 4096]) cuda:5 torch.float32
position_ids torch.Size([1, 4096]) cuda:5 torch.int64
Found 2 differentiable parameters
differentiable parameters: dict_keys(['input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 8192 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
----------
epoch=0
train loss=9.39e-03	
----------
epoch=1
train loss=9.32e-03	
----------
epoch=2
train loss=9.25e-03	
----------
epoch=3
train loss=9.19e-03	
----------
epoch=4
train loss=9.14e-03	
trying to convert back to original dtype
Fine tuned
layer original dtype torch.float16
17 self_attn.q_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
stopped after 49 iterations
19.838 s H_error tensor(331.9756, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3256, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
17 self_attn.k_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
reducing lr to  0.0009000000000000001
stopped after 49 iterations
18.468 s H_error tensor(337.6770, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3426, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
17 self_attn.v_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
18.465 s H_error tensor(139.7774, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3191, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
17 self_attn.o_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
18.468 s H_error tensor(10.0670, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3149, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
17 mlp.gate_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
stopped after 49 iterations
47.33 s H_error tensor(302.7211, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3161, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38553 MiB free out of 48676 MiB total
17 mlp.up_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
stopped after 49 iterations
47.561 s H_error tensor(272.7090, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3136, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38381 MiB free out of 48676 MiB total
17 mlp.down_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
stopped after 49 iterations
50.079 s H_error tensor(9.3068, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3183, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38209 MiB free out of 48676 MiB total
Fine tuning ...
LlamaDecoderLayer(
  (self_attn): LlamaAttention(
    (q_proj): QuantizedLinear()
    (k_proj): QuantizedLinear()
    (v_proj): QuantizedLinear()
    (o_proj): QuantizedLinear()
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (mlp): LlamaMLP(
    (gate_proj): QuantizedLinear()
    (up_proj): QuantizedLinear()
    (down_proj): QuantizedLinear()
    (act_fn): SiLUActivation()
  )
  (input_layernorm): LlamaRMSNorm()
  (post_attention_layernorm): LlamaRMSNorm()
)
attempting to cast to float32
attention_mask torch.Size([1, 1, 4096, 4096]) cuda:5 torch.float32
position_ids torch.Size([1, 4096]) cuda:5 torch.int64
Found 2 differentiable parameters
differentiable parameters: dict_keys(['input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 8192 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
----------
epoch=0
train loss=9.91e-03	
----------
epoch=1
train loss=9.80e-03	
----------
epoch=2
train loss=9.70e-03	
----------
epoch=3
train loss=9.61e-03	
----------
epoch=4
train loss=9.53e-03	
trying to convert back to original dtype
Fine tuned
layer original dtype torch.float16
18 self_attn.q_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
19.841 s H_error tensor(299.9431, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3712, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
18 self_attn.k_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
18.431 s H_error tensor(58.4429, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3952, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
18 self_attn.v_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
stopped after 49 iterations
18.463 s H_error tensor(169.7702, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3169, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
18 self_attn.o_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
stopped after 49 iterations
18.373 s H_error tensor(8.6750, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3162, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
18 mlp.gate_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
stopped after 49 iterations
47.637 s H_error tensor(344.8976, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3158, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38553 MiB free out of 48676 MiB total
18 mlp.up_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
reducing lr to  0.00020589113209464906
stopped after 49 iterations
47.544 s H_error tensor(302.3835, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3132, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38381 MiB free out of 48676 MiB total
18 mlp.down_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
stopped after 49 iterations
49.918 s H_error tensor(10.8768, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3198, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38209 MiB free out of 48676 MiB total
Fine tuning ...
LlamaDecoderLayer(
  (self_attn): LlamaAttention(
    (q_proj): QuantizedLinear()
    (k_proj): QuantizedLinear()
    (v_proj): QuantizedLinear()
    (o_proj): QuantizedLinear()
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (mlp): LlamaMLP(
    (gate_proj): QuantizedLinear()
    (up_proj): QuantizedLinear()
    (down_proj): QuantizedLinear()
    (act_fn): SiLUActivation()
  )
  (input_layernorm): LlamaRMSNorm()
  (post_attention_layernorm): LlamaRMSNorm()
)
attempting to cast to float32
attention_mask torch.Size([1, 1, 4096, 4096]) cuda:5 torch.float32
position_ids torch.Size([1, 4096]) cuda:5 torch.int64
Found 2 differentiable parameters
differentiable parameters: dict_keys(['input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 8192 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
----------
epoch=0
train loss=1.10e-02	
----------
epoch=1
train loss=1.09e-02	
----------
epoch=2
train loss=1.08e-02	
----------
epoch=3
train loss=1.07e-02	
----------
epoch=4
train loss=1.06e-02	
trying to convert back to original dtype
Fine tuned
layer original dtype torch.float16
19 self_attn.q_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
19.819 s H_error tensor(348.4545, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3252, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
19 self_attn.k_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
stopped after 49 iterations
18.484 s H_error tensor(351.0520, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3346, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
19 self_attn.v_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
18.336 s H_error tensor(174.3329, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3191, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
19 self_attn.o_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
18.334 s H_error tensor(9.4714, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3164, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
19 mlp.gate_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
reducing lr to  0.00020589113209464906
stopped after 49 iterations
47.417 s H_error tensor(369.9240, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3148, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38553 MiB free out of 48676 MiB total
19 mlp.up_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
reducing lr to  0.0009000000000000001
stopped after 49 iterations
47.447 s H_error tensor(324.9553, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3132, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38381 MiB free out of 48676 MiB total
19 mlp.down_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
49.916 s H_error tensor(11.7558, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3191, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38209 MiB free out of 48676 MiB total
Fine tuning ...
LlamaDecoderLayer(
  (self_attn): LlamaAttention(
    (q_proj): QuantizedLinear()
    (k_proj): QuantizedLinear()
    (v_proj): QuantizedLinear()
    (o_proj): QuantizedLinear()
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (mlp): LlamaMLP(
    (gate_proj): QuantizedLinear()
    (up_proj): QuantizedLinear()
    (down_proj): QuantizedLinear()
    (act_fn): SiLUActivation()
  )
  (input_layernorm): LlamaRMSNorm()
  (post_attention_layernorm): LlamaRMSNorm()
)
attempting to cast to float32
attention_mask torch.Size([1, 1, 4096, 4096]) cuda:5 torch.float32
position_ids torch.Size([1, 4096]) cuda:5 torch.int64
Found 2 differentiable parameters
differentiable parameters: dict_keys(['input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 8192 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
----------
epoch=0
train loss=1.21e-02	
----------
epoch=1
train loss=1.20e-02	
----------
epoch=2
train loss=1.19e-02	
----------
epoch=3
train loss=1.18e-02	
----------
epoch=4
train loss=1.17e-02	
trying to convert back to original dtype
Fine tuned
layer original dtype torch.float16
20 self_attn.q_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
stopped after 49 iterations
19.86 s H_error tensor(358.6292, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3255, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
20 self_attn.k_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
18.463 s H_error tensor(169.7808, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3905, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
20 self_attn.v_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
18.51 s H_error tensor(179.1410, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3167, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
20 self_attn.o_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
18.446 s H_error tensor(12.1326, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3229, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
20 mlp.gate_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
reducing lr to  0.00020589113209464906
reducing lr to  0.00018530201888518417
stopped after 49 iterations
47.554 s H_error tensor(400.5946, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3150, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38553 MiB free out of 48676 MiB total
20 mlp.up_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
stopped after 49 iterations
47.458 s H_error tensor(347.5341, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3132, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38381 MiB free out of 48676 MiB total
20 mlp.down_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
stopped after 49 iterations
49.991 s H_error tensor(14.7305, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3209, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38209 MiB free out of 48676 MiB total
Fine tuning ...
LlamaDecoderLayer(
  (self_attn): LlamaAttention(
    (q_proj): QuantizedLinear()
    (k_proj): QuantizedLinear()
    (v_proj): QuantizedLinear()
    (o_proj): QuantizedLinear()
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (mlp): LlamaMLP(
    (gate_proj): QuantizedLinear()
    (up_proj): QuantizedLinear()
    (down_proj): QuantizedLinear()
    (act_fn): SiLUActivation()
  )
  (input_layernorm): LlamaRMSNorm()
  (post_attention_layernorm): LlamaRMSNorm()
)
attempting to cast to float32
attention_mask torch.Size([1, 1, 4096, 4096]) cuda:5 torch.float32
position_ids torch.Size([1, 4096]) cuda:5 torch.int64
Found 2 differentiable parameters
differentiable parameters: dict_keys(['input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 8192 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
----------
epoch=0
train loss=1.55e-02	
----------
epoch=1
train loss=1.54e-02	
----------
epoch=2
train loss=1.53e-02	
----------
epoch=3
train loss=1.52e-02	
----------
epoch=4
train loss=1.51e-02	
trying to convert back to original dtype
Fine tuned
layer original dtype torch.float16
21 self_attn.q_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
19.833 s H_error tensor(290.2156, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3737, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
21 self_attn.k_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 48 iterations
18.46 s H_error tensor(-1.0272, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3917, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
21 self_attn.v_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
18.468 s H_error tensor(215.4886, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3159, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
21 self_attn.o_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
18.514 s H_error tensor(9.7035, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3196, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
21 mlp.gate_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
stopped after 49 iterations
47.517 s H_error tensor(432.9130, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3148, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38553 MiB free out of 48676 MiB total
21 mlp.up_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
stopped after 49 iterations
47.327 s H_error tensor(370.4259, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3127, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38381 MiB free out of 48676 MiB total
21 mlp.down_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
stopped after 49 iterations
50.401 s H_error tensor(15.4782, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3187, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38209 MiB free out of 48676 MiB total
Fine tuning ...
LlamaDecoderLayer(
  (self_attn): LlamaAttention(
    (q_proj): QuantizedLinear()
    (k_proj): QuantizedLinear()
    (v_proj): QuantizedLinear()
    (o_proj): QuantizedLinear()
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (mlp): LlamaMLP(
    (gate_proj): QuantizedLinear()
    (up_proj): QuantizedLinear()
    (down_proj): QuantizedLinear()
    (act_fn): SiLUActivation()
  )
  (input_layernorm): LlamaRMSNorm()
  (post_attention_layernorm): LlamaRMSNorm()
)
attempting to cast to float32
attention_mask torch.Size([1, 1, 4096, 4096]) cuda:5 torch.float32
position_ids torch.Size([1, 4096]) cuda:5 torch.int64
Found 2 differentiable parameters
differentiable parameters: dict_keys(['input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 8192 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
----------
epoch=0
train loss=1.56e-02	
----------
epoch=1
train loss=1.55e-02	
----------
epoch=2
train loss=1.54e-02	
----------
epoch=3
train loss=1.53e-02	
----------
epoch=4
train loss=1.52e-02	
trying to convert back to original dtype
Fine tuned
layer original dtype torch.float16
22 self_attn.q_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
19.818 s H_error tensor(326.9400, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3615, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
22 self_attn.k_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
18.46 s H_error tensor(60.3763, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3925, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
22 self_attn.v_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
18.443 s H_error tensor(223.7719, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3169, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
22 self_attn.o_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
18.381 s H_error tensor(36.2800, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3533, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
22 mlp.gate_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
reducing lr to  0.00020589113209464906
stopped after 49 iterations
47.518 s H_error tensor(455.7228, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3146, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38553 MiB free out of 48676 MiB total
22 mlp.up_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
reducing lr to  0.00020589113209464906
reducing lr to  0.00018530201888518417
stopped after 49 iterations
47.626 s H_error tensor(384.5070, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3126, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38381 MiB free out of 48676 MiB total
22 mlp.down_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
stopped after 49 iterations
50.022 s H_error tensor(16.8311, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3176, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38209 MiB free out of 48676 MiB total
Fine tuning ...
LlamaDecoderLayer(
  (self_attn): LlamaAttention(
    (q_proj): QuantizedLinear()
    (k_proj): QuantizedLinear()
    (v_proj): QuantizedLinear()
    (o_proj): QuantizedLinear()
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (mlp): LlamaMLP(
    (gate_proj): QuantizedLinear()
    (up_proj): QuantizedLinear()
    (down_proj): QuantizedLinear()
    (act_fn): SiLUActivation()
  )
  (input_layernorm): LlamaRMSNorm()
  (post_attention_layernorm): LlamaRMSNorm()
)
attempting to cast to float32
attention_mask torch.Size([1, 1, 4096, 4096]) cuda:5 torch.float32
position_ids torch.Size([1, 4096]) cuda:5 torch.int64
Found 2 differentiable parameters
differentiable parameters: dict_keys(['input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 8192 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
----------
epoch=0
train loss=2.22e-02	
----------
epoch=1
train loss=2.20e-02	
----------
epoch=2
train loss=2.19e-02	
----------
epoch=3
train loss=2.17e-02	
----------
epoch=4
train loss=2.15e-02	
trying to convert back to original dtype
Fine tuned
layer original dtype torch.float16
23 self_attn.q_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
19.866 s H_error tensor(354.7728, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3737, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
23 self_attn.k_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
18.5 s H_error tensor(30.3970, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.4016, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
23 self_attn.v_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
18.367 s H_error tensor(276.5641, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3155, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
23 self_attn.o_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
18.344 s H_error tensor(13.7710, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3151, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
23 mlp.gate_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
reducing lr to  0.00020589113209464906
reducing lr to  0.00018530201888518417
reducing lr to  0.00016677181699666576
stopped after 49 iterations
47.385 s H_error tensor(500.2028, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3139, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38553 MiB free out of 48676 MiB total
23 mlp.up_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
reducing lr to  0.00020589113209464906
reducing lr to  0.00018530201888518417
reducing lr to  0.00016677181699666576
stopped after 49 iterations
47.461 s H_error tensor(424.7379, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3121, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38381 MiB free out of 48676 MiB total
23 mlp.down_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
reducing lr to  0.00020589113209464906
stopped after 49 iterations
50.058 s H_error tensor(19.0085, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3167, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38209 MiB free out of 48676 MiB total
Fine tuning ...
LlamaDecoderLayer(
  (self_attn): LlamaAttention(
    (q_proj): QuantizedLinear()
    (k_proj): QuantizedLinear()
    (v_proj): QuantizedLinear()
    (o_proj): QuantizedLinear()
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (mlp): LlamaMLP(
    (gate_proj): QuantizedLinear()
    (up_proj): QuantizedLinear()
    (down_proj): QuantizedLinear()
    (act_fn): SiLUActivation()
  )
  (input_layernorm): LlamaRMSNorm()
  (post_attention_layernorm): LlamaRMSNorm()
)
attempting to cast to float32
attention_mask torch.Size([1, 1, 4096, 4096]) cuda:5 torch.float32
position_ids torch.Size([1, 4096]) cuda:5 torch.int64
Found 2 differentiable parameters
differentiable parameters: dict_keys(['input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 8192 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
----------
epoch=0
train loss=2.02e-02	
----------
epoch=1
train loss=2.00e-02	
----------
epoch=2
train loss=1.99e-02	
----------
epoch=3
train loss=1.98e-02	
----------
epoch=4
train loss=1.98e-02	
trying to convert back to original dtype
Fine tuned
layer original dtype torch.float16
24 self_attn.q_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
stopped after 49 iterations
19.793 s H_error tensor(414.8334, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3231, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
24 self_attn.k_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
stopped after 49 iterations
18.599 s H_error tensor(439.3902, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3249, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
24 self_attn.v_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
18.461 s H_error tensor(267.1737, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3170, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
24 self_attn.o_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
18.423 s H_error tensor(22.9553, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3378, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
24 mlp.gate_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
stopped after 49 iterations
47.378 s H_error tensor(525.6881, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3139, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38553 MiB free out of 48676 MiB total
24 mlp.up_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
reducing lr to  0.00020589113209464906
reducing lr to  0.00018530201888518417
reducing lr to  0.00016677181699666576
reducing lr to  0.0001500946352969992
stopped after 49 iterations
47.599 s H_error tensor(447.2346, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3118, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38381 MiB free out of 48676 MiB total
24 mlp.down_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
stopped after 49 iterations
50.004 s H_error tensor(20.2202, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3161, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38209 MiB free out of 48676 MiB total
Fine tuning ...
LlamaDecoderLayer(
  (self_attn): LlamaAttention(
    (q_proj): QuantizedLinear()
    (k_proj): QuantizedLinear()
    (v_proj): QuantizedLinear()
    (o_proj): QuantizedLinear()
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (mlp): LlamaMLP(
    (gate_proj): QuantizedLinear()
    (up_proj): QuantizedLinear()
    (down_proj): QuantizedLinear()
    (act_fn): SiLUActivation()
  )
  (input_layernorm): LlamaRMSNorm()
  (post_attention_layernorm): LlamaRMSNorm()
)
attempting to cast to float32
attention_mask torch.Size([1, 1, 4096, 4096]) cuda:5 torch.float32
position_ids torch.Size([1, 4096]) cuda:5 torch.int64
Found 2 differentiable parameters
differentiable parameters: dict_keys(['input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 8192 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
----------
epoch=0
train loss=2.21e-02	
----------
epoch=1
train loss=2.20e-02	
----------
epoch=2
train loss=2.19e-02	
----------
epoch=3
train loss=2.19e-02	
----------
epoch=4
train loss=2.18e-02	
trying to convert back to original dtype
Fine tuned
layer original dtype torch.float16
25 self_attn.q_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
19.868 s H_error tensor(480.2097, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3216, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
25 self_attn.k_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
18.5 s H_error tensor(311.4709, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3717, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
25 self_attn.v_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
18.426 s H_error tensor(334.0642, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3150, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
25 self_attn.o_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
stopped after 49 iterations
18.451 s H_error tensor(17.4549, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3179, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
25 mlp.gate_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
reducing lr to  0.00020589113209464906
stopped after 49 iterations
47.443 s H_error tensor(573.3929, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3139, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38553 MiB free out of 48676 MiB total
25 mlp.up_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
reducing lr to  0.00020589113209464906
reducing lr to  0.00018530201888518417
reducing lr to  0.00016677181699666576
reducing lr to  0.0001500946352969992
stopped after 49 iterations
47.395 s H_error tensor(488.2528, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3119, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38381 MiB free out of 48676 MiB total
25 mlp.down_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
stopped after 49 iterations
49.99 s H_error tensor(21.8799, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3171, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38209 MiB free out of 48676 MiB total
Fine tuning ...
LlamaDecoderLayer(
  (self_attn): LlamaAttention(
    (q_proj): QuantizedLinear()
    (k_proj): QuantizedLinear()
    (v_proj): QuantizedLinear()
    (o_proj): QuantizedLinear()
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (mlp): LlamaMLP(
    (gate_proj): QuantizedLinear()
    (up_proj): QuantizedLinear()
    (down_proj): QuantizedLinear()
    (act_fn): SiLUActivation()
  )
  (input_layernorm): LlamaRMSNorm()
  (post_attention_layernorm): LlamaRMSNorm()
)
attempting to cast to float32
attention_mask torch.Size([1, 1, 4096, 4096]) cuda:5 torch.float32
position_ids torch.Size([1, 4096]) cuda:5 torch.int64
Found 2 differentiable parameters
differentiable parameters: dict_keys(['input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 8192 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
----------
epoch=0
train loss=2.50e-02	
----------
epoch=1
train loss=2.49e-02	
----------
epoch=2
train loss=2.48e-02	
----------
epoch=3
train loss=2.47e-02	
----------
epoch=4
train loss=2.46e-02	
trying to convert back to original dtype
Fine tuned
layer original dtype torch.float16
26 self_attn.q_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
stopped after 49 iterations
19.871 s H_error tensor(453.6891, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3232, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
26 self_attn.k_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
stopped after 49 iterations
18.441 s H_error tensor(486.7951, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3247, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
26 self_attn.v_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
18.42 s H_error tensor(330.8477, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3162, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
26 self_attn.o_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
18.404 s H_error tensor(28.9977, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3355, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
26 mlp.gate_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
reducing lr to  0.00020589113209464906
stopped after 49 iterations
47.474 s H_error tensor(598.4126, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3144, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38553 MiB free out of 48676 MiB total
26 mlp.up_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
reducing lr to  0.00020589113209464906
reducing lr to  0.00018530201888518417
stopped after 49 iterations
47.409 s H_error tensor(515.5770, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3123, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38381 MiB free out of 48676 MiB total
26 mlp.down_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
reducing lr to  0.00020589113209464906
reducing lr to  0.00018530201888518417
stopped after 49 iterations
50.001 s H_error tensor(23.7287, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3177, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38209 MiB free out of 48676 MiB total
Fine tuning ...
LlamaDecoderLayer(
  (self_attn): LlamaAttention(
    (q_proj): QuantizedLinear()
    (k_proj): QuantizedLinear()
    (v_proj): QuantizedLinear()
    (o_proj): QuantizedLinear()
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (mlp): LlamaMLP(
    (gate_proj): QuantizedLinear()
    (up_proj): QuantizedLinear()
    (down_proj): QuantizedLinear()
    (act_fn): SiLUActivation()
  )
  (input_layernorm): LlamaRMSNorm()
  (post_attention_layernorm): LlamaRMSNorm()
)
attempting to cast to float32
attention_mask torch.Size([1, 1, 4096, 4096]) cuda:5 torch.float32
position_ids torch.Size([1, 4096]) cuda:5 torch.int64
Found 2 differentiable parameters
differentiable parameters: dict_keys(['input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 8192 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
----------
epoch=0
train loss=2.79e-02	
----------
epoch=1
train loss=2.78e-02	
----------
epoch=2
train loss=2.77e-02	
----------
epoch=3
train loss=2.76e-02	
----------
epoch=4
train loss=2.75e-02	
trying to convert back to original dtype
Fine tuned
layer original dtype torch.float16
27 self_attn.q_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
stopped after 49 iterations
19.846 s H_error tensor(502.3925, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3216, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
27 self_attn.k_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
stopped after 49 iterations
18.537 s H_error tensor(516.6641, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3239, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
27 self_attn.v_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
18.431 s H_error tensor(346.5056, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3144, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
27 self_attn.o_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
18.337 s H_error tensor(19.3370, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3216, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
27 mlp.gate_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
stopped after 49 iterations
47.569 s H_error tensor(648.1781, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3151, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38553 MiB free out of 48676 MiB total
27 mlp.up_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
stopped after 49 iterations
47.25 s H_error tensor(561.2466, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3135, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38381 MiB free out of 48676 MiB total
27 mlp.down_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
reducing lr to  0.00020589113209464906
reducing lr to  0.00018530201888518417
reducing lr to  0.00016677181699666576
reducing lr to  0.0001500946352969992
reducing lr to  0.0001350851717672993
reducing lr to  0.00012157665459056936
reducing lr to  0.00010941898913151243
reducing lr to  9.847709021836118e-05
reducing lr to  8.862938119652506e-05
reducing lr to  7.976644307687256e-05
reducing lr to  7.17897987691853e-05
stopped after 49 iterations
49.935 s H_error tensor(27.8118, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3200, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38209 MiB free out of 48676 MiB total
Fine tuning ...
LlamaDecoderLayer(
  (self_attn): LlamaAttention(
    (q_proj): QuantizedLinear()
    (k_proj): QuantizedLinear()
    (v_proj): QuantizedLinear()
    (o_proj): QuantizedLinear()
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (mlp): LlamaMLP(
    (gate_proj): QuantizedLinear()
    (up_proj): QuantizedLinear()
    (down_proj): QuantizedLinear()
    (act_fn): SiLUActivation()
  )
  (input_layernorm): LlamaRMSNorm()
  (post_attention_layernorm): LlamaRMSNorm()
)
attempting to cast to float32
attention_mask torch.Size([1, 1, 4096, 4096]) cuda:5 torch.float32
position_ids torch.Size([1, 4096]) cuda:5 torch.int64
Found 2 differentiable parameters
differentiable parameters: dict_keys(['input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 8192 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
----------
epoch=0
train loss=2.84e-02	
----------
epoch=1
train loss=2.83e-02	
----------
epoch=2
train loss=2.82e-02	
----------
epoch=3
train loss=2.82e-02	
----------
epoch=4
train loss=2.81e-02	
trying to convert back to original dtype
Fine tuned
layer original dtype torch.float16
28 self_attn.q_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
stopped after 49 iterations
19.804 s H_error tensor(479.9485, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3209, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
28 self_attn.k_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
stopped after 49 iterations
18.452 s H_error tensor(496.6056, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3230, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
28 self_attn.v_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
18.428 s H_error tensor(379.3374, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3147, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
28 self_attn.o_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
18.353 s H_error tensor(28.6351, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3194, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
28 mlp.gate_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
stopped after 49 iterations
47.384 s H_error tensor(679.5734, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3151, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38553 MiB free out of 48676 MiB total
28 mlp.up_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
47.522 s H_error tensor(609.8083, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3149, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38381 MiB free out of 48676 MiB total
28 mlp.down_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
49.95 s H_error tensor(33.2050, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3232, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38209 MiB free out of 48676 MiB total
Fine tuning ...
LlamaDecoderLayer(
  (self_attn): LlamaAttention(
    (q_proj): QuantizedLinear()
    (k_proj): QuantizedLinear()
    (v_proj): QuantizedLinear()
    (o_proj): QuantizedLinear()
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (mlp): LlamaMLP(
    (gate_proj): QuantizedLinear()
    (up_proj): QuantizedLinear()
    (down_proj): QuantizedLinear()
    (act_fn): SiLUActivation()
  )
  (input_layernorm): LlamaRMSNorm()
  (post_attention_layernorm): LlamaRMSNorm()
)
attempting to cast to float32
attention_mask torch.Size([1, 1, 4096, 4096]) cuda:5 torch.float32
position_ids torch.Size([1, 4096]) cuda:5 torch.int64
Found 2 differentiable parameters
differentiable parameters: dict_keys(['input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 8192 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
----------
epoch=0
train loss=3.67e-02	
----------
epoch=1
train loss=3.66e-02	
----------
epoch=2
train loss=3.65e-02	
----------
epoch=3
train loss=3.64e-02	
----------
epoch=4
train loss=3.63e-02	
trying to convert back to original dtype
Fine tuned
layer original dtype torch.float16
29 self_attn.q_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
19.846 s H_error tensor(369.9456, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3641, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
29 self_attn.k_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
18.501 s H_error tensor(231.9348, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3774, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
29 self_attn.v_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
18.534 s H_error tensor(354.5615, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3135, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
29 self_attn.o_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
18.427 s H_error tensor(28.0332, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3295, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
29 mlp.gate_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
stopped after 49 iterations
47.266 s H_error tensor(704.5640, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3164, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38553 MiB free out of 48676 MiB total
29 mlp.up_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
reducing lr to  0.0009000000000000001
stopped after 49 iterations
47.405 s H_error tensor(637.6387, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3166, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38381 MiB free out of 48676 MiB total
29 mlp.down_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
reducing lr to  0.00020589113209464906
reducing lr to  0.00018530201888518417
reducing lr to  0.00016677181699666576
reducing lr to  0.0001500946352969992
stopped after 49 iterations
50.111 s H_error tensor(39.6374, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3291, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38209 MiB free out of 48676 MiB total
Fine tuning ...
LlamaDecoderLayer(
  (self_attn): LlamaAttention(
    (q_proj): QuantizedLinear()
    (k_proj): QuantizedLinear()
    (v_proj): QuantizedLinear()
    (o_proj): QuantizedLinear()
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (mlp): LlamaMLP(
    (gate_proj): QuantizedLinear()
    (up_proj): QuantizedLinear()
    (down_proj): QuantizedLinear()
    (act_fn): SiLUActivation()
  )
  (input_layernorm): LlamaRMSNorm()
  (post_attention_layernorm): LlamaRMSNorm()
)
attempting to cast to float32
attention_mask torch.Size([1, 1, 4096, 4096]) cuda:5 torch.float32
position_ids torch.Size([1, 4096]) cuda:5 torch.int64
Found 2 differentiable parameters
differentiable parameters: dict_keys(['input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 8192 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
----------
epoch=0
train loss=4.39e-02	
----------
epoch=1
train loss=4.37e-02	
----------
epoch=2
train loss=4.35e-02	
----------
epoch=3
train loss=4.33e-02	
----------
epoch=4
train loss=4.31e-02	
trying to convert back to original dtype
Fine tuned
layer original dtype torch.float16
30 self_attn.q_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
stopped after 49 iterations
19.833 s H_error tensor(450.4611, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3215, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
30 self_attn.k_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
stopped after 49 iterations
18.455 s H_error tensor(465.7074, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3219, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
30 self_attn.v_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
18.46 s H_error tensor(400.1332, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3144, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
30 self_attn.o_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
18.462 s H_error tensor(33.2514, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3285, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
30 mlp.gate_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
stopped after 49 iterations
47.611 s H_error tensor(760.8640, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3193, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38553 MiB free out of 48676 MiB total
30 mlp.up_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
stopped after 49 iterations
47.515 s H_error tensor(669.4452, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3186, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38381 MiB free out of 48676 MiB total
30 mlp.down_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 47 iterations
49.541 s H_error tensor(-0.9526, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.5089, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38209 MiB free out of 48676 MiB total
Fine tuning ...
LlamaDecoderLayer(
  (self_attn): LlamaAttention(
    (q_proj): QuantizedLinear()
    (k_proj): QuantizedLinear()
    (v_proj): QuantizedLinear()
    (o_proj): QuantizedLinear()
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (mlp): LlamaMLP(
    (gate_proj): QuantizedLinear()
    (up_proj): QuantizedLinear()
    (down_proj): QuantizedLinear()
    (act_fn): SiLUActivation()
  )
  (input_layernorm): LlamaRMSNorm()
  (post_attention_layernorm): LlamaRMSNorm()
)
attempting to cast to float32
attention_mask torch.Size([1, 1, 4096, 4096]) cuda:5 torch.float32
position_ids torch.Size([1, 4096]) cuda:5 torch.int64
Found 2 differentiable parameters
differentiable parameters: dict_keys(['input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 8192 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
----------
epoch=0
train loss=9.99e-02	
----------
epoch=1
train loss=9.91e-02	
----------
epoch=2
train loss=9.83e-02	
----------
epoch=3
train loss=9.76e-02	
----------
epoch=4
train loss=9.69e-02	
trying to convert back to original dtype
Fine tuned
layer original dtype torch.float16
31 self_attn.q_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
stopped after 49 iterations
19.863 s H_error tensor(313.5607, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3204, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
31 self_attn.k_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
stopped after 49 iterations
18.427 s H_error tensor(343.4975, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3228, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
31 self_attn.v_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
reducing lr to  0.00020589113209464906
reducing lr to  0.00018530201888518417
reducing lr to  0.00016677181699666576
reducing lr to  0.0001500946352969992
reducing lr to  0.0001350851717672993
reducing lr to  0.00012157665459056936
reducing lr to  0.00010941898913151243
reducing lr to  9.847709021836118e-05
reducing lr to  8.862938119652506e-05
stopped after 49 iterations
18.306 s H_error tensor(225.9385, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3126, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
31 self_attn.o_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
18.355 s H_error tensor(105.4799, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3383, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
37873 MiB free out of 48676 MiB total
31 mlp.gate_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
stopped after 49 iterations
47.457 s H_error tensor(664.9456, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3208, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38553 MiB free out of 48676 MiB total
31 mlp.up_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
stopped after 49 iterations
47.631 s H_error tensor(602.8883, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3237, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38381 MiB free out of 48676 MiB total
31 mlp.down_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
stopped after 49 iterations
50.084 s H_error tensor(118.5819, device='cuda:5', grad_fn=<ViewBackward0>) average_error tensor(0.3983, device='cuda:5', grad_fn=<DivBackward0>)
to: args ('cuda:5',) kwargs {}
38209 MiB free out of 48676 MiB total
Fine tuning ...
LlamaDecoderLayer(
  (self_attn): LlamaAttention(
    (q_proj): QuantizedLinear()
    (k_proj): QuantizedLinear()
    (v_proj): QuantizedLinear()
    (o_proj): QuantizedLinear()
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (mlp): LlamaMLP(
    (gate_proj): QuantizedLinear()
    (up_proj): QuantizedLinear()
    (down_proj): QuantizedLinear()
    (act_fn): SiLUActivation()
  )
  (input_layernorm): LlamaRMSNorm()
  (post_attention_layernorm): LlamaRMSNorm()
)
attempting to cast to float32
attention_mask torch.Size([1, 1, 4096, 4096]) cuda:5 torch.float32
position_ids torch.Size([1, 4096]) cuda:5 torch.int64
Found 2 differentiable parameters
differentiable parameters: dict_keys(['input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 8192 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
----------
epoch=0
train loss=1.77e-01	
----------
epoch=1
train loss=1.74e-01	
----------
epoch=2
train loss=1.71e-01	
----------
epoch=3
train loss=1.69e-01	
----------
epoch=4
train loss=1.67e-01	
trying to convert back to original dtype
Fine tuned
Total bits: 12995657728.0 Total params: 6476005376
average bits per value: 2.0067397992227978
13149.368861913681
Loading tokenizer for meta-llama/Llama-2-7b-hf
/home/lliu/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Dataset: wikitext2
Evaluating ...
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
Perplexity: 7.804674
