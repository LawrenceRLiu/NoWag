/home/lliu/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.32s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.20it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.10it/s]
Model loaded. LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
4096
Loading tokenizer for meta-llama/Llama-2-7b-hf
Starting...
39011 MiB free out of 48676 MiB total
attention_mask torch.Size([1, 1, 4096, 4096]) cuda:7 torch.float16
position_ids torch.Size([1, 4096]) cuda:7 torch.int64
position_ids torch.Size([1, 4096])
attention_mask torch.Size([1, 1, 4096, 4096])
Ready.
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
0 self_attn.k_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.1756, 0.8264, 0.3096,  ..., 0.5891, 0.6551, 0.5814], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.0278, 1.4363, 1.2937,  ..., 1.3905, 1.6662, 0.9913], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
0 self_attn.v_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([0.7236, 0.7618, 0.5036,  ..., 0.7654, 0.7673, 0.6849], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.4949, 0.5030, 0.4779,  ..., 0.4897, 0.5060, 0.5031], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
0 self_attn.q_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([0.8849, 0.6529, 0.2560,  ..., 0.5371, 0.5559, 0.4512], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.6149, 1.0973, 1.2322,  ..., 1.2838, 1.3711, 0.9672], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
0 self_attn.o_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([0.2801, 0.2821, 0.2653,  ..., 0.2623, 0.2674, 0.2648], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.4009, 0.4159, 0.4015,  ..., 0.4328, 0.4183, 0.4196], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
0 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 77439248
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0001771905085661274 val loss: 8.10248420748394e-05
9021 MiB free out of 48676 MiB total
epoch 1 loss: 5.729961583256227e-05 val loss: 8.812623400444863e-05
9013 MiB free out of 48676 MiB total
epoch 2 loss: 3.625129025408569e-05 val loss: 0.00010074321289721411
9013 MiB free out of 48676 MiB total
epoch 3 loss: 2.6435473614583316e-05 val loss: 0.00011060137239837786
9013 MiB free out of 48676 MiB total
epoch 4 loss: 2.031387271017593e-05 val loss: 0.00011794075408033677
9013 MiB free out of 48676 MiB total
epoch 5 loss: 1.643374799442654e-05 val loss: 0.00012338462420302676
9013 MiB free out of 48676 MiB total
early stopping
39011 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
9013 MiB free out of 48676 MiB total
after cast to cpu
38565 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
1 self_attn.k_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([2.0254, 1.9664, 2.0364,  ..., 1.6627, 1.8973, 1.8084], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.5419, 1.7741, 1.8750,  ..., 0.6519, 0.6390, 0.5201], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
1 self_attn.v_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([0.5438, 0.5367, 0.5507,  ..., 0.6505, 0.5752, 0.6442], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.8513, 0.8723, 0.8107,  ..., 0.3544, 0.3659, 0.3639], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
1 self_attn.q_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.9255, 1.9044, 1.8602,  ..., 1.6792, 1.9344, 1.6705], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.9452, 1.6639, 1.9007,  ..., 0.3805, 0.3928, 0.4772], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
1 self_attn.o_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([0.8602, 0.8032, 0.6896,  ..., 0.2113, 0.2166, 0.2192], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.5290, 0.5332, 0.5174,  ..., 0.5289, 0.5185, 0.5111], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
1 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 77439248
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.025783067245356506 val loss: 0.0006868787713756319
8989 MiB free out of 48676 MiB total
epoch 1 loss: 0.0006144738927105209 val loss: 0.000698352432664251
8981 MiB free out of 48676 MiB total
epoch 2 loss: 0.00023343602157410714 val loss: 0.0007100250295479782
8981 MiB free out of 48676 MiB total
epoch 3 loss: 0.0001557746707590013 val loss: 0.0007302529957087245
8981 MiB free out of 48676 MiB total
epoch 4 loss: 0.00012890384022057333 val loss: 0.0007582261459901929
8981 MiB free out of 48676 MiB total
epoch 5 loss: 0.00012439067361924572 val loss: 0.0007807544097886421
8981 MiB free out of 48676 MiB total
early stopping
38565 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8981 MiB free out of 48676 MiB total
after cast to cpu
38533 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
2 self_attn.k_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.7840, 1.7515, 1.8243,  ..., 1.7953, 1.7517, 1.7821], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.7197, 1.3612, 1.5470,  ..., 2.5627, 0.6336, 2.3330], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
2 self_attn.v_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([0.9290, 0.9331, 0.9195,  ..., 0.9050, 0.9574, 0.9601], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.9509, 0.9649, 0.9703,  ..., 0.8383, 0.9783, 0.9524], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
2 self_attn.q_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.6843, 1.7193, 1.6812,  ..., 1.7487, 1.6237, 1.6486], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.0094, 1.2899, 1.5509,  ..., 2.1117, 0.7452, 2.0198], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
2 self_attn.o_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([0.9219, 0.9821, 0.9457,  ..., 0.9478, 0.9625, 0.9246], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.8827, 0.8924, 0.8919,  ..., 0.8741, 0.8675, 0.8812], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
2 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 77439248
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0015024719250504859 val loss: 0.0022595328773604706
8989 MiB free out of 48676 MiB total
epoch 1 loss: 0.0005685716737389157 val loss: 0.0022244496358325705
8981 MiB free out of 48676 MiB total
epoch 2 loss: 0.00047831563665567955 val loss: 0.002237075677840039
8981 MiB free out of 48676 MiB total
epoch 3 loss: 0.000426184964226195 val loss: 0.0022587555140489712
8981 MiB free out of 48676 MiB total
epoch 4 loss: 0.0003905776907231484 val loss: 0.0022812068928033113
8981 MiB free out of 48676 MiB total
epoch 5 loss: 0.00036460677392824437 val loss: 0.002303116722032428
8981 MiB free out of 48676 MiB total
epoch 6 loss: 0.0003447132419296395 val loss: 0.0023234689142555
8981 MiB free out of 48676 MiB total
early stopping
38533 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8981 MiB free out of 48676 MiB total
after cast to cpu
38565 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
3 self_attn.k_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.6463, 1.6737, 1.6654,  ..., 1.6477, 1.6560, 1.6845], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.1294, 1.3448, 1.3066,  ..., 2.7185, 2.9419, 3.0465], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
3 self_attn.v_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([0.8999, 0.8761, 0.8859,  ..., 0.8799, 0.8948, 0.8878], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.8146, 0.8202, 0.8095,  ..., 0.4050, 0.4349, 0.3835], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
3 self_attn.q_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.5567, 1.6199, 1.6101,  ..., 1.6199, 1.6003, 1.6185], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.1710, 1.3686, 1.3253,  ..., 2.6906, 2.8910, 3.0339], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
3 self_attn.o_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([0.8149, 0.8113, 0.7834,  ..., 0.3370, 0.3588, 0.3251], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.8344, 0.8513, 0.8366,  ..., 0.8515, 0.8315, 0.8545], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
3 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 77439248
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.008308674418003648 val loss: 0.00769570775446482
8989 MiB free out of 48676 MiB total
epoch 1 loss: 0.001990327523344604 val loss: 0.0077025098144076765
8981 MiB free out of 48676 MiB total
epoch 2 loss: 0.0014897524511070515 val loss: 0.007721935166046023
8981 MiB free out of 48676 MiB total
epoch 3 loss: 0.0012151352148066508 val loss: 0.0077343154407572
8981 MiB free out of 48676 MiB total
epoch 4 loss: 0.0010560448718024418 val loss: 0.007736660772934556
8981 MiB free out of 48676 MiB total
epoch 5 loss: 0.0009536230777484889 val loss: 0.0077303478319663554
8981 MiB free out of 48676 MiB total
early stopping
38565 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8981 MiB free out of 48676 MiB total
after cast to cpu
38533 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
4 self_attn.k_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.6928, 1.7265, 1.7171,  ..., 1.6686, 1.7117, 1.7124], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.9987, 1.3206, 1.3897,  ..., 2.4271, 2.2609, 2.4617], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
4 self_attn.v_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([0.9338, 0.9049, 0.9457,  ..., 0.9385, 0.9042, 0.9126], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.8676, 0.8579, 0.8606,  ..., 0.9248, 0.9169, 0.9350], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
4 self_attn.q_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.6591, 1.6807, 1.6291,  ..., 1.6611, 1.7065, 1.7137], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.9198, 1.3477, 1.4289,  ..., 2.3017, 2.2600, 2.2052], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
4 self_attn.o_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([0.8375, 0.8274, 0.8368,  ..., 0.9084, 0.9081, 0.9244], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.8678, 0.8990, 0.9099,  ..., 0.8872, 0.8625, 0.8959], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
4 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 77439248
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.010646862214343855 val loss: 0.007820075465133414
8989 MiB free out of 48676 MiB total
epoch 1 loss: 0.0024406095617450774 val loss: 0.007772962038870901
8981 MiB free out of 48676 MiB total
epoch 2 loss: 0.0017998968423853512 val loss: 0.007773157529300079
8981 MiB free out of 48676 MiB total
epoch 3 loss: 0.0014467519672507478 val loss: 0.007766752125462517
8981 MiB free out of 48676 MiB total
epoch 4 loss: 0.0012295950450607052 val loss: 0.00774469377938658
8981 MiB free out of 48676 MiB total
epoch 5 loss: 0.001087604053736868 val loss: 0.007716237450949848
8981 MiB free out of 48676 MiB total
epoch 6 loss: 0.0009882854128591134 val loss: 0.007692922779824585
8981 MiB free out of 48676 MiB total
epoch 7 loss: 0.0009146703851001803 val loss: 0.007664047938305885
8981 MiB free out of 48676 MiB total
epoch 8 loss: 0.000856067067161348 val loss: 0.007639586692675948
8981 MiB free out of 48676 MiB total
epoch 9 loss: 0.0008076605522546743 val loss: 0.0076167080260347575
8981 MiB free out of 48676 MiB total
38533 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8981 MiB free out of 48676 MiB total
after cast to cpu
38565 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
5 self_attn.k_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.7228, 1.7929, 1.7542,  ..., 1.6983, 1.7302, 1.7280], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.9647, 1.0825, 1.1445,  ..., 1.4308, 2.6816, 2.3936], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
5 self_attn.v_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([0.9815, 0.9241, 0.9573,  ..., 0.9756, 0.9689, 0.9269], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.9928, 0.9724, 0.9818,  ..., 0.8969, 0.9049, 0.9102], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
5 self_attn.q_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.6849, 1.6923, 1.6421,  ..., 1.6521, 1.6664, 1.7371], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.0021, 1.0970, 1.1329,  ..., 1.7344, 1.9421, 1.6516], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
5 self_attn.o_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([0.9817, 0.9510, 0.9640,  ..., 0.8447, 0.8656, 0.8768], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.9241, 0.9280, 0.9083,  ..., 0.9133, 0.8996, 0.9203], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
5 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 77439248
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.015652891039280803 val loss: 0.008833685627905652
8989 MiB free out of 48676 MiB total
epoch 1 loss: 0.004368779800643097 val loss: 0.00877961321384646
8981 MiB free out of 48676 MiB total
epoch 2 loss: 0.003052642793591076 val loss: 0.008751153713092208
8981 MiB free out of 48676 MiB total
epoch 3 loss: 0.0023579422440889175 val loss: 0.008731125562917441
8981 MiB free out of 48676 MiB total
epoch 4 loss: 0.001955112428731809 val loss: 0.008712484705029055
8981 MiB free out of 48676 MiB total
epoch 5 loss: 0.0017043765519701992 val loss: 0.008694289019331336
8981 MiB free out of 48676 MiB total
epoch 6 loss: 0.0015329893121815985 val loss: 0.008677777368575335
8981 MiB free out of 48676 MiB total
epoch 7 loss: 0.0014066636158531765 val loss: 0.008662222069688141
8981 MiB free out of 48676 MiB total
epoch 8 loss: 0.0013068512744212057 val loss: 0.008648292772704735
8981 MiB free out of 48676 MiB total
epoch 9 loss: 0.0012242233297001803 val loss: 0.00863348352140747
8981 MiB free out of 48676 MiB total
38565 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8981 MiB free out of 48676 MiB total
after cast to cpu
38533 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
6 self_attn.k_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.6115, 1.6701, 1.6337,  ..., 1.6253, 1.6097, 1.6225], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.2987, 1.3126, 1.3289,  ..., 2.0024, 2.0803, 2.1513], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
6 self_attn.v_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([0.9260, 0.8385, 0.8766,  ..., 0.9176, 0.8962, 0.8614], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.9808, 0.9636, 0.9873,  ..., 0.9508, 0.9518, 0.9481], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
6 self_attn.q_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.5404, 1.5738, 1.6013,  ..., 1.5337, 1.5906, 1.6066], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.3378, 1.3558, 1.3186,  ..., 2.1034, 2.0254, 2.1746], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
6 self_attn.o_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([0.9574, 0.9658, 0.9658,  ..., 0.9132, 0.9029, 0.9161], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.8659, 0.8919, 0.8469,  ..., 0.8721, 0.8547, 0.8473], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
6 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 77439248
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.02551482249327819 val loss: 0.008763450547121465
8989 MiB free out of 48676 MiB total
epoch 1 loss: 0.006649740782449953 val loss: 0.008811206498648971
8981 MiB free out of 48676 MiB total
epoch 2 loss: 0.004411251597048249 val loss: 0.008833605155814439
8981 MiB free out of 48676 MiB total
epoch 3 loss: 0.0033303847076240345 val loss: 0.008846405835356563
8981 MiB free out of 48676 MiB total
epoch 4 loss: 0.0027246217086940305 val loss: 0.008853149018250406
8981 MiB free out of 48676 MiB total
epoch 5 loss: 0.0023412503651343286 val loss: 0.008858127112034708
8981 MiB free out of 48676 MiB total
early stopping
38533 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8981 MiB free out of 48676 MiB total
after cast to cpu
38565 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
7 self_attn.k_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.5728, 1.6454, 1.5849,  ..., 1.5643, 1.5614, 1.5966], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.7458, 0.7266, 0.7555,  ..., 1.9018, 2.3899, 2.3772], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
7 self_attn.v_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([0.9560, 0.8397, 0.8979,  ..., 0.9439, 0.9081, 0.8830], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.9850, 0.9593, 0.9277,  ..., 0.8599, 0.8627, 0.8792], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
7 self_attn.q_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.5591, 1.5747, 1.5777,  ..., 1.5886, 1.5985, 1.6013], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.7524, 0.7359, 0.7657,  ..., 2.0297, 2.7848, 2.7748], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
7 self_attn.o_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.0083, 0.9239, 0.9096,  ..., 0.8438, 0.8473, 0.8590], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.8733, 0.9017, 0.8406,  ..., 0.8633, 0.8822, 0.8546], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
7 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 77439248
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.026592012549372157 val loss: 0.011778118030633777
8989 MiB free out of 48676 MiB total
epoch 1 loss: 0.008081598520220723 val loss: 0.011869410227518529
8981 MiB free out of 48676 MiB total
epoch 2 loss: 0.0055714077243465 val loss: 0.01194597885478288
8981 MiB free out of 48676 MiB total
epoch 3 loss: 0.004246015385433566 val loss: 0.012004037736915052
8981 MiB free out of 48676 MiB total
epoch 4 loss: 0.003489788088700152 val loss: 0.012046874267980456
8981 MiB free out of 48676 MiB total
epoch 5 loss: 0.0030072417812334606 val loss: 0.012083027686458081
8981 MiB free out of 48676 MiB total
early stopping
38565 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8981 MiB free out of 48676 MiB total
after cast to cpu
38533 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
8 self_attn.k_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.6222, 1.6256, 1.6392,  ..., 1.5824, 1.5826, 1.6133], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.8323, 1.0179, 0.9911,  ..., 1.9226, 2.3946, 2.4478], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
8 self_attn.v_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([0.9597, 0.8698, 0.8955,  ..., 0.9706, 0.9312, 0.9159], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.0011, 1.0007, 0.9877,  ..., 0.8776, 0.8515, 0.8656], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
8 self_attn.q_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.5808, 1.6306, 1.5969,  ..., 1.5672, 1.5837, 1.5975], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.8247, 1.0033, 0.9929,  ..., 2.3955, 3.1897, 3.1726], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
8 self_attn.o_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([0.9737, 0.9628, 0.9414,  ..., 0.8193, 0.8024, 0.8064], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.9048, 0.9030, 0.8637,  ..., 0.8852, 0.8910, 0.8782], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
8 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 77439248
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.024352203668968286 val loss: 0.012347458803560585
8989 MiB free out of 48676 MiB total
epoch 1 loss: 0.007633455557879643 val loss: 0.012358266743831336
8981 MiB free out of 48676 MiB total
epoch 2 loss: 0.00527443678038253 val loss: 0.012367970368359238
8981 MiB free out of 48676 MiB total
epoch 3 loss: 0.004061631483637029 val loss: 0.012376847851555794
8981 MiB free out of 48676 MiB total
epoch 4 loss: 0.003358307968483132 val loss: 0.012389026465825737
8981 MiB free out of 48676 MiB total
epoch 5 loss: 0.0028995817210670793 val loss: 0.012406651745550334
8981 MiB free out of 48676 MiB total
early stopping
38533 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8981 MiB free out of 48676 MiB total
after cast to cpu
38565 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
9 self_attn.k_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.6808, 1.6457, 1.6952,  ..., 1.6183, 1.6024, 1.6369], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.9793, 1.1443, 1.1281,  ..., 2.1393, 2.2283, 2.2846], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
9 self_attn.v_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([0.9727, 0.9112, 0.8904,  ..., 0.9682, 0.9803, 0.9293], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.0864, 1.0820, 1.0906,  ..., 0.8114, 0.8140, 0.7966], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
9 self_attn.q_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.5445, 1.6070, 1.5710,  ..., 1.6273, 1.5779, 1.6374], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.0707, 1.1604, 1.1292,  ..., 1.9872, 2.0199, 2.1190], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
9 self_attn.o_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.0343, 1.0450, 1.0397,  ..., 0.8190, 0.8186, 0.7914], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.9360, 0.9315, 0.8659,  ..., 0.9257, 0.9022, 0.9244], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
9 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 77439248
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.031628810917027295 val loss: 0.01685988623648882
8989 MiB free out of 48676 MiB total
epoch 1 loss: 0.009365251658891793 val loss: 0.016783975530415773
8981 MiB free out of 48676 MiB total
epoch 2 loss: 0.0064793412584549515 val loss: 0.01676276559010148
8981 MiB free out of 48676 MiB total
epoch 3 loss: 0.0049924846352951135 val loss: 0.016737011959776282
8981 MiB free out of 48676 MiB total
epoch 4 loss: 0.0041173098725266755 val loss: 0.01669822633266449
8981 MiB free out of 48676 MiB total
epoch 5 loss: 0.003534728717568214 val loss: 0.01665426220279187
8981 MiB free out of 48676 MiB total
epoch 6 loss: 0.0031188899447442964 val loss: 0.016613407409749925
8981 MiB free out of 48676 MiB total
epoch 7 loss: 0.0028109056902394514 val loss: 0.016577270696870983
8981 MiB free out of 48676 MiB total
epoch 8 loss: 0.002585727804216731 val loss: 0.016548168379813433
8981 MiB free out of 48676 MiB total
epoch 9 loss: 0.0023814823007342056 val loss: 0.016520440869498998
8981 MiB free out of 48676 MiB total
38565 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8981 MiB free out of 48676 MiB total
after cast to cpu
38533 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
10 self_attn.k_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.6543, 1.6721, 1.6871,  ..., 1.5862, 1.6025, 1.6724], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.7476, 0.8385, 0.8215,  ..., 2.1863, 2.1931, 2.1422], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
10 self_attn.v_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([0.9680, 0.9046, 0.8915,  ..., 0.9570, 0.9430, 0.9284], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.1302, 1.1081, 1.1169,  ..., 0.9286, 0.9147, 0.9270], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
10 self_attn.q_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.5843, 1.6006, 1.5543,  ..., 1.6136, 1.6117, 1.6147], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.8123, 0.8635, 0.8176,  ..., 2.0155, 2.2036, 2.1027], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
10 self_attn.o_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.0892, 1.0734, 1.0835,  ..., 0.8955, 0.8985, 0.8911], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.9230, 0.9006, 0.8636,  ..., 0.9054, 0.9069, 0.8883], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
10 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 77439248
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.03841331203148002 val loss: 0.023772778105922043
8989 MiB free out of 48676 MiB total
epoch 1 loss: 0.009899277672957396 val loss: 0.023851197329349816
8981 MiB free out of 48676 MiB total
epoch 2 loss: 0.006478367657109629 val loss: 0.023946339380927384
8981 MiB free out of 48676 MiB total
epoch 3 loss: 0.004901606262137648 val loss: 0.024023906560614705
8981 MiB free out of 48676 MiB total
epoch 4 loss: 0.004006749903055606 val loss: 0.024105166434310377
8981 MiB free out of 48676 MiB total
epoch 5 loss: 0.0034212445443699835 val loss: 0.02418073220178485
8981 MiB free out of 48676 MiB total
early stopping
38533 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8981 MiB free out of 48676 MiB total
after cast to cpu
38565 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
11 self_attn.k_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.4919, 1.5258, 1.5711,  ..., 1.5032, 1.4571, 1.5529], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.9077, 0.8226, 0.7986,  ..., 1.9173, 1.8759, 1.8916], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
11 self_attn.v_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([0.9840, 0.9265, 0.9024,  ..., 1.0096, 0.9917, 0.9475], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.9041, 0.9214, 0.9175,  ..., 1.0224, 1.0506, 1.0277], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
11 self_attn.q_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.5355, 1.5699, 1.5003,  ..., 1.5027, 1.5507, 1.5416], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.9395, 0.8253, 0.8040,  ..., 2.0374, 1.9353, 2.0648], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
11 self_attn.o_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([0.8975, 0.9156, 0.8993,  ..., 0.9782, 0.9988, 0.9837], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.9541, 0.9431, 0.8961,  ..., 0.9448, 0.9398, 0.9368], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
11 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 77439248
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.051610282993351575 val loss: 0.023905456066131592
8989 MiB free out of 48676 MiB total
epoch 1 loss: 0.015606943030434195 val loss: 0.023671615519560874
8981 MiB free out of 48676 MiB total
epoch 2 loss: 0.010614688340865541 val loss: 0.02345188334584236
8981 MiB free out of 48676 MiB total
epoch 3 loss: 0.008184843882190762 val loss: 0.023269134922884405
8981 MiB free out of 48676 MiB total
epoch 4 loss: 0.006762197277566884 val loss: 0.02311834751162678
8981 MiB free out of 48676 MiB total
epoch 5 loss: 0.005810639033370535 val loss: 0.022983879200182855
8981 MiB free out of 48676 MiB total
epoch 6 loss: 0.005123100349010201 val loss: 0.02285742328967899
8981 MiB free out of 48676 MiB total
epoch 7 loss: 0.004600897078489652 val loss: 0.02273374702781439
8981 MiB free out of 48676 MiB total
epoch 8 loss: 0.004189508503259276 val loss: 0.02262006001546979
8981 MiB free out of 48676 MiB total
epoch 9 loss: 0.0038559586409974145 val loss: 0.022513755946420133
8981 MiB free out of 48676 MiB total
38565 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8981 MiB free out of 48676 MiB total
after cast to cpu
38533 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
12 self_attn.k_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.6344, 1.6304, 1.6164,  ..., 1.5886, 1.5710, 1.6159], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.8726, 0.9940, 1.0412,  ..., 1.7685, 1.8328, 1.6122], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
12 self_attn.v_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([0.9805, 0.8820, 0.9145,  ..., 0.9917, 0.9768, 0.9477], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.9254, 0.9355, 0.9365,  ..., 0.7872, 0.8072, 0.7791], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
12 self_attn.q_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.5269, 1.5773, 1.5420,  ..., 1.5242, 1.5672, 1.5658], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.8878, 1.0026, 1.0124,  ..., 1.6680, 1.7091, 1.5400], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
12 self_attn.o_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([0.9019, 0.9160, 0.9263,  ..., 0.8095, 0.8280, 0.8232], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.9529, 0.9486, 0.9037,  ..., 0.9460, 0.9320, 0.9379], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
12 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 77439248
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.04782609017274808 val loss: 0.01921209879219532
8989 MiB free out of 48676 MiB total
epoch 1 loss: 0.01535718733430258 val loss: 0.019080902449786663
8981 MiB free out of 48676 MiB total
epoch 2 loss: 0.010446119707921753 val loss: 0.019049086375162005
8981 MiB free out of 48676 MiB total
epoch 3 loss: 0.008129363603075035 val loss: 0.01908079721033573
8981 MiB free out of 48676 MiB total
epoch 4 loss: 0.006776216861908324 val loss: 0.01912424387410283
8981 MiB free out of 48676 MiB total
epoch 5 loss: 0.0058677535998867825 val loss: 0.01916660077404231
8981 MiB free out of 48676 MiB total
epoch 6 loss: 0.00520823063015996 val loss: 0.019202779978513718
8981 MiB free out of 48676 MiB total
epoch 7 loss: 0.004705236930021783 val loss: 0.019227810204029083
8981 MiB free out of 48676 MiB total
early stopping
38533 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8981 MiB free out of 48676 MiB total
after cast to cpu
38565 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
13 self_attn.k_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.6126, 1.5780, 1.5905,  ..., 1.5597, 1.5394, 1.5684], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.8244, 1.0660, 1.0900,  ..., 1.8910, 1.9190, 1.7981], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
13 self_attn.v_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.0103, 0.9506, 0.9665,  ..., 0.9982, 1.0047, 0.9755], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.9152, 0.9285, 0.9353,  ..., 0.9549, 0.9411, 0.9663], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
13 self_attn.q_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.5021, 1.5756, 1.5207,  ..., 1.5473, 1.5423, 1.5433], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.0422, 1.0638, 1.0815,  ..., 1.8074, 1.7660, 1.7255], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
13 self_attn.o_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([0.8974, 0.9448, 0.9435,  ..., 0.9521, 0.9254, 0.9538], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.9631, 0.9864, 0.9225,  ..., 0.9686, 0.9673, 0.9634], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
13 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 77439248
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.054865409671037924 val loss: 0.023538797278888524
8989 MiB free out of 48676 MiB total
epoch 1 loss: 0.017937755703314906 val loss: 0.023422349360771477
8981 MiB free out of 48676 MiB total
epoch 2 loss: 0.012394572218909161 val loss: 0.02337910037022084
8981 MiB free out of 48676 MiB total
epoch 3 loss: 0.009703056668513454 val loss: 0.023374828859232366
8981 MiB free out of 48676 MiB total
epoch 4 loss: 0.008105724999040831 val loss: 0.023363314801827073
8981 MiB free out of 48676 MiB total
epoch 5 loss: 0.007020538756478345 val loss: 0.023345102090388536
8981 MiB free out of 48676 MiB total
epoch 6 loss: 0.006229662933037616 val loss: 0.023316816659644246
8981 MiB free out of 48676 MiB total
epoch 7 loss: 0.0056282689238287276 val loss: 0.023289773846045136
8981 MiB free out of 48676 MiB total
epoch 8 loss: 0.0051553757784859044 val loss: 0.023261354421265423
8981 MiB free out of 48676 MiB total
epoch 9 loss: 0.004772498199599795 val loss: 0.02323943329975009
8981 MiB free out of 48676 MiB total
38565 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8981 MiB free out of 48676 MiB total
after cast to cpu
38533 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
14 self_attn.k_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.6134, 1.5749, 1.5880,  ..., 1.5348, 1.5272, 1.5489], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.0522, 1.4499, 1.4432,  ..., 1.5537, 1.5905, 1.4958], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
14 self_attn.v_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([0.9699, 0.9163, 0.9656,  ..., 0.9915, 0.9925, 0.9866], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.9107, 0.9021, 0.9065,  ..., 1.0683, 1.0652, 1.0858], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
14 self_attn.q_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.5147, 1.5503, 1.5152,  ..., 1.5625, 1.5397, 1.5450], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.1204, 1.4476, 1.4390,  ..., 1.4891, 1.5953, 1.5113], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
14 self_attn.o_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([0.8922, 0.8743, 0.8909,  ..., 1.0444, 1.0262, 1.0497], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.9329, 0.9440, 0.9126,  ..., 0.9653, 0.9494, 0.9803], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
14 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 77439248
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.05060388913261704 val loss: 0.02421796612907201
8989 MiB free out of 48676 MiB total
epoch 1 loss: 0.015268237184500322 val loss: 0.023988244705833495
8981 MiB free out of 48676 MiB total
epoch 2 loss: 0.010312269696441945 val loss: 0.02384536270983517
8981 MiB free out of 48676 MiB total
epoch 3 loss: 0.007956940007716184 val loss: 0.023783009499311447
8981 MiB free out of 48676 MiB total
epoch 4 loss: 0.006569832476088777 val loss: 0.023764839163050056
8981 MiB free out of 48676 MiB total
epoch 5 loss: 0.005640453617161256 val loss: 0.023767224280163646
8981 MiB free out of 48676 MiB total
epoch 6 loss: 0.004970621563188615 val loss: 0.023773875436745584
8981 MiB free out of 48676 MiB total
epoch 7 loss: 0.004464304280190845 val loss: 0.0237834551371634
8981 MiB free out of 48676 MiB total
epoch 8 loss: 0.00406778260912688 val loss: 0.023790516308508813
8981 MiB free out of 48676 MiB total
epoch 9 loss: 0.0037481504268725985 val loss: 0.023792931344360113
8981 MiB free out of 48676 MiB total
early stopping
38533 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8981 MiB free out of 48676 MiB total
after cast to cpu
38565 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
15 self_attn.k_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.6233, 1.5785, 1.5584,  ..., 1.5217, 1.5787, 1.5373], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.8892, 0.9599, 0.9163,  ..., 1.9989, 2.0357, 2.1068], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
15 self_attn.v_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.0155, 0.9771, 0.9901,  ..., 1.0429, 1.0140, 1.0064], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.0221, 1.0299, 1.0216,  ..., 0.9816, 0.9779, 0.9640], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
15 self_attn.q_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.4933, 1.5343, 1.4894,  ..., 1.5105, 1.5305, 1.5341], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.8819, 0.9525, 0.9162,  ..., 1.9384, 1.9062, 1.9309], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
15 self_attn.o_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([0.9999, 1.0097, 1.0048,  ..., 0.9665, 0.9695, 0.9545], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.9702, 0.9879, 0.9669,  ..., 0.9994, 0.9682, 1.0115], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
15 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 77439248
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.06739235948043643 val loss: 0.028723902418278158
8989 MiB free out of 48676 MiB total
epoch 1 loss: 0.01831017902804888 val loss: 0.028626418788917363
8981 MiB free out of 48676 MiB total
epoch 2 loss: 0.01225982497271616 val loss: 0.02860901632811874
8981 MiB free out of 48676 MiB total
epoch 3 loss: 0.009461226960411295 val loss: 0.02864714793395251
8981 MiB free out of 48676 MiB total
epoch 4 loss: 0.00785067463220912 val loss: 0.028703499934636056
8981 MiB free out of 48676 MiB total
epoch 5 loss: 0.0067781186662614346 val loss: 0.02875137364026159
8981 MiB free out of 48676 MiB total
epoch 6 loss: 0.006002111011184752 val loss: 0.0287873565685004
8981 MiB free out of 48676 MiB total
epoch 7 loss: 0.0054114667618705425 val loss: 0.02881189389154315
8981 MiB free out of 48676 MiB total
early stopping
38565 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8981 MiB free out of 48676 MiB total
after cast to cpu
38533 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
16 self_attn.k_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.5899, 1.5800, 1.5417,  ..., 1.5215, 1.5134, 1.5515], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.1845, 1.2345, 1.2825,  ..., 1.7402, 1.7219, 1.7930], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
16 self_attn.v_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.0617, 1.0144, 1.0695,  ..., 1.0903, 1.0521, 1.0318], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.2892, 1.2590, 1.2708,  ..., 1.0814, 1.0957, 1.0983], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
16 self_attn.q_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.4894, 1.4845, 1.4596,  ..., 1.4817, 1.5148, 1.4989], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.1535, 1.2194, 1.2913,  ..., 1.6515, 1.6507, 1.6518], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
16 self_attn.o_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.2393, 1.2085, 1.2276,  ..., 1.0664, 1.0781, 1.0688], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.0223, 1.0249, 1.0036,  ..., 1.0444, 1.0177, 1.0147], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
16 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 77439248
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.07766311441082507 val loss: 0.04167245887219906
8989 MiB free out of 48676 MiB total
epoch 1 loss: 0.02353764799045166 val loss: 0.041578230215236545
8981 MiB free out of 48676 MiB total
epoch 2 loss: 0.015829656291316496 val loss: 0.04154615476727486
8981 MiB free out of 48676 MiB total
epoch 3 loss: 0.012258184171514586 val loss: 0.04153155465610325
8981 MiB free out of 48676 MiB total
epoch 4 loss: 0.010184327449678676 val loss: 0.04152168636210263
8981 MiB free out of 48676 MiB total
epoch 5 loss: 0.008801004638371523 val loss: 0.04150697938166559
8981 MiB free out of 48676 MiB total
epoch 6 loss: 0.007799088431056589 val loss: 0.04148800973780453
8981 MiB free out of 48676 MiB total
epoch 7 loss: 0.007034668300548219 val loss: 0.04146933276206255
8981 MiB free out of 48676 MiB total
epoch 8 loss: 0.006429510685848072 val loss: 0.04144983901642263
8981 MiB free out of 48676 MiB total
epoch 9 loss: 0.005936640145591809 val loss: 0.04143131268210709
8981 MiB free out of 48676 MiB total
38533 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8981 MiB free out of 48676 MiB total
after cast to cpu
38565 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
17 self_attn.k_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.5578, 1.5840, 1.5218,  ..., 1.4811, 1.4996, 1.5344], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.6770, 0.7048, 0.7073,  ..., 2.2019, 1.6335, 2.1880], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
17 self_attn.v_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.0450, 1.0147, 1.0800,  ..., 1.0646, 1.0705, 1.0441], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.9612, 0.9027, 0.9841,  ..., 1.0400, 1.0164, 1.0498], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
17 self_attn.q_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.4791, 1.4705, 1.4639,  ..., 1.5067, 1.5329, 1.4977], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.7025, 0.6941, 0.6939,  ..., 2.0938, 2.2398, 2.0421], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
17 self_attn.o_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.0241, 1.0220, 0.9856,  ..., 1.0233, 0.9882, 1.0276], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.0365, 1.0348, 1.0110,  ..., 1.0520, 1.0362, 1.0467], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
17 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 77439248
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.07125414785696194 val loss: 0.09987495793029666
8989 MiB free out of 48676 MiB total
epoch 1 loss: 0.023296684565139003 val loss: 0.09901235159486532
8981 MiB free out of 48676 MiB total
epoch 2 loss: 0.015615777138009435 val loss: 0.09848784981295466
8981 MiB free out of 48676 MiB total
epoch 3 loss: 0.012039558048854815 val loss: 0.09808327024802566
8981 MiB free out of 48676 MiB total
epoch 4 loss: 0.009952994492778089 val loss: 0.09770583221688867
8981 MiB free out of 48676 MiB total
epoch 5 loss: 0.008563813509681495 val loss: 0.09740506624802947
8981 MiB free out of 48676 MiB total
epoch 6 loss: 0.007562477727333317 val loss: 0.09708863217383623
8981 MiB free out of 48676 MiB total
epoch 7 loss: 0.006801910441936343 val loss: 0.09677649289369583
8981 MiB free out of 48676 MiB total
epoch 8 loss: 0.006202409262186848 val loss: 0.09647945873439312
8981 MiB free out of 48676 MiB total
epoch 9 loss: 0.005716743846278405 val loss: 0.09618428535759449
8981 MiB free out of 48676 MiB total
38565 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8981 MiB free out of 48676 MiB total
after cast to cpu
38533 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
18 self_attn.k_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.5252, 1.5307, 1.4716,  ..., 1.4937, 1.4664, 1.4463], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.0710, 1.1842, 1.2079,  ..., 2.1710, 2.3075, 2.3298], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
18 self_attn.v_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.0789, 1.0596, 1.0864,  ..., 1.1258, 1.1193, 1.1207], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.2060, 1.2129, 1.2298,  ..., 0.9829, 0.9974, 1.0103], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
18 self_attn.q_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.4467, 1.4891, 1.4806,  ..., 1.4545, 1.4652, 1.4646], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.0691, 1.1817, 1.1950,  ..., 1.9673, 2.0273, 2.0164], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
18 self_attn.o_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.1691, 1.1865, 1.1922,  ..., 0.9777, 0.9909, 0.9968], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.0622, 1.0683, 1.0524,  ..., 1.0709, 1.0725, 1.0642], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
18 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 77439248
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.09316509118070826 val loss: 0.045709883561357856
8989 MiB free out of 48676 MiB total
epoch 1 loss: 0.031872321873379406 val loss: 0.0456015570089221
8981 MiB free out of 48676 MiB total
epoch 2 loss: 0.021937176985375118 val loss: 0.04556703055277467
8981 MiB free out of 48676 MiB total
epoch 3 loss: 0.017205449956236407 val loss: 0.045571957249194384
8981 MiB free out of 48676 MiB total
epoch 4 loss: 0.014382766203198116 val loss: 0.045593110378831625
8981 MiB free out of 48676 MiB total
epoch 5 loss: 0.012470826437493088 val loss: 0.045624409802258015
8981 MiB free out of 48676 MiB total
epoch 6 loss: 0.011075020036514616 val loss: 0.0456540766172111
8981 MiB free out of 48676 MiB total
epoch 7 loss: 0.01000533987826202 val loss: 0.045680651208385825
8981 MiB free out of 48676 MiB total
early stopping
38533 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8981 MiB free out of 48676 MiB total
after cast to cpu
38565 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
19 self_attn.k_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.4944, 1.5200, 1.4792,  ..., 1.4335, 1.4468, 1.4712], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.8642, 1.1287, 1.1200,  ..., 2.0235, 2.0642, 2.0212], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
19 self_attn.v_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.0984, 1.0858, 1.1186,  ..., 1.1277, 1.1063, 1.0958], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.1414, 1.1464, 1.1453,  ..., 1.1046, 1.0892, 1.0853], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
19 self_attn.q_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.4172, 1.4455, 1.4177,  ..., 1.4245, 1.4529, 1.4584], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.9218, 1.1143, 1.1046,  ..., 1.9034, 1.8568, 1.8653], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
19 self_attn.o_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.1254, 1.1186, 1.1293,  ..., 1.0896, 1.0747, 1.0727], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.0850, 1.0817, 1.0711,  ..., 1.0988, 1.0905, 1.0791], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
19 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 77439248
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.07855663675582036 val loss: 0.04254200984723866
8989 MiB free out of 48676 MiB total
epoch 1 loss: 0.02757227175607113 val loss: 0.04253330663777888
8981 MiB free out of 48676 MiB total
epoch 2 loss: 0.019067661713052075 val loss: 0.04249654454179108
8981 MiB free out of 48676 MiB total
epoch 3 loss: 0.015028285801236052 val loss: 0.042463617864996195
8981 MiB free out of 48676 MiB total
epoch 4 loss: 0.01261524836809258 val loss: 0.04243442486040294
8981 MiB free out of 48676 MiB total
epoch 5 loss: 0.01097865640258533 val loss: 0.04241335880942643
8981 MiB free out of 48676 MiB total
epoch 6 loss: 0.009779859148693504 val loss: 0.042388666421175
8981 MiB free out of 48676 MiB total
epoch 7 loss: 0.008856319469487062 val loss: 0.04237360833212733
8981 MiB free out of 48676 MiB total
epoch 8 loss: 0.008119704234559322 val loss: 0.042365210596472025
8981 MiB free out of 48676 MiB total
epoch 9 loss: 0.0075161483146075625 val loss: 0.04236305318772793
8981 MiB free out of 48676 MiB total
38565 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8981 MiB free out of 48676 MiB total
after cast to cpu
38533 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
20 self_attn.k_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.5040, 1.5277, 1.4980,  ..., 1.4534, 1.4546, 1.4831], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.5292, 0.5662, 0.5834,  ..., 1.8763, 2.0239, 2.0330], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
20 self_attn.v_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.1192, 1.0834, 1.1373,  ..., 1.1337, 1.1346, 1.1265], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.0799, 1.1011, 1.0996,  ..., 1.0353, 1.0458, 1.0728], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
20 self_attn.q_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.4390, 1.4916, 1.4176,  ..., 1.4356, 1.4500, 1.4538], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.5440, 0.5997, 0.6395,  ..., 1.7327, 1.8238, 1.5132], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
20 self_attn.o_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.0672, 1.0759, 1.0829,  ..., 1.0529, 1.0531, 1.0652], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.1243, 1.1123, 1.0981,  ..., 1.0983, 1.1009, 1.1329], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
20 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 77439248
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.09988352406071499 val loss: 0.06212218594737351
8989 MiB free out of 48676 MiB total
epoch 1 loss: 0.034210257275844924 val loss: 0.06240528845228255
8981 MiB free out of 48676 MiB total
epoch 2 loss: 0.023373545496724546 val loss: 0.06251878733746707
8981 MiB free out of 48676 MiB total
epoch 3 loss: 0.018305293684534263 val loss: 0.06252778973430395
8981 MiB free out of 48676 MiB total
epoch 4 loss: 0.01529761071287794 val loss: 0.06245523155666888
8981 MiB free out of 48676 MiB total
epoch 5 loss: 0.013269542243506294 val loss: 0.06233031675219536
8981 MiB free out of 48676 MiB total
early stopping
38533 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8981 MiB free out of 48676 MiB total
after cast to cpu
38565 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
21 self_attn.k_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.4550, 1.4713, 1.4318,  ..., 1.4118, 1.4379, 1.4315], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.8120, 0.9468, 0.9507,  ..., 2.0480, 1.7255, 1.9224], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
21 self_attn.v_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.1393, 1.1365, 1.1569,  ..., 1.1675, 1.1572, 1.1763], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.0815, 1.0853, 1.1115,  ..., 1.1556, 1.1620, 1.1422], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
21 self_attn.q_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.4144, 1.4360, 1.4147,  ..., 1.3882, 1.4228, 1.3867], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.8719, 0.9523, 0.9489,  ..., 1.8164, 1.4733, 1.9335], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
21 self_attn.o_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.0900, 1.0793, 1.1069,  ..., 1.1355, 1.1423, 1.1257], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.1539, 1.1705, 1.1258,  ..., 1.1432, 1.1037, 1.1304], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
21 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 77439248
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.11092586992890574 val loss: 0.0771513907238841
8989 MiB free out of 48676 MiB total
epoch 1 loss: 0.03954083348799031 val loss: 0.07701198430731893
8981 MiB free out of 48676 MiB total
epoch 2 loss: 0.02755214542412432 val loss: 0.07697686366736889
8981 MiB free out of 48676 MiB total
epoch 3 loss: 0.02173500706703635 val loss: 0.0769776999950409
8981 MiB free out of 48676 MiB total
epoch 4 loss: 0.018221041085780598 val loss: 0.07698975224047899
8981 MiB free out of 48676 MiB total
epoch 5 loss: 0.015828867311938666 val loss: 0.07700931606814265
8981 MiB free out of 48676 MiB total
epoch 6 loss: 0.014075561713980278 val loss: 0.07702436298131943
8981 MiB free out of 48676 MiB total
epoch 7 loss: 0.01272652683837805 val loss: 0.07703735353425145
8981 MiB free out of 48676 MiB total
early stopping
38565 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8981 MiB free out of 48676 MiB total
after cast to cpu
38533 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
22 self_attn.k_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.4745, 1.4774, 1.4902,  ..., 1.4655, 1.4788, 1.4825], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.4680, 1.4894, 1.5025,  ..., 1.2110, 1.9322, 1.9817], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
22 self_attn.v_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.1536, 1.1395, 1.1547,  ..., 1.1419, 1.1616, 1.1692], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.2635, 1.2563, 1.2672,  ..., 1.2306, 1.2221, 1.2212], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
22 self_attn.q_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.4339, 1.4647, 1.4494,  ..., 1.4377, 1.4301, 1.4364], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.4712, 1.4866, 1.5007,  ..., 1.9845, 1.9318, 2.1921], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
22 self_attn.o_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.2228, 1.2182, 1.2207,  ..., 1.1587, 1.1975, 1.1741], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.1274, 1.1286, 1.1356,  ..., 1.1232, 1.1369, 1.1320], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
22 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 77439248
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.11877301913045812 val loss: 0.06934704212471843
8989 MiB free out of 48676 MiB total
epoch 1 loss: 0.04268140693602618 val loss: 0.06995857041329145
8981 MiB free out of 48676 MiB total
epoch 2 loss: 0.029890306366723962 val loss: 0.0703164185397327
8981 MiB free out of 48676 MiB total
epoch 3 loss: 0.023725631690467708 val loss: 0.07058677589520812
8981 MiB free out of 48676 MiB total
epoch 4 loss: 0.019990188295196276 val loss: 0.07080666301771998
8981 MiB free out of 48676 MiB total
epoch 5 loss: 0.017435649264371023 val loss: 0.07098104758188128
8981 MiB free out of 48676 MiB total
early stopping
38533 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8981 MiB free out of 48676 MiB total
after cast to cpu
38565 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
23 self_attn.k_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.4515, 1.4864, 1.4677,  ..., 1.4692, 1.4491, 1.4700], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.7387, 0.7335, 0.7738,  ..., 1.5745, 1.3933, 1.6235], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
23 self_attn.v_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.2002, 1.1744, 1.2148,  ..., 1.1841, 1.2298, 1.2296], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.1003, 1.0871, 1.1005,  ..., 1.1547, 1.1473, 1.1477], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
23 self_attn.q_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.4343, 1.4455, 1.4322,  ..., 1.4610, 1.4012, 1.4335], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.7342, 0.7344, 0.7733,  ..., 1.6962, 1.9142, 1.6451], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
23 self_attn.o_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.0890, 1.0794, 1.0946,  ..., 1.1393, 1.1247, 1.1352], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.1980, 1.2021, 1.1762,  ..., 1.1868, 1.1935, 1.1843], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
23 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 77439248
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.16328551855986007 val loss: 0.09563933545723557
8989 MiB free out of 48676 MiB total
epoch 1 loss: 0.05906956229591742 val loss: 0.09555639931932092
8981 MiB free out of 48676 MiB total
epoch 2 loss: 0.04215710946300533 val loss: 0.09557969169691205
8981 MiB free out of 48676 MiB total
epoch 3 loss: 0.03380899969488382 val loss: 0.09558531967923045
8981 MiB free out of 48676 MiB total
epoch 4 loss: 0.02863547846209258 val loss: 0.09563147043809295
8981 MiB free out of 48676 MiB total
epoch 5 loss: 0.0250479019669001 val loss: 0.09569487534463406
8981 MiB free out of 48676 MiB total
epoch 6 loss: 0.022376965280272998 val loss: 0.0957779516465962
8981 MiB free out of 48676 MiB total
early stopping
38565 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8981 MiB free out of 48676 MiB total
after cast to cpu
38533 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
24 self_attn.k_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.4107, 1.4404, 1.4210,  ..., 1.3930, 1.3714, 1.3905], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.3660, 1.3716, 1.4155,  ..., 1.6762, 1.7466, 1.7519], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
24 self_attn.v_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.2120, 1.1798, 1.1826,  ..., 1.2159, 1.1909, 1.2084], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.2397, 1.2433, 1.2546,  ..., 1.1785, 1.1836, 1.1816], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
24 self_attn.q_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.3881, 1.3955, 1.3956,  ..., 1.4213, 1.4150, 1.4089], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.3741, 1.3726, 1.4193,  ..., 1.6439, 1.7088, 1.6957], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
24 self_attn.o_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.2159, 1.2214, 1.2327,  ..., 1.1572, 1.1644, 1.1607], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.1998, 1.1973, 1.1921,  ..., 1.1950, 1.1950, 1.1825], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
24 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 77439248
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.14143395840073936 val loss: 0.10594484442844987
8989 MiB free out of 48676 MiB total
epoch 1 loss: 0.05166221245599445 val loss: 0.10544660221785307
8981 MiB free out of 48676 MiB total
epoch 2 loss: 0.03707865849719383 val loss: 0.10517368232831359
8981 MiB free out of 48676 MiB total
epoch 3 loss: 0.029933581026853062 val loss: 0.10500453133136034
8981 MiB free out of 48676 MiB total
epoch 4 loss: 0.025515464345517103 val loss: 0.10488129546865821
8981 MiB free out of 48676 MiB total
epoch 5 loss: 0.022452907905972097 val loss: 0.10482925875112414
8981 MiB free out of 48676 MiB total
epoch 6 loss: 0.02016992209973978 val loss: 0.10479558305814862
8981 MiB free out of 48676 MiB total
epoch 7 loss: 0.01838337617664365 val loss: 0.10481058480218053
8981 MiB free out of 48676 MiB total
epoch 8 loss: 0.016938114349613898 val loss: 0.1048387149348855
8981 MiB free out of 48676 MiB total
epoch 9 loss: 0.015739505317469593 val loss: 0.10488697467371821
8981 MiB free out of 48676 MiB total
38533 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8981 MiB free out of 48676 MiB total
after cast to cpu
38565 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
25 self_attn.k_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.4192, 1.4690, 1.4127,  ..., 1.4213, 1.3962, 1.4222], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.5660, 0.5315, 0.5776,  ..., 1.6343, 1.7744, 1.7085], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
25 self_attn.v_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.2550, 1.2244, 1.2299,  ..., 1.2444, 1.2465, 1.2497], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.3124, 1.3050, 1.3245,  ..., 1.2849, 1.3018, 1.2839], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
25 self_attn.q_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.4169, 1.3969, 1.4229,  ..., 1.4106, 1.3910, 1.4140], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.5655, 0.5461, 0.5966,  ..., 1.6563, 1.7658, 1.6648], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
25 self_attn.o_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.2653, 1.2502, 1.2811,  ..., 1.2720, 1.2811, 1.2801], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.2426, 1.2505, 1.2296,  ..., 1.2162, 1.2488, 1.2485], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
25 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 77439248
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.19616783084347844 val loss: 0.15197242982685566
8989 MiB free out of 48676 MiB total
epoch 1 loss: 0.07070075543015264 val loss: 0.15056891087442636
8981 MiB free out of 48676 MiB total
epoch 2 loss: 0.05078364706423599 val loss: 0.1496987733989954
8981 MiB free out of 48676 MiB total
epoch 3 loss: 0.0410396461666096 val loss: 0.1491265669465065
8981 MiB free out of 48676 MiB total
epoch 4 loss: 0.03498672606656328 val loss: 0.14862980600446463
8981 MiB free out of 48676 MiB total
epoch 5 loss: 0.030786260685999878 val loss: 0.14818865340203047
8981 MiB free out of 48676 MiB total
epoch 6 loss: 0.02765150736377109 val loss: 0.14779842365533113
8981 MiB free out of 48676 MiB total
epoch 7 loss: 0.025197267823386937 val loss: 0.14745537657290697
8981 MiB free out of 48676 MiB total
epoch 8 loss: 0.02321094374929089 val loss: 0.14718614984303713
8981 MiB free out of 48676 MiB total
epoch 9 loss: 0.021563687558227684 val loss: 0.14697055332362652
8981 MiB free out of 48676 MiB total
38565 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8981 MiB free out of 48676 MiB total
after cast to cpu
38533 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
26 self_attn.k_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.3885, 1.4287, 1.3924,  ..., 1.4006, 1.3950, 1.3818], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.1274, 1.1214, 1.1409,  ..., 1.8750, 1.8001, 1.7344], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
26 self_attn.v_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.2695, 1.2609, 1.2552,  ..., 1.2922, 1.2361, 1.2380], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.3299, 1.3174, 1.3327,  ..., 1.2755, 1.2536, 1.2610], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
26 self_attn.q_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.3872, 1.3984, 1.4091,  ..., 1.3887, 1.3689, 1.4181], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.1334, 1.1177, 1.1413,  ..., 0.8847, 1.7802, 1.8081], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
26 self_attn.o_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.3113, 1.3014, 1.3114,  ..., 1.2673, 1.2453, 1.2514], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.2578, 1.2535, 1.2387,  ..., 1.2614, 1.2616, 1.2322], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
26 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 77439248
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.1934903368819505 val loss: 0.1503591574728489
8989 MiB free out of 48676 MiB total
epoch 1 loss: 0.07262055005412549 val loss: 0.15006451215595007
8981 MiB free out of 48676 MiB total
epoch 2 loss: 0.05307522229850292 val loss: 0.15013120230287313
8981 MiB free out of 48676 MiB total
epoch 3 loss: 0.04341792411287315 val loss: 0.1503019966185093
8981 MiB free out of 48676 MiB total
epoch 4 loss: 0.037392182610346936 val loss: 0.15040664561092854
8981 MiB free out of 48676 MiB total
epoch 5 loss: 0.033200963836861774 val loss: 0.1505279652774334
8981 MiB free out of 48676 MiB total
epoch 6 loss: 0.03006047455710359 val loss: 0.15061792824417353
8981 MiB free out of 48676 MiB total
early stopping
38533 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8981 MiB free out of 48676 MiB total
after cast to cpu
38565 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
27 self_attn.k_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.4432, 1.4596, 1.4588,  ..., 1.4564, 1.4578, 1.4376], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.2920, 1.3262, 1.3285,  ..., 1.5665, 1.5050, 1.6066], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
27 self_attn.v_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.2719, 1.2507, 1.2797,  ..., 1.2662, 1.2425, 1.2610], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.2851, 1.2833, 1.2784,  ..., 1.4586, 1.4483, 1.4834], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
27 self_attn.q_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.4110, 1.4780, 1.4380,  ..., 1.4506, 1.4507, 1.4779], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.3049, 1.3313, 1.3302,  ..., 1.5344, 1.4997, 1.0326], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
27 self_attn.o_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.2766, 1.2844, 1.2753,  ..., 1.5237, 1.4909, 1.5521], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.2326, 1.2740, 1.2605,  ..., 1.2930, 1.2936, 1.2707], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
27 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 77439248
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.2419182711164467 val loss: 0.2615882661193609
8989 MiB free out of 48676 MiB total
epoch 1 loss: 0.0841284615744371 val loss: 0.26200726721435785
8981 MiB free out of 48676 MiB total
epoch 2 loss: 0.060406777847674675 val loss: 0.26143444050103426
8981 MiB free out of 48676 MiB total
epoch 3 loss: 0.04869096138281748 val loss: 0.2606367403641343
8981 MiB free out of 48676 MiB total
epoch 4 loss: 0.04142560379114002 val loss: 0.25937916804105043
8981 MiB free out of 48676 MiB total
epoch 5 loss: 0.03643161940271966 val loss: 0.257943999953568
8981 MiB free out of 48676 MiB total
epoch 6 loss: 0.032733285494032316 val loss: 0.25647463742643595
8981 MiB free out of 48676 MiB total
epoch 7 loss: 0.029854917418560944 val loss: 0.2549180993810296
8981 MiB free out of 48676 MiB total
epoch 8 loss: 0.027535862209333573 val loss: 0.2533480813726783
8981 MiB free out of 48676 MiB total
epoch 9 loss: 0.02561759096715832 val loss: 0.25196044985204935
8981 MiB free out of 48676 MiB total
38565 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8981 MiB free out of 48676 MiB total
after cast to cpu
38533 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
28 self_attn.k_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.4238, 1.4306, 1.4203,  ..., 1.4233, 1.4031, 1.4396], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.7822, 0.7447, 0.7523,  ..., 1.7620, 1.7353, 1.7418], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
28 self_attn.v_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.2854, 1.2936, 1.3251,  ..., 1.2990, 1.3177, 1.3131], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.4762, 1.4871, 1.5179,  ..., 1.4363, 1.4516, 1.4401], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
28 self_attn.q_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.3899, 1.4050, 1.4191,  ..., 1.3927, 1.3823, 1.4056], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.6978, 0.7669, 0.7800,  ..., 1.5473, 1.6398, 1.7314], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
28 self_attn.o_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.5029, 1.4821, 1.5049,  ..., 1.4681, 1.4612, 1.4610], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.2865, 1.2987, 1.3002,  ..., 1.3400, 1.3499, 1.3088], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
28 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 77439248
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.2786483463132754 val loss: 0.5983279012143612
8989 MiB free out of 48676 MiB total
epoch 1 loss: 0.10057797076296993 val loss: 0.6024248152971268
8981 MiB free out of 48676 MiB total
epoch 2 loss: 0.07346962296287529 val loss: 0.6046369150280952
8981 MiB free out of 48676 MiB total
epoch 3 loss: 0.06008083665801678 val loss: 0.6067528985440731
8981 MiB free out of 48676 MiB total
epoch 4 loss: 0.05171853880165145 val loss: 0.609246451407671
8981 MiB free out of 48676 MiB total
epoch 5 loss: 0.04592159732419532 val loss: 0.6117176655679941
8981 MiB free out of 48676 MiB total
early stopping
38533 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8981 MiB free out of 48676 MiB total
after cast to cpu
38565 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
29 self_attn.k_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.3682, 1.4166, 1.3816,  ..., 1.4090, 1.3446, 1.3857], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.9166, 0.9375, 0.9786,  ..., 1.5665, 1.7960, 1.9091], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
29 self_attn.v_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.3328, 1.2889, 1.3329,  ..., 1.2950, 1.3074, 1.3143], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.2784, 1.2410, 1.2657,  ..., 1.2642, 1.2732, 1.2683], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
29 self_attn.q_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.3790, 1.3933, 1.3722,  ..., 1.3567, 1.3229, 1.3394], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.8758, 0.9288, 0.9866,  ..., 1.5234, 1.7083, 1.3679], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
29 self_attn.o_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.2741, 1.2564, 1.2702,  ..., 1.2577, 1.2643, 1.2598], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.3297, 1.2958, 1.2935,  ..., 1.3316, 1.3571, 1.3266], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
29 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 77439248
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.24847447028150782 val loss: 5.006660968065262
8989 MiB free out of 48676 MiB total
epoch 1 loss: 0.08856187135097571 val loss: 5.112448833882809
8981 MiB free out of 48676 MiB total
epoch 2 loss: 0.06425701828266028 val loss: 5.039790824055672
8981 MiB free out of 48676 MiB total
epoch 3 loss: 0.05208737897919491 val loss: 4.927007749676704
8981 MiB free out of 48676 MiB total
epoch 4 loss: 0.04449362315062899 val loss: 4.808376342058182
8981 MiB free out of 48676 MiB total
epoch 5 loss: 0.03925806385814212 val loss: 4.692788682878017
8981 MiB free out of 48676 MiB total
epoch 6 loss: 0.03537840279750526 val loss: 4.576319590210915
8981 MiB free out of 48676 MiB total
epoch 7 loss: 0.032381034674472176 val loss: 4.463940881192684
8981 MiB free out of 48676 MiB total
epoch 8 loss: 0.030019213387276977 val loss: 4.32183700799942
8981 MiB free out of 48676 MiB total
epoch 9 loss: 0.027968239191977773 val loss: 4.21208356320858
8981 MiB free out of 48676 MiB total
38565 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8981 MiB free out of 48676 MiB total
after cast to cpu
38533 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
30 self_attn.k_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.4019, 1.4178, 1.3976,  ..., 1.3864, 1.3602, 1.3972], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.0182, 1.0110, 1.0278,  ..., 1.4493, 1.4746, 1.4525], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
30 self_attn.v_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.3517, 1.3246, 1.3371,  ..., 1.3470, 1.3623, 1.3652], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.3536, 1.3972, 1.3478,  ..., 1.4740, 1.4633, 1.5104], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
30 self_attn.q_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.3731, 1.4261, 1.3994,  ..., 1.3547, 1.3360, 1.3441], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.0392, 1.0339, 1.0422,  ..., 1.4349, 1.4045, 1.4240], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
30 self_attn.o_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.3750, 1.4141, 1.3648,  ..., 1.4676, 1.4893, 1.4816], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.3430, 1.3294, 1.3135,  ..., 1.3429, 1.3585, 1.3574], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
30 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 77439248
n accumulation steps: 1
n batches: 128
epoch 0 loss: 2.375995184353087 val loss: inf
8989 MiB free out of 48676 MiB total
epoch 1 loss: 0.17253357492154464 val loss: inf
8981 MiB free out of 48676 MiB total
epoch 2 loss: 0.10801186313619837 val loss: inf
8981 MiB free out of 48676 MiB total
epoch 3 loss: 0.08655134588479996 val loss: inf
8981 MiB free out of 48676 MiB total
epoch 4 loss: 0.07295733137289062 val loss: inf
8981 MiB free out of 48676 MiB total
epoch 5 loss: 0.07144312410673592 val loss: inf
8981 MiB free out of 48676 MiB total
epoch 6 loss: 0.0579118602181552 val loss: inf
8981 MiB free out of 48676 MiB total
epoch 7 loss: 0.05223513371311128 val loss: inf
8981 MiB free out of 48676 MiB total
epoch 8 loss: 0.04715408832998946 val loss: inf
8981 MiB free out of 48676 MiB total
epoch 9 loss: 0.04298739376827143 val loss: inf
8981 MiB free out of 48676 MiB total
38533 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8981 MiB free out of 48676 MiB total
after cast to cpu
38565 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
31 self_attn.k_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.4408, 1.4562, 1.4368,  ..., 1.4340, 1.4106, 1.4039], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.9789, 0.9707, 1.0469,  ..., 1.6500, 1.6098, 1.6455], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
31 self_attn.v_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.1832, 1.2165, 1.2963,  ..., 1.1931, 1.2608, 1.2214], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.1387, 1.1403, 1.1530,  ..., 1.1925, 1.2135, 1.2031], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
31 self_attn.q_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.3578, 1.4003, 1.4142,  ..., 1.4286, 1.3539, 1.3484], device='cuda:7')]
sparse_frac =  0.01
[tensor([0.9185, 0.9745, 1.0542,  ..., 1.5913, 1.5933, 1.6090], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
31 self_attn.o_proj
Pruning ...
using low rank =  256
0.01 ['weight'] 0.01 ['weight']
[tensor([1.2308, 1.2274, 1.2312,  ..., 1.2517, 1.2610, 1.2420], device='cuda:7')]
sparse_frac =  0.01
[tensor([1.1774, 1.2117, 1.2020,  ..., 1.2123, 1.2554, 1.2226], device='cuda:7')]
sparse_frac =  0.01
row mask =  4052 column mask =  4052
31 mlp
Pruning ...
using importances
keeping 5508 top channels and 0 bottom channels
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp']
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 77439248
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.1538737741066143 val loss: 30.110501170158386
8989 MiB free out of 48676 MiB total
epoch 1 loss: 0.05295773531543091 val loss: 29.352400422096252
8981 MiB free out of 48676 MiB total
epoch 2 loss: 0.03737328691931907 val loss: 28.837207913398743
8981 MiB free out of 48676 MiB total
epoch 3 loss: 0.029645783557498362 val loss: 28.45669174194336
8981 MiB free out of 48676 MiB total
epoch 4 loss: 0.025086529421969317 val loss: 28.23447549343109
8981 MiB free out of 48676 MiB total
epoch 5 loss: 0.021747885111835785 val loss: 28.024678826332092
8981 MiB free out of 48676 MiB total
epoch 6 loss: 0.019513487044605426 val loss: 27.998186349868774
8981 MiB free out of 48676 MiB total
epoch 7 loss: 0.01769762775802519 val loss: 27.85199773311615
8981 MiB free out of 48676 MiB total
epoch 8 loss: 0.016206334192247596 val loss: 27.890080094337463
8981 MiB free out of 48676 MiB total
epoch 9 loss: 0.015113214190932922 val loss: 27.583515405654907
8981 MiB free out of 48676 MiB total
38565 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
8981 MiB free out of 48676 MiB total
after cast to cpu
38533 MiB free out of 48676 MiB total
Total bits: tensor(39652999168, device='cuda:7') Total params: 6476005376
average bits per value: tensor(6.1231, device='cuda:7')
total time taken: 7549.0129935741425
Loading tokenizer for meta-llama/Llama-2-7b-hf
/home/lliu/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Dataset: wikitext2
Evaluating ...
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
Perplexity: 200994.125000
