{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "import cvxpy as cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Quantizer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 initial_codebook:torch.Tensor,\n",
    "                 H: torch.Tensor,\n",
    "                 Weights: torch.Tensor,\n",
    "                 ):\n",
    "        \"\"\"Vector Quantizer\n",
    "\n",
    "        Args:\n",
    "            initial_codebook (torch.Tensor): the initial codebook (n,k)\n",
    "            H (torch.Tensor): the Hessian of shape (n,n)\n",
    "            Weights (torch.Tensor): weights of shape (n,n)\n",
    "        \"\"\"\n",
    "        super(Quantizer, self).__init__()\n",
    "\n",
    "        # self.codebook = nn.Parameter(initial_codebook, requires_grad=True)\n",
    "        self.codebook = nn.Parameter(initial_codebook)\n",
    "        self.H = H\n",
    "        self.Weights = Weights\n",
    "        n,k = initial_codebook.shape\n",
    "        #randomly initialize the initial assignments through xavier uniform initialization\n",
    "        #a matrix of shape (k,n)\n",
    "\n",
    "        #logistic initialization\n",
    "        initial_assignments = torch.empty(k,n,device = initial_codebook.device,\n",
    "                                                            dtype = initial_codebook.dtype\n",
    "                                                            ).uniform_(0,1)\n",
    "        \n",
    "\n",
    "        self.initial_assignments = nn.Parameter(#take the logistic of the initial assignments\n",
    "                                                torch.log(initial_assignments / (1 - initial_assignments)),\n",
    "                                                requires_grad = True\n",
    "                                                )\n",
    "\n",
    "        self.activation = nn.PReLU().to(initial_codebook.device)\n",
    "        # self.initial_assignments = torch.empty(n,k,device = initial_codebook.device,\n",
    "        #                                                     dtype = initial_codebook.dtype\n",
    "        #                                                     ).uniform_(-1,1)\n",
    "\n",
    "        print(self.H.dtype,self.Weights.dtype,self.codebook.dtype,self.initial_assignments.dtype)\n",
    "\n",
    "\n",
    "    def sparse_penalty(self, assignments: torch.Tensor, target_sparsity:float, \n",
    "                       greater_penalty:float):\n",
    "        \"\"\"Penalty function for the assignments\n",
    "\n",
    "        Args:\n",
    "            assignments (torch.Tensor): the assignments of shape (k,n)\n",
    "                assumed to be past through \n",
    "            target_sparsity (float): the target sparsity of the assignments, sparsity is column wise\n",
    "            greater_penalty (float): the penalty for greater sparsity\n",
    "            lesser_penalty (float): the penalty for lesser sparsity\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: the penalty\n",
    "        \"\"\"\n",
    "        sparsity = torch.sum(assignments, dim = 0) #shape (n,)\n",
    "        sparsity = sparsity / assignments.shape[0]\n",
    "        penalty = torch.relu(sparsity - target_sparsity)\n",
    "        return torch.sum(greater_penalty * penalty), torch.sum(assignments)\n",
    "\n",
    "    def penalty(self, assignments, beta, penalty_1_weight,\n",
    "                penalty_2_weight):\n",
    "        \"\"\"Penalty function for the assignments\n",
    "\n",
    "        Args:\n",
    "            assignments (torch.Tensor): the assignments of shape (k,n)\n",
    "                assumed to be past through \n",
    "            beta (float): a hyperparameter for the first penalty, this penalty\n",
    "            is 1- |2*a_ij - 1|**(beta)\n",
    "\n",
    "            penalty_1_weight (float): the weight of the first penalty\n",
    "            penalty_2_weight (float): the weight of the second penalty\n",
    "            this penalty is the sum of the assignments, therefore enforceing \n",
    "            sparsity\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: the penalty\n",
    "        \"\"\"\n",
    "        # assert torch.isfinite(torch.abs(2*assignments - 1)).all()\n",
    "        penalty_1 = torch.sum(1 - torch.abs(2*assignments - 1)**beta)\n",
    "        # print(penalty_1)\n",
    "        # penalty_2, sparsity = self.sparse_penalty(assignments, 0.5, 1)\n",
    "        # return penalty_1_weight * penalty_1 + penalty_2_weight * penalty_2, penalty_1, sparsity\n",
    "        penalty_2 = torch.sum(assignments)\n",
    "        # print(penalty_2)\n",
    "        return penalty_1_weight * penalty_1 + penalty_2_weight * penalty_2, penalty_1, penalty_2\n",
    "    \n",
    "    def forward(self,beta,penalty_1_weight,\n",
    "                penalty_2_weight):\n",
    "        \"\"\"Forward pass of the quantizer\n",
    "\n",
    "        Args:\n",
    "            beta (float): a hyperparameter for the first penalty, this penalty\n",
    "            is 1- |2*a_ij - 1|**(beta)\n",
    "\n",
    "            penalty_1_weight (float): the weight of the first penalty\n",
    "            penalty_2_weight (float): the weight of the second penalty\n",
    "            this penalty is the sum of the assignments, therefore enforceing \n",
    "            sparsity\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: the quantized codebook\n",
    "        \"\"\"\n",
    "        #get the assignments\n",
    "        assignments = torch.sigmoid(self.initial_assignments)*1.2 - 0.1\n",
    "        # print(assignments)\n",
    "        # assert torch.isfinite(assignments).all()\n",
    "        assignments = torch.clip(assignments,0,1)\n",
    "        # assert torch.isfinite(assignments).all()\n",
    "\n",
    "        #get the quantized_weights\n",
    "        quantized_weights = self.codebook @ assignments\n",
    "        assert torch.isfinite(quantized_weights).all()\n",
    "        #get the difference between the quantized weights and the original weights\n",
    "        diff = quantized_weights - self.Weights\n",
    "\n",
    "        #get the loss\n",
    "        loss = torch.einsum('ik,kl,il->', diff, self.H, diff)\n",
    "        # assert torch.isclose(loss,torch.sum(diff**2))\n",
    "        # loss = torch.sum(diff**2)\n",
    "        #get the penalty\n",
    "        penalty,p1,p2 = self.penalty(assignments, beta, penalty_1_weight, penalty_2_weight)\n",
    "        # assert torch.isfinite(penalty)\n",
    "        return loss + penalty, loss, penalty,p1,p2\n",
    "    \n",
    "    def get_rounded_error(self):\n",
    "\n",
    "        #get the assignments\n",
    "        assignments = torch.sigmoid(self.initial_assignments)*1.2 - 0.1\n",
    "        assignments = torch.clip(assignments,0,1)\n",
    "        assignments = torch.round(assignments)  \n",
    "        #get the quantized_weights\n",
    "        quantized_weights = self.codebook @ assignments\n",
    "\n",
    "        #get the difference between the quantized weights and the original weights\n",
    "        diff = quantized_weights - self.Weights\n",
    "\n",
    "        #get the loss\n",
    "        # loss = torch.einsum('ik,kl,il->', diff, self.H, diff)\n",
    "        # assert torch.isclose(loss,torch.sum(diff**2))\n",
    "        # loss = torch.sum(diff**2)\n",
    "        loss = torch.einsum('ik,kl,il->', diff, self.H, diff)\n",
    "        return loss\n",
    "    \n",
    "    def update_codebook(self):\n",
    "        raise NotImplementedError\n",
    "        with torch.no_grad():\n",
    "            assignments = torch.sigmoid(self.initial_assignments)\n",
    "\n",
    "            #cast to numpy\n",
    "            assignments = assignments.cpu().numpy()\n",
    "            weights = self.Weights.cpu().numpy()\n",
    "            H = self.H.cpu().numpy()\n",
    "\n",
    "            #use cvxpy to solve the optimization problem\n",
    "            k,n = assignments.shape\n",
    "            codebook = cp.Variable(self.codebook.shape)\n",
    "            objective = cp.Minimize(cp.trace((weights - codebook @ assignments).T @ H @ (weights - codebook @ assignments)))\n",
    "            constraints = []\n",
    "            \n",
    "            #no constraints\n",
    "            prob = cp.Problem(objective, constraints)\n",
    "            prob.solve()\n",
    "            self.codebook = torch.tensor(codebook.value,device = self.codebook.device,dtype = self.codebook.dtype)\n",
    "            \n",
    "        return self.codebook\n",
    "\n",
    "    def update_assignments(self):\n",
    "        raise NotImplementedError\n",
    "        #closed form update \n",
    "        with torch.no_grad():\n",
    "            # assignments = self.initial_assignments\n",
    "            # print(assignments.shape)\n",
    "            #update formula for the assignments is\n",
    "            #A = W^T C (C^T C)^-1\n",
    "\n",
    "            codebook_inner_product = self.codebook.T @ self.H @ self.codebook\n",
    "            inverse = torch.inverse(codebook_inner_product)\n",
    "\n",
    "            self.initial_assignments = self.Weights @ self.H @ self.codebook @ inverse\n",
    "            print(self.initial_assignments)\n",
    "            assert torch.all(torch.isfinite(self.initial_assignments))\n",
    "        \n",
    "        #take the mean of the assignments\n",
    "        mean = torch.mean(torch.abs(self.initial_assignments))\n",
    "        #rescale the assignments\n",
    "        self.initial_assignments = self.initial_assignments / mean\n",
    "        #scale up the codebook\n",
    "        self.codebook = self.codebook * mean\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.load(\"/home/lliu/huffman/test/original_weights.pt\")\n",
    "weights = data['weights'].float().to(torch.device(\"cuda:6\"))\n",
    "H = data['H'].float().to(torch.device(\"cuda:6\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def BetaScheduler_(i, T, scale, bias):\n",
    "\n",
    "#     signal = (np.sin(((i%T)/T-0.5)*np.pi)+1)*0.5\n",
    "\n",
    "#     return scale * signal + bias\n",
    "\n",
    "# def BetaScheduler_(i, T, scale, bias):\n",
    "\n",
    "#     signal = (np.sin(((i)/T)*np.pi)+1) - 1\n",
    "\n",
    "#     return scale * signal + bias\n",
    "\n",
    "def BetaScheduler_(i, T, scale, bias):\n",
    "\n",
    "    # signal = (np.sin(((i%T)/T-0.5)*np.pi)+1)*0.5\n",
    "    signal = (T-i)/T\n",
    "    # signal = (np.cos(((i)/T)*np.pi*2))*0.5+0.5\n",
    "    return scale * signal + bias\n",
    "\n",
    "# def BetaScheduler_(i, T, scale, bias):\n",
    "\n",
    "#     # signal = (np.sin(((i%T)/T-0.5)*np.pi)+1)*0.5\n",
    "#     # signal = abs((i-T/2)*2/T)**5\n",
    "#     signal = (i/T)**5\n",
    "#     return scale * signal + bias\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "k = 256\n",
    "\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "#try to initalize the codebook from the eigenvectors of the weights\n",
    "eigenvalues, eigenvectors = torch.linalg.eigh(weights)\n",
    "print(eigenvalues.shape, eigenvectors.shape)\n",
    "# eigenvectors = eigenvalues.vectors\n",
    "indexs = torch.argsort(eigenvalues,descending=True)[:k]\n",
    "#renormalize the eigenvectors\n",
    "initial_codebook = eigenvectors[indexs,:].T / k\n",
    "print(initial_codebook.shape)\n",
    "quantizer = Quantizer(\n",
    "                        # weights[indexs,:].T/k\n",
    "                        initial_codebook\n",
    "                    #   , torch.eye(H.shape[0],device = H.device,dtype = H.dtype)\n",
    "                    , H\n",
    "                      , weights)\n",
    "\n",
    "\n",
    "n_iters = 10000\n",
    "n_subiters = 10000\n",
    "n_subiters_multiple = 1.1\n",
    "\n",
    "beta_min, beta_max = 1, 4\n",
    "\n",
    "penalty_1_weight = 1\n",
    "penalty_2_weight = 1\n",
    "penalty_2_multiple = 1 \n",
    " \n",
    "beta = 5 \n",
    "penalty_2_weight = 0.01\n",
    "\n",
    "losses = []\n",
    "rounded_errors = []\n",
    "i_s = []\n",
    "\n",
    "errors = []\n",
    "penalties = []\n",
    "binary_penalty = []\n",
    "sparsity_penalty = []\n",
    "betas = []\n",
    "penalty_2_weights = []\n",
    "lrs = []\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam([{'params': quantizer.codebook, 'lr': 5e-4},\n",
    "                                {'params': quantizer.initial_assignments, 'lr': 1e-3}])\n",
    "# scheduler_up = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.95)\n",
    "# scheduler_down = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=1.005)\n",
    "\n",
    "j = 0\n",
    "prev_loss = float('inf')\n",
    "patience_up = 25\n",
    "patience_down = 25\n",
    "impatience_1 = 0\n",
    "impatience_2 = 0\n",
    "\n",
    "\n",
    "for i in tqdm.tqdm(range(n_iters)):\n",
    "    beta = BetaScheduler_(j, n_subiters, beta_max - beta_min, beta_min)\n",
    "    # penalty_2_weight = BetaScheduler_(j, n_subiters*penalty_2_multiple, 0.2, 0.0)\n",
    "    # print(beta)\n",
    "    # print(beta)\n",
    "    loss, loss1, loss2, p1, p2 = quantizer(beta, penalty_1_weight, penalty_2_weight)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # print(loss)\n",
    "\n",
    "    assert torch.all(torch.isfinite(loss))\n",
    "\n",
    "    losses.append(loss.item())\n",
    "    errors.append(loss1.item())\n",
    "    penalties.append(loss2.item())\n",
    "    binary_penalty.append(p1.item())\n",
    "    sparsity_penalty.append(p2.item())\n",
    "    betas.append(beta)\n",
    "    penalty_2_weights.append(penalty_2_weight)\n",
    "    j+=1\n",
    "    if i % 1000 == 0:\n",
    "        # print(f\"Loss: {loss.item()}, Error: {loss1.item()/torch.sum(torch.abs(weights**2))}, Loss2: {loss2.item()}, P1: {p1.item()}, sparsity: {p2.item()/(H.shape[0]*k)}, beta: {beta}, penalty_2_weight: {penalty_2_weight}\")\n",
    "        print(f\"Loss: {loss.item()}, Error: {loss1.item()/H.shape[0]}, Loss2: {loss2.item()}, P1: {p1.item()}, sparsity: {p2.item()/(H.shape[0]*k)}, beta: {beta}, penalty_2_weight: {penalty_2_weight}\")\n",
    "        # wandb.log({\"Loss\": loss.item(), \"Error\": loss1.item()/H.shape[0], \"Penalty\": loss2.item(), \"BinaryPenalty\": p1.item(), \"Sparsity\": p2.item()/(H.shape[0]**2),\n",
    "        #            \"penalty_2_weight\": penalty_2_weight, \"beta\": beta})\n",
    "        fig = plt.figure()\n",
    "        plt.hist(np.clip(torch.sigmoid(quantizer.initial_assignments).detach().cpu().numpy().flatten()*1.2-0.1,0,1), bins = 100,density = True)\n",
    "        plt.xlim(0,1)\n",
    "        plt.ylim(1e-5,1)\n",
    "        plt.yscale('log')\n",
    "        plt.title(\"Assignments\")\n",
    "        plt.savefig(f\"test/assignments{i}.png\")  \n",
    "        plt.close(fig)  \n",
    "\n",
    "        rounded_error = quantizer.get_rounded_error()\n",
    "        rounded_errors.append(rounded_error.item())\n",
    "        i_s.append(i)\n",
    "        # print(f\"Rounded Error: {rounded_error.item()/torch.sum(torch.abs(weights**2))}\")\n",
    "        print(f\"Rounded Error: {rounded_error.item()/H.shape[0]}\")\n",
    "    if j % n_subiters == n_subiters - 1:\n",
    "        n_subiters = int(n_subiters * n_subiters_multiple)\n",
    "        j = 0\n",
    "        impatience_1 = 0\n",
    "\n",
    "    # if loss1.item() > prev_loss:\n",
    "    #     impatience_1 += 1\n",
    "    #     if impatience_1 > patience_up:\n",
    "    #         scheduler_up.step()\n",
    "    #         # print(\"Decreasing learning rate to \", scheduler_up.get_last_lr())\n",
    "    #         impatience_1 = 0\n",
    "    # if loss1.item() < prev_loss:\n",
    "    #     impatience_2 += 1\n",
    "    #     if impatience_2 > patience_down:\n",
    "    #         scheduler_down.step()\n",
    "    #         # print(\"Increasing learning rate to \", scheduler_down.get_last_lr())\n",
    "    #         impatience_2 = 0 \n",
    "\n",
    "    prev_loss = loss1.item()\n",
    "\n",
    "    lrs.append(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "    \n",
    "\n",
    "print(loss1.item()/H.shape[0], loss2.item(), p1.item(), p2.item()/(H.shape[0]**2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1/1.0001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(errors[100:])\n",
    "plt.plot(i_s[1:],rounded_errors[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(np.array(losses)/losses[0], label = \"total loss\")\n",
    "plt.plot(np.array(errors)/errors[0], label = \"error\")\n",
    "plt.plot(np.array(penalties)/penalties[0], label = \"penalty\")\n",
    "plt.plot(np.array(binary_penalty)/binary_penalty[0], label = \"binary penalty\")\n",
    "plt.plot(np.array(sparsity_penalty)/sparsity_penalty[0], label = \"sparsity penalty\")\n",
    "plt.legend()\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array(sparsity_penalty)/(H.shape[0]*k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array(betas)/np.max(betas), label = \"beta\")\n",
    "plt.plot(np.array(pe|nalty_2_weights)/np.max(penalty_2_weights), label = \"penalty_2_weight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.arange(1000)\n",
    "T = 1000\n",
    "plt.plot((np.cos(((i)/T)*np.pi*2))*0.5+0.5)\n",
    "# (np.sin(((i)/T)*np.pi)+1)*0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(errors)/weights.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the assignments\n",
    "assignments = torch.sigmoid(quantizer.initial_assignments)*1.2 - 0.1\n",
    "assignments = torch.clip(assignments,0,1)\n",
    "# assignments = torch.round(assignments)\n",
    "print(assignments)\n",
    "#round the assignments\n",
    "# assignments = torch.round(assignments)\n",
    "print(assignments)\n",
    "#get the quantized_weights\n",
    "quantized_weights = quantizer.codebook @ assignments\n",
    "print(quantized_weights)\n",
    "#get the difference between the quantized weights and the original weights\n",
    "diff = quantized_weights - quantizer.Weights\n",
    "print(quantizer.Weights)\n",
    "print(torch.mean(torch.abs(diff**2))/torch.mean(torch.abs(quantizer.Weights**2)))\n",
    "# print(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.einsum('ik,kl,il->', diff, quantizer.H, diff)/(quantizer.Weights.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(torch.round(torch.sigmoid(quantizer.initial_assignments)).detach().cpu().numpy(),interpolation='nearest', aspect='auto')\n",
    "#scale it to be a square image\n",
    "#add a colorbar\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(torch.sigmoid(quantizer.initial_assignments).detach().cpu().numpy()*1.2-0.1,interpolation='nearest', aspect='auto')\n",
    "#scale it to be a square image\n",
    "#add a colorbar\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sum(assignments,dim = 0)/k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(assignments.detach().cpu().numpy().flatten(),bins=100)\n",
    "# plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = assignments.detach().cpu().numpy().flatten()\n",
    "\n",
    "plt.plot(np.sort(a), np.linspace(0,1,len(a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "\n",
    "#create a gif from soem figures\n",
    "\n",
    "import imageio\n",
    "\n",
    "images = []\n",
    "for fn in sorted(glob.glob(\"test/*.png\")):\n",
    "    images.append(imageio.imread(fn))\n",
    "print(len(images))\n",
    "imageio.mimsave('test.gif', images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(quantizer.Weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
