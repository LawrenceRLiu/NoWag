/home/lliu/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.17s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.37it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.26it/s]
Model loaded. LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
4096
Loading tokenizer for meta-llama/Llama-2-7b-hf
Starting...
38748 MiB free out of 48676 MiB total
attention_mask torch.Size([1, 1, 4096, 4096]) cuda:5 torch.float16
position_ids torch.Size([1, 4096]) cuda:5 torch.int64
position_ids torch.Size([1, 4096])
attention_mask torch.Size([1, 1, 4096, 4096])
Ready.
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
0 self_attn.q_proj
Pruning ...
256
iter 0, train loss 1.539240837097168, val loss None
iter 10, train loss 1.2978596687316895, val loss None
iter 20, train loss 1.3093628883361816, val loss None
iter 30, train loss 1.0551574230194092, val loss None
iter 40, train loss 0.860085129737854, val loss None
iter 50, train loss 0.800328254699707, val loss None
iter 60, train loss 0.8180774450302124, val loss None
iter 70, train loss 0.7689632177352905, val loss None
iter 80, train loss 0.7255842089653015, val loss None
iter 90, train loss 0.7013158798217773, val loss None
best loss 0.6685052514076233
not here
quantized in 34.35959076881409 seconds
36458 MiB free out of 48676 MiB total
0 self_attn.k_proj
Pruning ...
256
iter 0, train loss 1.238694429397583, val loss None
iter 10, train loss 0.9339669942855835, val loss None
iter 20, train loss 0.837529182434082, val loss None
iter 30, train loss 0.8083577156066895, val loss None
iter 40, train loss 0.7075798511505127, val loss None
iter 50, train loss 0.6352445483207703, val loss None
iter 60, train loss 0.6172502040863037, val loss None
iter 70, train loss 0.5792560577392578, val loss None
iter 80, train loss 0.5484219193458557, val loss None
iter 90, train loss 0.5455954074859619, val loss None
best loss 0.531893789768219
not here
quantized in 32.920170545578 seconds
36458 MiB free out of 48676 MiB total
0 self_attn.v_proj
Pruning ...
256
iter 0, train loss 0.0800636038184166, val loss None
iter 10, train loss 0.07447576522827148, val loss None
iter 20, train loss 0.07439971715211868, val loss None
iter 30, train loss 0.07253052294254303, val loss None
iter 40, train loss 0.07138670235872269, val loss None
iter 50, train loss 0.07059690356254578, val loss None
iter 60, train loss 0.06976059079170227, val loss None
iter 70, train loss 0.06949888914823532, val loss None
iter 80, train loss 0.06935397535562515, val loss None
iter 90, train loss 0.06921853870153427, val loss None
best loss 0.0690288320183754
not here
quantized in 33.225441694259644 seconds
36426 MiB free out of 48676 MiB total
0 self_attn.o_proj
Pruning ...
256
iter 0, train loss 0.010799665004014969, val loss None
iter 10, train loss 0.010456979274749756, val loss None
iter 20, train loss 0.009622030891478062, val loss None
iter 30, train loss 0.008947975002229214, val loss None
iter 40, train loss 0.009357023052871227, val loss None
iter 50, train loss 0.008780964650213718, val loss None
iter 60, train loss 0.008766630664467812, val loss None
iter 70, train loss 0.008607775904238224, val loss None
iter 80, train loss 0.008524302393198013, val loss None
iter 90, train loss 0.00839562714099884, val loss None
best loss 0.008367662318050861
not here
quantized in 31.97284197807312 seconds
36394 MiB free out of 48676 MiB total
0 mlp.gate_proj
Pruning ...
256
iter 0, train loss 3.9480443000793457, val loss None
iter 10, train loss 4.089966297149658, val loss None
iter 20, train loss 4.111600875854492, val loss None
iter 30, train loss 4.058868408203125, val loss None
iter 40, train loss 4.043213844299316, val loss None
iter 50, train loss 4.019326210021973, val loss None
iter 60, train loss 4.011330604553223, val loss None
iter 70, train loss 4.0084967613220215, val loss None
iter 80, train loss 4.002608299255371, val loss None
iter 90, train loss 3.9955668449401855, val loss None
best loss 3.771836280822754
not here
quantized in 87.46595025062561 seconds
36092 MiB free out of 48676 MiB total
0 mlp.up_proj
Pruning ...
256
iter 0, train loss 3.5419487953186035, val loss None
iter 10, train loss 3.728330135345459, val loss None
iter 20, train loss 3.563784599304199, val loss None
iter 30, train loss 3.5496413707733154, val loss None
iter 40, train loss 3.550842761993408, val loss None
iter 50, train loss 3.5388143062591553, val loss None
iter 60, train loss 3.5343408584594727, val loss None
iter 70, train loss 3.5310163497924805, val loss None
iter 80, train loss 3.5320088863372803, val loss None
iter 90, train loss 3.5289602279663086, val loss None
best loss 3.3886497020721436
not here
quantized in 87.66388392448425 seconds
35898 MiB free out of 48676 MiB total
0 mlp.down_proj
Pruning ...
256
iter 0, train loss 0.010733775794506073, val loss None
iter 10, train loss 0.011703618802130222, val loss None
iter 20, train loss 0.011523718945682049, val loss None
iter 30, train loss 0.01128207053989172, val loss None
iter 40, train loss 0.011101317591965199, val loss None
iter 50, train loss 0.011035358533263206, val loss None
iter 60, train loss 0.010999227873980999, val loss None
iter 70, train loss 0.010980161838233471, val loss None
iter 80, train loss 0.010967947542667389, val loss None
iter 90, train loss 0.010961516760289669, val loss None
best loss 0.01010395772755146
not here
quantized in 94.19870138168335 seconds
35704 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35704 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31608 MiB free out of 48676 MiB total
after cast to cpu
38316 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
1 self_attn.q_proj
Pruning ...
256
iter 0, train loss 18.76083755493164, val loss None
iter 10, train loss 16.48206901550293, val loss None
iter 20, train loss 15.98734188079834, val loss None
iter 30, train loss 14.809237480163574, val loss None
iter 40, train loss 14.952863693237305, val loss None
iter 50, train loss 14.445342063903809, val loss None
iter 60, train loss 15.004846572875977, val loss None
iter 70, train loss 15.173236846923828, val loss None
iter 80, train loss 15.243929862976074, val loss None
iter 90, train loss 15.271383285522461, val loss None
best loss 14.395072937011719
not here
quantized in 36.771976709365845 seconds
36424 MiB free out of 48676 MiB total
1 self_attn.k_proj
Pruning ...
256
iter 0, train loss 17.661075592041016, val loss None
iter 10, train loss 14.76048469543457, val loss None
iter 20, train loss 13.904890060424805, val loss None
iter 30, train loss 13.366608619689941, val loss None
iter 40, train loss 13.154579162597656, val loss None
iter 50, train loss 12.69942855834961, val loss None
iter 60, train loss 12.424192428588867, val loss None
iter 70, train loss 12.293048858642578, val loss None
iter 80, train loss 12.126314163208008, val loss None
iter 90, train loss 12.248148918151855, val loss None
best loss 11.92302131652832
not here
quantized in 35.050365924835205 seconds
36414 MiB free out of 48676 MiB total
1 self_attn.v_proj
Pruning ...
256
iter 0, train loss 0.8288251161575317, val loss None
iter 10, train loss 0.8563949465751648, val loss None
iter 20, train loss 0.8106459379196167, val loss None
iter 30, train loss 0.7987585067749023, val loss None
iter 40, train loss 0.7931344509124756, val loss None
iter 50, train loss 0.78861403465271, val loss None
iter 60, train loss 0.7884389162063599, val loss None
iter 70, train loss 0.7881389856338501, val loss None
iter 80, train loss 0.7871423363685608, val loss None
iter 90, train loss 0.7866451144218445, val loss None
best loss 0.7760974764823914
not here
quantized in 32.62094759941101 seconds
36404 MiB free out of 48676 MiB total
1 self_attn.o_proj
Pruning ...
256
iter 0, train loss 0.07652617245912552, val loss None
iter 10, train loss 0.0746942088007927, val loss None
iter 20, train loss 0.07333004474639893, val loss None
iter 30, train loss 0.0712466910481453, val loss None
iter 40, train loss 0.07041455805301666, val loss None
iter 50, train loss 0.07049521803855896, val loss None
iter 60, train loss 0.06996019184589386, val loss None
iter 70, train loss 0.06983217597007751, val loss None
iter 80, train loss 0.06978895515203476, val loss None
iter 90, train loss 0.06939717382192612, val loss None
best loss 0.06919366121292114
not here
quantized in 31.674312353134155 seconds
36404 MiB free out of 48676 MiB total
1 mlp.gate_proj
Pruning ...
256
iter 0, train loss 16.104001998901367, val loss None
iter 10, train loss 16.934019088745117, val loss None
iter 20, train loss 16.555734634399414, val loss None
iter 30, train loss 16.451574325561523, val loss None
iter 40, train loss 16.558931350708008, val loss None
iter 50, train loss 16.663227081298828, val loss None
iter 60, train loss 16.73223876953125, val loss None
iter 70, train loss 16.701862335205078, val loss None
iter 80, train loss 16.808691024780273, val loss None
iter 90, train loss 16.792423248291016, val loss None
best loss 15.483592987060547
not here
quantized in 88.1098690032959 seconds
36102 MiB free out of 48676 MiB total
1 mlp.up_proj
Pruning ...
256
iter 0, train loss 13.291582107543945, val loss None
iter 10, train loss 13.77094554901123, val loss None
iter 20, train loss 13.452192306518555, val loss None
iter 30, train loss 13.513118743896484, val loss None
iter 40, train loss 13.499256134033203, val loss None
iter 50, train loss 13.478814125061035, val loss None
iter 60, train loss 13.472936630249023, val loss None
iter 70, train loss 13.487518310546875, val loss None
iter 80, train loss 13.496064186096191, val loss None
iter 90, train loss 13.493453025817871, val loss None
best loss 13.131227493286133
not here
quantized in 85.66134309768677 seconds
35908 MiB free out of 48676 MiB total
1 mlp.down_proj
Pruning ...
256
iter 0, train loss 0.08552630990743637, val loss None
iter 10, train loss 0.23954519629478455, val loss None
iter 20, train loss 0.1668168008327484, val loss None
iter 30, train loss 0.17032822966575623, val loss None
iter 40, train loss 0.13459020853042603, val loss None
iter 50, train loss 0.11776907742023468, val loss None
iter 60, train loss 0.10951609909534454, val loss None
iter 70, train loss 0.10814031958580017, val loss None
iter 80, train loss 0.10576677322387695, val loss None
iter 90, train loss 0.10464362800121307, val loss None
best loss 0.08552630990743637
not here
quantized in 92.44492530822754 seconds
35714 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35714 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31618 MiB free out of 48676 MiB total
after cast to cpu
38316 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
2 self_attn.q_proj
Pruning ...
256
iter 0, train loss 62.27800750732422, val loss None
iter 10, train loss 60.884864807128906, val loss None
iter 20, train loss 61.25555419921875, val loss None
iter 30, train loss 59.519737243652344, val loss None
iter 40, train loss 59.31387710571289, val loss None
iter 50, train loss 59.136539459228516, val loss None
iter 60, train loss 59.00605010986328, val loss None
iter 70, train loss 58.89927291870117, val loss None
iter 80, train loss 58.8203239440918, val loss None
iter 90, train loss 58.670494079589844, val loss None
best loss 56.52482604980469
not here
quantized in 34.82601022720337 seconds
36424 MiB free out of 48676 MiB total
2 self_attn.k_proj
Pruning ...
256
iter 0, train loss 72.63776397705078, val loss None
iter 10, train loss 69.06150817871094, val loss None
iter 20, train loss 70.24339294433594, val loss None
iter 30, train loss 69.00662231445312, val loss None
iter 40, train loss 68.31861877441406, val loss None
iter 50, train loss 67.6989517211914, val loss None
iter 60, train loss 67.49322509765625, val loss None
iter 70, train loss 67.48736572265625, val loss None
iter 80, train loss 67.11994171142578, val loss None
iter 90, train loss 66.8909912109375, val loss None
best loss 65.3441162109375
not here
quantized in 33.62976145744324 seconds
36414 MiB free out of 48676 MiB total
2 self_attn.v_proj
Pruning ...
256
iter 0, train loss 13.208587646484375, val loss None
iter 10, train loss 13.784114837646484, val loss None
iter 20, train loss 13.396904945373535, val loss None
iter 30, train loss 13.414051055908203, val loss None
iter 40, train loss 13.414722442626953, val loss None
iter 50, train loss 13.392223358154297, val loss None
iter 60, train loss 13.363180160522461, val loss None
iter 70, train loss 13.349753379821777, val loss None
iter 80, train loss 13.337381362915039, val loss None
iter 90, train loss 13.338226318359375, val loss None
best loss 12.996993064880371
not here
quantized in 32.36526894569397 seconds
36404 MiB free out of 48676 MiB total
2 self_attn.o_proj
Pruning ...
256
iter 0, train loss 0.13640592992305756, val loss None
iter 10, train loss 0.13692869246006012, val loss None
iter 20, train loss 0.13402654230594635, val loss None
iter 30, train loss 0.1337689310312271, val loss None
iter 40, train loss 0.13360992074012756, val loss None
iter 50, train loss 0.13318704068660736, val loss None
iter 60, train loss 0.13314194977283478, val loss None
iter 70, train loss 0.13308681547641754, val loss None
iter 80, train loss 0.13283677399158478, val loss None
iter 90, train loss 0.13284803926944733, val loss None
best loss 0.13277913630008698
not here
quantized in 32.019187450408936 seconds
36372 MiB free out of 48676 MiB total
2 mlp.gate_proj
Pruning ...
256
iter 0, train loss 33.68585968017578, val loss None
iter 10, train loss 34.90552520751953, val loss None
iter 20, train loss 33.68452453613281, val loss None
iter 30, train loss 33.801490783691406, val loss None
iter 40, train loss 33.74048614501953, val loss None
iter 50, train loss 33.76359558105469, val loss None
iter 60, train loss 33.729698181152344, val loss None
iter 70, train loss 33.73030090332031, val loss None
iter 80, train loss 33.74517059326172, val loss None
iter 90, train loss 33.73856735229492, val loss None
best loss 33.05324935913086
not here
quantized in 86.48402285575867 seconds
36070 MiB free out of 48676 MiB total
2 mlp.up_proj
Pruning ...
256
iter 0, train loss 27.705486297607422, val loss None
iter 10, train loss 27.873077392578125, val loss None
iter 20, train loss 27.827434539794922, val loss None
iter 30, train loss 27.747108459472656, val loss None
iter 40, train loss 27.776885986328125, val loss None
iter 50, train loss 27.782337188720703, val loss None
iter 60, train loss 27.789987564086914, val loss None
iter 70, train loss 27.79559326171875, val loss None
iter 80, train loss 27.79165267944336, val loss None
iter 90, train loss 27.793506622314453, val loss None
best loss 27.542552947998047
not here
quantized in 83.73175501823425 seconds
35876 MiB free out of 48676 MiB total
2 mlp.down_proj
Pruning ...
256
iter 0, train loss 0.1607731133699417, val loss None
iter 10, train loss 0.1608160138130188, val loss None
iter 20, train loss 0.15999600291252136, val loss None
iter 30, train loss 0.15943662822246552, val loss None
iter 40, train loss 0.15911152958869934, val loss None
iter 50, train loss 0.1590239703655243, val loss None
iter 60, train loss 0.15883269906044006, val loss None
iter 70, train loss 0.15868180990219116, val loss None
iter 80, train loss 0.15859122574329376, val loss None
iter 90, train loss 0.15843719244003296, val loss None
best loss 0.15841060876846313
not here
quantized in 89.22136044502258 seconds
35682 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35682 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31586 MiB free out of 48676 MiB total
after cast to cpu
38316 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
3 self_attn.q_proj
Pruning ...
256
iter 0, train loss 146.14138793945312, val loss None
iter 10, train loss 151.37933349609375, val loss None
iter 20, train loss 150.22409057617188, val loss None
iter 30, train loss 145.39010620117188, val loss None
iter 40, train loss 144.1849365234375, val loss None
iter 50, train loss 143.75274658203125, val loss None
iter 60, train loss 143.471435546875, val loss None
iter 70, train loss 143.00399780273438, val loss None
iter 80, train loss 142.90853881835938, val loss None
iter 90, train loss 142.97288513183594, val loss None
best loss 137.9447021484375
not here
quantized in 34.259345293045044 seconds
36392 MiB free out of 48676 MiB total
3 self_attn.k_proj
Pruning ...
256
iter 0, train loss 168.2652587890625, val loss None
iter 10, train loss 166.62643432617188, val loss None
iter 20, train loss 172.91717529296875, val loss None
iter 30, train loss 167.01498413085938, val loss None
iter 40, train loss 166.20614624023438, val loss None
iter 50, train loss 164.8784637451172, val loss None
iter 60, train loss 164.21975708007812, val loss None
iter 70, train loss 164.07162475585938, val loss None
iter 80, train loss 163.85272216796875, val loss None
iter 90, train loss 163.6049041748047, val loss None
best loss 156.53179931640625
not here
quantized in 33.391364336013794 seconds
36382 MiB free out of 48676 MiB total
3 self_attn.v_proj
Pruning ...
256
iter 0, train loss 34.74956130981445, val loss None
iter 10, train loss 35.27287292480469, val loss None
iter 20, train loss 34.963218688964844, val loss None
iter 30, train loss 34.87660598754883, val loss None
iter 40, train loss 34.82225036621094, val loss None
iter 50, train loss 34.81769561767578, val loss None
iter 60, train loss 34.84453582763672, val loss None
iter 70, train loss 34.77662658691406, val loss None
iter 80, train loss 34.76887130737305, val loss None
iter 90, train loss 34.7665901184082, val loss None
best loss 34.63420104980469
not here
quantized in 31.73392367362976 seconds
36404 MiB free out of 48676 MiB total
3 self_attn.o_proj
Pruning ...
256
iter 0, train loss 0.2203216850757599, val loss None
iter 10, train loss 0.2079305648803711, val loss None
iter 20, train loss 0.20277738571166992, val loss None
iter 30, train loss 0.20142416656017303, val loss None
iter 40, train loss 0.202531635761261, val loss None
iter 50, train loss 0.20375308394432068, val loss None
iter 60, train loss 0.20651870965957642, val loss None
iter 70, train loss 0.20609307289123535, val loss None
iter 80, train loss 0.20503245294094086, val loss None
iter 90, train loss 0.2045966386795044, val loss None
best loss 0.20102477073669434
not here
quantized in 31.794440984725952 seconds
36372 MiB free out of 48676 MiB total
3 mlp.gate_proj
Pruning ...
256
iter 0, train loss 53.88377380371094, val loss None
iter 10, train loss 54.642154693603516, val loss None
iter 20, train loss 54.22886657714844, val loss None
iter 30, train loss 54.238311767578125, val loss None
iter 40, train loss 54.204437255859375, val loss None
iter 50, train loss 54.2003059387207, val loss None
iter 60, train loss 54.13090133666992, val loss None
iter 70, train loss 54.13502883911133, val loss None
iter 80, train loss 54.11505126953125, val loss None
iter 90, train loss 54.12005615234375, val loss None
best loss 53.53572082519531
not here
quantized in 84.44667410850525 seconds
36070 MiB free out of 48676 MiB total
3 mlp.up_proj
Pruning ...
256
iter 0, train loss 44.437477111816406, val loss None
iter 10, train loss 44.47459030151367, val loss None
iter 20, train loss 44.47473907470703, val loss None
iter 30, train loss 44.475582122802734, val loss None
iter 40, train loss 44.512298583984375, val loss None
iter 50, train loss 44.493812561035156, val loss None
iter 60, train loss 44.4786376953125, val loss None
iter 70, train loss 44.48802185058594, val loss None
iter 80, train loss 44.47481918334961, val loss None
iter 90, train loss 44.47825622558594, val loss None
best loss 44.437477111816406
not here
quantized in 82.75072050094604 seconds
35876 MiB free out of 48676 MiB total
3 mlp.down_proj
Pruning ...
256
iter 0, train loss 0.29910025000572205, val loss None
iter 10, train loss 0.29924777150154114, val loss None
iter 20, train loss 0.2968912422657013, val loss None
iter 30, train loss 0.2951011657714844, val loss None
iter 40, train loss 0.29360711574554443, val loss None
iter 50, train loss 0.2927204668521881, val loss None
iter 60, train loss 0.2919767498970032, val loss None
iter 70, train loss 0.2911105751991272, val loss None
iter 80, train loss 0.29064199328422546, val loss None
iter 90, train loss 0.29053497314453125, val loss None
best loss 0.2905031442642212
not here
quantized in 89.1330246925354 seconds
35682 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35682 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31586 MiB free out of 48676 MiB total
after cast to cpu
38316 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
4 self_attn.q_proj
Pruning ...
256
iter 0, train loss 137.63845825195312, val loss None
iter 10, train loss 145.83834838867188, val loss None
iter 20, train loss 141.33566284179688, val loss None
iter 30, train loss 138.91650390625, val loss None
iter 40, train loss 138.1729736328125, val loss None
iter 50, train loss 137.63211059570312, val loss None
iter 60, train loss 137.08851623535156, val loss None
iter 70, train loss 136.76934814453125, val loss None
iter 80, train loss 136.5761260986328, val loss None
iter 90, train loss 136.69847106933594, val loss None
best loss 132.67959594726562
not here
quantized in 34.20445537567139 seconds
36392 MiB free out of 48676 MiB total
4 self_attn.k_proj
Pruning ...
256
iter 0, train loss 152.75521850585938, val loss None
iter 10, train loss 158.58932495117188, val loss None
iter 20, train loss 159.48333740234375, val loss None
iter 30, train loss 154.52969360351562, val loss None
iter 40, train loss 152.38848876953125, val loss None
iter 50, train loss 151.01573181152344, val loss None
iter 60, train loss 151.02584838867188, val loss None
iter 70, train loss 151.07977294921875, val loss None
iter 80, train loss 150.94776916503906, val loss None
iter 90, train loss 150.630126953125, val loss None
best loss 147.6947784423828
not here
quantized in 33.06362223625183 seconds
36382 MiB free out of 48676 MiB total
4 self_attn.v_proj
Pruning ...
256
iter 0, train loss 34.982669830322266, val loss None
iter 10, train loss 35.45399856567383, val loss None
iter 20, train loss 35.256996154785156, val loss None
iter 30, train loss 35.217899322509766, val loss None
iter 40, train loss 35.18613815307617, val loss None
iter 50, train loss 35.17792510986328, val loss None
iter 60, train loss 35.14753341674805, val loss None
iter 70, train loss 35.142921447753906, val loss None
iter 80, train loss 35.10067367553711, val loss None
iter 90, train loss 35.083858489990234, val loss None
best loss 34.85832977294922
not here
quantized in 31.800848245620728 seconds
36404 MiB free out of 48676 MiB total
4 self_attn.o_proj
Pruning ...
256
iter 0, train loss 0.593012809753418, val loss None
iter 10, train loss 0.5474398136138916, val loss None
iter 20, train loss 0.5315046310424805, val loss None
iter 30, train loss 0.5216609835624695, val loss None
iter 40, train loss 0.5167128443717957, val loss None
iter 50, train loss 0.5110918283462524, val loss None
iter 60, train loss 0.510344386100769, val loss None
iter 70, train loss 0.5091605186462402, val loss None
iter 80, train loss 0.5067007541656494, val loss None
iter 90, train loss 0.5048015117645264, val loss None
best loss 0.5026335716247559
not here
quantized in 31.739710330963135 seconds
36372 MiB free out of 48676 MiB total
4 mlp.gate_proj
Pruning ...
256
iter 0, train loss 79.90623474121094, val loss None
iter 10, train loss 81.58345031738281, val loss None
iter 20, train loss 80.73902130126953, val loss None
iter 30, train loss 80.518798828125, val loss None
iter 40, train loss 80.3790512084961, val loss None
iter 50, train loss 80.39142608642578, val loss None
iter 60, train loss 80.36357116699219, val loss None
iter 70, train loss 80.42799377441406, val loss None
iter 80, train loss 80.45335388183594, val loss None
iter 90, train loss 80.47048950195312, val loss None
best loss 79.29362487792969
not here
quantized in 84.53816771507263 seconds
36070 MiB free out of 48676 MiB total
4 mlp.up_proj
Pruning ...
256
iter 0, train loss 62.65779113769531, val loss None
iter 10, train loss 62.75099182128906, val loss None
iter 20, train loss 62.78009796142578, val loss None
iter 30, train loss 62.68782424926758, val loss None
iter 40, train loss 62.64759063720703, val loss None
iter 50, train loss 62.62413787841797, val loss None
iter 60, train loss 62.62614059448242, val loss None
iter 70, train loss 62.61382293701172, val loss None
iter 80, train loss 62.604469299316406, val loss None
iter 90, train loss 62.574073791503906, val loss None
best loss 62.56299591064453
not here
quantized in 83.05292201042175 seconds
35876 MiB free out of 48676 MiB total
4 mlp.down_proj
Pruning ...
256
iter 0, train loss 0.604773759841919, val loss None
iter 10, train loss 0.6056751012802124, val loss None
iter 20, train loss 0.6001694202423096, val loss None
iter 30, train loss 0.5985884070396423, val loss None
iter 40, train loss 0.598404049873352, val loss None
iter 50, train loss 0.5973423719406128, val loss None
iter 60, train loss 0.595443606376648, val loss None
iter 70, train loss 0.5947940349578857, val loss None
iter 80, train loss 0.5941169261932373, val loss None
iter 90, train loss 0.594372034072876, val loss None
best loss 0.5938757061958313
not here
quantized in 88.69978857040405 seconds
35682 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35682 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31586 MiB free out of 48676 MiB total
after cast to cpu
38316 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
5 self_attn.q_proj
Pruning ...
256
iter 0, train loss 158.26702880859375, val loss None
iter 10, train loss 167.22474670410156, val loss None
iter 20, train loss 163.73121643066406, val loss None
iter 30, train loss 157.7774658203125, val loss None
iter 40, train loss 156.75051879882812, val loss None
iter 50, train loss 156.3480224609375, val loss None
iter 60, train loss 155.9352569580078, val loss None
iter 70, train loss 155.57830810546875, val loss None
iter 80, train loss 155.4839324951172, val loss None
iter 90, train loss 155.12237548828125, val loss None
best loss 151.68197631835938
not here
quantized in 34.01518368721008 seconds
36424 MiB free out of 48676 MiB total
5 self_attn.k_proj
Pruning ...
256
iter 0, train loss 186.0988006591797, val loss None
iter 10, train loss 190.6905059814453, val loss None
iter 20, train loss 191.44046020507812, val loss None
iter 30, train loss 186.24896240234375, val loss None
iter 40, train loss 184.18368530273438, val loss None
iter 50, train loss 182.4742431640625, val loss None
iter 60, train loss 181.58474731445312, val loss None
iter 70, train loss 180.84906005859375, val loss None
iter 80, train loss 180.29107666015625, val loss None
iter 90, train loss 180.16851806640625, val loss None
best loss 175.21615600585938
not here
quantized in 32.85890221595764 seconds
36446 MiB free out of 48676 MiB total
5 self_attn.v_proj
Pruning ...
256
iter 0, train loss 40.979881286621094, val loss None
iter 10, train loss 41.331214904785156, val loss None
iter 20, train loss 41.11980438232422, val loss None
iter 30, train loss 41.10786819458008, val loss None
iter 40, train loss 41.03839874267578, val loss None
iter 50, train loss 41.00403594970703, val loss None
iter 60, train loss 40.98786926269531, val loss None
iter 70, train loss 40.9820556640625, val loss None
iter 80, train loss 40.97483444213867, val loss None
iter 90, train loss 40.982765197753906, val loss None
best loss 40.96875762939453
not here
quantized in 31.43101978302002 seconds
36436 MiB free out of 48676 MiB total
5 self_attn.o_proj
Pruning ...
256
iter 0, train loss 0.7684517502784729, val loss None
iter 10, train loss 0.7596806287765503, val loss None
iter 20, train loss 0.7528550028800964, val loss None
iter 30, train loss 0.7486977577209473, val loss None
iter 40, train loss 0.7480151653289795, val loss None
iter 50, train loss 0.7451595664024353, val loss None
iter 60, train loss 0.7443716526031494, val loss None
iter 70, train loss 0.7438226938247681, val loss None
iter 80, train loss 0.743704080581665, val loss None
iter 90, train loss 0.7416227459907532, val loss None
best loss 0.741603672504425
not here
quantized in 31.185743808746338 seconds
36404 MiB free out of 48676 MiB total
5 mlp.gate_proj
Pruning ...
256
iter 0, train loss 103.08714294433594, val loss None
iter 10, train loss 104.5634765625, val loss None
iter 20, train loss 103.7764892578125, val loss None
iter 30, train loss 103.59477233886719, val loss None
iter 40, train loss 103.41893768310547, val loss None
iter 50, train loss 103.44678497314453, val loss None
iter 60, train loss 103.31204223632812, val loss None
iter 70, train loss 103.13899993896484, val loss None
iter 80, train loss 103.14984893798828, val loss None
iter 90, train loss 103.12202453613281, val loss None
best loss 102.58988952636719
not here
quantized in 84.16806697845459 seconds
36102 MiB free out of 48676 MiB total
5 mlp.up_proj
Pruning ...
256
iter 0, train loss 79.9719467163086, val loss None
iter 10, train loss 80.19522094726562, val loss None
iter 20, train loss 80.19873046875, val loss None
iter 30, train loss 80.19171905517578, val loss None
iter 40, train loss 80.14382934570312, val loss None
iter 50, train loss 80.19461822509766, val loss None
iter 60, train loss 80.18595123291016, val loss None
iter 70, train loss 80.19131469726562, val loss None
iter 80, train loss 80.19991302490234, val loss None
iter 90, train loss 80.18732452392578, val loss None
best loss 79.9719467163086
not here
quantized in 84.11929893493652 seconds
35908 MiB free out of 48676 MiB total
5 mlp.down_proj
Pruning ...
256
iter 0, train loss 0.8655641078948975, val loss None
iter 10, train loss 0.8664693832397461, val loss None
iter 20, train loss 0.8625096082687378, val loss None
iter 30, train loss 0.8605923652648926, val loss None
iter 40, train loss 0.8594247102737427, val loss None
iter 50, train loss 0.8591532707214355, val loss None
iter 60, train loss 0.8588160276412964, val loss None
iter 70, train loss 0.858376145362854, val loss None
iter 80, train loss 0.8580073118209839, val loss None
iter 90, train loss 0.8578397035598755, val loss None
best loss 0.8575797080993652
not here
quantized in 89.77465677261353 seconds
35714 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35714 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31618 MiB free out of 48676 MiB total
after cast to cpu
38316 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
6 self_attn.q_proj
Pruning ...
256
iter 0, train loss 231.38818359375, val loss None
iter 10, train loss 241.0986328125, val loss None
iter 20, train loss 239.96792602539062, val loss None
iter 30, train loss 234.62176513671875, val loss None
iter 40, train loss 233.08387756347656, val loss None
iter 50, train loss 231.36160278320312, val loss None
iter 60, train loss 230.00830078125, val loss None
iter 70, train loss 230.0770263671875, val loss None
iter 80, train loss 230.1323699951172, val loss None
iter 90, train loss 230.22984313964844, val loss None
best loss 214.6068572998047
not here
quantized in 34.244460344314575 seconds
36392 MiB free out of 48676 MiB total
6 self_attn.k_proj
Pruning ...
256
iter 0, train loss 250.42636108398438, val loss None
iter 10, train loss 255.59170532226562, val loss None
iter 20, train loss 259.77191162109375, val loss None
iter 30, train loss 253.8298797607422, val loss None
iter 40, train loss 251.6568603515625, val loss None
iter 50, train loss 250.5220947265625, val loss None
iter 60, train loss 249.3504638671875, val loss None
iter 70, train loss 249.2296142578125, val loss None
iter 80, train loss 249.10238647460938, val loss None
iter 90, train loss 248.7078857421875, val loss None
best loss 226.7896728515625
not here
quantized in 33.827993631362915 seconds
36382 MiB free out of 48676 MiB total
6 self_attn.v_proj
Pruning ...
256
iter 0, train loss 57.976741790771484, val loss None
iter 10, train loss 58.72327423095703, val loss None
iter 20, train loss 58.35647964477539, val loss None
iter 30, train loss 58.34846496582031, val loss None
iter 40, train loss 58.144901275634766, val loss None
iter 50, train loss 58.14463424682617, val loss None
iter 60, train loss 57.98837661743164, val loss None
iter 70, train loss 57.98699951171875, val loss None
iter 80, train loss 57.98945617675781, val loss None
iter 90, train loss 58.03303527832031, val loss None
best loss 57.976741790771484
not here
quantized in 31.580098152160645 seconds
36404 MiB free out of 48676 MiB total
6 self_attn.o_proj
Pruning ...
256
iter 0, train loss 1.3411898612976074, val loss None
iter 10, train loss 1.3546714782714844, val loss None
iter 20, train loss 1.331457495689392, val loss None
iter 30, train loss 1.3240489959716797, val loss None
iter 40, train loss 1.3079935312271118, val loss None
iter 50, train loss 1.3033734560012817, val loss None
iter 60, train loss 1.3014695644378662, val loss None
iter 70, train loss 1.298081398010254, val loss None
iter 80, train loss 1.2948452234268188, val loss None
iter 90, train loss 1.2939646244049072, val loss None
best loss 1.292171597480774
not here
quantized in 31.11168146133423 seconds
36404 MiB free out of 48676 MiB total
6 mlp.gate_proj
Pruning ...
256
iter 0, train loss 126.64212036132812, val loss None
iter 10, train loss 130.54537963867188, val loss None
iter 20, train loss 128.6975860595703, val loss None
iter 30, train loss 128.40322875976562, val loss None
iter 40, train loss 128.15213012695312, val loss None
iter 50, train loss 127.98686218261719, val loss None
iter 60, train loss 127.88783264160156, val loss None
iter 70, train loss 127.76548767089844, val loss None
iter 80, train loss 127.75250244140625, val loss None
iter 90, train loss 127.7344970703125, val loss None
best loss 125.89474487304688
not here
quantized in 84.27952003479004 seconds
36102 MiB free out of 48676 MiB total
6 mlp.up_proj
Pruning ...
256
iter 0, train loss 94.46263885498047, val loss None
iter 10, train loss 94.43695068359375, val loss None
iter 20, train loss 94.44034576416016, val loss None
iter 30, train loss 94.37850189208984, val loss None
iter 40, train loss 94.32925415039062, val loss None
iter 50, train loss 94.44676208496094, val loss None
iter 60, train loss 94.37117004394531, val loss None
iter 70, train loss 94.28755187988281, val loss None
iter 80, train loss 94.23146057128906, val loss None
iter 90, train loss 94.28181457519531, val loss None
best loss 94.23146057128906
not here
quantized in 83.48870348930359 seconds
35908 MiB free out of 48676 MiB total
6 mlp.down_proj
Pruning ...
256
iter 0, train loss 1.269624948501587, val loss None
iter 10, train loss 1.2729244232177734, val loss None
iter 20, train loss 1.2679897546768188, val loss None
iter 30, train loss 1.2630125284194946, val loss None
iter 40, train loss 1.2603175640106201, val loss None
iter 50, train loss 1.2573624849319458, val loss None
iter 60, train loss 1.2557148933410645, val loss None
iter 70, train loss 1.2516889572143555, val loss None
iter 80, train loss 1.252315640449524, val loss None
iter 90, train loss 1.2495783567428589, val loss None
best loss 1.2495783567428589
not here
quantized in 88.41547346115112 seconds
35714 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35714 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31618 MiB free out of 48676 MiB total
after cast to cpu
38316 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
7 self_attn.q_proj
Pruning ...
256
iter 0, train loss 252.52676391601562, val loss None
iter 10, train loss 266.413818359375, val loss None
iter 20, train loss 263.0091552734375, val loss None
iter 30, train loss 257.4980163574219, val loss None
iter 40, train loss 255.83436584472656, val loss None
iter 50, train loss 254.9530029296875, val loss None
iter 60, train loss 254.27357482910156, val loss None
iter 70, train loss 253.65939331054688, val loss None
iter 80, train loss 253.35633850097656, val loss None
iter 90, train loss 253.46128845214844, val loss None
best loss 233.0325927734375
not here
quantized in 34.17743992805481 seconds
36392 MiB free out of 48676 MiB total
7 self_attn.k_proj
Pruning ...
256
iter 0, train loss 264.8933410644531, val loss None
iter 10, train loss 271.18487548828125, val loss None
iter 20, train loss 276.7169189453125, val loss None
iter 30, train loss 269.6287841796875, val loss None
iter 40, train loss 266.2068176269531, val loss None
iter 50, train loss 264.4909973144531, val loss None
iter 60, train loss 262.6142883300781, val loss None
iter 70, train loss 261.74420166015625, val loss None
iter 80, train loss 261.2811584472656, val loss None
iter 90, train loss 261.1471252441406, val loss None
best loss 241.97665405273438
not here
quantized in 33.00818729400635 seconds
36382 MiB free out of 48676 MiB total
7 self_attn.v_proj
Pruning ...
256
iter 0, train loss 64.87458801269531, val loss None
iter 10, train loss 65.41743469238281, val loss None
iter 20, train loss 65.00638580322266, val loss None
iter 30, train loss 64.87995147705078, val loss None
iter 40, train loss 64.8212890625, val loss None
iter 50, train loss 64.82860565185547, val loss None
iter 60, train loss 64.86737060546875, val loss None
iter 70, train loss 64.95337677001953, val loss None
iter 80, train loss 64.89681243896484, val loss None
iter 90, train loss 64.87960052490234, val loss None
best loss 64.7288818359375
not here
quantized in 31.821801900863647 seconds
36404 MiB free out of 48676 MiB total
7 self_attn.o_proj
Pruning ...
256
iter 0, train loss 2.3492002487182617, val loss None
iter 10, train loss 2.222245931625366, val loss None
iter 20, train loss 2.2100636959075928, val loss None
iter 30, train loss 2.1917316913604736, val loss None
iter 40, train loss 2.1698803901672363, val loss None
iter 50, train loss 2.160287380218506, val loss None
iter 60, train loss 2.1620688438415527, val loss None
iter 70, train loss 2.151108503341675, val loss None
iter 80, train loss 2.151453733444214, val loss None
iter 90, train loss 2.1441752910614014, val loss None
best loss 2.1390459537506104
not here
quantized in 31.59273886680603 seconds
36372 MiB free out of 48676 MiB total
7 mlp.gate_proj
Pruning ...
256
iter 0, train loss 145.49720764160156, val loss None
iter 10, train loss 151.2374267578125, val loss None
iter 20, train loss 148.06930541992188, val loss None
iter 30, train loss 147.82723999023438, val loss None
iter 40, train loss 147.14781188964844, val loss None
iter 50, train loss 147.02481079101562, val loss None
iter 60, train loss 146.9972686767578, val loss None
iter 70, train loss 146.8914337158203, val loss None
iter 80, train loss 146.90350341796875, val loss None
iter 90, train loss 146.8765106201172, val loss None
best loss 143.9998016357422
not here
quantized in 85.37630939483643 seconds
36070 MiB free out of 48676 MiB total
7 mlp.up_proj
Pruning ...
256
iter 0, train loss 108.78433227539062, val loss None
iter 10, train loss 109.04202270507812, val loss None
iter 20, train loss 109.02100372314453, val loss None
iter 30, train loss 108.92366027832031, val loss None
iter 40, train loss 108.90274810791016, val loss None
iter 50, train loss 108.79436492919922, val loss None
iter 60, train loss 108.79145050048828, val loss None
iter 70, train loss 108.81961822509766, val loss None
iter 80, train loss 108.77726745605469, val loss None
iter 90, train loss 108.72557067871094, val loss None
best loss 108.688720703125
not here
quantized in 83.34823489189148 seconds
35876 MiB free out of 48676 MiB total
7 mlp.down_proj
Pruning ...
256
iter 0, train loss 1.7699575424194336, val loss None
iter 10, train loss 1.7708837985992432, val loss None
iter 20, train loss 1.7568961381912231, val loss None
iter 30, train loss 1.7539880275726318, val loss None
iter 40, train loss 1.745886206626892, val loss None
iter 50, train loss 1.7432231903076172, val loss None
iter 60, train loss 1.7408359050750732, val loss None
iter 70, train loss 1.7405145168304443, val loss None
iter 80, train loss 1.7392584085464478, val loss None
iter 90, train loss 1.7371163368225098, val loss None
best loss 1.7355363368988037
not here
quantized in 88.52388405799866 seconds
35682 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35682 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31586 MiB free out of 48676 MiB total
after cast to cpu
38316 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
8 self_attn.q_proj
Pruning ...
256
iter 0, train loss 244.29263305664062, val loss None
iter 10, train loss 256.2992858886719, val loss None
iter 20, train loss 250.7205047607422, val loss None
iter 30, train loss 247.5460205078125, val loss None
iter 40, train loss 245.34872436523438, val loss None
iter 50, train loss 244.97515869140625, val loss None
iter 60, train loss 244.26556396484375, val loss None
iter 70, train loss 243.7146759033203, val loss None
iter 80, train loss 243.37167358398438, val loss None
iter 90, train loss 243.47247314453125, val loss None
best loss 229.71658325195312
not here
quantized in 33.98134994506836 seconds
36392 MiB free out of 48676 MiB total
8 self_attn.k_proj
Pruning ...
256
iter 0, train loss 253.8037872314453, val loss None
iter 10, train loss 260.8194885253906, val loss None
iter 20, train loss 265.7465515136719, val loss None
iter 30, train loss 260.0881652832031, val loss None
iter 40, train loss 258.136962890625, val loss None
iter 50, train loss 256.3839111328125, val loss None
iter 60, train loss 256.250732421875, val loss None
iter 70, train loss 255.2523193359375, val loss None
iter 80, train loss 254.8296356201172, val loss None
iter 90, train loss 255.039306640625, val loss None
best loss 239.51034545898438
not here
quantized in 32.830986738204956 seconds
36382 MiB free out of 48676 MiB total
8 self_attn.v_proj
Pruning ...
256
iter 0, train loss 67.13825988769531, val loss None
iter 10, train loss 68.15817260742188, val loss None
iter 20, train loss 67.56362915039062, val loss None
iter 30, train loss 67.43522644042969, val loss None
iter 40, train loss 67.1697769165039, val loss None
iter 50, train loss 67.2123031616211, val loss None
iter 60, train loss 67.16114807128906, val loss None
iter 70, train loss 67.1933364868164, val loss None
iter 80, train loss 67.18020629882812, val loss None
iter 90, train loss 67.1683349609375, val loss None
best loss 66.8487777709961
not here
quantized in 31.739877700805664 seconds
36404 MiB free out of 48676 MiB total
8 self_attn.o_proj
Pruning ...
256
iter 0, train loss 4.000096321105957, val loss None
iter 10, train loss 3.740124225616455, val loss None
iter 20, train loss 3.652249574661255, val loss None
iter 30, train loss 3.5731492042541504, val loss None
iter 40, train loss 3.5092813968658447, val loss None
iter 50, train loss 3.4802281856536865, val loss None
iter 60, train loss 3.4413225650787354, val loss None
iter 70, train loss 3.433002471923828, val loss None
iter 80, train loss 3.413712978363037, val loss None
iter 90, train loss 3.4032344818115234, val loss None
best loss 3.3884148597717285
not here
quantized in 31.159313678741455 seconds
36372 MiB free out of 48676 MiB total
8 mlp.gate_proj
Pruning ...
256
iter 0, train loss 151.20062255859375, val loss None
iter 10, train loss 157.10971069335938, val loss None
iter 20, train loss 153.32455444335938, val loss None
iter 30, train loss 153.72235107421875, val loss None
iter 40, train loss 153.0558319091797, val loss None
iter 50, train loss 152.79457092285156, val loss None
iter 60, train loss 152.5843963623047, val loss None
iter 70, train loss 152.48448181152344, val loss None
iter 80, train loss 152.56353759765625, val loss None
iter 90, train loss 152.5441436767578, val loss None
best loss 149.85794067382812
not here
quantized in 84.49940991401672 seconds
36070 MiB free out of 48676 MiB total
8 mlp.up_proj
Pruning ...
256
iter 0, train loss 120.18073272705078, val loss None
iter 10, train loss 120.70751190185547, val loss None
iter 20, train loss 120.81757354736328, val loss None
iter 30, train loss 120.62818145751953, val loss None
iter 40, train loss 120.62408447265625, val loss None
iter 50, train loss 120.44501495361328, val loss None
iter 60, train loss 120.3985824584961, val loss None
iter 70, train loss 120.4332504272461, val loss None
iter 80, train loss 120.44053649902344, val loss None
iter 90, train loss 120.38626098632812, val loss None
best loss 120.08195495605469
not here
quantized in 83.94308185577393 seconds
35876 MiB free out of 48676 MiB total
8 mlp.down_proj
Pruning ...
256
iter 0, train loss 2.184788465499878, val loss None
iter 10, train loss 2.1860687732696533, val loss None
iter 20, train loss 2.178767681121826, val loss None
iter 30, train loss 2.172961711883545, val loss None
iter 40, train loss 2.1683969497680664, val loss None
iter 50, train loss 2.164606809616089, val loss None
iter 60, train loss 2.1633620262145996, val loss None
iter 70, train loss 2.1605782508850098, val loss None
iter 80, train loss 2.1602697372436523, val loss None
iter 90, train loss 2.1589972972869873, val loss None
best loss 2.1573257446289062
not here
quantized in 89.5136730670929 seconds
35682 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35682 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31586 MiB free out of 48676 MiB total
after cast to cpu
38316 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
9 self_attn.q_proj
Pruning ...
256
iter 0, train loss 253.73117065429688, val loss None
iter 10, train loss 266.34844970703125, val loss None
iter 20, train loss 258.2789611816406, val loss None
iter 30, train loss 252.58432006835938, val loss None
iter 40, train loss 252.20523071289062, val loss None
iter 50, train loss 251.77841186523438, val loss None
iter 60, train loss 251.43153381347656, val loss None
iter 70, train loss 251.23031616210938, val loss None
iter 80, train loss 251.11532592773438, val loss None
iter 90, train loss 250.65234375, val loss None
best loss 240.64151000976562
not here
quantized in 34.33173584938049 seconds
36392 MiB free out of 48676 MiB total
9 self_attn.k_proj
Pruning ...
256
iter 0, train loss 274.5245056152344, val loss None
iter 10, train loss 286.90496826171875, val loss None
iter 20, train loss 288.693603515625, val loss None
iter 30, train loss 279.7743835449219, val loss None
iter 40, train loss 275.32879638671875, val loss None
iter 50, train loss 273.42877197265625, val loss None
iter 60, train loss 273.1407165527344, val loss None
iter 70, train loss 273.37347412109375, val loss None
iter 80, train loss 272.86602783203125, val loss None
iter 90, train loss 272.9127197265625, val loss None
best loss 260.8568115234375
not here
quantized in 32.98568272590637 seconds
36382 MiB free out of 48676 MiB total
9 self_attn.v_proj
Pruning ...
256
iter 0, train loss 72.30142211914062, val loss None
iter 10, train loss 72.93415069580078, val loss None
iter 20, train loss 72.53362274169922, val loss None
iter 30, train loss 72.44622039794922, val loss None
iter 40, train loss 72.30821990966797, val loss None
iter 50, train loss 72.24758911132812, val loss None
iter 60, train loss 72.15165710449219, val loss None
iter 70, train loss 72.0523681640625, val loss None
iter 80, train loss 72.04253387451172, val loss None
iter 90, train loss 71.98553466796875, val loss None
best loss 71.98553466796875
not here
quantized in 31.93483805656433 seconds
36404 MiB free out of 48676 MiB total
9 self_attn.o_proj
Pruning ...
256
iter 0, train loss 4.832009792327881, val loss None
iter 10, train loss 4.593649864196777, val loss None
iter 20, train loss 4.531299591064453, val loss None
iter 30, train loss 4.479120254516602, val loss None
iter 40, train loss 4.448266983032227, val loss None
iter 50, train loss 4.417224407196045, val loss None
iter 60, train loss 4.410520076751709, val loss None
iter 70, train loss 4.397378921508789, val loss None
iter 80, train loss 4.386119842529297, val loss None
iter 90, train loss 4.370792388916016, val loss None
best loss 4.359441757202148
not here
quantized in 31.729875564575195 seconds
36372 MiB free out of 48676 MiB total
9 mlp.gate_proj
Pruning ...
256
iter 0, train loss 156.2671356201172, val loss None
iter 10, train loss 162.8196563720703, val loss None
iter 20, train loss 158.68008422851562, val loss None
iter 30, train loss 158.656982421875, val loss None
iter 40, train loss 158.1585235595703, val loss None
iter 50, train loss 158.14019775390625, val loss None
iter 60, train loss 157.8646240234375, val loss None
iter 70, train loss 157.75357055664062, val loss None
iter 80, train loss 157.62155151367188, val loss None
iter 90, train loss 157.51539611816406, val loss None
best loss 154.50416564941406
not here
quantized in 85.21878933906555 seconds
36070 MiB free out of 48676 MiB total
9 mlp.up_proj
Pruning ...
256
iter 0, train loss 128.47203063964844, val loss None
iter 10, train loss 129.25210571289062, val loss None
iter 20, train loss 129.20590209960938, val loss None
iter 30, train loss 129.00265502929688, val loss None
iter 40, train loss 128.9478759765625, val loss None
iter 50, train loss 128.90985107421875, val loss None
iter 60, train loss 129.00352478027344, val loss None
iter 70, train loss 128.99313354492188, val loss None
iter 80, train loss 128.94027709960938, val loss None
iter 90, train loss 128.9977264404297, val loss None
best loss 128.30821228027344
not here
quantized in 83.65826725959778 seconds
35876 MiB free out of 48676 MiB total
9 mlp.down_proj
Pruning ...
256
iter 0, train loss 2.620701789855957, val loss None
iter 10, train loss 2.6192212104797363, val loss None
iter 20, train loss 2.6067018508911133, val loss None
iter 30, train loss 2.602752685546875, val loss None
iter 40, train loss 2.5960683822631836, val loss None
iter 50, train loss 2.594113826751709, val loss None
iter 60, train loss 2.5918755531311035, val loss None
iter 70, train loss 2.5879459381103516, val loss None
iter 80, train loss 2.5840988159179688, val loss None
iter 90, train loss 2.583672523498535, val loss None
best loss 2.5821738243103027
not here
quantized in 88.19633078575134 seconds
35682 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35682 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31586 MiB free out of 48676 MiB total
after cast to cpu
38316 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
10 self_attn.q_proj
Pruning ...
256
iter 0, train loss 255.5940704345703, val loss None
iter 10, train loss 267.4598388671875, val loss None
iter 20, train loss 260.27294921875, val loss None
iter 30, train loss 256.4958801269531, val loss None
iter 40, train loss 255.70791625976562, val loss None
iter 50, train loss 255.25343322753906, val loss None
iter 60, train loss 255.57260131835938, val loss None
iter 70, train loss 255.20516967773438, val loss None
iter 80, train loss 254.78790283203125, val loss None
iter 90, train loss 254.49185180664062, val loss None
best loss 242.97499084472656
not here
quantized in 34.21463966369629 seconds
36392 MiB free out of 48676 MiB total
10 self_attn.k_proj
Pruning ...
256
iter 0, train loss 283.03851318359375, val loss None
iter 10, train loss 294.8088684082031, val loss None
iter 20, train loss 299.4448547363281, val loss None
iter 30, train loss 290.4461669921875, val loss None
iter 40, train loss 287.9460754394531, val loss None
iter 50, train loss 285.789306640625, val loss None
iter 60, train loss 285.3406677246094, val loss None
iter 70, train loss 284.7047424316406, val loss None
iter 80, train loss 284.5704650878906, val loss None
iter 90, train loss 284.1582946777344, val loss None
best loss 270.9957580566406
not here
quantized in 32.70532298088074 seconds
36382 MiB free out of 48676 MiB total
10 self_attn.v_proj
Pruning ...
256
iter 0, train loss 72.07465362548828, val loss None
iter 10, train loss 72.80674743652344, val loss None
iter 20, train loss 72.564697265625, val loss None
iter 30, train loss 72.52925109863281, val loss None
iter 40, train loss 72.31990814208984, val loss None
iter 50, train loss 72.15324401855469, val loss None
iter 60, train loss 72.10748291015625, val loss None
iter 70, train loss 72.14027404785156, val loss None
iter 80, train loss 72.11479949951172, val loss None
iter 90, train loss 72.09352111816406, val loss None
best loss 71.92607116699219
not here
quantized in 31.596205711364746 seconds
36404 MiB free out of 48676 MiB total
10 self_attn.o_proj
Pruning ...
256
iter 0, train loss 6.979745388031006, val loss None
iter 10, train loss 6.574070453643799, val loss None
iter 20, train loss 6.356821537017822, val loss None
iter 30, train loss 6.15882682800293, val loss None
iter 40, train loss 6.0247578620910645, val loss None
iter 50, train loss 5.950340270996094, val loss None
iter 60, train loss 5.921217918395996, val loss None
iter 70, train loss 5.894680023193359, val loss None
iter 80, train loss 5.875550746917725, val loss None
iter 90, train loss 5.85551118850708, val loss None
best loss 5.844943046569824
not here
quantized in 30.907909393310547 seconds
36372 MiB free out of 48676 MiB total
10 mlp.gate_proj
Pruning ...
256
iter 0, train loss 161.84808349609375, val loss None
iter 10, train loss 168.78570556640625, val loss None
iter 20, train loss 165.30889892578125, val loss None
iter 30, train loss 165.37689208984375, val loss None
iter 40, train loss 164.6331024169922, val loss None
iter 50, train loss 164.6433868408203, val loss None
iter 60, train loss 164.3656005859375, val loss None
iter 70, train loss 164.4095458984375, val loss None
iter 80, train loss 164.33139038085938, val loss None
iter 90, train loss 164.34466552734375, val loss None
best loss 160.4006805419922
not here
quantized in 84.94195222854614 seconds
36070 MiB free out of 48676 MiB total
10 mlp.up_proj
Pruning ...
256
iter 0, train loss 136.99774169921875, val loss None
iter 10, train loss 137.8331298828125, val loss None
iter 20, train loss 137.7991485595703, val loss None
iter 30, train loss 137.45066833496094, val loss None
iter 40, train loss 137.4338836669922, val loss None
iter 50, train loss 137.4058074951172, val loss None
iter 60, train loss 137.3918914794922, val loss None
iter 70, train loss 137.36727905273438, val loss None
iter 80, train loss 137.31314086914062, val loss None
iter 90, train loss 137.25753784179688, val loss None
best loss 136.80804443359375
not here
quantized in 85.05104160308838 seconds
35876 MiB free out of 48676 MiB total
10 mlp.down_proj
Pruning ...
256
iter 0, train loss 2.937892436981201, val loss None
iter 10, train loss 2.940361738204956, val loss None
iter 20, train loss 2.9309334754943848, val loss None
iter 30, train loss 2.916468381881714, val loss None
iter 40, train loss 2.904148578643799, val loss None
iter 50, train loss 2.899614095687866, val loss None
iter 60, train loss 2.891997814178467, val loss None
iter 70, train loss 2.8880035877227783, val loss None
iter 80, train loss 2.8879952430725098, val loss None
iter 90, train loss 2.8890774250030518, val loss None
best loss 2.8869271278381348
not here
quantized in 89.32023477554321 seconds
35682 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35682 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31586 MiB free out of 48676 MiB total
after cast to cpu
38316 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
11 self_attn.q_proj
Pruning ...
256
iter 0, train loss 289.73907470703125, val loss None
iter 10, train loss 306.38238525390625, val loss None
iter 20, train loss 301.1779479980469, val loss None
iter 30, train loss 295.8912658691406, val loss None
iter 40, train loss 293.2771911621094, val loss None
iter 50, train loss 292.2277526855469, val loss None
iter 60, train loss 291.9360656738281, val loss None
iter 70, train loss 291.5108642578125, val loss None
iter 80, train loss 290.95947265625, val loss None
iter 90, train loss 290.58245849609375, val loss None
best loss 273.3929443359375
not here
quantized in 33.87197494506836 seconds
36392 MiB free out of 48676 MiB total
11 self_attn.k_proj
Pruning ...
256
iter 0, train loss 292.2492980957031, val loss None
iter 10, train loss 307.79827880859375, val loss None
iter 20, train loss 308.8270568847656, val loss None
iter 30, train loss 304.7317199707031, val loss None
iter 40, train loss 304.38275146484375, val loss None
iter 50, train loss 302.1595764160156, val loss None
iter 60, train loss 300.8189697265625, val loss None
iter 70, train loss 299.4717102050781, val loss None
iter 80, train loss 298.5965881347656, val loss None
iter 90, train loss 298.5770263671875, val loss None
best loss 277.41387939453125
not here
quantized in 32.771745443344116 seconds
36382 MiB free out of 48676 MiB total
11 self_attn.v_proj
Pruning ...
256
iter 0, train loss 97.10638427734375, val loss None
iter 10, train loss 97.95741271972656, val loss None
iter 20, train loss 97.65605163574219, val loss None
iter 30, train loss 97.46958923339844, val loss None
iter 40, train loss 97.34469604492188, val loss None
iter 50, train loss 97.15142822265625, val loss None
iter 60, train loss 97.04126739501953, val loss None
iter 70, train loss 97.02941131591797, val loss None
iter 80, train loss 96.98358154296875, val loss None
iter 90, train loss 97.0065689086914, val loss None
best loss 96.97183227539062
not here
quantized in 31.66567826271057 seconds
36372 MiB free out of 48676 MiB total
11 self_attn.o_proj
Pruning ...
256
iter 0, train loss 7.819901466369629, val loss None
iter 10, train loss 7.710820198059082, val loss None
iter 20, train loss 7.631711959838867, val loss None
iter 30, train loss 7.602810859680176, val loss None
iter 40, train loss 7.598045349121094, val loss None
iter 50, train loss 7.559082508087158, val loss None
iter 60, train loss 7.544768333435059, val loss None
iter 70, train loss 7.541848182678223, val loss None
iter 80, train loss 7.527885437011719, val loss None
iter 90, train loss 7.673598766326904, val loss None
best loss 7.5199761390686035
not here
quantized in 30.85884928703308 seconds
36372 MiB free out of 48676 MiB total
11 mlp.gate_proj
Pruning ...
256
iter 0, train loss 172.4320831298828, val loss None
iter 10, train loss 180.77487182617188, val loss None
iter 20, train loss 177.02383422851562, val loss None
iter 30, train loss 177.07289123535156, val loss None
iter 40, train loss 176.033203125, val loss None
iter 50, train loss 175.54139709472656, val loss None
iter 60, train loss 175.45034790039062, val loss None
iter 70, train loss 175.2886199951172, val loss None
iter 80, train loss 175.14630126953125, val loss None
iter 90, train loss 175.1269073486328, val loss None
best loss 170.24871826171875
not here
quantized in 112.0074234008789 seconds
36070 MiB free out of 48676 MiB total
11 mlp.up_proj
Pruning ...
256
iter 0, train loss 149.5220184326172, val loss None
iter 10, train loss 150.24107360839844, val loss None
iter 20, train loss 150.4982147216797, val loss None
iter 30, train loss 150.13418579101562, val loss None
iter 40, train loss 150.11431884765625, val loss None
iter 50, train loss 150.1697998046875, val loss None
iter 60, train loss 150.09515380859375, val loss None
iter 70, train loss 150.05343627929688, val loss None
iter 80, train loss 150.06307983398438, val loss None
iter 90, train loss 150.104736328125, val loss None
best loss 149.12310791015625
not here
quantized in 96.9591052532196 seconds
35876 MiB free out of 48676 MiB total
11 mlp.down_proj
Pruning ...
256
iter 0, train loss 3.416823148727417, val loss None
iter 10, train loss 3.4120888710021973, val loss None
iter 20, train loss 3.3840043544769287, val loss None
iter 30, train loss 3.359997272491455, val loss None
iter 40, train loss 3.349017381668091, val loss None
iter 50, train loss 3.350064992904663, val loss None
iter 60, train loss 3.337815046310425, val loss None
iter 70, train loss 3.3262710571289062, val loss None
iter 80, train loss 3.3249928951263428, val loss None
iter 90, train loss 3.326524257659912, val loss None
best loss 3.3241381645202637
not here
quantized in 116.37880730628967 seconds
35682 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35682 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31586 MiB free out of 48676 MiB total
after cast to cpu
38316 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
12 self_attn.q_proj
Pruning ...
256
iter 0, train loss 298.93402099609375, val loss None
iter 10, train loss 316.16912841796875, val loss None
iter 20, train loss 307.61578369140625, val loss None
iter 30, train loss 304.9774475097656, val loss None
iter 40, train loss 301.79376220703125, val loss None
iter 50, train loss 300.7403564453125, val loss None
iter 60, train loss 300.5206298828125, val loss None
iter 70, train loss 300.4012451171875, val loss None
iter 80, train loss 300.120361328125, val loss None
iter 90, train loss 300.14691162109375, val loss None
best loss 282.74810791015625
not here
quantized in 34.448302030563354 seconds
36392 MiB free out of 48676 MiB total
12 self_attn.k_proj
Pruning ...
256
iter 0, train loss 330.5894470214844, val loss None
iter 10, train loss 347.5142822265625, val loss None
iter 20, train loss 346.3231201171875, val loss None
iter 30, train loss 339.88299560546875, val loss None
iter 40, train loss 338.3961181640625, val loss None
iter 50, train loss 336.71966552734375, val loss None
iter 60, train loss 336.1213684082031, val loss None
iter 70, train loss 334.8807373046875, val loss None
iter 80, train loss 334.323974609375, val loss None
iter 90, train loss 333.8895263671875, val loss None
best loss 311.0816345214844
not here
quantized in 33.50632381439209 seconds
36382 MiB free out of 48676 MiB total
12 self_attn.v_proj
Pruning ...
256
iter 0, train loss 95.31002044677734, val loss None
iter 10, train loss 95.98204040527344, val loss None
iter 20, train loss 95.7950439453125, val loss None
iter 30, train loss 95.65074157714844, val loss None
iter 40, train loss 95.53437805175781, val loss None
iter 50, train loss 95.38775634765625, val loss None
iter 60, train loss 95.4935073852539, val loss None
iter 70, train loss 95.408447265625, val loss None
iter 80, train loss 95.38182067871094, val loss None
iter 90, train loss 95.34521484375, val loss None
best loss 95.21142578125
not here
quantized in 31.7814519405365 seconds
36404 MiB free out of 48676 MiB total
12 self_attn.o_proj
Pruning ...
256
iter 0, train loss 8.260231971740723, val loss None
iter 10, train loss 8.164329528808594, val loss None
iter 20, train loss 8.114028930664062, val loss None
iter 30, train loss 8.025030136108398, val loss None
iter 40, train loss 8.000353813171387, val loss None
iter 50, train loss 8.005841255187988, val loss None
iter 60, train loss 7.978097915649414, val loss None
iter 70, train loss 7.933014869689941, val loss None
iter 80, train loss 7.908685207366943, val loss None
iter 90, train loss 7.875741004943848, val loss None
best loss 7.8612823486328125
not here
quantized in 31.350245475769043 seconds
36404 MiB free out of 48676 MiB total
12 mlp.gate_proj
Pruning ...
256
iter 0, train loss 184.25245666503906, val loss None
iter 10, train loss 192.58193969726562, val loss None
iter 20, train loss 187.86105346679688, val loss None
iter 30, train loss 187.973388671875, val loss None
iter 40, train loss 187.367431640625, val loss None
iter 50, train loss 187.37387084960938, val loss None
iter 60, train loss 187.11932373046875, val loss None
iter 70, train loss 186.9234619140625, val loss None
iter 80, train loss 186.88034057617188, val loss None
iter 90, train loss 186.70550537109375, val loss None
best loss 182.43865966796875
not here
quantized in 85.88629841804504 seconds
36102 MiB free out of 48676 MiB total
12 mlp.up_proj
Pruning ...
256
iter 0, train loss 164.9062957763672, val loss None
iter 10, train loss 165.97113037109375, val loss None
iter 20, train loss 165.77163696289062, val loss None
iter 30, train loss 165.4482879638672, val loss None
iter 40, train loss 165.2412109375, val loss None
iter 50, train loss 165.2256317138672, val loss None
iter 60, train loss 165.38339233398438, val loss None
iter 70, train loss 165.3710479736328, val loss None
iter 80, train loss 165.274658203125, val loss None
iter 90, train loss 165.22547912597656, val loss None
best loss 164.9062957763672
not here
quantized in 83.90420913696289 seconds
35908 MiB free out of 48676 MiB total
12 mlp.down_proj
Pruning ...
256
iter 0, train loss 3.7437169551849365, val loss None
iter 10, train loss 3.7618985176086426, val loss None
iter 20, train loss 3.7512803077697754, val loss None
iter 30, train loss 3.748257875442505, val loss None
iter 40, train loss 3.7489805221557617, val loss None
iter 50, train loss 3.7488627433776855, val loss None
iter 60, train loss 3.747687339782715, val loss None
iter 70, train loss 3.744616985321045, val loss None
iter 80, train loss 3.7449169158935547, val loss None
iter 90, train loss 3.7449567317962646, val loss None
best loss 3.7437169551849365
not here
quantized in 87.76453399658203 seconds
35714 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35714 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31618 MiB free out of 48676 MiB total
after cast to cpu
38316 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
13 self_attn.q_proj
Pruning ...
256
iter 0, train loss 306.405029296875, val loss None
iter 10, train loss 322.2506103515625, val loss None
iter 20, train loss 310.3799743652344, val loss None
iter 30, train loss 309.22918701171875, val loss None
iter 40, train loss 307.929443359375, val loss None
iter 50, train loss 306.7817687988281, val loss None
iter 60, train loss 305.5417785644531, val loss None
iter 70, train loss 304.8681945800781, val loss None
iter 80, train loss 304.74053955078125, val loss None
iter 90, train loss 304.4669189453125, val loss None
best loss 288.2865295410156
not here
quantized in 33.49238061904907 seconds
36392 MiB free out of 48676 MiB total
13 self_attn.k_proj
Pruning ...
256
iter 0, train loss 329.120849609375, val loss None
iter 10, train loss 345.14459228515625, val loss None
iter 20, train loss 343.1920166015625, val loss None
iter 30, train loss 334.92218017578125, val loss None
iter 40, train loss 331.41766357421875, val loss None
iter 50, train loss 330.0267333984375, val loss None
iter 60, train loss 328.9649353027344, val loss None
iter 70, train loss 328.0399169921875, val loss None
iter 80, train loss 327.416015625, val loss None
iter 90, train loss 326.86297607421875, val loss None
best loss 307.910400390625
not here
quantized in 32.74880027770996 seconds
36382 MiB free out of 48676 MiB total
13 self_attn.v_proj
Pruning ...
256
iter 0, train loss 105.89179992675781, val loss None
iter 10, train loss 105.9780502319336, val loss None
iter 20, train loss 105.99755859375, val loss None
iter 30, train loss 105.76499938964844, val loss None
iter 40, train loss 105.56189727783203, val loss None
iter 50, train loss 105.64610290527344, val loss None
iter 60, train loss 105.51414489746094, val loss None
iter 70, train loss 105.42868041992188, val loss None
iter 80, train loss 105.37757110595703, val loss None
iter 90, train loss 105.34202575683594, val loss None
best loss 105.32443237304688
not here
quantized in 31.308032751083374 seconds
36404 MiB free out of 48676 MiB total
13 self_attn.o_proj
Pruning ...
256
iter 0, train loss 8.626374244689941, val loss None
iter 10, train loss 8.167913436889648, val loss None
iter 20, train loss 8.121294021606445, val loss None
iter 30, train loss 8.069135665893555, val loss None
iter 40, train loss 7.998417854309082, val loss None
iter 50, train loss 7.9474382400512695, val loss None
iter 60, train loss 7.92475700378418, val loss None
iter 70, train loss 7.87367582321167, val loss None
iter 80, train loss 7.866482734680176, val loss None
iter 90, train loss 7.83828067779541, val loss None
best loss 7.828598499298096
not here
quantized in 30.874907970428467 seconds
36372 MiB free out of 48676 MiB total
13 mlp.gate_proj
Pruning ...
256
iter 0, train loss 194.42984008789062, val loss None
iter 10, train loss 202.58810424804688, val loss None
iter 20, train loss 198.4746856689453, val loss None
iter 30, train loss 198.01309204101562, val loss None
iter 40, train loss 197.3565673828125, val loss None
iter 50, train loss 196.77825927734375, val loss None
iter 60, train loss 196.54791259765625, val loss None
iter 70, train loss 196.48818969726562, val loss None
iter 80, train loss 196.34805297851562, val loss None
iter 90, train loss 196.2587890625, val loss None
best loss 192.3968963623047
not here
quantized in 84.13212442398071 seconds
36070 MiB free out of 48676 MiB total
13 mlp.up_proj
Pruning ...
256
iter 0, train loss 178.250732421875, val loss None
iter 10, train loss 178.92337036132812, val loss None
iter 20, train loss 179.11843872070312, val loss None
iter 30, train loss 178.7647705078125, val loss None
iter 40, train loss 178.93310546875, val loss None
iter 50, train loss 178.65870666503906, val loss None
iter 60, train loss 178.59803771972656, val loss None
iter 70, train loss 178.54421997070312, val loss None
iter 80, train loss 178.51022338867188, val loss None
iter 90, train loss 178.5223846435547, val loss None
best loss 178.0728302001953
not here
quantized in 83.46474385261536 seconds
35876 MiB free out of 48676 MiB total
13 mlp.down_proj
Pruning ...
256
iter 0, train loss 4.414822578430176, val loss None
iter 10, train loss 4.420198440551758, val loss None
iter 20, train loss 4.398874282836914, val loss None
iter 30, train loss 4.391740798950195, val loss None
iter 40, train loss 4.376347064971924, val loss None
iter 50, train loss 4.373700141906738, val loss None
iter 60, train loss 4.366138935089111, val loss None
iter 70, train loss 4.361790657043457, val loss None
iter 80, train loss 4.358305931091309, val loss None
iter 90, train loss 4.356621742248535, val loss None
best loss 4.353711128234863
not here
quantized in 88.15272331237793 seconds
35682 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35682 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31586 MiB free out of 48676 MiB total
after cast to cpu
38316 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
14 self_attn.q_proj
Pruning ...
256
iter 0, train loss 323.7491149902344, val loss None
iter 10, train loss 342.4091491699219, val loss None
iter 20, train loss 331.3387451171875, val loss None
iter 30, train loss 327.12701416015625, val loss None
iter 40, train loss 322.8551940917969, val loss None
iter 50, train loss 321.2685241699219, val loss None
iter 60, train loss 321.1312561035156, val loss None
iter 70, train loss 320.9543151855469, val loss None
iter 80, train loss 320.9953918457031, val loss None
iter 90, train loss 320.7776794433594, val loss None
best loss 300.8115234375
not here
quantized in 34.08129405975342 seconds
36392 MiB free out of 48676 MiB total
14 self_attn.k_proj
Pruning ...
256
iter 0, train loss 350.2248229980469, val loss None
iter 10, train loss 366.9510192871094, val loss None
iter 20, train loss 366.35394287109375, val loss None
iter 30, train loss 356.2164001464844, val loss None
iter 40, train loss 353.5136413574219, val loss None
iter 50, train loss 350.709228515625, val loss None
iter 60, train loss 349.17205810546875, val loss None
iter 70, train loss 349.2928466796875, val loss None
iter 80, train loss 348.8359680175781, val loss None
iter 90, train loss 348.76275634765625, val loss None
best loss 322.07305908203125
not here
quantized in 33.08366417884827 seconds
36382 MiB free out of 48676 MiB total
14 self_attn.v_proj
Pruning ...
256
iter 0, train loss 106.29081726074219, val loss None
iter 10, train loss 106.29782104492188, val loss None
iter 20, train loss 106.06529998779297, val loss None
iter 30, train loss 105.9018783569336, val loss None
iter 40, train loss 105.76539611816406, val loss None
iter 50, train loss 105.8487548828125, val loss None
iter 60, train loss 105.80030822753906, val loss None
iter 70, train loss 105.82017517089844, val loss None
iter 80, train loss 105.84272766113281, val loss None
iter 90, train loss 105.78390502929688, val loss None
best loss 105.74520874023438
not here
quantized in 31.153779983520508 seconds
36372 MiB free out of 48676 MiB total
14 self_attn.o_proj
Pruning ...
256
iter 0, train loss 10.505728721618652, val loss None
iter 10, train loss 10.387170791625977, val loss None
iter 20, train loss 10.295600891113281, val loss None
iter 30, train loss 10.2721586227417, val loss None
iter 40, train loss 10.218549728393555, val loss None
iter 50, train loss 10.17574691772461, val loss None
iter 60, train loss 10.155900955200195, val loss None
iter 70, train loss 10.101245880126953, val loss None
iter 80, train loss 10.081427574157715, val loss None
iter 90, train loss 10.075045585632324, val loss None
best loss 10.046646118164062
not here
quantized in 30.49479603767395 seconds
36372 MiB free out of 48676 MiB total
14 mlp.gate_proj
Pruning ...
256
iter 0, train loss 212.52825927734375, val loss None
iter 10, train loss 222.70138549804688, val loss None
iter 20, train loss 216.63131713867188, val loss None
iter 30, train loss 217.04931640625, val loss None
iter 40, train loss 216.4455108642578, val loss None
iter 50, train loss 216.06771850585938, val loss None
iter 60, train loss 215.83604431152344, val loss None
iter 70, train loss 215.8363800048828, val loss None
iter 80, train loss 215.7670440673828, val loss None
iter 90, train loss 215.65211486816406, val loss None
best loss 210.07757568359375
not here
quantized in 84.45515584945679 seconds
36070 MiB free out of 48676 MiB total
14 mlp.up_proj
Pruning ...
256
iter 0, train loss 195.40167236328125, val loss None
iter 10, train loss 196.6140899658203, val loss None
iter 20, train loss 196.37213134765625, val loss None
iter 30, train loss 196.30349731445312, val loss None
iter 40, train loss 196.3615264892578, val loss None
iter 50, train loss 196.19847106933594, val loss None
iter 60, train loss 196.19741821289062, val loss None
iter 70, train loss 196.06031799316406, val loss None
iter 80, train loss 195.9525146484375, val loss None
iter 90, train loss 195.89523315429688, val loss None
best loss 195.40167236328125
not here
quantized in 82.47965049743652 seconds
35876 MiB free out of 48676 MiB total
14 mlp.down_proj
Pruning ...
256
iter 0, train loss 5.126226425170898, val loss None
iter 10, train loss 5.1402587890625, val loss None
iter 20, train loss 5.114809989929199, val loss None
iter 30, train loss 5.110424518585205, val loss None
iter 40, train loss 5.103254795074463, val loss None
iter 50, train loss 5.102675437927246, val loss None
iter 60, train loss 5.100421905517578, val loss None
iter 70, train loss 5.094394683837891, val loss None
iter 80, train loss 5.098722457885742, val loss None
iter 90, train loss 5.097342491149902, val loss None
best loss 5.092392921447754
not here
quantized in 105.61760354042053 seconds
35682 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35682 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31586 MiB free out of 48676 MiB total
after cast to cpu
38316 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
15 self_attn.q_proj
Pruning ...
256
iter 0, train loss 302.7221984863281, val loss None
iter 10, train loss 318.04510498046875, val loss None
iter 20, train loss 305.5411682128906, val loss None
iter 30, train loss 302.69879150390625, val loss None
iter 40, train loss 301.87548828125, val loss None
iter 50, train loss 301.0850830078125, val loss None
iter 60, train loss 299.91357421875, val loss None
iter 70, train loss 299.7196044921875, val loss None
iter 80, train loss 299.58636474609375, val loss None
iter 90, train loss 299.5727233886719, val loss None
best loss 281.3565673828125
not here
quantized in 34.00523591041565 seconds
36392 MiB free out of 48676 MiB total
15 self_attn.k_proj
Pruning ...
256
iter 0, train loss 342.0773010253906, val loss None
iter 10, train loss 355.1751708984375, val loss None
iter 20, train loss 357.27734375, val loss None
iter 30, train loss 350.15716552734375, val loss None
iter 40, train loss 347.7857971191406, val loss None
iter 50, train loss 345.9906311035156, val loss None
iter 60, train loss 344.6528625488281, val loss None
iter 70, train loss 344.2510070800781, val loss None
iter 80, train loss 343.68865966796875, val loss None
iter 90, train loss 343.6236572265625, val loss None
best loss 311.30755615234375
not here
quantized in 33.03907370567322 seconds
36382 MiB free out of 48676 MiB total
15 self_attn.v_proj
Pruning ...
256
iter 0, train loss 110.48726654052734, val loss None
iter 10, train loss 111.01240539550781, val loss None
iter 20, train loss 110.86296844482422, val loss None
iter 30, train loss 110.45861053466797, val loss None
iter 40, train loss 110.41241455078125, val loss None
iter 50, train loss 110.27619934082031, val loss None
iter 60, train loss 110.19540405273438, val loss None
iter 70, train loss 110.17888641357422, val loss None
iter 80, train loss 110.13191223144531, val loss None
iter 90, train loss 110.09445190429688, val loss None
best loss 110.05357360839844
not here
quantized in 31.334528923034668 seconds
36404 MiB free out of 48676 MiB total
15 self_attn.o_proj
Pruning ...
256
iter 0, train loss 10.3049955368042, val loss None
iter 10, train loss 10.122893333435059, val loss None
iter 20, train loss 10.02787971496582, val loss None
iter 30, train loss 9.968950271606445, val loss None
iter 40, train loss 9.86921215057373, val loss None
iter 50, train loss 9.858741760253906, val loss None
iter 60, train loss 9.794317245483398, val loss None
iter 70, train loss 9.79863166809082, val loss None
iter 80, train loss 9.802502632141113, val loss None
iter 90, train loss 9.800753593444824, val loss None
best loss 9.77587890625
not here
quantized in 31.161568880081177 seconds
36372 MiB free out of 48676 MiB total
15 mlp.gate_proj
Pruning ...
256
iter 0, train loss 230.5938262939453, val loss None
iter 10, train loss 240.63156127929688, val loss None
iter 20, train loss 235.041259765625, val loss None
iter 30, train loss 235.64706420898438, val loss None
iter 40, train loss 235.05825805664062, val loss None
iter 50, train loss 235.38233947753906, val loss None
iter 60, train loss 235.37608337402344, val loss None
iter 70, train loss 235.4790802001953, val loss None
iter 80, train loss 235.4874725341797, val loss None
iter 90, train loss 235.41195678710938, val loss None
best loss 227.52728271484375
not here
quantized in 85.19107913970947 seconds
36070 MiB free out of 48676 MiB total
15 mlp.up_proj
Pruning ...
256
iter 0, train loss 212.644775390625, val loss None
iter 10, train loss 213.5014190673828, val loss None
iter 20, train loss 213.83692932128906, val loss None
iter 30, train loss 213.38763427734375, val loss None
iter 40, train loss 213.44863891601562, val loss None
iter 50, train loss 213.27127075195312, val loss None
iter 60, train loss 213.18258666992188, val loss None
iter 70, train loss 213.20108032226562, val loss None
iter 80, train loss 213.24148559570312, val loss None
iter 90, train loss 213.08163452148438, val loss None
best loss 212.51492309570312
not here
quantized in 83.65151333808899 seconds
35876 MiB free out of 48676 MiB total
15 mlp.down_proj
Pruning ...
256
iter 0, train loss 6.204716682434082, val loss None
iter 10, train loss 6.221798419952393, val loss None
iter 20, train loss 6.19114875793457, val loss None
iter 30, train loss 6.181678771972656, val loss None
iter 40, train loss 6.189530372619629, val loss None
iter 50, train loss 6.18705415725708, val loss None
iter 60, train loss 6.1837592124938965, val loss None
iter 70, train loss 6.177859306335449, val loss None
iter 80, train loss 6.173375129699707, val loss None
iter 90, train loss 6.1717047691345215, val loss None
best loss 6.169418811798096
not here
quantized in 87.87650847434998 seconds
35682 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35682 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31586 MiB free out of 48676 MiB total
after cast to cpu
38316 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
16 self_attn.q_proj
Pruning ...
256
iter 0, train loss 318.9951171875, val loss None
iter 10, train loss 332.8056640625, val loss None
iter 20, train loss 322.89990234375, val loss None
iter 30, train loss 319.0457763671875, val loss None
iter 40, train loss 315.31060791015625, val loss None
iter 50, train loss 313.7606201171875, val loss None
iter 60, train loss 313.0420227050781, val loss None
iter 70, train loss 312.71795654296875, val loss None
iter 80, train loss 312.1204833984375, val loss None
iter 90, train loss 311.9852600097656, val loss None
best loss 296.19171142578125
not here
quantized in 33.92068529129028 seconds
36392 MiB free out of 48676 MiB total
16 self_attn.k_proj
Pruning ...
256
iter 0, train loss 354.9114685058594, val loss None
iter 10, train loss 367.4002380371094, val loss None
iter 20, train loss 367.6046142578125, val loss None
iter 30, train loss 363.0091552734375, val loss None
iter 40, train loss 356.5250549316406, val loss None
iter 50, train loss 354.435302734375, val loss None
iter 60, train loss 352.48077392578125, val loss None
iter 70, train loss 351.2714538574219, val loss None
iter 80, train loss 351.08306884765625, val loss None
iter 90, train loss 350.5929260253906, val loss None
best loss 322.9601135253906
not here
quantized in 32.763263463974 seconds
36382 MiB free out of 48676 MiB total
16 self_attn.v_proj
Pruning ...
256
iter 0, train loss 125.77173614501953, val loss None
iter 10, train loss 126.04719543457031, val loss None
iter 20, train loss 125.65260314941406, val loss None
iter 30, train loss 125.59759521484375, val loss None
iter 40, train loss 125.24278259277344, val loss None
iter 50, train loss 125.23245239257812, val loss None
iter 60, train loss 125.03523254394531, val loss None
iter 70, train loss 125.02743530273438, val loss None
iter 80, train loss 124.89163208007812, val loss None
iter 90, train loss 124.89875793457031, val loss None
best loss 124.81109619140625
not here
quantized in 31.027138233184814 seconds
36372 MiB free out of 48676 MiB total
16 self_attn.o_proj
Pruning ...
256
iter 0, train loss 12.085884094238281, val loss None
iter 10, train loss 11.972585678100586, val loss None
iter 20, train loss 11.886480331420898, val loss None
iter 30, train loss 11.826350212097168, val loss None
iter 40, train loss 11.815993309020996, val loss None
iter 50, train loss 11.763290405273438, val loss None
iter 60, train loss 11.732022285461426, val loss None
iter 70, train loss 11.702898025512695, val loss None
iter 80, train loss 11.69306755065918, val loss None
iter 90, train loss 11.65322494506836, val loss None
best loss 11.635835647583008
not here
quantized in 30.942769050598145 seconds
36372 MiB free out of 48676 MiB total
16 mlp.gate_proj
Pruning ...
256
iter 0, train loss 268.3559875488281, val loss None
iter 10, train loss 282.0706787109375, val loss None
iter 20, train loss 275.22772216796875, val loss None
iter 30, train loss 275.24810791015625, val loss None
iter 40, train loss 273.95684814453125, val loss None
iter 50, train loss 273.737548828125, val loss None
iter 60, train loss 273.17578125, val loss None
iter 70, train loss 272.58319091796875, val loss None
iter 80, train loss 272.4474182128906, val loss None
iter 90, train loss 272.4259033203125, val loss None
best loss 263.7976379394531
not here
quantized in 84.49126076698303 seconds
36070 MiB free out of 48676 MiB total
16 mlp.up_proj
Pruning ...
256
iter 0, train loss 243.1584014892578, val loss None
iter 10, train loss 243.73776245117188, val loss None
iter 20, train loss 243.24130249023438, val loss None
iter 30, train loss 242.8130645751953, val loss None
iter 40, train loss 242.86781311035156, val loss None
iter 50, train loss 242.7910614013672, val loss None
iter 60, train loss 242.731689453125, val loss None
iter 70, train loss 242.53628540039062, val loss None
iter 80, train loss 242.52725219726562, val loss None
iter 90, train loss 242.54373168945312, val loss None
best loss 242.40847778320312
not here
quantized in 83.0729079246521 seconds
35876 MiB free out of 48676 MiB total
16 mlp.down_proj
Pruning ...
256
iter 0, train loss 8.226089477539062, val loss None
iter 10, train loss 8.231234550476074, val loss None
iter 20, train loss 8.19711971282959, val loss None
iter 30, train loss 8.187398910522461, val loss None
iter 40, train loss 8.183168411254883, val loss None
iter 50, train loss 8.184918403625488, val loss None
iter 60, train loss 8.173139572143555, val loss None
iter 70, train loss 8.168191909790039, val loss None
iter 80, train loss 8.162948608398438, val loss None
iter 90, train loss 8.152984619140625, val loss None
best loss 8.152740478515625
not here
quantized in 87.55095911026001 seconds
35682 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35682 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31586 MiB free out of 48676 MiB total
after cast to cpu
38316 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
17 self_attn.q_proj
Pruning ...
256
iter 0, train loss 332.5975341796875, val loss None
iter 10, train loss 352.5007019042969, val loss None
iter 20, train loss 342.406005859375, val loss None
iter 30, train loss 337.4036865234375, val loss None
iter 40, train loss 333.3568115234375, val loss None
iter 50, train loss 330.95404052734375, val loss None
iter 60, train loss 329.95135498046875, val loss None
iter 70, train loss 329.4112854003906, val loss None
iter 80, train loss 329.2210693359375, val loss None
iter 90, train loss 328.68145751953125, val loss None
best loss 306.74298095703125
not here
quantized in 34.25491714477539 seconds
36392 MiB free out of 48676 MiB total
17 self_attn.k_proj
Pruning ...
256
iter 0, train loss 362.3670654296875, val loss None
iter 10, train loss 381.4434509277344, val loss None
iter 20, train loss 381.6409912109375, val loss None
iter 30, train loss 374.21533203125, val loss None
iter 40, train loss 371.1477966308594, val loss None
iter 50, train loss 368.16949462890625, val loss None
iter 60, train loss 366.9892883300781, val loss None
iter 70, train loss 365.9882507324219, val loss None
iter 80, train loss 365.757080078125, val loss None
iter 90, train loss 365.3287353515625, val loss None
best loss 329.7947692871094
not here
quantized in 33.956430196762085 seconds
36382 MiB free out of 48676 MiB total
17 self_attn.v_proj
Pruning ...
256
iter 0, train loss 134.15826416015625, val loss None
iter 10, train loss 134.6258544921875, val loss None
iter 20, train loss 134.56549072265625, val loss None
iter 30, train loss 134.2813720703125, val loss None
iter 40, train loss 133.8651123046875, val loss None
iter 50, train loss 133.84417724609375, val loss None
iter 60, train loss 133.81124877929688, val loss None
iter 70, train loss 133.7781982421875, val loss None
iter 80, train loss 133.5544891357422, val loss None
iter 90, train loss 133.5726318359375, val loss None
best loss 133.51943969726562
not here
quantized in 31.394971132278442 seconds
36404 MiB free out of 48676 MiB total
17 self_attn.o_proj
Pruning ...
256
iter 0, train loss 9.444477081298828, val loss None
iter 10, train loss 9.042922973632812, val loss None
iter 20, train loss 8.865584373474121, val loss None
iter 30, train loss 8.841658592224121, val loss None
iter 40, train loss 8.785451889038086, val loss None
iter 50, train loss 8.83831787109375, val loss None
iter 60, train loss 8.795825958251953, val loss None
iter 70, train loss 8.749004364013672, val loss None
iter 80, train loss 8.7256441116333, val loss None
iter 90, train loss 8.716239929199219, val loss None
best loss 8.710294723510742
not here
quantized in 31.660539627075195 seconds
36404 MiB free out of 48676 MiB total
17 mlp.gate_proj
Pruning ...
256
iter 0, train loss 304.45538330078125, val loss None
iter 10, train loss 317.4798889160156, val loss None
iter 20, train loss 311.55877685546875, val loss None
iter 30, train loss 312.8865051269531, val loss None
iter 40, train loss 311.8436584472656, val loss None
iter 50, train loss 311.76702880859375, val loss None
iter 60, train loss 311.27520751953125, val loss None
iter 70, train loss 311.13916015625, val loss None
iter 80, train loss 310.92584228515625, val loss None
iter 90, train loss 310.81793212890625, val loss None
best loss 300.1724548339844
not here
quantized in 84.84930539131165 seconds
36102 MiB free out of 48676 MiB total
17 mlp.up_proj
Pruning ...
256
iter 0, train loss 267.3935546875, val loss None
iter 10, train loss 267.46490478515625, val loss None
iter 20, train loss 268.1363220214844, val loss None
iter 30, train loss 268.3250732421875, val loss None
iter 40, train loss 268.4315185546875, val loss None
iter 50, train loss 268.3992919921875, val loss None
iter 60, train loss 268.2365417480469, val loss None
iter 70, train loss 268.1387023925781, val loss None
iter 80, train loss 268.19329833984375, val loss None
iter 90, train loss 268.1580810546875, val loss None
best loss 266.8930358886719
not here
quantized in 83.08054494857788 seconds
35908 MiB free out of 48676 MiB total
17 mlp.down_proj
Pruning ...
256
iter 0, train loss 8.835625648498535, val loss None
iter 10, train loss 8.836677551269531, val loss None
iter 20, train loss 8.807083129882812, val loss None
iter 30, train loss 8.79815673828125, val loss None
iter 40, train loss 8.799357414245605, val loss None
iter 50, train loss 8.792243003845215, val loss None
iter 60, train loss 8.787769317626953, val loss None
iter 70, train loss 8.786705017089844, val loss None
iter 80, train loss 8.784542083740234, val loss None
iter 90, train loss 8.777202606201172, val loss None
best loss 8.777202606201172
not here
quantized in 87.52974343299866 seconds
35714 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35714 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31618 MiB free out of 48676 MiB total
after cast to cpu
38316 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
18 self_attn.q_proj
Pruning ...
256
iter 0, train loss 363.6089782714844, val loss None
iter 10, train loss 380.21826171875, val loss None
iter 20, train loss 365.7138977050781, val loss None
iter 30, train loss 357.44329833984375, val loss None
iter 40, train loss 353.0425720214844, val loss None
iter 50, train loss 352.0076904296875, val loss None
iter 60, train loss 350.994873046875, val loss None
iter 70, train loss 350.1744079589844, val loss None
iter 80, train loss 349.8460998535156, val loss None
iter 90, train loss 349.605224609375, val loss None
best loss 331.6770935058594
not here
quantized in 34.013450145721436 seconds
36392 MiB free out of 48676 MiB total
18 self_attn.k_proj
Pruning ...
256
iter 0, train loss 387.7427673339844, val loss None
iter 10, train loss 407.4991455078125, val loss None
iter 20, train loss 399.78509521484375, val loss None
iter 30, train loss 394.0079345703125, val loss None
iter 40, train loss 387.01885986328125, val loss None
iter 50, train loss 384.49884033203125, val loss None
iter 60, train loss 382.1899719238281, val loss None
iter 70, train loss 380.70135498046875, val loss None
iter 80, train loss 380.0419616699219, val loss None
iter 90, train loss 379.62841796875, val loss None
best loss 351.60504150390625
not here
quantized in 32.93325638771057 seconds
36382 MiB free out of 48676 MiB total
18 self_attn.v_proj
Pruning ...
256
iter 0, train loss 163.05307006835938, val loss None
iter 10, train loss 162.83274841308594, val loss None
iter 20, train loss 162.759765625, val loss None
iter 30, train loss 162.71493530273438, val loss None
iter 40, train loss 162.416259765625, val loss None
iter 50, train loss 162.6290283203125, val loss None
iter 60, train loss 162.24122619628906, val loss None
iter 70, train loss 162.05401611328125, val loss None
iter 80, train loss 162.08551025390625, val loss None
iter 90, train loss 162.04354858398438, val loss None
best loss 161.92755126953125
not here
quantized in 31.191010236740112 seconds
36404 MiB free out of 48676 MiB total
18 self_attn.o_proj
Pruning ...
256
iter 0, train loss 8.002867698669434, val loss None
iter 10, train loss 7.78547477722168, val loss None
iter 20, train loss 7.647040843963623, val loss None
iter 30, train loss 7.542506694793701, val loss None
iter 40, train loss 7.465152740478516, val loss None
iter 50, train loss 7.409364223480225, val loss None
iter 60, train loss 7.394893169403076, val loss None
iter 70, train loss 7.375661373138428, val loss None
iter 80, train loss 7.337860584259033, val loss None
iter 90, train loss 7.33870792388916, val loss None
best loss 7.324451446533203
not here
quantized in 31.599076747894287 seconds
36372 MiB free out of 48676 MiB total
18 mlp.gate_proj
Pruning ...
256
iter 0, train loss 343.8045654296875, val loss None
iter 10, train loss 358.494140625, val loss None
iter 20, train loss 352.55621337890625, val loss None
iter 30, train loss 351.6213073730469, val loss None
iter 40, train loss 351.7298278808594, val loss None
iter 50, train loss 351.285400390625, val loss None
iter 60, train loss 351.0748596191406, val loss None
iter 70, train loss 351.06048583984375, val loss None
iter 80, train loss 350.94647216796875, val loss None
iter 90, train loss 350.894287109375, val loss None
best loss 339.9788818359375
not here
quantized in 84.51159238815308 seconds
36070 MiB free out of 48676 MiB total
18 mlp.up_proj
Pruning ...
256
iter 0, train loss 294.4507141113281, val loss None
iter 10, train loss 294.81207275390625, val loss None
iter 20, train loss 294.8139953613281, val loss None
iter 30, train loss 294.27630615234375, val loss None
iter 40, train loss 294.3328857421875, val loss None
iter 50, train loss 294.33624267578125, val loss None
iter 60, train loss 294.5024719238281, val loss None
iter 70, train loss 294.54022216796875, val loss None
iter 80, train loss 294.4197082519531, val loss None
iter 90, train loss 294.4204406738281, val loss None
best loss 294.1885070800781
not here
quantized in 83.24110770225525 seconds
35876 MiB free out of 48676 MiB total
18 mlp.down_proj
Pruning ...
256
iter 0, train loss 10.499679565429688, val loss None
iter 10, train loss 10.483640670776367, val loss None
iter 20, train loss 10.430012702941895, val loss None
iter 30, train loss 10.399802207946777, val loss None
iter 40, train loss 10.389248847961426, val loss None
iter 50, train loss 10.373024940490723, val loss None
iter 60, train loss 10.361654281616211, val loss None
iter 70, train loss 10.343605041503906, val loss None
iter 80, train loss 10.335650444030762, val loss None
iter 90, train loss 10.324134826660156, val loss None
best loss 10.31926441192627
not here
quantized in 88.09630537033081 seconds
35682 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35682 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31586 MiB free out of 48676 MiB total
after cast to cpu
38316 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
19 self_attn.q_proj
Pruning ...
256
iter 0, train loss 343.93408203125, val loss None
iter 10, train loss 363.4947509765625, val loss None
iter 20, train loss 347.8731689453125, val loss None
iter 30, train loss 342.57659912109375, val loss None
iter 40, train loss 340.88763427734375, val loss None
iter 50, train loss 339.3546447753906, val loss None
iter 60, train loss 339.1138610839844, val loss None
iter 70, train loss 338.55474853515625, val loss None
iter 80, train loss 338.3490295410156, val loss None
iter 90, train loss 338.2920837402344, val loss None
best loss 318.51910400390625
not here
quantized in 34.07458686828613 seconds
36392 MiB free out of 48676 MiB total
19 self_attn.k_proj
Pruning ...
256
iter 0, train loss 369.7852478027344, val loss None
iter 10, train loss 385.28509521484375, val loss None
iter 20, train loss 382.14874267578125, val loss None
iter 30, train loss 375.4205322265625, val loss None
iter 40, train loss 371.63616943359375, val loss None
iter 50, train loss 370.64349365234375, val loss None
iter 60, train loss 369.344970703125, val loss None
iter 70, train loss 368.2134094238281, val loss None
iter 80, train loss 367.1292724609375, val loss None
iter 90, train loss 366.62982177734375, val loss None
best loss 337.75927734375
not here
quantized in 32.88445281982422 seconds
36382 MiB free out of 48676 MiB total
19 self_attn.v_proj
Pruning ...
256
iter 0, train loss 164.43882751464844, val loss None
iter 10, train loss 164.564697265625, val loss None
iter 20, train loss 164.4947509765625, val loss None
iter 30, train loss 164.307861328125, val loss None
iter 40, train loss 164.0338134765625, val loss None
iter 50, train loss 163.89988708496094, val loss None
iter 60, train loss 163.69277954101562, val loss None
iter 70, train loss 163.5137176513672, val loss None
iter 80, train loss 163.44822692871094, val loss None
iter 90, train loss 163.38133239746094, val loss None
best loss 163.32481384277344
not here
quantized in 31.209455490112305 seconds
36372 MiB free out of 48676 MiB total
19 self_attn.o_proj
Pruning ...
256
iter 0, train loss 8.508294105529785, val loss None
iter 10, train loss 8.106195449829102, val loss None
iter 20, train loss 8.025167465209961, val loss None
iter 30, train loss 7.953927993774414, val loss None
iter 40, train loss 7.885617256164551, val loss None
iter 50, train loss 7.87116003036499, val loss None
iter 60, train loss 7.833004951477051, val loss None
iter 70, train loss 7.803319454193115, val loss None
iter 80, train loss 7.792136192321777, val loss None
iter 90, train loss 7.79136848449707, val loss None
best loss 7.770890235900879
not here
quantized in 31.444604635238647 seconds
36372 MiB free out of 48676 MiB total
19 mlp.gate_proj
Pruning ...
256
iter 0, train loss 366.3642578125, val loss None
iter 10, train loss 380.2433166503906, val loss None
iter 20, train loss 376.5439758300781, val loss None
iter 30, train loss 377.2713317871094, val loss None
iter 40, train loss 377.001708984375, val loss None
iter 50, train loss 377.0948181152344, val loss None
iter 60, train loss 377.0889587402344, val loss None
iter 70, train loss 377.1678466796875, val loss None
iter 80, train loss 377.334716796875, val loss None
iter 90, train loss 377.3562316894531, val loss None
best loss 362.99700927734375
not here
quantized in 84.70809388160706 seconds
36070 MiB free out of 48676 MiB total
19 mlp.up_proj
Pruning ...
256
iter 0, train loss 314.71893310546875, val loss None
iter 10, train loss 314.3792419433594, val loss None
iter 20, train loss 315.06463623046875, val loss None
iter 30, train loss 315.0007629394531, val loss None
iter 40, train loss 315.161376953125, val loss None
iter 50, train loss 315.2499084472656, val loss None
iter 60, train loss 314.9268798828125, val loss None
iter 70, train loss 314.8743591308594, val loss None
iter 80, train loss 314.8821105957031, val loss None
iter 90, train loss 314.88897705078125, val loss None
best loss 314.16217041015625
not here
quantized in 83.44109535217285 seconds
35876 MiB free out of 48676 MiB total
19 mlp.down_proj
Pruning ...
256
iter 0, train loss 11.292244911193848, val loss None
iter 10, train loss 11.289192199707031, val loss None
iter 20, train loss 11.261638641357422, val loss None
iter 30, train loss 11.233489990234375, val loss None
iter 40, train loss 11.22746467590332, val loss None
iter 50, train loss 11.221678733825684, val loss None
iter 60, train loss 11.222489356994629, val loss None
iter 70, train loss 11.219344139099121, val loss None
iter 80, train loss 11.211686134338379, val loss None
iter 90, train loss 11.213812828063965, val loss None
best loss 11.207427978515625
not here
quantized in 87.43518137931824 seconds
35682 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35682 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31586 MiB free out of 48676 MiB total
after cast to cpu
38316 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
20 self_attn.q_proj
Pruning ...
256
iter 0, train loss 357.4976806640625, val loss None
iter 10, train loss 373.66510009765625, val loss None
iter 20, train loss 365.1379699707031, val loss None
iter 30, train loss 357.963623046875, val loss None
iter 40, train loss 354.5197448730469, val loss None
iter 50, train loss 350.25982666015625, val loss None
iter 60, train loss 349.4108581542969, val loss None
iter 70, train loss 348.9765319824219, val loss None
iter 80, train loss 348.39752197265625, val loss None
iter 90, train loss 347.87872314453125, val loss None
best loss 329.78411865234375
not here
quantized in 33.90190148353577 seconds
36392 MiB free out of 48676 MiB total
20 self_attn.k_proj
Pruning ...
256
iter 0, train loss 379.68768310546875, val loss None
iter 10, train loss 397.9279479980469, val loss None
iter 20, train loss 394.6248779296875, val loss None
iter 30, train loss 388.80535888671875, val loss None
iter 40, train loss 384.53167724609375, val loss None
iter 50, train loss 382.5837707519531, val loss None
iter 60, train loss 381.0048828125, val loss None
iter 70, train loss 380.47491455078125, val loss None
iter 80, train loss 379.7644958496094, val loss None
iter 90, train loss 379.5146179199219, val loss None
best loss 348.4600524902344
not here
quantized in 32.6830530166626 seconds
36382 MiB free out of 48676 MiB total
20 self_attn.v_proj
Pruning ...
256
iter 0, train loss 170.76602172851562, val loss None
iter 10, train loss 170.38336181640625, val loss None
iter 20, train loss 170.34872436523438, val loss None
iter 30, train loss 170.28750610351562, val loss None
iter 40, train loss 170.0349884033203, val loss None
iter 50, train loss 169.7602996826172, val loss None
iter 60, train loss 169.72482299804688, val loss None
iter 70, train loss 169.3806610107422, val loss None
iter 80, train loss 169.4319305419922, val loss None
iter 90, train loss 169.5394287109375, val loss None
best loss 169.3748779296875
not here
quantized in 31.0266432762146 seconds
36404 MiB free out of 48676 MiB total
20 self_attn.o_proj
Pruning ...
256
iter 0, train loss 12.994750022888184, val loss None
iter 10, train loss 11.482819557189941, val loss None
iter 20, train loss 10.698726654052734, val loss None
iter 30, train loss 10.054605484008789, val loss None
iter 40, train loss 9.87716293334961, val loss None
iter 50, train loss 9.698038101196289, val loss None
iter 60, train loss 9.596763610839844, val loss None
iter 70, train loss 9.575708389282227, val loss None
iter 80, train loss 9.53035831451416, val loss None
iter 90, train loss 9.490368843078613, val loss None
best loss 9.45470905303955
not here
quantized in 31.90498423576355 seconds
36372 MiB free out of 48676 MiB total
20 mlp.gate_proj
Pruning ...
256
iter 0, train loss 397.8997497558594, val loss None
iter 10, train loss 414.1456604003906, val loss None
iter 20, train loss 409.4697265625, val loss None
iter 30, train loss 409.67218017578125, val loss None
iter 40, train loss 408.8426513671875, val loss None
iter 50, train loss 408.3284606933594, val loss None
iter 60, train loss 408.1302490234375, val loss None
iter 70, train loss 407.9087829589844, val loss None
iter 80, train loss 407.97900390625, val loss None
iter 90, train loss 408.3042907714844, val loss None
best loss 393.7396240234375
not here
quantized in 84.22539925575256 seconds
36070 MiB free out of 48676 MiB total
20 mlp.up_proj
Pruning ...
256
iter 0, train loss 338.4665222167969, val loss None
iter 10, train loss 339.064697265625, val loss None
iter 20, train loss 340.08392333984375, val loss None
iter 30, train loss 339.67254638671875, val loss None
iter 40, train loss 339.81695556640625, val loss None
iter 50, train loss 339.6101989746094, val loss None
iter 60, train loss 339.96844482421875, val loss None
iter 70, train loss 340.04669189453125, val loss None
iter 80, train loss 339.8129577636719, val loss None
iter 90, train loss 339.696533203125, val loss None
best loss 338.2867431640625
not here
quantized in 83.19009447097778 seconds
35876 MiB free out of 48676 MiB total
20 mlp.down_proj
Pruning ...
256
iter 0, train loss 14.069886207580566, val loss None
iter 10, train loss 14.040152549743652, val loss None
iter 20, train loss 13.97131061553955, val loss None
iter 30, train loss 13.937190055847168, val loss None
iter 40, train loss 13.914108276367188, val loss None
iter 50, train loss 13.899988174438477, val loss None
iter 60, train loss 13.89320182800293, val loss None
iter 70, train loss 13.88321590423584, val loss None
iter 80, train loss 13.875870704650879, val loss None
iter 90, train loss 13.872774124145508, val loss None
best loss 13.862434387207031
not here
quantized in 87.61149477958679 seconds
35682 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35682 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31586 MiB free out of 48676 MiB total
after cast to cpu
38316 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
21 self_attn.q_proj
Pruning ...
256
iter 0, train loss 369.11700439453125, val loss None
iter 10, train loss 387.69683837890625, val loss None
iter 20, train loss 377.3990783691406, val loss None
iter 30, train loss 370.0777893066406, val loss None
iter 40, train loss 367.4933166503906, val loss None
iter 50, train loss 365.5157470703125, val loss None
iter 60, train loss 364.4365234375, val loss None
iter 70, train loss 363.6777038574219, val loss None
iter 80, train loss 362.91259765625, val loss None
iter 90, train loss 362.5357360839844, val loss None
best loss 347.1548767089844
not here
quantized in 33.75329327583313 seconds
36392 MiB free out of 48676 MiB total
21 self_attn.k_proj
Pruning ...
256
iter 0, train loss 384.5835876464844, val loss None
iter 10, train loss 404.02215576171875, val loss None
iter 20, train loss 391.8774108886719, val loss None
iter 30, train loss 384.91766357421875, val loss None
iter 40, train loss 382.38079833984375, val loss None
iter 50, train loss 381.22442626953125, val loss None
iter 60, train loss 379.82666015625, val loss None
iter 70, train loss 379.40234375, val loss None
iter 80, train loss 378.90789794921875, val loss None
iter 90, train loss 378.9279479980469, val loss None
best loss 363.1358642578125
not here
quantized in 32.24542546272278 seconds
36382 MiB free out of 48676 MiB total
21 self_attn.v_proj
Pruning ...
256
iter 0, train loss 202.6490478515625, val loss None
iter 10, train loss 202.52838134765625, val loss None
iter 20, train loss 201.98956298828125, val loss None
iter 30, train loss 201.71034240722656, val loss None
iter 40, train loss 201.70115661621094, val loss None
iter 50, train loss 201.60501098632812, val loss None
iter 60, train loss 201.1133270263672, val loss None
iter 70, train loss 200.962158203125, val loss None
iter 80, train loss 200.9514923095703, val loss None
iter 90, train loss 200.6455078125, val loss None
best loss 200.4985809326172
not here
quantized in 31.087849855422974 seconds
36404 MiB free out of 48676 MiB total
21 self_attn.o_proj
Pruning ...
256
iter 0, train loss 9.288582801818848, val loss None
iter 10, train loss 8.598600387573242, val loss None
iter 20, train loss 8.342557907104492, val loss None
iter 30, train loss 8.245584487915039, val loss None
iter 40, train loss 8.118995666503906, val loss None
iter 50, train loss 8.063029289245605, val loss None
iter 60, train loss 7.987333297729492, val loss None
iter 70, train loss 7.957552433013916, val loss None
iter 80, train loss 7.936233043670654, val loss None
iter 90, train loss 7.923833847045898, val loss None
best loss 7.900420188903809
not here
quantized in 31.68135714530945 seconds
36404 MiB free out of 48676 MiB total
21 mlp.gate_proj
Pruning ...
256
iter 0, train loss 423.510009765625, val loss None
iter 10, train loss 440.3016357421875, val loss None
iter 20, train loss 436.1874694824219, val loss None
iter 30, train loss 435.7509765625, val loss None
iter 40, train loss 434.94110107421875, val loss None
iter 50, train loss 435.0016174316406, val loss None
iter 60, train loss 435.1729736328125, val loss None
iter 70, train loss 434.7873229980469, val loss None
iter 80, train loss 434.8096008300781, val loss None
iter 90, train loss 434.6434631347656, val loss None
best loss 419.908203125
not here
quantized in 84.12693786621094 seconds
36102 MiB free out of 48676 MiB total
21 mlp.up_proj
Pruning ...
256
iter 0, train loss 356.5351257324219, val loss None
iter 10, train loss 356.118408203125, val loss None
iter 20, train loss 357.5152282714844, val loss None
iter 30, train loss 358.03271484375, val loss None
iter 40, train loss 358.0904541015625, val loss None
iter 50, train loss 358.2026062011719, val loss None
iter 60, train loss 357.8848571777344, val loss None
iter 70, train loss 357.9462890625, val loss None
iter 80, train loss 358.01416015625, val loss None
iter 90, train loss 357.99774169921875, val loss None
best loss 355.88543701171875
not here
quantized in 83.6220178604126 seconds
35908 MiB free out of 48676 MiB total
21 mlp.down_proj
Pruning ...
256
iter 0, train loss 14.592113494873047, val loss None
iter 10, train loss 14.556572914123535, val loss None
iter 20, train loss 14.437688827514648, val loss None
iter 30, train loss 14.354920387268066, val loss None
iter 40, train loss 14.281919479370117, val loss None
iter 50, train loss 14.242589950561523, val loss None
iter 60, train loss 14.213000297546387, val loss None
iter 70, train loss 14.199952125549316, val loss None
iter 80, train loss 14.180726051330566, val loss None
iter 90, train loss 14.162084579467773, val loss None
best loss 14.157464981079102
not here
quantized in 88.0687358379364 seconds
35714 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35714 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31618 MiB free out of 48676 MiB total
after cast to cpu
38316 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
22 self_attn.q_proj
Pruning ...
256
iter 0, train loss 389.9894104003906, val loss None
iter 10, train loss 414.9422607421875, val loss None
iter 20, train loss 399.12774658203125, val loss None
iter 30, train loss 392.072509765625, val loss None
iter 40, train loss 387.3503112792969, val loss None
iter 50, train loss 385.96868896484375, val loss None
iter 60, train loss 385.27777099609375, val loss None
iter 70, train loss 384.65228271484375, val loss None
iter 80, train loss 384.2250671386719, val loss None
iter 90, train loss 383.9366760253906, val loss None
best loss 371.2781982421875
not here
quantized in 33.63652563095093 seconds
36392 MiB free out of 48676 MiB total
22 self_attn.k_proj
Pruning ...
256
iter 0, train loss 416.39007568359375, val loss None
iter 10, train loss 432.4591979980469, val loss None
iter 20, train loss 424.4452819824219, val loss None
iter 30, train loss 417.17266845703125, val loss None
iter 40, train loss 413.56768798828125, val loss None
iter 50, train loss 412.30401611328125, val loss None
iter 60, train loss 412.02886962890625, val loss None
iter 70, train loss 412.17822265625, val loss None
iter 80, train loss 411.3975830078125, val loss None
iter 90, train loss 410.97259521484375, val loss None
best loss 390.87359619140625
not here
quantized in 32.50059938430786 seconds
36382 MiB free out of 48676 MiB total
22 self_attn.v_proj
Pruning ...
256
iter 0, train loss 209.32012939453125, val loss None
iter 10, train loss 209.17349243164062, val loss None
iter 20, train loss 209.1243896484375, val loss None
iter 30, train loss 208.96986389160156, val loss None
iter 40, train loss 208.54930114746094, val loss None
iter 50, train loss 208.50877380371094, val loss None
iter 60, train loss 208.6451873779297, val loss None
iter 70, train loss 208.5440216064453, val loss None
iter 80, train loss 208.3131103515625, val loss None
iter 90, train loss 208.20791625976562, val loss None
best loss 208.20791625976562
not here
quantized in 31.119375467300415 seconds
36404 MiB free out of 48676 MiB total
22 self_attn.o_proj
Pruning ...
256
iter 0, train loss 52.70599365234375, val loss None
iter 10, train loss 36.518638610839844, val loss None
iter 20, train loss 28.009599685668945, val loss None
iter 30, train loss 20.005176544189453, val loss None
iter 40, train loss 16.72870445251465, val loss None
iter 50, train loss 14.8836669921875, val loss None
iter 60, train loss 14.719057083129883, val loss None
iter 70, train loss 14.282767295837402, val loss None
iter 80, train loss 14.256874084472656, val loss None
iter 90, train loss 14.055459976196289, val loss None
best loss 13.991211891174316
not here
quantized in 32.656296491622925 seconds
36372 MiB free out of 48676 MiB total
22 mlp.gate_proj
Pruning ...
256
iter 0, train loss 435.8123779296875, val loss None
iter 10, train loss 450.20123291015625, val loss None
iter 20, train loss 447.3675537109375, val loss None
iter 30, train loss 446.70513916015625, val loss None
iter 40, train loss 446.05596923828125, val loss None
iter 50, train loss 446.43817138671875, val loss None
iter 60, train loss 446.50787353515625, val loss None
iter 70, train loss 446.5172119140625, val loss None
iter 80, train loss 446.5932312011719, val loss None
iter 90, train loss 446.38970947265625, val loss None
best loss 432.4262390136719
not here
quantized in 84.46707010269165 seconds
36070 MiB free out of 48676 MiB total
22 mlp.up_proj
Pruning ...
256
iter 0, train loss 362.70611572265625, val loss None
iter 10, train loss 363.3460693359375, val loss None
iter 20, train loss 363.5850830078125, val loss None
iter 30, train loss 363.69781494140625, val loss None
iter 40, train loss 363.9658203125, val loss None
iter 50, train loss 364.06768798828125, val loss None
iter 60, train loss 363.922119140625, val loss None
iter 70, train loss 363.9413757324219, val loss None
iter 80, train loss 363.9979248046875, val loss None
iter 90, train loss 364.0203552246094, val loss None
best loss 362.70611572265625
not here
quantized in 83.61224460601807 seconds
35876 MiB free out of 48676 MiB total
22 mlp.down_proj
Pruning ...
256
iter 0, train loss 15.31747817993164, val loss None
iter 10, train loss 15.337116241455078, val loss None
iter 20, train loss 15.28509521484375, val loss None
iter 30, train loss 15.267446517944336, val loss None
iter 40, train loss 15.283121109008789, val loss None
iter 50, train loss 15.268745422363281, val loss None
iter 60, train loss 15.246675491333008, val loss None
iter 70, train loss 15.244156837463379, val loss None
iter 80, train loss 15.248100280761719, val loss None
iter 90, train loss 15.235301971435547, val loss None
best loss 15.235301971435547
not here
quantized in 88.11778283119202 seconds
35682 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35682 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31586 MiB free out of 48676 MiB total
after cast to cpu
38316 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
23 self_attn.q_proj
Pruning ...
256
iter 0, train loss 426.7716064453125, val loss None
iter 10, train loss 439.5893859863281, val loss None
iter 20, train loss 427.21209716796875, val loss None
iter 30, train loss 426.37152099609375, val loss None
iter 40, train loss 424.02899169921875, val loss None
iter 50, train loss 422.4444885253906, val loss None
iter 60, train loss 422.01025390625, val loss None
iter 70, train loss 421.961181640625, val loss None
iter 80, train loss 421.7875671386719, val loss None
iter 90, train loss 421.38287353515625, val loss None
best loss 410.566162109375
not here
quantized in 33.69705843925476 seconds
36392 MiB free out of 48676 MiB total
23 self_attn.k_proj
Pruning ...
256
iter 0, train loss 443.0794677734375, val loss None
iter 10, train loss 470.7133483886719, val loss None
iter 20, train loss 457.91552734375, val loss None
iter 30, train loss 447.5960693359375, val loss None
iter 40, train loss 445.17413330078125, val loss None
iter 50, train loss 442.3646240234375, val loss None
iter 60, train loss 441.18939208984375, val loss None
iter 70, train loss 439.6338195800781, val loss None
iter 80, train loss 438.7545471191406, val loss None
iter 90, train loss 438.5963439941406, val loss None
best loss 424.26397705078125
not here
quantized in 32.81941771507263 seconds
36382 MiB free out of 48676 MiB total
23 self_attn.v_proj
Pruning ...
256
iter 0, train loss 258.93377685546875, val loss None
iter 10, train loss 258.6898193359375, val loss None
iter 20, train loss 258.2419738769531, val loss None
iter 30, train loss 258.678466796875, val loss None
iter 40, train loss 257.9963684082031, val loss None
iter 50, train loss 257.4888610839844, val loss None
iter 60, train loss 257.8740539550781, val loss None
iter 70, train loss 257.5857238769531, val loss None
iter 80, train loss 257.570556640625, val loss None
iter 90, train loss 257.7715148925781, val loss None
best loss 257.4261779785156
not here
quantized in 31.420294523239136 seconds
36404 MiB free out of 48676 MiB total
23 self_attn.o_proj
Pruning ...
256
iter 0, train loss 11.527525901794434, val loss None
iter 10, train loss 11.389487266540527, val loss None
iter 20, train loss 11.343737602233887, val loss None
iter 30, train loss 11.274948120117188, val loss None
iter 40, train loss 11.229422569274902, val loss None
iter 50, train loss 11.191959381103516, val loss None
iter 60, train loss 11.198773384094238, val loss None
iter 70, train loss 11.230920791625977, val loss None
iter 80, train loss 11.21219539642334, val loss None
iter 90, train loss 11.18525218963623, val loss None
best loss 11.157150268554688
not here
quantized in 31.709357738494873 seconds
36372 MiB free out of 48676 MiB total
23 mlp.gate_proj
Pruning ...
256
iter 0, train loss 486.06597900390625, val loss None
iter 10, train loss 497.9043884277344, val loss None
iter 20, train loss 495.490966796875, val loss None
iter 30, train loss 495.7657470703125, val loss None
iter 40, train loss 496.0581359863281, val loss None
iter 50, train loss 496.26666259765625, val loss None
iter 60, train loss 496.07122802734375, val loss None
iter 70, train loss 495.65704345703125, val loss None
iter 80, train loss 495.3773498535156, val loss None
iter 90, train loss 495.3036193847656, val loss None
best loss 483.38970947265625
not here
quantized in 84.60380244255066 seconds
36070 MiB free out of 48676 MiB total
23 mlp.up_proj
Pruning ...
256
iter 0, train loss 406.09857177734375, val loss None
iter 10, train loss 405.65802001953125, val loss None
iter 20, train loss 406.1836242675781, val loss None
iter 30, train loss 406.3721618652344, val loss None
iter 40, train loss 406.2086181640625, val loss None
iter 50, train loss 406.4840087890625, val loss None
iter 60, train loss 406.3015441894531, val loss None
iter 70, train loss 406.35955810546875, val loss None
iter 80, train loss 406.2218017578125, val loss None
iter 90, train loss 406.11871337890625, val loss None
best loss 405.4078674316406
not here
quantized in 82.95650792121887 seconds
35876 MiB free out of 48676 MiB total
23 mlp.down_proj
Pruning ...
256
iter 0, train loss 17.814048767089844, val loss None
iter 10, train loss 17.80783462524414, val loss None
iter 20, train loss 17.7708740234375, val loss None
iter 30, train loss 17.75623321533203, val loss None
iter 40, train loss 17.744792938232422, val loss None
iter 50, train loss 17.730857849121094, val loss None
iter 60, train loss 17.713180541992188, val loss None
iter 70, train loss 17.70775032043457, val loss None
iter 80, train loss 17.70455551147461, val loss None
iter 90, train loss 17.696073532104492, val loss None
best loss 17.69489097595215
not here
quantized in 87.28819036483765 seconds
35682 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35682 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31586 MiB free out of 48676 MiB total
after cast to cpu
38316 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
24 self_attn.q_proj
Pruning ...
256
iter 0, train loss 396.07025146484375, val loss None
iter 10, train loss 406.71331787109375, val loss None
iter 20, train loss 390.8262939453125, val loss None
iter 30, train loss 390.8555908203125, val loss None
iter 40, train loss 389.81695556640625, val loss None
iter 50, train loss 389.76220703125, val loss None
iter 60, train loss 389.76666259765625, val loss None
iter 70, train loss 389.150390625, val loss None
iter 80, train loss 389.00189208984375, val loss None
iter 90, train loss 388.90234375, val loss None
best loss 377.9822082519531
not here
quantized in 36.959407567977905 seconds
36392 MiB free out of 48676 MiB total
24 self_attn.k_proj
Pruning ...
256
iter 0, train loss 408.066650390625, val loss None
iter 10, train loss 428.5695495605469, val loss None
iter 20, train loss 420.7886657714844, val loss None
iter 30, train loss 413.5626220703125, val loss None
iter 40, train loss 411.12725830078125, val loss None
iter 50, train loss 409.47735595703125, val loss None
iter 60, train loss 407.7159118652344, val loss None
iter 70, train loss 405.4664001464844, val loss None
iter 80, train loss 405.314453125, val loss None
iter 90, train loss 405.177001953125, val loss None
best loss 389.31427001953125
not here
quantized in 46.399502992630005 seconds
36382 MiB free out of 48676 MiB total
24 self_attn.v_proj
Pruning ...
256
iter 0, train loss 247.50424194335938, val loss None
iter 10, train loss 247.197509765625, val loss None
iter 20, train loss 246.64224243164062, val loss None
iter 30, train loss 246.373779296875, val loss None
iter 40, train loss 246.6094207763672, val loss None
iter 50, train loss 246.12747192382812, val loss None
iter 60, train loss 246.0172576904297, val loss None
iter 70, train loss 246.0263671875, val loss None
iter 80, train loss 246.12020874023438, val loss None
iter 90, train loss 246.13235473632812, val loss None
best loss 245.91580200195312
not here
quantized in 42.89543294906616 seconds
36372 MiB free out of 48676 MiB total
24 self_attn.o_proj
Pruning ...
256
iter 0, train loss 28.134838104248047, val loss None
iter 10, train loss 21.802751541137695, val loss None
iter 20, train loss 18.40604019165039, val loss None
iter 30, train loss 16.614852905273438, val loss None
iter 40, train loss 16.075613021850586, val loss None
iter 50, train loss 15.866641998291016, val loss None
iter 60, train loss 15.802642822265625, val loss None
iter 70, train loss 15.624507904052734, val loss None
iter 80, train loss 15.55392074584961, val loss None
iter 90, train loss 15.485901832580566, val loss None
best loss 15.45376205444336
not here
quantized in 37.37874484062195 seconds
36372 MiB free out of 48676 MiB total
24 mlp.gate_proj
Pruning ...
256
iter 0, train loss 506.9034729003906, val loss None
iter 10, train loss 519.354736328125, val loss None
iter 20, train loss 517.7862548828125, val loss None
iter 30, train loss 516.9439697265625, val loss None
iter 40, train loss 516.7073974609375, val loss None
iter 50, train loss 516.593505859375, val loss None
iter 60, train loss 516.3303833007812, val loss None
iter 70, train loss 516.0435180664062, val loss None
iter 80, train loss 515.8451538085938, val loss None
iter 90, train loss 515.5955200195312, val loss None
best loss 504.3601989746094
not here
quantized in 84.43569254875183 seconds
36070 MiB free out of 48676 MiB total
24 mlp.up_proj
Pruning ...
256
iter 0, train loss 425.75396728515625, val loss None
iter 10, train loss 426.267578125, val loss None
iter 20, train loss 426.25115966796875, val loss None
iter 30, train loss 426.642578125, val loss None
iter 40, train loss 426.6741027832031, val loss None
iter 50, train loss 427.04058837890625, val loss None
iter 60, train loss 426.92523193359375, val loss None
iter 70, train loss 426.844482421875, val loss None
iter 80, train loss 426.85638427734375, val loss None
iter 90, train loss 426.9515380859375, val loss None
best loss 425.75396728515625
not here
quantized in 83.05975651741028 seconds
35876 MiB free out of 48676 MiB total
24 mlp.down_proj
Pruning ...
256
iter 0, train loss 18.804893493652344, val loss None
iter 10, train loss 18.815670013427734, val loss None
iter 20, train loss 18.772966384887695, val loss None
iter 30, train loss 18.74806022644043, val loss None
iter 40, train loss 18.735830307006836, val loss None
iter 50, train loss 18.825002670288086, val loss None
iter 60, train loss 18.815841674804688, val loss None
iter 70, train loss 18.81291961669922, val loss None
iter 80, train loss 18.818370819091797, val loss None
iter 90, train loss 18.81319808959961, val loss None
best loss 18.72992515563965
not here
quantized in 87.6249852180481 seconds
35682 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35682 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31586 MiB free out of 48676 MiB total
after cast to cpu
38316 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
25 self_attn.q_proj
Pruning ...
256
iter 0, train loss 453.2187805175781, val loss None
iter 10, train loss 469.4902648925781, val loss None
iter 20, train loss 447.21453857421875, val loss None
iter 30, train loss 447.6728515625, val loss None
iter 40, train loss 449.65960693359375, val loss None
iter 50, train loss 448.5724792480469, val loss None
iter 60, train loss 447.06329345703125, val loss None
iter 70, train loss 446.3564147949219, val loss None
iter 80, train loss 445.96435546875, val loss None
iter 90, train loss 445.6933288574219, val loss None
best loss 438.1131896972656
not here
quantized in 33.44666504859924 seconds
36424 MiB free out of 48676 MiB total
25 self_attn.k_proj
Pruning ...
256
iter 0, train loss 470.5440979003906, val loss None
iter 10, train loss 492.89312744140625, val loss None
iter 20, train loss 475.97564697265625, val loss None
iter 30, train loss 465.2196350097656, val loss None
iter 40, train loss 463.40362548828125, val loss None
iter 50, train loss 463.9566345214844, val loss None
iter 60, train loss 463.1902160644531, val loss None
iter 70, train loss 463.40765380859375, val loss None
iter 80, train loss 462.89886474609375, val loss None
iter 90, train loss 462.5461120605469, val loss None
best loss 451.8793029785156
not here
quantized in 32.10547637939453 seconds
36414 MiB free out of 48676 MiB total
25 self_attn.v_proj
Pruning ...
256
iter 0, train loss 308.19964599609375, val loss None
iter 10, train loss 307.9971618652344, val loss None
iter 20, train loss 307.968505859375, val loss None
iter 30, train loss 307.68511962890625, val loss None
iter 40, train loss 307.1490173339844, val loss None
iter 50, train loss 307.5330505371094, val loss None
iter 60, train loss 306.9995422363281, val loss None
iter 70, train loss 307.18438720703125, val loss None
iter 80, train loss 306.9915771484375, val loss None
iter 90, train loss 306.8923645019531, val loss None
best loss 306.742919921875
not here
quantized in 31.187670469284058 seconds
36404 MiB free out of 48676 MiB total
25 self_attn.o_proj
Pruning ...
256
iter 0, train loss 16.152862548828125, val loss None
iter 10, train loss 16.43769073486328, val loss None
iter 20, train loss 16.815048217773438, val loss None
iter 30, train loss 16.573259353637695, val loss None
iter 40, train loss 16.317628860473633, val loss None
iter 50, train loss 16.21112060546875, val loss None
iter 60, train loss 16.19412612915039, val loss None
iter 70, train loss 16.102476119995117, val loss None
iter 80, train loss 16.103515625, val loss None
iter 90, train loss 16.166669845581055, val loss None
best loss 15.921381950378418
not here
quantized in 31.676814794540405 seconds
36372 MiB free out of 48676 MiB total
25 mlp.gate_proj
Pruning ...
256
iter 0, train loss 548.5563354492188, val loss None
iter 10, train loss 563.7935791015625, val loss None
iter 20, train loss 563.90771484375, val loss None
iter 30, train loss 562.2913208007812, val loss None
iter 40, train loss 560.9995727539062, val loss None
iter 50, train loss 561.2225341796875, val loss None
iter 60, train loss 561.8057861328125, val loss None
iter 70, train loss 561.9468994140625, val loss None
iter 80, train loss 561.939208984375, val loss None
iter 90, train loss 561.9678955078125, val loss None
best loss 545.3250732421875
not here
quantized in 85.01635932922363 seconds
36070 MiB free out of 48676 MiB total
25 mlp.up_proj
Pruning ...
256
iter 0, train loss 460.56781005859375, val loss None
iter 10, train loss 460.73956298828125, val loss None
iter 20, train loss 459.6347961425781, val loss None
iter 30, train loss 460.18994140625, val loss None
iter 40, train loss 460.169921875, val loss None
iter 50, train loss 460.26922607421875, val loss None
iter 60, train loss 460.4422607421875, val loss None
iter 70, train loss 460.3695983886719, val loss None
iter 80, train loss 460.2366638183594, val loss None
iter 90, train loss 460.0925598144531, val loss None
best loss 459.4368591308594
not here
quantized in 83.34371185302734 seconds
35876 MiB free out of 48676 MiB total
25 mlp.down_proj
Pruning ...
256
iter 0, train loss 19.867511749267578, val loss None
iter 10, train loss 19.885692596435547, val loss None
iter 20, train loss 19.841697692871094, val loss None
iter 30, train loss 19.80516242980957, val loss None
iter 40, train loss 19.798391342163086, val loss None
iter 50, train loss 19.785680770874023, val loss None
iter 60, train loss 19.788471221923828, val loss None
iter 70, train loss 19.783414840698242, val loss None
iter 80, train loss 19.778350830078125, val loss None
iter 90, train loss 19.76907730102539, val loss None
best loss 19.768089294433594
not here
quantized in 87.2025089263916 seconds
35682 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35682 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31586 MiB free out of 48676 MiB total
after cast to cpu
38316 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
26 self_attn.q_proj
Pruning ...
256
iter 0, train loss 432.35723876953125, val loss None
iter 10, train loss 453.65130615234375, val loss None
iter 20, train loss 440.913330078125, val loss None
iter 30, train loss 431.2801513671875, val loss None
iter 40, train loss 428.6549072265625, val loss None
iter 50, train loss 426.37799072265625, val loss None
iter 60, train loss 424.18841552734375, val loss None
iter 70, train loss 423.39312744140625, val loss None
iter 80, train loss 423.8170166015625, val loss None
iter 90, train loss 423.7323913574219, val loss None
best loss 411.27606201171875
not here
quantized in 33.53357791900635 seconds
36392 MiB free out of 48676 MiB total
26 self_attn.k_proj
Pruning ...
256
iter 0, train loss 454.3604736328125, val loss None
iter 10, train loss 478.1477966308594, val loss None
iter 20, train loss 469.3955078125, val loss None
iter 30, train loss 461.77685546875, val loss None
iter 40, train loss 460.90423583984375, val loss None
iter 50, train loss 457.84014892578125, val loss None
iter 60, train loss 456.7564697265625, val loss None
iter 70, train loss 456.0704040527344, val loss None
iter 80, train loss 455.3736572265625, val loss None
iter 90, train loss 454.32666015625, val loss None
best loss 427.0035400390625
not here
quantized in 32.40343880653381 seconds
36382 MiB free out of 48676 MiB total
26 self_attn.v_proj
Pruning ...
256
iter 0, train loss 304.6563720703125, val loss None
iter 10, train loss 304.24652099609375, val loss None
iter 20, train loss 304.04290771484375, val loss None
iter 30, train loss 303.3543395996094, val loss None
iter 40, train loss 303.4547424316406, val loss None
iter 50, train loss 302.56866455078125, val loss None
iter 60, train loss 302.55718994140625, val loss None
iter 70, train loss 302.618896484375, val loss None
iter 80, train loss 302.70184326171875, val loss None
iter 90, train loss 302.619140625, val loss None
best loss 302.4499206542969
not here
quantized in 31.268406867980957 seconds
36404 MiB free out of 48676 MiB total
26 self_attn.o_proj
Pruning ...
256
iter 0, train loss 35.86603927612305, val loss None
iter 10, train loss 27.102123260498047, val loss None
iter 20, train loss 22.455717086791992, val loss None
iter 30, train loss 20.73025131225586, val loss None
iter 40, train loss 20.046001434326172, val loss None
iter 50, train loss 19.721288681030273, val loss None
iter 60, train loss 19.40664291381836, val loss None
iter 70, train loss 19.280941009521484, val loss None
iter 80, train loss 19.146041870117188, val loss None
iter 90, train loss 19.13435173034668, val loss None
best loss 19.05048179626465
not here
quantized in 31.917300939559937 seconds
36404 MiB free out of 48676 MiB total
26 mlp.gate_proj
Pruning ...
256
iter 0, train loss 573.5517578125, val loss None
iter 10, train loss 595.6795654296875, val loss None
iter 20, train loss 593.4063720703125, val loss None
iter 30, train loss 593.7708129882812, val loss None
iter 40, train loss 591.9547729492188, val loss None
iter 50, train loss 591.6878662109375, val loss None
iter 60, train loss 592.2772827148438, val loss None
iter 70, train loss 592.515869140625, val loss None
iter 80, train loss 592.2199096679688, val loss None
iter 90, train loss 592.2784423828125, val loss None
best loss 567.5054321289062
not here
quantized in 84.70626950263977 seconds
36102 MiB free out of 48676 MiB total
26 mlp.up_proj
Pruning ...
256
iter 0, train loss 484.88922119140625, val loss None
iter 10, train loss 484.54522705078125, val loss None
iter 20, train loss 485.9648132324219, val loss None
iter 30, train loss 484.9089050292969, val loss None
iter 40, train loss 485.8508605957031, val loss None
iter 50, train loss 485.6355895996094, val loss None
iter 60, train loss 485.38458251953125, val loss None
iter 70, train loss 485.5092468261719, val loss None
iter 80, train loss 485.4986267089844, val loss None
iter 90, train loss 485.5812683105469, val loss None
best loss 483.75250244140625
not here
quantized in 83.61557483673096 seconds
35908 MiB free out of 48676 MiB total
26 mlp.down_proj
Pruning ...
256
iter 0, train loss 21.474163055419922, val loss None
iter 10, train loss 21.512176513671875, val loss None
iter 20, train loss 21.445083618164062, val loss None
iter 30, train loss 21.44747543334961, val loss None
iter 40, train loss 21.420982360839844, val loss None
iter 50, train loss 21.42230796813965, val loss None
iter 60, train loss 21.406902313232422, val loss None
iter 70, train loss 21.39272117614746, val loss None
iter 80, train loss 21.384172439575195, val loss None
iter 90, train loss 21.393543243408203, val loss None
best loss 21.382793426513672
not here
quantized in 87.42357873916626 seconds
35714 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35714 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31618 MiB free out of 48676 MiB total
after cast to cpu
38316 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
27 self_attn.q_proj
Pruning ...
256
iter 0, train loss 466.947265625, val loss None
iter 10, train loss 492.2071838378906, val loss None
iter 20, train loss 473.5422058105469, val loss None
iter 30, train loss 469.1006774902344, val loss None
iter 40, train loss 465.9575500488281, val loss None
iter 50, train loss 465.99981689453125, val loss None
iter 60, train loss 464.1806640625, val loss None
iter 70, train loss 463.7884521484375, val loss None
iter 80, train loss 463.03070068359375, val loss None
iter 90, train loss 462.7474060058594, val loss None
best loss 445.51702880859375
not here
quantized in 33.42803072929382 seconds
36392 MiB free out of 48676 MiB total
27 self_attn.k_proj
Pruning ...
256
iter 0, train loss 507.45159912109375, val loss None
iter 10, train loss 530.50439453125, val loss None
iter 20, train loss 507.05682373046875, val loss None
iter 30, train loss 496.44830322265625, val loss None
iter 40, train loss 493.19952392578125, val loss None
iter 50, train loss 493.39508056640625, val loss None
iter 60, train loss 493.2152404785156, val loss None
iter 70, train loss 491.79998779296875, val loss None
iter 80, train loss 491.697265625, val loss None
iter 90, train loss 492.2543640136719, val loss None
best loss 468.73516845703125
not here
quantized in 32.552650451660156 seconds
36382 MiB free out of 48676 MiB total
27 self_attn.v_proj
Pruning ...
256
iter 0, train loss 315.9350891113281, val loss None
iter 10, train loss 316.7251281738281, val loss None
iter 20, train loss 314.695556640625, val loss None
iter 30, train loss 315.2462463378906, val loss None
iter 40, train loss 314.9384765625, val loss None
iter 50, train loss 314.59088134765625, val loss None
iter 60, train loss 314.5010986328125, val loss None
iter 70, train loss 314.64599609375, val loss None
iter 80, train loss 314.1257019042969, val loss None
iter 90, train loss 314.071044921875, val loss None
best loss 313.57159423828125
not here
quantized in 31.20657205581665 seconds
36372 MiB free out of 48676 MiB total
27 self_attn.o_proj
Pruning ...
256
iter 0, train loss 19.407302856445312, val loss None
iter 10, train loss 18.574596405029297, val loss None
iter 20, train loss 18.139869689941406, val loss None
iter 30, train loss 17.770368576049805, val loss None
iter 40, train loss 17.59112548828125, val loss None
iter 50, train loss 17.364309310913086, val loss None
iter 60, train loss 17.223651885986328, val loss None
iter 70, train loss 17.1173152923584, val loss None
iter 80, train loss 16.986225128173828, val loss None
iter 90, train loss 16.898590087890625, val loss None
best loss 16.863853454589844
not here
quantized in 31.515339374542236 seconds
36372 MiB free out of 48676 MiB total
27 mlp.gate_proj
Pruning ...
256
iter 0, train loss 618.4573364257812, val loss None
iter 10, train loss 641.2513427734375, val loss None
iter 20, train loss 638.5385131835938, val loss None
iter 30, train loss 638.0375366210938, val loss None
iter 40, train loss 637.4522094726562, val loss None
iter 50, train loss 637.6221923828125, val loss None
iter 60, train loss 637.7068481445312, val loss None
iter 70, train loss 637.1681518554688, val loss None
iter 80, train loss 636.6943359375, val loss None
iter 90, train loss 636.6370849609375, val loss None
best loss 612.36181640625
not here
quantized in 84.4130220413208 seconds
36070 MiB free out of 48676 MiB total
27 mlp.up_proj
Pruning ...
256
iter 0, train loss 525.9371948242188, val loss None
iter 10, train loss 526.0535888671875, val loss None
iter 20, train loss 527.0726318359375, val loss None
iter 30, train loss 527.0125122070312, val loss None
iter 40, train loss 526.810302734375, val loss None
iter 50, train loss 526.259521484375, val loss None
iter 60, train loss 526.2044067382812, val loss None
iter 70, train loss 526.2633666992188, val loss None
iter 80, train loss 526.2035522460938, val loss None
iter 90, train loss 526.286865234375, val loss None
best loss 524.4622192382812
not here
quantized in 83.83720922470093 seconds
35876 MiB free out of 48676 MiB total
27 mlp.down_proj
Pruning ...
256
iter 0, train loss 25.12622833251953, val loss None
iter 10, train loss 25.176284790039062, val loss None
iter 20, train loss 25.12575912475586, val loss None
iter 30, train loss 25.102806091308594, val loss None
iter 40, train loss 25.09294891357422, val loss None
iter 50, train loss 25.0462646484375, val loss None
iter 60, train loss 25.07565689086914, val loss None
iter 70, train loss 25.065988540649414, val loss None
iter 80, train loss 25.049945831298828, val loss None
iter 90, train loss 25.05461883544922, val loss None
best loss 25.040462493896484
not here
quantized in 87.19700598716736 seconds
35682 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35682 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31586 MiB free out of 48676 MiB total
after cast to cpu
38316 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
28 self_attn.q_proj
Pruning ...
256
iter 0, train loss 473.6885986328125, val loss None
iter 10, train loss 505.43841552734375, val loss None
iter 20, train loss 484.8727722167969, val loss None
iter 30, train loss 474.57061767578125, val loss None
iter 40, train loss 472.1949157714844, val loss None
iter 50, train loss 471.1955871582031, val loss None
iter 60, train loss 471.7752685546875, val loss None
iter 70, train loss 471.7516174316406, val loss None
iter 80, train loss 471.69024658203125, val loss None
iter 90, train loss 471.30804443359375, val loss None
best loss 444.13885498046875
not here
quantized in 33.776926040649414 seconds
36392 MiB free out of 48676 MiB total
28 self_attn.k_proj
Pruning ...
256
iter 0, train loss 510.2318115234375, val loss None
iter 10, train loss 538.4293823242188, val loss None
iter 20, train loss 516.07275390625, val loss None
iter 30, train loss 509.96337890625, val loss None
iter 40, train loss 501.9305419921875, val loss None
iter 50, train loss 498.75677490234375, val loss None
iter 60, train loss 496.8212585449219, val loss None
iter 70, train loss 496.35809326171875, val loss None
iter 80, train loss 495.51507568359375, val loss None
iter 90, train loss 495.06884765625, val loss None
best loss 463.5224609375
not here
quantized in 33.10998606681824 seconds
36382 MiB free out of 48676 MiB total
28 self_attn.v_proj
Pruning ...
256
iter 0, train loss 351.780029296875, val loss None
iter 10, train loss 352.2287902832031, val loss None
iter 20, train loss 351.8348388671875, val loss None
iter 30, train loss 351.2528076171875, val loss None
iter 40, train loss 351.63775634765625, val loss None
iter 50, train loss 350.74334716796875, val loss None
iter 60, train loss 350.86309814453125, val loss None
iter 70, train loss 350.89862060546875, val loss None
iter 80, train loss 350.8507995605469, val loss None
iter 90, train loss 350.8528747558594, val loss None
best loss 350.703857421875
not here
quantized in 31.31374168395996 seconds
36404 MiB free out of 48676 MiB total
28 self_attn.o_proj
Pruning ...
256
iter 0, train loss 29.124767303466797, val loss None
iter 10, train loss 27.778636932373047, val loss None
iter 20, train loss 27.058731079101562, val loss None
iter 30, train loss 27.143964767456055, val loss None
iter 40, train loss 26.66444969177246, val loss None
iter 50, train loss 26.713491439819336, val loss None
iter 60, train loss 26.773090362548828, val loss None
iter 70, train loss 26.65038299560547, val loss None
iter 80, train loss 26.59390640258789, val loss None
iter 90, train loss 26.535137176513672, val loss None
best loss 26.506689071655273
not here
quantized in 31.535722732543945 seconds
36372 MiB free out of 48676 MiB total
28 mlp.gate_proj
Pruning ...
256
iter 0, train loss 654.2701416015625, val loss None
iter 10, train loss 682.2637939453125, val loss None
iter 20, train loss 671.9168701171875, val loss None
iter 30, train loss 673.8868408203125, val loss None
iter 40, train loss 671.6896362304688, val loss None
iter 50, train loss 669.857177734375, val loss None
iter 60, train loss 669.4848022460938, val loss None
iter 70, train loss 669.3189697265625, val loss None
iter 80, train loss 669.3267211914062, val loss None
iter 90, train loss 669.3854370117188, val loss None
best loss 645.8663330078125
not here
quantized in 84.76704430580139 seconds
36070 MiB free out of 48676 MiB total
28 mlp.up_proj
Pruning ...
256
iter 0, train loss 579.3809814453125, val loss None
iter 10, train loss 586.8734130859375, val loss None
iter 20, train loss 577.1102905273438, val loss None
iter 30, train loss 578.5426025390625, val loss None
iter 40, train loss 576.5745239257812, val loss None
iter 50, train loss 576.6800537109375, val loss None
iter 60, train loss 576.0501708984375, val loss None
iter 70, train loss 575.7138671875, val loss None
iter 80, train loss 575.91552734375, val loss None
iter 90, train loss 575.95458984375, val loss None
best loss 574.4340209960938
not here
quantized in 84.74938464164734 seconds
35876 MiB free out of 48676 MiB total
28 mlp.down_proj
Pruning ...
256
iter 0, train loss 30.288131713867188, val loss None
iter 10, train loss 30.381771087646484, val loss None
iter 20, train loss 30.360431671142578, val loss None
iter 30, train loss 30.308135986328125, val loss None
iter 40, train loss 30.247859954833984, val loss None
iter 50, train loss 30.224361419677734, val loss None
iter 60, train loss 30.19302749633789, val loss None
iter 70, train loss 30.142044067382812, val loss None
iter 80, train loss 30.14472198486328, val loss None
iter 90, train loss 30.160919189453125, val loss None
best loss 30.133935928344727
not here
quantized in 87.07463431358337 seconds
35682 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35682 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31586 MiB free out of 48676 MiB total
after cast to cpu
38316 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
29 self_attn.q_proj
Pruning ...
256
iter 0, train loss 433.7162170410156, val loss None
iter 10, train loss 464.0552978515625, val loss None
iter 20, train loss 444.7705993652344, val loss None
iter 30, train loss 437.3664245605469, val loss None
iter 40, train loss 429.9554138183594, val loss None
iter 50, train loss 427.529052734375, val loss None
iter 60, train loss 425.9944763183594, val loss None
iter 70, train loss 425.1974182128906, val loss None
iter 80, train loss 424.8852233886719, val loss None
iter 90, train loss 424.78692626953125, val loss None
best loss 396.54632568359375
not here
quantized in 34.20952248573303 seconds
36392 MiB free out of 48676 MiB total
29 self_attn.k_proj
Pruning ...
256
iter 0, train loss 468.4088439941406, val loss None
iter 10, train loss 491.0063171386719, val loss None
iter 20, train loss 478.04705810546875, val loss None
iter 30, train loss 464.482666015625, val loss None
iter 40, train loss 455.9746398925781, val loss None
iter 50, train loss 451.97686767578125, val loss None
iter 60, train loss 450.1822509765625, val loss None
iter 70, train loss 449.87506103515625, val loss None
iter 80, train loss 449.5079040527344, val loss None
iter 90, train loss 449.12652587890625, val loss None
best loss 415.21429443359375
not here
quantized in 33.28188467025757 seconds
36382 MiB free out of 48676 MiB total
29 self_attn.v_proj
Pruning ...
256
iter 0, train loss 331.133056640625, val loss None
iter 10, train loss 330.7550048828125, val loss None
iter 20, train loss 330.1664123535156, val loss None
iter 30, train loss 329.9993591308594, val loss None
iter 40, train loss 329.7296447753906, val loss None
iter 50, train loss 329.6937561035156, val loss None
iter 60, train loss 329.78729248046875, val loss None
iter 70, train loss 329.5035400390625, val loss None
iter 80, train loss 328.9976806640625, val loss None
iter 90, train loss 328.76153564453125, val loss None
best loss 328.5098571777344
not here
quantized in 31.201432943344116 seconds
36404 MiB free out of 48676 MiB total
29 self_attn.o_proj
Pruning ...
256
iter 0, train loss 30.74822425842285, val loss None
iter 10, train loss 25.832725524902344, val loss None
iter 20, train loss 24.932239532470703, val loss None
iter 30, train loss 24.586605072021484, val loss None
iter 40, train loss 24.045970916748047, val loss None
iter 50, train loss 23.617952346801758, val loss None
iter 60, train loss 23.565990447998047, val loss None
iter 70, train loss 23.53314208984375, val loss None
iter 80, train loss 23.23904800415039, val loss None
iter 90, train loss 23.17962646484375, val loss None
best loss 23.051509857177734
not here
quantized in 31.8815815448761 seconds
36372 MiB free out of 48676 MiB total
29 mlp.gate_proj
Pruning ...
256
iter 0, train loss 688.2498779296875, val loss None
iter 10, train loss 720.044921875, val loss None
iter 20, train loss 705.6643676757812, val loss None
iter 30, train loss 706.2531127929688, val loss None
iter 40, train loss 703.9420166015625, val loss None
iter 50, train loss 703.00390625, val loss None
iter 60, train loss 701.82763671875, val loss None
iter 70, train loss 701.318115234375, val loss None
iter 80, train loss 701.2421875, val loss None
iter 90, train loss 701.126953125, val loss None
best loss 673.139404296875
not here
quantized in 85.7593948841095 seconds
36070 MiB free out of 48676 MiB total
29 mlp.up_proj
Pruning ...
256
iter 0, train loss 619.6017456054688, val loss None
iter 10, train loss 640.480224609375, val loss None
iter 20, train loss 613.8458251953125, val loss None
iter 30, train loss 610.7509765625, val loss None
iter 40, train loss 609.029296875, val loss None
iter 50, train loss 608.1314697265625, val loss None
iter 60, train loss 608.437255859375, val loss None
iter 70, train loss 607.625244140625, val loss None
iter 80, train loss 607.0387573242188, val loss None
iter 90, train loss 606.77734375, val loss None
best loss 606.679443359375
not here
quantized in 85.25088214874268 seconds
35876 MiB free out of 48676 MiB total
29 mlp.down_proj
Pruning ...
256
iter 0, train loss 36.82067108154297, val loss None
iter 10, train loss 36.95996856689453, val loss None
iter 20, train loss 36.895606994628906, val loss None
iter 30, train loss 36.81324768066406, val loss None
iter 40, train loss 36.799224853515625, val loss None
iter 50, train loss 36.794010162353516, val loss None
iter 60, train loss 36.73369598388672, val loss None
iter 70, train loss 36.72580337524414, val loss None
iter 80, train loss 36.654327392578125, val loss None
iter 90, train loss 36.64122009277344, val loss None
best loss 36.60777282714844
not here
quantized in 87.47077345848083 seconds
35682 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35682 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31586 MiB free out of 48676 MiB total
after cast to cpu
38316 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
30 self_attn.q_proj
Pruning ...
256
iter 0, train loss 467.19183349609375, val loss None
iter 10, train loss 497.96685791015625, val loss None
iter 20, train loss 480.19329833984375, val loss None
iter 30, train loss 467.7419128417969, val loss None
iter 40, train loss 464.65338134765625, val loss None
iter 50, train loss 463.9104309082031, val loss None
iter 60, train loss 463.3563232421875, val loss None
iter 70, train loss 461.59625244140625, val loss None
iter 80, train loss 460.464111328125, val loss None
iter 90, train loss 459.5489807128906, val loss None
best loss 425.1075744628906
not here
quantized in 34.36547112464905 seconds
36392 MiB free out of 48676 MiB total
30 self_attn.k_proj
Pruning ...
256
iter 0, train loss 501.8850402832031, val loss None
iter 10, train loss 533.7686157226562, val loss None
iter 20, train loss 512.179443359375, val loss None
iter 30, train loss 499.75897216796875, val loss None
iter 40, train loss 489.55426025390625, val loss None
iter 50, train loss 484.11395263671875, val loss None
iter 60, train loss 483.1612548828125, val loss None
iter 70, train loss 481.728515625, val loss None
iter 80, train loss 480.49755859375, val loss None
iter 90, train loss 480.1716613769531, val loss None
best loss 442.9529113769531
not here
quantized in 33.258580923080444 seconds
36382 MiB free out of 48676 MiB total
30 self_attn.v_proj
Pruning ...
256
iter 0, train loss 375.8223876953125, val loss None
iter 10, train loss 378.301513671875, val loss None
iter 20, train loss 376.8587341308594, val loss None
iter 30, train loss 376.7842712402344, val loss None
iter 40, train loss 375.56011962890625, val loss None
iter 50, train loss 374.3139343261719, val loss None
iter 60, train loss 374.3116455078125, val loss None
iter 70, train loss 374.2324523925781, val loss None
iter 80, train loss 374.455322265625, val loss None
iter 90, train loss 374.62933349609375, val loss None
best loss 373.8303527832031
not here
quantized in 31.311212062835693 seconds
36372 MiB free out of 48676 MiB total
30 self_attn.o_proj
Pruning ...
256
iter 0, train loss 38.65102767944336, val loss None
iter 10, train loss 32.2155647277832, val loss None
iter 20, train loss 30.015216827392578, val loss None
iter 30, train loss 28.42542266845703, val loss None
iter 40, train loss 27.89923667907715, val loss None
iter 50, train loss 27.711688995361328, val loss None
iter 60, train loss 27.3925724029541, val loss None
iter 70, train loss 27.245195388793945, val loss None
iter 80, train loss 27.222362518310547, val loss None
iter 90, train loss 27.083194732666016, val loss None
best loss 26.88758087158203
not here
quantized in 31.87152624130249 seconds
36340 MiB free out of 48676 MiB total
30 mlp.gate_proj
Pruning ...
256
iter 0, train loss 783.4583740234375, val loss None
iter 10, train loss 821.1546020507812, val loss None
iter 20, train loss 783.5339965820312, val loss None
iter 30, train loss 766.1751098632812, val loss None
iter 40, train loss 761.6826171875, val loss None
iter 50, train loss 761.0971069335938, val loss None
iter 60, train loss 760.19287109375, val loss None
iter 70, train loss 760.04833984375, val loss None
iter 80, train loss 758.87548828125, val loss None
iter 90, train loss 759.1763916015625, val loss None
best loss 724.4733276367188
not here
quantized in 86.90885162353516 seconds
36038 MiB free out of 48676 MiB total
30 mlp.up_proj
Pruning ...
256
iter 0, train loss 678.0689086914062, val loss None
iter 10, train loss 711.2861328125, val loss None
iter 20, train loss 673.5028076171875, val loss None
iter 30, train loss 656.748779296875, val loss None
iter 40, train loss 649.6687622070312, val loss None
iter 50, train loss 648.3593139648438, val loss None
iter 60, train loss 648.1210327148438, val loss None
iter 70, train loss 647.68017578125, val loss None
iter 80, train loss 647.2208862304688, val loss None
iter 90, train loss 646.3133544921875, val loss None
best loss 634.4390869140625
not here
quantized in 86.26447010040283 seconds
35844 MiB free out of 48676 MiB total
30 mlp.down_proj
Pruning ...
256
iter 0, train loss 52.152713775634766, val loss None
iter 10, train loss 52.82468795776367, val loss None
iter 20, train loss 52.1693229675293, val loss None
iter 30, train loss 52.093040466308594, val loss None
iter 40, train loss 51.821022033691406, val loss None
iter 50, train loss 51.63331985473633, val loss None
iter 60, train loss 51.55207061767578, val loss None
iter 70, train loss 51.51879119873047, val loss None
iter 80, train loss 51.42873764038086, val loss None
iter 90, train loss 51.285057067871094, val loss None
best loss 51.23688507080078
not here
quantized in 88.63552355766296 seconds
35650 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35650 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31554 MiB free out of 48676 MiB total
after cast to cpu
38316 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
31 self_attn.q_proj
Pruning ...
256
iter 0, train loss 352.87420654296875, val loss None
iter 10, train loss 369.81304931640625, val loss None
iter 20, train loss 360.3341979980469, val loss None
iter 30, train loss 347.6524353027344, val loss None
iter 40, train loss 343.14129638671875, val loss None
iter 50, train loss 341.0828857421875, val loss None
iter 60, train loss 340.8830871582031, val loss None
iter 70, train loss 340.0856018066406, val loss None
iter 80, train loss 339.609130859375, val loss None
iter 90, train loss 339.25787353515625, val loss None
best loss 304.2283020019531
not here
quantized in 34.97708344459534 seconds
36392 MiB free out of 48676 MiB total
31 self_attn.k_proj
Pruning ...
256
iter 0, train loss 410.3641662597656, val loss None
iter 10, train loss 417.1654052734375, val loss None
iter 20, train loss 411.8365173339844, val loss None
iter 30, train loss 395.7669677734375, val loss None
iter 40, train loss 391.904541015625, val loss None
iter 50, train loss 387.20770263671875, val loss None
iter 60, train loss 385.0836181640625, val loss None
iter 70, train loss 384.1943359375, val loss None
iter 80, train loss 383.69091796875, val loss None
iter 90, train loss 383.68603515625, val loss None
best loss 331.6520690917969
not here
quantized in 34.42050004005432 seconds
36382 MiB free out of 48676 MiB total
31 self_attn.v_proj
Pruning ...
256
iter 0, train loss 214.60366821289062, val loss None
iter 10, train loss 215.18060302734375, val loss None
iter 20, train loss 214.4082489013672, val loss None
iter 30, train loss 214.82557678222656, val loss None
iter 40, train loss 214.7891845703125, val loss None
iter 50, train loss 214.07656860351562, val loss None
iter 60, train loss 214.1563720703125, val loss None
iter 70, train loss 214.00892639160156, val loss None
iter 80, train loss 213.65371704101562, val loss None
iter 90, train loss 213.80535888671875, val loss None
best loss 213.5379638671875
not here
quantized in 31.364750385284424 seconds
36372 MiB free out of 48676 MiB total
31 self_attn.o_proj
Pruning ...
256
iter 0, train loss 176.88076782226562, val loss None
iter 10, train loss 140.19900512695312, val loss None
iter 20, train loss 124.9512939453125, val loss None
iter 30, train loss 105.67514038085938, val loss None
iter 40, train loss 95.74552917480469, val loss None
iter 50, train loss 91.04277801513672, val loss None
iter 60, train loss 89.96055603027344, val loss None
iter 70, train loss 87.10092163085938, val loss None
iter 80, train loss 83.43241119384766, val loss None
iter 90, train loss 82.19284057617188, val loss None
best loss 81.77592468261719
not here
quantized in 32.778231143951416 seconds
36340 MiB free out of 48676 MiB total
31 mlp.gate_proj
Pruning ...
256
iter 0, train loss 705.8810424804688, val loss None
iter 10, train loss 719.218994140625, val loss None
iter 20, train loss 687.4933471679688, val loss None
iter 30, train loss 669.588623046875, val loss None
iter 40, train loss 664.3837890625, val loss None
iter 50, train loss 664.8775634765625, val loss None
iter 60, train loss 665.4337158203125, val loss None
iter 70, train loss 664.6365356445312, val loss None
iter 80, train loss 663.6649169921875, val loss None
iter 90, train loss 663.7686767578125, val loss None
best loss 614.208251953125
not here
quantized in 87.86070990562439 seconds
36038 MiB free out of 48676 MiB total
31 mlp.up_proj
Pruning ...
256
iter 0, train loss 674.9302978515625, val loss None
iter 10, train loss 680.3150634765625, val loss None
iter 20, train loss 660.987548828125, val loss None
iter 30, train loss 638.3834228515625, val loss None
iter 40, train loss 627.010498046875, val loss None
iter 50, train loss 621.2314453125, val loss None
iter 60, train loss 618.7505493164062, val loss None
iter 70, train loss 617.6412963867188, val loss None
iter 80, train loss 616.3097534179688, val loss None
iter 90, train loss 615.0902099609375, val loss None
best loss 561.7080688476562
not here
quantized in 90.71045660972595 seconds
35844 MiB free out of 48676 MiB total
31 mlp.down_proj
Pruning ...
256
iter 0, train loss 96.46250915527344, val loss None
iter 10, train loss 107.14913940429688, val loss None
iter 20, train loss 100.62306213378906, val loss None
iter 30, train loss 97.43448638916016, val loss None
iter 40, train loss 95.3720474243164, val loss None
iter 50, train loss 93.54672241210938, val loss None
iter 60, train loss 92.72769927978516, val loss None
iter 70, train loss 92.18592071533203, val loss None
iter 80, train loss 91.63250732421875, val loss None
iter 90, train loss 90.8907470703125, val loss None
best loss 90.47654724121094
not here
quantized in 91.71998858451843 seconds
35650 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
35650 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
31554 MiB free out of 48676 MiB total
after cast to cpu
38316 MiB free out of 48676 MiB total
Total bits: 13017415680.0 Total params: 6476005376
average bits per value: 2.0100995790155443
total time taken: 13271.081764221191
Loading tokenizer for meta-llama/Llama-2-7b-hf
/home/lliu/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Dataset: wikitext2
Evaluating ...
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
Perplexity: 7.647955
