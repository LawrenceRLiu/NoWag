/home/lliu/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.04s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.50it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.39it/s]
Model loaded. LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
4096
Loading tokenizer for meta-llama/Llama-2-7b-hf
Starting...
39836 MiB free out of 48676 MiB total
attention_mask torch.Size([1, 1, 4096, 4096]) cuda:7 torch.float16
position_ids torch.Size([1, 4096]) cuda:7 torch.int64
position_ids torch.Size([1, 4096])
attention_mask torch.Size([1, 1, 4096, 4096])
Ready.
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
0 self_attn.q_proj
Pruning ...
256
iter 0, train loss 1.2186399698257446, val loss None
iter 10, train loss 1.119516134262085, val loss None
iter 20, train loss 0.9884368181228638, val loss None
iter 30, train loss 1.046722173690796, val loss None
iter 40, train loss 0.984609842300415, val loss None
iter 50, train loss 0.9213398694992065, val loss None
iter 60, train loss 0.834230899810791, val loss None
iter 70, train loss 0.8075946569442749, val loss None
iter 80, train loss 0.7496522068977356, val loss None
iter 90, train loss 0.7204086184501648, val loss None
best loss 0.7134543061256409
not here
quantized in 34.503390312194824 seconds
37514 MiB free out of 48676 MiB total
0 self_attn.k_proj
Pruning ...
256
iter 0, train loss 0.9332952499389648, val loss None
iter 10, train loss 0.7587932348251343, val loss None
iter 20, train loss 0.7534413933753967, val loss None
iter 30, train loss 0.703830361366272, val loss None
iter 40, train loss 0.6517475843429565, val loss None
iter 50, train loss 0.630162239074707, val loss None
iter 60, train loss 0.6182987689971924, val loss None
iter 70, train loss 0.5846052169799805, val loss None
iter 80, train loss 0.575898289680481, val loss None
iter 90, train loss 0.5881614685058594, val loss None
best loss 0.561808168888092
not here
quantized in 32.903404712677 seconds
37450 MiB free out of 48676 MiB total
0 self_attn.v_proj
Pruning ...
256
iter 0, train loss 0.09436560422182083, val loss None
iter 10, train loss 0.08059316128492355, val loss None
iter 20, train loss 0.0804050862789154, val loss None
iter 30, train loss 0.07679381221532822, val loss None
iter 40, train loss 0.07414661347866058, val loss None
iter 50, train loss 0.07325101643800735, val loss None
iter 60, train loss 0.07180514931678772, val loss None
iter 70, train loss 0.07146237045526505, val loss None
iter 80, train loss 0.0706852525472641, val loss None
iter 90, train loss 0.0700930804014206, val loss None
best loss 0.06996148079633713
not here
quantized in 33.17185401916504 seconds
37450 MiB free out of 48676 MiB total
0 self_attn.o_proj
Pruning ...
256
iter 0, train loss 0.011464841663837433, val loss None
iter 10, train loss 0.010397965088486671, val loss None
iter 20, train loss 0.010019666515290737, val loss None
iter 30, train loss 0.00937157217413187, val loss None
iter 40, train loss 0.009089747443795204, val loss None
iter 50, train loss 0.008979199454188347, val loss None
iter 60, train loss 0.00871557742357254, val loss None
iter 70, train loss 0.008652668446302414, val loss None
iter 80, train loss 0.008620553649961948, val loss None
iter 90, train loss 0.008694020099937916, val loss None
best loss 0.008584421128034592
not here
quantized in 32.16788864135742 seconds
37386 MiB free out of 48676 MiB total
0 mlp.gate_proj
Pruning ...
256
iter 0, train loss 3.914763927459717, val loss None
iter 10, train loss 4.074483394622803, val loss None
iter 20, train loss 3.913628101348877, val loss None
iter 30, train loss 3.8993492126464844, val loss None
iter 40, train loss 3.8955917358398438, val loss None
iter 50, train loss 3.8993489742279053, val loss None
iter 60, train loss 3.8939061164855957, val loss None
iter 70, train loss 3.8909707069396973, val loss None
iter 80, train loss 3.8890702724456787, val loss None
iter 90, train loss 3.8866477012634277, val loss None
best loss 3.7343382835388184
not here
quantized in 87.35989713668823 seconds
36998 MiB free out of 48676 MiB total
0 mlp.up_proj
Pruning ...
256
iter 0, train loss 3.5440115928649902, val loss None
iter 10, train loss 3.751767635345459, val loss None
iter 20, train loss 3.577709436416626, val loss None
iter 30, train loss 3.562427043914795, val loss None
iter 40, train loss 3.5612680912017822, val loss None
iter 50, train loss 3.555222749710083, val loss None
iter 60, train loss 3.5544967651367188, val loss None
iter 70, train loss 3.5540828704833984, val loss None
iter 80, train loss 3.5538244247436523, val loss None
iter 90, train loss 3.5520124435424805, val loss None
best loss 3.391655206680298
not here
quantized in 87.24313712120056 seconds
36718 MiB free out of 48676 MiB total
0 mlp.down_proj
Pruning ...
256
iter 0, train loss 0.010795675218105316, val loss None
iter 10, train loss 0.01196495071053505, val loss None
iter 20, train loss 0.011627107858657837, val loss None
iter 30, train loss 0.01134019996970892, val loss None
iter 40, train loss 0.011228447780013084, val loss None
iter 50, train loss 0.011204478330910206, val loss None
iter 60, train loss 0.011173442006111145, val loss None
iter 70, train loss 0.011148728430271149, val loss None
iter 80, train loss 0.011124704964458942, val loss None
iter 90, train loss 0.011111089028418064, val loss None
best loss 0.010174737311899662
not here
quantized in 94.47773551940918 seconds
36438 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.codebook', 'self_attn.q_proj.quantizer.norms_1', 'self_attn.q_proj.quantizer.norms_0', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.codebook', 'self_attn.k_proj.quantizer.norms_1', 'self_attn.k_proj.quantizer.norms_0', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.codebook', 'self_attn.v_proj.quantizer.norms_1', 'self_attn.v_proj.quantizer.norms_0', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.codebook', 'self_attn.o_proj.quantizer.norms_1', 'self_attn.o_proj.quantizer.norms_0', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.codebook', 'mlp.gate_proj.quantizer.norms_1', 'mlp.gate_proj.quantizer.norms_0', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.codebook', 'mlp.up_proj.quantizer.norms_1', 'mlp.up_proj.quantizer.norms_0', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.codebook', 'mlp.down_proj.quantizer.norms_1', 'mlp.down_proj.quantizer.norms_0', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135936
n accumulation steps: 1
n batches: 128
epoch 0 loss: 5.815701381806093e-06
9714 MiB free out of 48676 MiB total
epoch 1 loss: 5.346113230331184e-06
9714 MiB free out of 48676 MiB total
epoch 2 loss: 5.258946639941087e-06
9714 MiB free out of 48676 MiB total
epoch 3 loss: 5.212464081694179e-06
9714 MiB free out of 48676 MiB total
epoch 4 loss: 5.182328376207579e-06
9714 MiB free out of 48676 MiB total
36438 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
9714 MiB free out of 48676 MiB total
after cast to cpu
39370 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
1 self_attn.q_proj
Pruning ...
256
iter 0, train loss 17.163822174072266, val loss None
iter 10, train loss 14.360633850097656, val loss None
iter 20, train loss 16.245254516601562, val loss None
iter 30, train loss 14.757781982421875, val loss None
iter 40, train loss 13.965330123901367, val loss None
iter 50, train loss 13.743480682373047, val loss None
iter 60, train loss 13.368125915527344, val loss None
iter 70, train loss 13.172981262207031, val loss None
iter 80, train loss 13.097610473632812, val loss None
iter 90, train loss 12.962808609008789, val loss None
best loss 12.908007621765137
not here
quantized in 37.281193256378174 seconds
37478 MiB free out of 48676 MiB total
1 self_attn.k_proj
Pruning ...
256
iter 0, train loss 17.66086196899414, val loss None
iter 10, train loss 14.862323760986328, val loss None
iter 20, train loss 14.049379348754883, val loss None
iter 30, train loss 13.530954360961914, val loss None
iter 40, train loss 13.591726303100586, val loss None
iter 50, train loss 13.71668815612793, val loss None
iter 60, train loss 13.502216339111328, val loss None
iter 70, train loss 13.385174751281738, val loss None
iter 80, train loss 13.1788330078125, val loss None
iter 90, train loss 13.048715591430664, val loss None
best loss 12.91349983215332
not here
quantized in 36.19434690475464 seconds
37468 MiB free out of 48676 MiB total
1 self_attn.v_proj
Pruning ...
256
iter 0, train loss 0.9256487488746643, val loss None
iter 10, train loss 0.9458835124969482, val loss None
iter 20, train loss 0.8911916613578796, val loss None
iter 30, train loss 0.8702602386474609, val loss None
iter 40, train loss 0.8614788055419922, val loss None
iter 50, train loss 0.8571851849555969, val loss None
iter 60, train loss 0.854447603225708, val loss None
iter 70, train loss 0.8491106629371643, val loss None
iter 80, train loss 0.8444417715072632, val loss None
iter 90, train loss 0.8425191640853882, val loss None
best loss 0.8412299156188965
not here
quantized in 32.812416553497314 seconds
37426 MiB free out of 48676 MiB total
1 self_attn.o_proj
Pruning ...
256
iter 0, train loss 0.08715854585170746, val loss None
iter 10, train loss 0.08258514106273651, val loss None
iter 20, train loss 0.08074139058589935, val loss None
iter 30, train loss 0.07942299544811249, val loss None
iter 40, train loss 0.07841719686985016, val loss None
iter 50, train loss 0.07798255980014801, val loss None
iter 60, train loss 0.07752829790115356, val loss None
iter 70, train loss 0.07712552696466446, val loss None
iter 80, train loss 0.07654345035552979, val loss None
iter 90, train loss 0.07617370784282684, val loss None
best loss 0.07585951685905457
not here
quantized in 31.65869903564453 seconds
37362 MiB free out of 48676 MiB total
1 mlp.gate_proj
Pruning ...
256
iter 0, train loss 16.370426177978516, val loss None
iter 10, train loss 17.405370712280273, val loss None
iter 20, train loss 17.051925659179688, val loss None
iter 30, train loss 16.84934425354004, val loss None
iter 40, train loss 16.839096069335938, val loss None
iter 50, train loss 16.779008865356445, val loss None
iter 60, train loss 16.764888763427734, val loss None
iter 70, train loss 16.784324645996094, val loss None
iter 80, train loss 16.769620895385742, val loss None
iter 90, train loss 16.747425079345703, val loss None
best loss 15.38759708404541
not here
quantized in 88.75338411331177 seconds
36974 MiB free out of 48676 MiB total
1 mlp.up_proj
Pruning ...
256
iter 0, train loss 13.24544906616211, val loss None
iter 10, train loss 13.690667152404785, val loss None
iter 20, train loss 13.429544448852539, val loss None
iter 30, train loss 13.453993797302246, val loss None
iter 40, train loss 13.444010734558105, val loss None
iter 50, train loss 13.445799827575684, val loss None
iter 60, train loss 13.439323425292969, val loss None
iter 70, train loss 13.435157775878906, val loss None
iter 80, train loss 13.437856674194336, val loss None
iter 90, train loss 13.441581726074219, val loss None
best loss 13.098968505859375
not here
quantized in 86.34500312805176 seconds
36694 MiB free out of 48676 MiB total
1 mlp.down_proj
Pruning ...
256
iter 0, train loss 0.08793056011199951, val loss None
iter 10, train loss 0.1552051603794098, val loss None
iter 20, train loss 0.16007742285728455, val loss None
iter 30, train loss 0.12585516273975372, val loss None
iter 40, train loss 0.11884839087724686, val loss None
iter 50, train loss 0.11061196029186249, val loss None
iter 60, train loss 0.10530126094818115, val loss None
iter 70, train loss 0.1029689684510231, val loss None
iter 80, train loss 0.10282927006483078, val loss None
iter 90, train loss 0.10131385922431946, val loss None
best loss 0.08793056011199951
not here
quantized in 92.10705852508545 seconds
36414 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.codebook', 'self_attn.q_proj.quantizer.norms_1', 'self_attn.q_proj.quantizer.norms_0', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.codebook', 'self_attn.k_proj.quantizer.norms_1', 'self_attn.k_proj.quantizer.norms_0', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.codebook', 'self_attn.v_proj.quantizer.norms_1', 'self_attn.v_proj.quantizer.norms_0', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.codebook', 'self_attn.o_proj.quantizer.norms_1', 'self_attn.o_proj.quantizer.norms_0', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.codebook', 'mlp.gate_proj.quantizer.norms_1', 'mlp.gate_proj.quantizer.norms_0', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.codebook', 'mlp.up_proj.quantizer.norms_1', 'mlp.up_proj.quantizer.norms_0', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.codebook', 'mlp.down_proj.quantizer.norms_1', 'mlp.down_proj.quantizer.norms_0', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135936
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0013582791238491154
9692 MiB free out of 48676 MiB total
epoch 1 loss: 0.00020178777313617502
9692 MiB free out of 48676 MiB total
epoch 2 loss: 0.0001619627961133574
9692 MiB free out of 48676 MiB total
epoch 3 loss: 0.00013767097073014156
9692 MiB free out of 48676 MiB total
epoch 4 loss: 0.00012058567929784658
9692 MiB free out of 48676 MiB total
36414 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
9692 MiB free out of 48676 MiB total
after cast to cpu
39402 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
2 self_attn.q_proj
Pruning ...
256
iter 0, train loss 62.82225036621094, val loss None
iter 10, train loss 62.62944030761719, val loss None
iter 20, train loss 62.66545867919922, val loss None
iter 30, train loss 61.350547790527344, val loss None
iter 40, train loss 60.99163055419922, val loss None
iter 50, train loss 60.50293731689453, val loss None
iter 60, train loss 60.247047424316406, val loss None
iter 70, train loss 60.22689437866211, val loss None
iter 80, train loss 60.119224548339844, val loss None
iter 90, train loss 60.01688003540039, val loss None
best loss 57.1174201965332
not here
quantized in 39.2047803401947 seconds
37478 MiB free out of 48676 MiB total
2 self_attn.k_proj
Pruning ...
256
iter 0, train loss 74.71372985839844, val loss None
iter 10, train loss 71.28164672851562, val loss None
iter 20, train loss 73.14675903320312, val loss None
iter 30, train loss 71.45347595214844, val loss None
iter 40, train loss 70.390869140625, val loss None
iter 50, train loss 69.90200805664062, val loss None
iter 60, train loss 69.61448669433594, val loss None
iter 70, train loss 69.3935775756836, val loss None
iter 80, train loss 69.2005615234375, val loss None
iter 90, train loss 69.15505981445312, val loss None
best loss 66.62980651855469
not here
quantized in 38.351757287979126 seconds
37436 MiB free out of 48676 MiB total
2 self_attn.v_proj
Pruning ...
256
iter 0, train loss 13.888402938842773, val loss None
iter 10, train loss 14.466136932373047, val loss None
iter 20, train loss 14.015803337097168, val loss None
iter 30, train loss 14.019786834716797, val loss None
iter 40, train loss 13.975661277770996, val loss None
iter 50, train loss 13.9470796585083, val loss None
iter 60, train loss 13.929193496704102, val loss None
iter 70, train loss 13.904903411865234, val loss None
iter 80, train loss 13.905420303344727, val loss None
iter 90, train loss 13.883138656616211, val loss None
best loss 13.625001907348633
not here
quantized in 36.88267374038696 seconds
37394 MiB free out of 48676 MiB total
2 self_attn.o_proj
Pruning ...
256
iter 0, train loss 0.1427522897720337, val loss None
iter 10, train loss 0.1416923701763153, val loss None
iter 20, train loss 0.1374967396259308, val loss None
iter 30, train loss 0.13840404152870178, val loss None
iter 40, train loss 0.13815715909004211, val loss None
iter 50, train loss 0.1377851665019989, val loss None
iter 60, train loss 0.1375831961631775, val loss None
iter 70, train loss 0.13750463724136353, val loss None
iter 80, train loss 0.13715068995952606, val loss None
iter 90, train loss 0.137058824300766, val loss None
best loss 0.13696809113025665
not here
quantized in 36.681215047836304 seconds
37394 MiB free out of 48676 MiB total
2 mlp.gate_proj
Pruning ...
256
iter 0, train loss 33.55299377441406, val loss None
iter 10, train loss 34.61381149291992, val loss None
iter 20, train loss 33.652244567871094, val loss None
iter 30, train loss 33.904823303222656, val loss None
iter 40, train loss 33.843807220458984, val loss None
iter 50, train loss 33.95542526245117, val loss None
iter 60, train loss 33.94084930419922, val loss None
iter 70, train loss 33.936824798583984, val loss None
iter 80, train loss 33.9322509765625, val loss None
iter 90, train loss 33.921260833740234, val loss None
best loss 32.99226379394531
not here
quantized in 97.19765663146973 seconds
37006 MiB free out of 48676 MiB total
2 mlp.up_proj
Pruning ...
256
iter 0, train loss 27.489334106445312, val loss None
iter 10, train loss 27.730300903320312, val loss None
iter 20, train loss 27.751970291137695, val loss None
iter 30, train loss 27.69771385192871, val loss None
iter 40, train loss 27.737529754638672, val loss None
iter 50, train loss 27.734323501586914, val loss None
iter 60, train loss 27.727867126464844, val loss None
iter 70, train loss 27.737850189208984, val loss None
iter 80, train loss 27.738786697387695, val loss None
iter 90, train loss 27.72150421142578, val loss None
best loss 27.4071044921875
not here
quantized in 96.26323914527893 seconds
36726 MiB free out of 48676 MiB total
2 mlp.down_proj
Pruning ...
256
iter 0, train loss 0.15278717875480652, val loss None
iter 10, train loss 0.15264970064163208, val loss None
iter 20, train loss 0.15246915817260742, val loss None
iter 30, train loss 0.15160584449768066, val loss None
iter 40, train loss 0.151291161775589, val loss None
iter 50, train loss 0.15097057819366455, val loss None
iter 60, train loss 0.1506543755531311, val loss None
iter 70, train loss 0.15058529376983643, val loss None
iter 80, train loss 0.15045052766799927, val loss None
iter 90, train loss 0.1505523920059204, val loss None
best loss 0.15040349960327148
not here
quantized in 100.44956946372986 seconds
36446 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.codebook', 'self_attn.q_proj.quantizer.norms_1', 'self_attn.q_proj.quantizer.norms_0', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.codebook', 'self_attn.k_proj.quantizer.norms_1', 'self_attn.k_proj.quantizer.norms_0', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.codebook', 'self_attn.v_proj.quantizer.norms_1', 'self_attn.v_proj.quantizer.norms_0', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.codebook', 'self_attn.o_proj.quantizer.norms_1', 'self_attn.o_proj.quantizer.norms_0', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.codebook', 'mlp.gate_proj.quantizer.norms_1', 'mlp.gate_proj.quantizer.norms_0', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.codebook', 'mlp.up_proj.quantizer.norms_1', 'mlp.up_proj.quantizer.norms_0', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.codebook', 'mlp.down_proj.quantizer.norms_1', 'mlp.down_proj.quantizer.norms_0', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135936
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.00013977229104966682
10684 MiB free out of 48676 MiB total
epoch 1 loss: 0.00013324273476200688
10684 MiB free out of 48676 MiB total
epoch 2 loss: 0.00013184032781055066
10684 MiB free out of 48676 MiB total
epoch 3 loss: 0.0001310501641000883
10684 MiB free out of 48676 MiB total
epoch 4 loss: 0.00013046889148427
10684 MiB free out of 48676 MiB total
36446 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
10684 MiB free out of 48676 MiB total
after cast to cpu
39370 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
3 self_attn.q_proj
Pruning ...
256
iter 0, train loss 145.46890258789062, val loss None
iter 10, train loss 150.26632690429688, val loss None
iter 20, train loss 149.32899475097656, val loss None
iter 30, train loss 144.49951171875, val loss None
iter 40, train loss 142.5517578125, val loss None
iter 50, train loss 142.5229034423828, val loss None
iter 60, train loss 142.53732299804688, val loss None
iter 70, train loss 142.38143920898438, val loss None
iter 80, train loss 142.2718505859375, val loss None
iter 90, train loss 142.3430938720703, val loss None
best loss 138.4945831298828
not here
quantized in 38.396169900894165 seconds
37478 MiB free out of 48676 MiB total
3 self_attn.k_proj
Pruning ...
256
iter 0, train loss 164.55738830566406, val loss None
iter 10, train loss 165.8311309814453, val loss None
iter 20, train loss 172.5789031982422, val loss None
iter 30, train loss 168.29498291015625, val loss None
iter 40, train loss 166.1270751953125, val loss None
iter 50, train loss 164.41709899902344, val loss None
iter 60, train loss 163.5355224609375, val loss None
iter 70, train loss 163.38221740722656, val loss None
iter 80, train loss 163.35446166992188, val loss None
iter 90, train loss 163.10850524902344, val loss None
best loss 153.72340393066406
not here
quantized in 37.65903973579407 seconds
37468 MiB free out of 48676 MiB total
3 self_attn.v_proj
Pruning ...
256
iter 0, train loss 35.376930236816406, val loss None
iter 10, train loss 35.896507263183594, val loss None
iter 20, train loss 35.296173095703125, val loss None
iter 30, train loss 35.267948150634766, val loss None
iter 40, train loss 35.156253814697266, val loss None
iter 50, train loss 35.119056701660156, val loss None
iter 60, train loss 35.09553146362305, val loss None
iter 70, train loss 35.07093811035156, val loss None
iter 80, train loss 35.060611724853516, val loss None
iter 90, train loss 35.03965377807617, val loss None
best loss 35.012535095214844
not here
quantized in 36.334590911865234 seconds
37458 MiB free out of 48676 MiB total
3 self_attn.o_proj
Pruning ...
256
iter 0, train loss 0.256911963224411, val loss None
iter 10, train loss 0.23021799325942993, val loss None
iter 20, train loss 0.2135743647813797, val loss None
iter 30, train loss 0.2065407633781433, val loss None
iter 40, train loss 0.2016136646270752, val loss None
iter 50, train loss 0.20349806547164917, val loss None
iter 60, train loss 0.199663907289505, val loss None
iter 70, train loss 0.19936293363571167, val loss None
iter 80, train loss 0.19874125719070435, val loss None
iter 90, train loss 0.19785280525684357, val loss None
best loss 0.1976698487997055
not here
quantized in 36.83530855178833 seconds
37394 MiB free out of 48676 MiB total
3 mlp.gate_proj
Pruning ...
256
iter 0, train loss 53.87746047973633, val loss None
iter 10, train loss 54.69832992553711, val loss None
iter 20, train loss 54.445716857910156, val loss None
iter 30, train loss 54.31382751464844, val loss None
iter 40, train loss 54.40770721435547, val loss None
iter 50, train loss 54.37608337402344, val loss None
iter 60, train loss 54.34632110595703, val loss None
iter 70, train loss 54.32886505126953, val loss None
iter 80, train loss 54.31951141357422, val loss None
iter 90, train loss 54.32539367675781, val loss None
best loss 53.55110168457031
not here
quantized in 96.58167672157288 seconds
37006 MiB free out of 48676 MiB total
3 mlp.up_proj
Pruning ...
256
iter 0, train loss 44.70659255981445, val loss None
iter 10, train loss 44.80851745605469, val loss None
iter 20, train loss 44.87004852294922, val loss None
iter 30, train loss 44.856040954589844, val loss None
iter 40, train loss 44.820011138916016, val loss None
iter 50, train loss 44.803070068359375, val loss None
iter 60, train loss 44.80146408081055, val loss None
iter 70, train loss 44.794822692871094, val loss None
iter 80, train loss 44.78440856933594, val loss None
iter 90, train loss 44.77595520019531, val loss None
best loss 44.70659255981445
not here
quantized in 94.85365843772888 seconds
36726 MiB free out of 48676 MiB total
3 mlp.down_proj
Pruning ...
256
iter 0, train loss 0.3027932643890381, val loss None
iter 10, train loss 0.30369818210601807, val loss None
iter 20, train loss 0.3020777702331543, val loss None
iter 30, train loss 0.3014625012874603, val loss None
iter 40, train loss 0.3007510006427765, val loss None
iter 50, train loss 0.3004012703895569, val loss None
iter 60, train loss 0.299868106842041, val loss None
iter 70, train loss 0.2995446026325226, val loss None
iter 80, train loss 0.29930567741394043, val loss None
iter 90, train loss 0.29894891381263733, val loss None
best loss 0.29882514476776123
not here
quantized in 99.5320885181427 seconds
36446 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.codebook', 'self_attn.q_proj.quantizer.norms_1', 'self_attn.q_proj.quantizer.norms_0', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.codebook', 'self_attn.k_proj.quantizer.norms_1', 'self_attn.k_proj.quantizer.norms_0', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.codebook', 'self_attn.v_proj.quantizer.norms_1', 'self_attn.v_proj.quantizer.norms_0', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.codebook', 'self_attn.o_proj.quantizer.norms_1', 'self_attn.o_proj.quantizer.norms_0', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.codebook', 'mlp.gate_proj.quantizer.norms_1', 'mlp.gate_proj.quantizer.norms_0', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.codebook', 'mlp.up_proj.quantizer.norms_1', 'mlp.up_proj.quantizer.norms_0', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.codebook', 'mlp.down_proj.quantizer.norms_1', 'mlp.down_proj.quantizer.norms_0', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135936
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.00029120503040758194
10684 MiB free out of 48676 MiB total
epoch 1 loss: 0.00026902863191935467
10684 MiB free out of 48676 MiB total
epoch 2 loss: 0.00026410557063627493
10684 MiB free out of 48676 MiB total
epoch 3 loss: 0.00026200081481420057
10684 MiB free out of 48676 MiB total
epoch 4 loss: 0.0002606875732453773
10684 MiB free out of 48676 MiB total
36446 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
10684 MiB free out of 48676 MiB total
after cast to cpu
39402 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
4 self_attn.q_proj
Pruning ...
256
iter 0, train loss 129.65505981445312, val loss None
iter 10, train loss 137.14231872558594, val loss None
iter 20, train loss 134.09609985351562, val loss None
iter 30, train loss 132.25088500976562, val loss None
iter 40, train loss 131.6201171875, val loss None
iter 50, train loss 130.85198974609375, val loss None
iter 60, train loss 130.71194458007812, val loss None
iter 70, train loss 130.31353759765625, val loss None
iter 80, train loss 130.30563354492188, val loss None
iter 90, train loss 130.13357543945312, val loss None
best loss 124.94115447998047
not here
quantized in 35.22038960456848 seconds
37446 MiB free out of 48676 MiB total
4 self_attn.k_proj
Pruning ...
256
iter 0, train loss 143.046142578125, val loss None
iter 10, train loss 149.8921661376953, val loss None
iter 20, train loss 151.7726287841797, val loss None
iter 30, train loss 146.86451721191406, val loss None
iter 40, train loss 145.1400146484375, val loss None
iter 50, train loss 144.5576171875, val loss None
iter 60, train loss 144.32156372070312, val loss None
iter 70, train loss 144.1300811767578, val loss None
iter 80, train loss 143.82293701171875, val loss None
iter 90, train loss 143.61380004882812, val loss None
best loss 137.16824340820312
not here
quantized in 33.983118772506714 seconds
37404 MiB free out of 48676 MiB total
4 self_attn.v_proj
Pruning ...
256
iter 0, train loss 33.602989196777344, val loss None
iter 10, train loss 34.10417938232422, val loss None
iter 20, train loss 33.58993148803711, val loss None
iter 30, train loss 33.545677185058594, val loss None
iter 40, train loss 33.45451736450195, val loss None
iter 50, train loss 33.44990539550781, val loss None
iter 60, train loss 33.395751953125, val loss None
iter 70, train loss 33.38202667236328, val loss None
iter 80, train loss 33.38574981689453, val loss None
iter 90, train loss 33.373268127441406, val loss None
best loss 33.369361877441406
not here
quantized in 32.554632902145386 seconds
37426 MiB free out of 48676 MiB total
4 self_attn.o_proj
Pruning ...
256
iter 0, train loss 0.6723642349243164, val loss None
iter 10, train loss 0.5912437438964844, val loss None
iter 20, train loss 0.5498145222663879, val loss None
iter 30, train loss 0.5334305763244629, val loss None
iter 40, train loss 0.5220691561698914, val loss None
iter 50, train loss 0.5055888295173645, val loss None
iter 60, train loss 0.5016210079193115, val loss None
iter 70, train loss 0.49556973576545715, val loss None
iter 80, train loss 0.4906475245952606, val loss None
iter 90, train loss 0.48940715193748474, val loss None
best loss 0.4862087070941925
not here
quantized in 33.19709873199463 seconds
37426 MiB free out of 48676 MiB total
4 mlp.gate_proj
Pruning ...
256
iter 0, train loss 74.47319030761719, val loss None
iter 10, train loss 76.90049743652344, val loss None
iter 20, train loss 75.53761291503906, val loss None
iter 30, train loss 75.70204162597656, val loss None
iter 40, train loss 75.50013732910156, val loss None
iter 50, train loss 75.47526550292969, val loss None
iter 60, train loss 75.39575958251953, val loss None
iter 70, train loss 75.42110443115234, val loss None
iter 80, train loss 75.40653991699219, val loss None
iter 90, train loss 75.44174194335938, val loss None
best loss 73.7570571899414
not here
quantized in 87.45501661300659 seconds
37038 MiB free out of 48676 MiB total
4 mlp.up_proj
Pruning ...
256
iter 0, train loss 58.28718948364258, val loss None
iter 10, train loss 58.54670715332031, val loss None
iter 20, train loss 58.60540771484375, val loss None
iter 30, train loss 58.54896926879883, val loss None
iter 40, train loss 58.550498962402344, val loss None
iter 50, train loss 58.5123176574707, val loss None
iter 60, train loss 58.51298522949219, val loss None
iter 70, train loss 58.473365783691406, val loss None
iter 80, train loss 58.446224212646484, val loss None
iter 90, train loss 58.46174621582031, val loss None
best loss 58.28718948364258
not here
quantized in 85.28434324264526 seconds
36758 MiB free out of 48676 MiB total
4 mlp.down_proj
Pruning ...
256
iter 0, train loss 0.5618411302566528, val loss None
iter 10, train loss 0.5620654225349426, val loss None
iter 20, train loss 0.5592424273490906, val loss None
iter 30, train loss 0.5585968494415283, val loss None
iter 40, train loss 0.5582943558692932, val loss None
iter 50, train loss 0.5572065114974976, val loss None
iter 60, train loss 0.5561156272888184, val loss None
iter 70, train loss 0.555540144443512, val loss None
iter 80, train loss 0.5555163621902466, val loss None
iter 90, train loss 0.5552515983581543, val loss None
best loss 0.5550721883773804
not here
quantized in 91.15868043899536 seconds
36478 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.codebook', 'self_attn.q_proj.quantizer.norms_1', 'self_attn.q_proj.quantizer.norms_0', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.codebook', 'self_attn.k_proj.quantizer.norms_1', 'self_attn.k_proj.quantizer.norms_0', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.codebook', 'self_attn.v_proj.quantizer.norms_1', 'self_attn.v_proj.quantizer.norms_0', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.codebook', 'self_attn.o_proj.quantizer.norms_1', 'self_attn.o_proj.quantizer.norms_0', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.codebook', 'mlp.gate_proj.quantizer.norms_1', 'mlp.gate_proj.quantizer.norms_0', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.codebook', 'mlp.up_proj.quantizer.norms_1', 'mlp.up_proj.quantizer.norms_0', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.codebook', 'mlp.down_proj.quantizer.norms_1', 'mlp.down_proj.quantizer.norms_0', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135936
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0005403012751230563
9692 MiB free out of 48676 MiB total
epoch 1 loss: 0.0004959535042416974
9692 MiB free out of 48676 MiB total
epoch 2 loss: 0.0004856048196870688
9692 MiB free out of 48676 MiB total
epoch 3 loss: 0.0004807389177585719
9692 MiB free out of 48676 MiB total
epoch 4 loss: 0.00047762496865288995
9692 MiB free out of 48676 MiB total
36478 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
9692 MiB free out of 48676 MiB total
after cast to cpu
39370 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
5 self_attn.q_proj
Pruning ...
256
iter 0, train loss 145.70455932617188, val loss None
iter 10, train loss 155.21182250976562, val loss None
iter 20, train loss 151.63597106933594, val loss None
iter 30, train loss 147.43707275390625, val loss None
iter 40, train loss 146.25418090820312, val loss None
iter 50, train loss 145.87786865234375, val loss None
iter 60, train loss 145.5315704345703, val loss None
iter 70, train loss 145.4617156982422, val loss None
iter 80, train loss 145.23135375976562, val loss None
iter 90, train loss 145.06040954589844, val loss None
best loss 141.03375244140625
not here
quantized in 35.015567779541016 seconds
37478 MiB free out of 48676 MiB total
5 self_attn.k_proj
Pruning ...
256
iter 0, train loss 170.13143920898438, val loss None
iter 10, train loss 175.75621032714844, val loss None
iter 20, train loss 177.57835388183594, val loss None
iter 30, train loss 172.9783172607422, val loss None
iter 40, train loss 170.32325744628906, val loss None
iter 50, train loss 169.06582641601562, val loss None
iter 60, train loss 168.29193115234375, val loss None
iter 70, train loss 167.99964904785156, val loss None
iter 80, train loss 167.657470703125, val loss None
iter 90, train loss 167.64877319335938, val loss None
best loss 162.59910583496094
not here
quantized in 33.73653554916382 seconds
37436 MiB free out of 48676 MiB total
5 self_attn.v_proj
Pruning ...
256
iter 0, train loss 39.239288330078125, val loss None
iter 10, train loss 39.58470916748047, val loss None
iter 20, train loss 39.19641876220703, val loss None
iter 30, train loss 39.10600280761719, val loss None
iter 40, train loss 39.04167175292969, val loss None
iter 50, train loss 38.975059509277344, val loss None
iter 60, train loss 38.83124542236328, val loss None
iter 70, train loss 38.83330154418945, val loss None
iter 80, train loss 38.80055236816406, val loss None
iter 90, train loss 38.762939453125, val loss None
best loss 38.75654983520508
not here
quantized in 32.385823011398315 seconds
37394 MiB free out of 48676 MiB total
5 self_attn.o_proj
Pruning ...
256
iter 0, train loss 0.9056212306022644, val loss None
iter 10, train loss 0.884375810623169, val loss None
iter 20, train loss 0.8666493892669678, val loss None
iter 30, train loss 0.8658403158187866, val loss None
iter 40, train loss 0.8590866923332214, val loss None
iter 50, train loss 0.8561214208602905, val loss None
iter 60, train loss 0.8527865409851074, val loss None
iter 70, train loss 0.8527302742004395, val loss None
iter 80, train loss 0.8498281240463257, val loss None
iter 90, train loss 0.8614364266395569, val loss None
best loss 0.8498281240463257
not here
quantized in 32.427467584609985 seconds
37394 MiB free out of 48676 MiB total
5 mlp.gate_proj
Pruning ...
256
iter 0, train loss 93.9916763305664, val loss None
iter 10, train loss 97.81050109863281, val loss None
iter 20, train loss 95.67931365966797, val loss None
iter 30, train loss 95.95830535888672, val loss None
iter 40, train loss 95.59772491455078, val loss None
iter 50, train loss 95.68746948242188, val loss None
iter 60, train loss 95.63584899902344, val loss None
iter 70, train loss 95.48028564453125, val loss None
iter 80, train loss 95.42660522460938, val loss None
iter 90, train loss 95.37432861328125, val loss None
best loss 92.94286346435547
not here
quantized in 87.76270079612732 seconds
37092 MiB free out of 48676 MiB total
5 mlp.up_proj
Pruning ...
256
iter 0, train loss 72.39634704589844, val loss None
iter 10, train loss 72.64269256591797, val loss None
iter 20, train loss 72.67506408691406, val loss None
iter 30, train loss 72.60821533203125, val loss None
iter 40, train loss 72.62320709228516, val loss None
iter 50, train loss 72.6082763671875, val loss None
iter 60, train loss 72.62061309814453, val loss None
iter 70, train loss 72.61781311035156, val loss None
iter 80, train loss 72.60494232177734, val loss None
iter 90, train loss 72.60929870605469, val loss None
best loss 72.39634704589844
not here
quantized in 86.78337860107422 seconds
36812 MiB free out of 48676 MiB total
5 mlp.down_proj
Pruning ...
256
iter 0, train loss 0.8510590195655823, val loss None
iter 10, train loss 0.8530442714691162, val loss None
iter 20, train loss 0.8465261459350586, val loss None
iter 30, train loss 0.8454768657684326, val loss None
iter 40, train loss 0.8438630104064941, val loss None
iter 50, train loss 0.8419281244277954, val loss None
iter 60, train loss 0.8406052589416504, val loss None
iter 70, train loss 0.8390617370605469, val loss None
iter 80, train loss 0.8391330242156982, val loss None
iter 90, train loss 0.838833749294281, val loss None
best loss 0.8385937213897705
not here
quantized in 91.32462096214294 seconds
36618 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.codebook', 'self_attn.q_proj.quantizer.norms_1', 'self_attn.q_proj.quantizer.norms_0', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.codebook', 'self_attn.k_proj.quantizer.norms_1', 'self_attn.k_proj.quantizer.norms_0', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.codebook', 'self_attn.v_proj.quantizer.norms_1', 'self_attn.v_proj.quantizer.norms_0', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.codebook', 'self_attn.o_proj.quantizer.norms_1', 'self_attn.o_proj.quantizer.norms_0', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.codebook', 'mlp.gate_proj.quantizer.norms_1', 'mlp.gate_proj.quantizer.norms_0', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.codebook', 'mlp.up_proj.quantizer.norms_1', 'mlp.up_proj.quantizer.norms_0', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.codebook', 'mlp.down_proj.quantizer.norms_1', 'mlp.down_proj.quantizer.norms_0', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135936
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.000821980796445132
9724 MiB free out of 48676 MiB total
epoch 1 loss: 0.0007637469461769797
9724 MiB free out of 48676 MiB total
epoch 2 loss: 0.0007479175324078824
9724 MiB free out of 48676 MiB total
epoch 3 loss: 0.0007407336270262022
9724 MiB free out of 48676 MiB total
epoch 4 loss: 0.0007366974055003084
9724 MiB free out of 48676 MiB total
36618 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
9724 MiB free out of 48676 MiB total
after cast to cpu
39402 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
6 self_attn.q_proj
Pruning ...
256
iter 0, train loss 219.77130126953125, val loss None
iter 10, train loss 230.28411865234375, val loss None
iter 20, train loss 228.7410125732422, val loss None
iter 30, train loss 224.15199279785156, val loss None
iter 40, train loss 221.4036407470703, val loss None
iter 50, train loss 220.07974243164062, val loss None
iter 60, train loss 220.00607299804688, val loss None
iter 70, train loss 219.6697998046875, val loss None
iter 80, train loss 219.29583740234375, val loss None
iter 90, train loss 219.05801391601562, val loss None
best loss 205.0989532470703
not here
quantized in 35.037132024765015 seconds
37446 MiB free out of 48676 MiB total
6 self_attn.k_proj
Pruning ...
256
iter 0, train loss 238.912109375, val loss None
iter 10, train loss 242.27682495117188, val loss None
iter 20, train loss 246.84254455566406, val loss None
iter 30, train loss 242.83489990234375, val loss None
iter 40, train loss 239.90634155273438, val loss None
iter 50, train loss 238.05728149414062, val loss None
iter 60, train loss 236.71116638183594, val loss None
iter 70, train loss 236.1664276123047, val loss None
iter 80, train loss 235.72640991210938, val loss None
iter 90, train loss 235.52365112304688, val loss None
best loss 217.45777893066406
not here
quantized in 34.003647804260254 seconds
37404 MiB free out of 48676 MiB total
6 self_attn.v_proj
Pruning ...
256
iter 0, train loss 56.15771484375, val loss None
iter 10, train loss 56.74366760253906, val loss None
iter 20, train loss 56.275123596191406, val loss None
iter 30, train loss 56.10483932495117, val loss None
iter 40, train loss 55.920509338378906, val loss None
iter 50, train loss 55.90068054199219, val loss None
iter 60, train loss 55.86650085449219, val loss None
iter 70, train loss 55.80685806274414, val loss None
iter 80, train loss 55.77565383911133, val loss None
iter 90, train loss 55.67301940917969, val loss None
best loss 55.64491271972656
not here
quantized in 32.78540277481079 seconds
37426 MiB free out of 48676 MiB total
6 self_attn.o_proj
Pruning ...
256
iter 0, train loss 1.5149532556533813, val loss None
iter 10, train loss 1.4247328042984009, val loss None
iter 20, train loss 1.385420322418213, val loss None
iter 30, train loss 1.355628490447998, val loss None
iter 40, train loss 1.336961269378662, val loss None
iter 50, train loss 1.3299756050109863, val loss None
iter 60, train loss 1.3229362964630127, val loss None
iter 70, train loss 1.3126435279846191, val loss None
iter 80, train loss 1.306969404220581, val loss None
iter 90, train loss 1.3050774335861206, val loss None
best loss 1.3005554676055908
not here
quantized in 32.48186278343201 seconds
37362 MiB free out of 48676 MiB total
6 mlp.gate_proj
Pruning ...
256
iter 0, train loss 120.06566619873047, val loss None
iter 10, train loss 126.86148071289062, val loss None
iter 20, train loss 123.63284301757812, val loss None
iter 30, train loss 124.02645111083984, val loss None
iter 40, train loss 123.39228820800781, val loss None
iter 50, train loss 123.41075134277344, val loss None
iter 60, train loss 123.21835327148438, val loss None
iter 70, train loss 123.104248046875, val loss None
iter 80, train loss 123.09037780761719, val loss None
iter 90, train loss 123.0968017578125, val loss None
best loss 118.35197448730469
not here
quantized in 89.02187967300415 seconds
36974 MiB free out of 48676 MiB total
6 mlp.up_proj
Pruning ...
256
iter 0, train loss 88.16403198242188, val loss None
iter 10, train loss 88.53604125976562, val loss None
iter 20, train loss 88.53305053710938, val loss None
iter 30, train loss 88.56465148925781, val loss None
iter 40, train loss 88.50059509277344, val loss None
iter 50, train loss 88.4757308959961, val loss None
iter 60, train loss 88.48001861572266, val loss None
iter 70, train loss 88.48248291015625, val loss None
iter 80, train loss 88.43289947509766, val loss None
iter 90, train loss 88.42659759521484, val loss None
best loss 88.16403198242188
not here
quantized in 86.22511672973633 seconds
36694 MiB free out of 48676 MiB total
6 mlp.down_proj
Pruning ...
256
iter 0, train loss 1.3284986019134521, val loss None
iter 10, train loss 1.3211396932601929, val loss None
iter 20, train loss 1.3128833770751953, val loss None
iter 30, train loss 1.305018663406372, val loss None
iter 40, train loss 1.2984912395477295, val loss None
iter 50, train loss 1.2999738454818726, val loss None
iter 60, train loss 1.296458125114441, val loss None
iter 70, train loss 1.2978265285491943, val loss None
iter 80, train loss 1.2960574626922607, val loss None
iter 90, train loss 1.2934796810150146, val loss None
best loss 1.2926360368728638
not here
quantized in 90.86937165260315 seconds
36414 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.codebook', 'self_attn.q_proj.quantizer.norms_1', 'self_attn.q_proj.quantizer.norms_0', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.codebook', 'self_attn.k_proj.quantizer.norms_1', 'self_attn.k_proj.quantizer.norms_0', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.codebook', 'self_attn.v_proj.quantizer.norms_1', 'self_attn.v_proj.quantizer.norms_0', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.codebook', 'self_attn.o_proj.quantizer.norms_1', 'self_attn.o_proj.quantizer.norms_0', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.codebook', 'mlp.gate_proj.quantizer.norms_1', 'mlp.gate_proj.quantizer.norms_0', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.codebook', 'mlp.up_proj.quantizer.norms_1', 'mlp.up_proj.quantizer.norms_0', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.codebook', 'mlp.down_proj.quantizer.norms_1', 'mlp.down_proj.quantizer.norms_0', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135936
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0012679521105383174
9692 MiB free out of 48676 MiB total
epoch 1 loss: 0.0011813520059149596
9692 MiB free out of 48676 MiB total
epoch 2 loss: 0.0011577226314329891
9692 MiB free out of 48676 MiB total
epoch 3 loss: 0.0011457582941147848
9692 MiB free out of 48676 MiB total
epoch 4 loss: 0.0011384732242731843
9692 MiB free out of 48676 MiB total
36414 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
9692 MiB free out of 48676 MiB total
after cast to cpu
39370 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
7 self_attn.q_proj
Pruning ...
256
iter 0, train loss 240.04391479492188, val loss None
iter 10, train loss 253.77427673339844, val loss None
iter 20, train loss 252.1220245361328, val loss None
iter 30, train loss 247.14398193359375, val loss None
iter 40, train loss 244.54144287109375, val loss None
iter 50, train loss 243.53173828125, val loss None
iter 60, train loss 243.1173858642578, val loss None
iter 70, train loss 243.07798767089844, val loss None
iter 80, train loss 242.94656372070312, val loss None
iter 90, train loss 242.6248016357422, val loss None
best loss 223.2991485595703
not here
quantized in 35.0887393951416 seconds
37478 MiB free out of 48676 MiB total
7 self_attn.k_proj
Pruning ...
256
iter 0, train loss 252.57827758789062, val loss None
iter 10, train loss 261.1412048339844, val loss None
iter 20, train loss 265.9671630859375, val loss None
iter 30, train loss 260.2621154785156, val loss None
iter 40, train loss 255.69552612304688, val loss None
iter 50, train loss 253.42645263671875, val loss None
iter 60, train loss 251.95867919921875, val loss None
iter 70, train loss 251.1187286376953, val loss None
iter 80, train loss 250.70584106445312, val loss None
iter 90, train loss 250.39981079101562, val loss None
best loss 231.64212036132812
not here
quantized in 34.350987672805786 seconds
37468 MiB free out of 48676 MiB total
7 self_attn.v_proj
Pruning ...
256
iter 0, train loss 63.7320556640625, val loss None
iter 10, train loss 64.08930206298828, val loss None
iter 20, train loss 63.7071533203125, val loss None
iter 30, train loss 63.36939239501953, val loss None
iter 40, train loss 63.24107360839844, val loss None
iter 50, train loss 63.169837951660156, val loss None
iter 60, train loss 63.130680084228516, val loss None
iter 70, train loss 62.995994567871094, val loss None
iter 80, train loss 62.97530746459961, val loss None
iter 90, train loss 63.00724792480469, val loss None
best loss 62.96833419799805
not here
quantized in 32.57240962982178 seconds
37458 MiB free out of 48676 MiB total
7 self_attn.o_proj
Pruning ...
256
iter 0, train loss 2.131927728652954, val loss None
iter 10, train loss 2.0502047538757324, val loss None
iter 20, train loss 2.022289276123047, val loss None
iter 30, train loss 2.0173661708831787, val loss None
iter 40, train loss 2.0044925212860107, val loss None
iter 50, train loss 2.0014166831970215, val loss None
iter 60, train loss 1.9910657405853271, val loss None
iter 70, train loss 1.9890470504760742, val loss None
iter 80, train loss 1.9862712621688843, val loss None
iter 90, train loss 1.9815473556518555, val loss None
best loss 1.9782299995422363
not here
quantized in 32.26234674453735 seconds
37394 MiB free out of 48676 MiB total
7 mlp.gate_proj
Pruning ...
256
iter 0, train loss 139.39207458496094, val loss None
iter 10, train loss 147.427978515625, val loss None
iter 20, train loss 143.76693725585938, val loss None
iter 30, train loss 144.30850219726562, val loss None
iter 40, train loss 143.7769317626953, val loss None
iter 50, train loss 143.69224548339844, val loss None
iter 60, train loss 143.60072326660156, val loss None
iter 70, train loss 143.5596923828125, val loss None
iter 80, train loss 143.55067443847656, val loss None
iter 90, train loss 143.4761199951172, val loss None
best loss 137.62722778320312
not here
quantized in 87.53894019126892 seconds
37006 MiB free out of 48676 MiB total
7 mlp.up_proj
Pruning ...
256
iter 0, train loss 103.86671447753906, val loss None
iter 10, train loss 104.282470703125, val loss None
iter 20, train loss 104.44577026367188, val loss None
iter 30, train loss 104.31069946289062, val loss None
iter 40, train loss 104.22782135009766, val loss None
iter 50, train loss 104.2052001953125, val loss None
iter 60, train loss 104.16641235351562, val loss None
iter 70, train loss 104.1740493774414, val loss None
iter 80, train loss 104.16627502441406, val loss None
iter 90, train loss 104.13839721679688, val loss None
best loss 103.86671447753906
not here
quantized in 85.43588495254517 seconds
36726 MiB free out of 48676 MiB total
7 mlp.down_proj
Pruning ...
256
iter 0, train loss 1.8046841621398926, val loss None
iter 10, train loss 1.8039073944091797, val loss None
iter 20, train loss 1.800594687461853, val loss None
iter 30, train loss 1.7971696853637695, val loss None
iter 40, train loss 1.7952319383621216, val loss None
iter 50, train loss 1.7942112684249878, val loss None
iter 60, train loss 1.790615439414978, val loss None
iter 70, train loss 1.7892180681228638, val loss None
iter 80, train loss 1.7872562408447266, val loss None
iter 90, train loss 1.7875006198883057, val loss None
best loss 1.7868024110794067
not here
quantized in 90.51440763473511 seconds
36446 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.codebook', 'self_attn.q_proj.quantizer.norms_1', 'self_attn.q_proj.quantizer.norms_0', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.codebook', 'self_attn.k_proj.quantizer.norms_1', 'self_attn.k_proj.quantizer.norms_0', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.codebook', 'self_attn.v_proj.quantizer.norms_1', 'self_attn.v_proj.quantizer.norms_0', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.codebook', 'self_attn.o_proj.quantizer.norms_1', 'self_attn.o_proj.quantizer.norms_0', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.codebook', 'mlp.gate_proj.quantizer.norms_1', 'mlp.gate_proj.quantizer.norms_0', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.codebook', 'mlp.up_proj.quantizer.norms_1', 'mlp.up_proj.quantizer.norms_0', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.codebook', 'mlp.down_proj.quantizer.norms_1', 'mlp.down_proj.quantizer.norms_0', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135936
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0017180082149934606
10684 MiB free out of 48676 MiB total
epoch 1 loss: 0.0016435132156402688
10684 MiB free out of 48676 MiB total
epoch 2 loss: 0.0016198311695916345
10684 MiB free out of 48676 MiB total
epoch 3 loss: 0.00160678032625583
10684 MiB free out of 48676 MiB total
epoch 4 loss: 0.001598476271283289
10684 MiB free out of 48676 MiB total
36446 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
10684 MiB free out of 48676 MiB total
after cast to cpu
39402 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
8 self_attn.q_proj
Pruning ...
256
iter 0, train loss 234.6906280517578, val loss None
iter 10, train loss 246.15289306640625, val loss None
iter 20, train loss 240.58212280273438, val loss None
iter 30, train loss 237.54931640625, val loss None
iter 40, train loss 234.87840270996094, val loss None
iter 50, train loss 233.87722778320312, val loss None
iter 60, train loss 234.150634765625, val loss None
iter 70, train loss 234.10276794433594, val loss None
iter 80, train loss 234.05657958984375, val loss None
iter 90, train loss 234.2126922607422, val loss None
best loss 221.28131103515625
not here
quantized in 34.55559754371643 seconds
37446 MiB free out of 48676 MiB total
8 self_attn.k_proj
Pruning ...
256
iter 0, train loss 244.18679809570312, val loss None
iter 10, train loss 251.72991943359375, val loss None
iter 20, train loss 255.3772735595703, val loss None
iter 30, train loss 248.80352783203125, val loss None
iter 40, train loss 247.21197509765625, val loss None
iter 50, train loss 246.91932678222656, val loss None
iter 60, train loss 245.6803436279297, val loss None
iter 70, train loss 245.086669921875, val loss None
iter 80, train loss 244.5276641845703, val loss None
iter 90, train loss 244.4104766845703, val loss None
best loss 232.94149780273438
not here
quantized in 33.63275170326233 seconds
37404 MiB free out of 48676 MiB total
8 self_attn.v_proj
Pruning ...
256
iter 0, train loss 65.67082214355469, val loss None
iter 10, train loss 66.60302734375, val loss None
iter 20, train loss 65.724365234375, val loss None
iter 30, train loss 65.53291320800781, val loss None
iter 40, train loss 65.36035919189453, val loss None
iter 50, train loss 65.28767395019531, val loss None
iter 60, train loss 65.17578125, val loss None
iter 70, train loss 65.1535873413086, val loss None
iter 80, train loss 65.10371398925781, val loss None
iter 90, train loss 65.0625, val loss None
best loss 65.03339385986328
not here
quantized in 32.75545310974121 seconds
37426 MiB free out of 48676 MiB total
8 self_attn.o_proj
Pruning ...
256
iter 0, train loss 3.8484811782836914, val loss None
iter 10, train loss 3.661776542663574, val loss None
iter 20, train loss 3.5772056579589844, val loss None
iter 30, train loss 3.496103525161743, val loss None
iter 40, train loss 3.4186811447143555, val loss None
iter 50, train loss 3.379739761352539, val loss None
iter 60, train loss 3.361917018890381, val loss None
iter 70, train loss 3.3445897102355957, val loss None
iter 80, train loss 3.322834014892578, val loss None
iter 90, train loss 3.3160414695739746, val loss None
best loss 3.3037331104278564
not here
quantized in 32.33139991760254 seconds
37362 MiB free out of 48676 MiB total
8 mlp.gate_proj
Pruning ...
256
iter 0, train loss 146.13902282714844, val loss None
iter 10, train loss 153.62252807617188, val loss None
iter 20, train loss 149.82398986816406, val loss None
iter 30, train loss 150.04367065429688, val loss None
iter 40, train loss 149.53482055664062, val loss None
iter 50, train loss 149.3736572265625, val loss None
iter 60, train loss 149.12210083007812, val loss None
iter 70, train loss 148.92344665527344, val loss None
iter 80, train loss 148.92431640625, val loss None
iter 90, train loss 148.9491729736328, val loss None
best loss 144.51084899902344
not here
quantized in 87.58414626121521 seconds
36974 MiB free out of 48676 MiB total
8 mlp.up_proj
Pruning ...
256
iter 0, train loss 115.88255310058594, val loss None
iter 10, train loss 116.78167724609375, val loss None
iter 20, train loss 116.89604187011719, val loss None
iter 30, train loss 116.72834777832031, val loss None
iter 40, train loss 116.65797424316406, val loss None
iter 50, train loss 116.60440063476562, val loss None
iter 60, train loss 116.57659149169922, val loss None
iter 70, train loss 116.5042953491211, val loss None
iter 80, train loss 116.45638275146484, val loss None
iter 90, train loss 116.44027709960938, val loss None
best loss 115.88255310058594
not here
quantized in 86.37377214431763 seconds
36694 MiB free out of 48676 MiB total
8 mlp.down_proj
Pruning ...
256
iter 0, train loss 2.279899835586548, val loss None
iter 10, train loss 2.2866718769073486, val loss None
iter 20, train loss 2.2732505798339844, val loss None
iter 30, train loss 2.2697219848632812, val loss None
iter 40, train loss 2.266066789627075, val loss None
iter 50, train loss 2.261369228363037, val loss None
iter 60, train loss 2.2592806816101074, val loss None
iter 70, train loss 2.2589380741119385, val loss None
iter 80, train loss 2.25907301902771, val loss None
iter 90, train loss 2.2587456703186035, val loss None
best loss 2.2576241493225098
not here
quantized in 90.2118513584137 seconds
36414 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.codebook', 'self_attn.q_proj.quantizer.norms_1', 'self_attn.q_proj.quantizer.norms_0', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.codebook', 'self_attn.k_proj.quantizer.norms_1', 'self_attn.k_proj.quantizer.norms_0', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.codebook', 'self_attn.v_proj.quantizer.norms_1', 'self_attn.v_proj.quantizer.norms_0', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.codebook', 'self_attn.o_proj.quantizer.norms_1', 'self_attn.o_proj.quantizer.norms_0', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.codebook', 'mlp.gate_proj.quantizer.norms_1', 'mlp.gate_proj.quantizer.norms_0', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.codebook', 'mlp.up_proj.quantizer.norms_1', 'mlp.up_proj.quantizer.norms_0', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.codebook', 'mlp.down_proj.quantizer.norms_1', 'mlp.down_proj.quantizer.norms_0', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135936
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.002295760010383674
9692 MiB free out of 48676 MiB total
epoch 1 loss: 0.0021935900822427357
9692 MiB free out of 48676 MiB total
epoch 2 loss: 0.0021602310516755097
9692 MiB free out of 48676 MiB total
epoch 3 loss: 0.0021413353742900654
9692 MiB free out of 48676 MiB total
epoch 4 loss: 0.0021289805463311495
9692 MiB free out of 48676 MiB total
36414 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
9692 MiB free out of 48676 MiB total
after cast to cpu
39370 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
9 self_attn.q_proj
Pruning ...
256
iter 0, train loss 242.82969665527344, val loss None
iter 10, train loss 258.7447509765625, val loss None
iter 20, train loss 250.21958923339844, val loss None
iter 30, train loss 246.47610473632812, val loss None
iter 40, train loss 244.60147094726562, val loss None
iter 50, train loss 243.58212280273438, val loss None
iter 60, train loss 243.53677368164062, val loss None
iter 70, train loss 243.449951171875, val loss None
iter 80, train loss 243.02203369140625, val loss None
iter 90, train loss 242.90234375, val loss None
best loss 232.4320831298828
not here
quantized in 35.12703204154968 seconds
37478 MiB free out of 48676 MiB total
9 self_attn.k_proj
Pruning ...
256
iter 0, train loss 265.6424865722656, val loss None
iter 10, train loss 277.83892822265625, val loss None
iter 20, train loss 279.1625671386719, val loss None
iter 30, train loss 272.127197265625, val loss None
iter 40, train loss 268.7062072753906, val loss None
iter 50, train loss 267.4138488769531, val loss None
iter 60, train loss 266.9123229980469, val loss None
iter 70, train loss 266.5976867675781, val loss None
iter 80, train loss 266.56304931640625, val loss None
iter 90, train loss 266.5103454589844, val loss None
best loss 254.35281372070312
not here
quantized in 34.260517597198486 seconds
37468 MiB free out of 48676 MiB total
9 self_attn.v_proj
Pruning ...
256
iter 0, train loss 71.95406341552734, val loss None
iter 10, train loss 72.47506713867188, val loss None
iter 20, train loss 71.94648742675781, val loss None
iter 30, train loss 71.64215087890625, val loss None
iter 40, train loss 71.5131607055664, val loss None
iter 50, train loss 71.41980743408203, val loss None
iter 60, train loss 71.38726806640625, val loss None
iter 70, train loss 71.40791320800781, val loss None
iter 80, train loss 71.30685424804688, val loss None
iter 90, train loss 71.24900817871094, val loss None
best loss 71.23368835449219
not here
quantized in 32.57401204109192 seconds
37490 MiB free out of 48676 MiB total
9 self_attn.o_proj
Pruning ...
256
iter 0, train loss 4.935539245605469, val loss None
iter 10, train loss 4.778722286224365, val loss None
iter 20, train loss 4.741721153259277, val loss None
iter 30, train loss 4.699539661407471, val loss None
iter 40, train loss 4.665382385253906, val loss None
iter 50, train loss 4.624451637268066, val loss None
iter 60, train loss 4.605523109436035, val loss None
iter 70, train loss 4.619955062866211, val loss None
iter 80, train loss 4.611334323883057, val loss None
iter 90, train loss 4.594109058380127, val loss None
best loss 4.5903472900390625
not here
quantized in 32.30731511116028 seconds
37458 MiB free out of 48676 MiB total
9 mlp.gate_proj
Pruning ...
256
iter 0, train loss 154.97018432617188, val loss None
iter 10, train loss 163.49374389648438, val loss None
iter 20, train loss 158.73171997070312, val loss None
iter 30, train loss 159.54095458984375, val loss None
iter 40, train loss 159.06939697265625, val loss None
iter 50, train loss 158.83486938476562, val loss None
iter 60, train loss 158.51670837402344, val loss None
iter 70, train loss 158.3781280517578, val loss None
iter 80, train loss 158.24343872070312, val loss None
iter 90, train loss 158.26588439941406, val loss None
best loss 152.56478881835938
not here
quantized in 88.01323676109314 seconds
37070 MiB free out of 48676 MiB total
9 mlp.up_proj
Pruning ...
256
iter 0, train loss 127.10066986083984, val loss None
iter 10, train loss 127.95171356201172, val loss None
iter 20, train loss 127.99470520019531, val loss None
iter 30, train loss 127.83991241455078, val loss None
iter 40, train loss 127.8450927734375, val loss None
iter 50, train loss 127.77782440185547, val loss None
iter 60, train loss 127.74578857421875, val loss None
iter 70, train loss 127.74336242675781, val loss None
iter 80, train loss 127.66362762451172, val loss None
iter 90, train loss 127.629638671875, val loss None
best loss 126.76585388183594
not here
quantized in 86.19514179229736 seconds
36790 MiB free out of 48676 MiB total
9 mlp.down_proj
Pruning ...
256
iter 0, train loss 2.718780517578125, val loss None
iter 10, train loss 2.712423324584961, val loss None
iter 20, train loss 2.7073264122009277, val loss None
iter 30, train loss 2.6942896842956543, val loss None
iter 40, train loss 2.6877236366271973, val loss None
iter 50, train loss 2.6849365234375, val loss None
iter 60, train loss 2.6829283237457275, val loss None
iter 70, train loss 2.68023419380188, val loss None
iter 80, train loss 2.6768546104431152, val loss None
iter 90, train loss 2.678255081176758, val loss None
best loss 2.6764049530029297
not here
quantized in 90.28386068344116 seconds
36510 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.codebook', 'self_attn.q_proj.quantizer.norms_1', 'self_attn.q_proj.quantizer.norms_0', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.codebook', 'self_attn.k_proj.quantizer.norms_1', 'self_attn.k_proj.quantizer.norms_0', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.codebook', 'self_attn.v_proj.quantizer.norms_1', 'self_attn.v_proj.quantizer.norms_0', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.codebook', 'self_attn.o_proj.quantizer.norms_1', 'self_attn.o_proj.quantizer.norms_0', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.codebook', 'mlp.gate_proj.quantizer.norms_1', 'mlp.gate_proj.quantizer.norms_0', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.codebook', 'mlp.up_proj.quantizer.norms_1', 'mlp.up_proj.quantizer.norms_0', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.codebook', 'mlp.down_proj.quantizer.norms_1', 'mlp.down_proj.quantizer.norms_0', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135936
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0028109191116527654
9702 MiB free out of 48676 MiB total
epoch 1 loss: 0.002704102538700681
9702 MiB free out of 48676 MiB total
epoch 2 loss: 0.002663890496478416
9702 MiB free out of 48676 MiB total
epoch 3 loss: 0.0026408196117699845
9702 MiB free out of 48676 MiB total
epoch 4 loss: 0.0026258221314492403
9702 MiB free out of 48676 MiB total
36510 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
9702 MiB free out of 48676 MiB total
after cast to cpu
39402 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
10 self_attn.q_proj
Pruning ...
256
iter 0, train loss 252.16195678710938, val loss None
iter 10, train loss 264.16058349609375, val loss None
iter 20, train loss 255.18399047851562, val loss None
iter 30, train loss 252.1029510498047, val loss None
iter 40, train loss 250.9412841796875, val loss None
iter 50, train loss 250.54193115234375, val loss None
iter 60, train loss 250.21340942382812, val loss None
iter 70, train loss 249.79983520507812, val loss None
iter 80, train loss 249.6602325439453, val loss None
iter 90, train loss 249.5238494873047, val loss None
best loss 239.7147216796875
not here
quantized in 34.91354417800903 seconds
37446 MiB free out of 48676 MiB total
10 self_attn.k_proj
Pruning ...
256
iter 0, train loss 279.7113037109375, val loss None
iter 10, train loss 290.9736328125, val loss None
iter 20, train loss 294.8740539550781, val loss None
iter 30, train loss 288.2125244140625, val loss None
iter 40, train loss 284.2838134765625, val loss None
iter 50, train loss 282.3589782714844, val loss None
iter 60, train loss 282.8704833984375, val loss None
iter 70, train loss 282.51153564453125, val loss None
iter 80, train loss 282.03271484375, val loss None
iter 90, train loss 281.815185546875, val loss None
best loss 268.1202697753906
not here
quantized in 33.4952597618103 seconds
37404 MiB free out of 48676 MiB total
10 self_attn.v_proj
Pruning ...
256
iter 0, train loss 72.7371597290039, val loss None
iter 10, train loss 73.38948059082031, val loss None
iter 20, train loss 72.74917602539062, val loss None
iter 30, train loss 72.57270050048828, val loss None
iter 40, train loss 72.41812896728516, val loss None
iter 50, train loss 72.38711547851562, val loss None
iter 60, train loss 72.33004760742188, val loss None
iter 70, train loss 72.31503295898438, val loss None
iter 80, train loss 72.29105377197266, val loss None
iter 90, train loss 72.25921630859375, val loss None
best loss 72.24071502685547
not here
quantized in 32.52196550369263 seconds
37426 MiB free out of 48676 MiB total
10 self_attn.o_proj
Pruning ...
256
iter 0, train loss 7.837725639343262, val loss None
iter 10, train loss 7.390902519226074, val loss None
iter 20, train loss 7.136240005493164, val loss None
iter 30, train loss 6.87928581237793, val loss None
iter 40, train loss 6.715683937072754, val loss None
iter 50, train loss 6.6674089431762695, val loss None
iter 60, train loss 6.615970611572266, val loss None
iter 70, train loss 6.58138370513916, val loss None
iter 80, train loss 6.579213619232178, val loss None
iter 90, train loss 6.549314975738525, val loss None
best loss 6.535506248474121
not here
quantized in 31.911247730255127 seconds
37426 MiB free out of 48676 MiB total
10 mlp.gate_proj
Pruning ...
256
iter 0, train loss 161.86842346191406, val loss None
iter 10, train loss 171.42807006835938, val loss None
iter 20, train loss 167.0188446044922, val loss None
iter 30, train loss 166.843994140625, val loss None
iter 40, train loss 166.361083984375, val loss None
iter 50, train loss 166.21697998046875, val loss None
iter 60, train loss 166.05282592773438, val loss None
iter 70, train loss 165.9833984375, val loss None
iter 80, train loss 165.84669494628906, val loss None
iter 90, train loss 165.8065185546875, val loss None
best loss 159.4904327392578
not here
quantized in 88.13678121566772 seconds
37038 MiB free out of 48676 MiB total
10 mlp.up_proj
Pruning ...
256
iter 0, train loss 135.79083251953125, val loss None
iter 10, train loss 137.4303741455078, val loss None
iter 20, train loss 137.2206268310547, val loss None
iter 30, train loss 136.90017700195312, val loss None
iter 40, train loss 136.9139404296875, val loss None
iter 50, train loss 136.81008911132812, val loss None
iter 60, train loss 136.73355102539062, val loss None
iter 70, train loss 136.70460510253906, val loss None
iter 80, train loss 136.70574951171875, val loss None
iter 90, train loss 136.68203735351562, val loss None
best loss 135.42193603515625
not here
quantized in 86.47146558761597 seconds
36758 MiB free out of 48676 MiB total
10 mlp.down_proj
Pruning ...
256
iter 0, train loss 3.2973175048828125, val loss None
iter 10, train loss 3.2926979064941406, val loss None
iter 20, train loss 3.2566778659820557, val loss None
iter 30, train loss 3.223942995071411, val loss None
iter 40, train loss 3.2143497467041016, val loss None
iter 50, train loss 3.1988701820373535, val loss None
iter 60, train loss 3.193805456161499, val loss None
iter 70, train loss 3.191086530685425, val loss None
iter 80, train loss 3.186687469482422, val loss None
iter 90, train loss 3.1832025051116943, val loss None
best loss 3.1827118396759033
not here
quantized in 91.06127619743347 seconds
36478 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.codebook', 'self_attn.q_proj.quantizer.norms_1', 'self_attn.q_proj.quantizer.norms_0', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.codebook', 'self_attn.k_proj.quantizer.norms_1', 'self_attn.k_proj.quantizer.norms_0', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.codebook', 'self_attn.v_proj.quantizer.norms_1', 'self_attn.v_proj.quantizer.norms_0', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.codebook', 'self_attn.o_proj.quantizer.norms_1', 'self_attn.o_proj.quantizer.norms_0', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.codebook', 'mlp.gate_proj.quantizer.norms_1', 'mlp.gate_proj.quantizer.norms_0', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.codebook', 'mlp.up_proj.quantizer.norms_1', 'mlp.up_proj.quantizer.norms_0', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.codebook', 'mlp.down_proj.quantizer.norms_1', 'mlp.down_proj.quantizer.norms_0', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135936
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0034764017564157257
9692 MiB free out of 48676 MiB total
epoch 1 loss: 0.0033440460883866763
9692 MiB free out of 48676 MiB total
epoch 2 loss: 0.0032922956943366444
9692 MiB free out of 48676 MiB total
epoch 3 loss: 0.0032616740099911112
9692 MiB free out of 48676 MiB total
epoch 4 loss: 0.0032410560561402235
9692 MiB free out of 48676 MiB total
36478 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
9692 MiB free out of 48676 MiB total
after cast to cpu
39370 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
11 self_attn.q_proj
Pruning ...
256
iter 0, train loss 284.2253112792969, val loss None
iter 10, train loss 303.30731201171875, val loss None
iter 20, train loss 298.15191650390625, val loss None
iter 30, train loss 293.8396911621094, val loss None
iter 40, train loss 290.22235107421875, val loss None
iter 50, train loss 287.76861572265625, val loss None
iter 60, train loss 287.6066589355469, val loss None
iter 70, train loss 287.602783203125, val loss None
iter 80, train loss 287.41912841796875, val loss None
iter 90, train loss 287.00469970703125, val loss None
best loss 269.72796630859375
not here
quantized in 34.621915340423584 seconds
37478 MiB free out of 48676 MiB total
11 self_attn.k_proj
Pruning ...
256
iter 0, train loss 293.77838134765625, val loss None
iter 10, train loss 307.4789733886719, val loss None
iter 20, train loss 309.46099853515625, val loss None
iter 30, train loss 302.68701171875, val loss None
iter 40, train loss 300.53326416015625, val loss None
iter 50, train loss 298.0972900390625, val loss None
iter 60, train loss 296.753173828125, val loss None
iter 70, train loss 296.3452453613281, val loss None
iter 80, train loss 295.90020751953125, val loss None
iter 90, train loss 295.4164123535156, val loss None
best loss 278.42022705078125
not here
quantized in 33.51910877227783 seconds
37468 MiB free out of 48676 MiB total
11 self_attn.v_proj
Pruning ...
256
iter 0, train loss 98.7347412109375, val loss None
iter 10, train loss 99.41220092773438, val loss None
iter 20, train loss 98.88307189941406, val loss None
iter 30, train loss 98.40547943115234, val loss None
iter 40, train loss 98.32311248779297, val loss None
iter 50, train loss 98.15121459960938, val loss None
iter 60, train loss 98.09806823730469, val loss None
iter 70, train loss 97.89449310302734, val loss None
iter 80, train loss 97.8405990600586, val loss None
iter 90, train loss 97.8674087524414, val loss None
best loss 97.755615234375
not here
quantized in 32.40995526313782 seconds
37458 MiB free out of 48676 MiB total
11 self_attn.o_proj
Pruning ...
256
iter 0, train loss 7.609523296356201, val loss None
iter 10, train loss 7.527207374572754, val loss None
iter 20, train loss 7.502410888671875, val loss None
iter 30, train loss 7.529999732971191, val loss None
iter 40, train loss 7.496074199676514, val loss None
iter 50, train loss 7.492966651916504, val loss None
iter 60, train loss 7.4744062423706055, val loss None
iter 70, train loss 7.437629699707031, val loss None
iter 80, train loss 7.4060468673706055, val loss None
iter 90, train loss 7.424080848693848, val loss None
best loss 7.403453826904297
not here
quantized in 31.643188953399658 seconds
37394 MiB free out of 48676 MiB total
11 mlp.gate_proj
Pruning ...
256
iter 0, train loss 175.5012969970703, val loss None
iter 10, train loss 186.8127899169922, val loss None
iter 20, train loss 182.71299743652344, val loss None
iter 30, train loss 182.94073486328125, val loss None
iter 40, train loss 182.34280395507812, val loss None
iter 50, train loss 181.64614868164062, val loss None
iter 60, train loss 181.21499633789062, val loss None
iter 70, train loss 181.1202850341797, val loss None
iter 80, train loss 181.01783752441406, val loss None
iter 90, train loss 180.9801025390625, val loss None
best loss 172.85000610351562
not here
quantized in 87.62725377082825 seconds
37006 MiB free out of 48676 MiB total
11 mlp.up_proj
Pruning ...
256
iter 0, train loss 151.1798095703125, val loss None
iter 10, train loss 152.80484008789062, val loss None
iter 20, train loss 152.89627075195312, val loss None
iter 30, train loss 152.6700439453125, val loss None
iter 40, train loss 152.75253295898438, val loss None
iter 50, train loss 152.58595275878906, val loss None
iter 60, train loss 152.51315307617188, val loss None
iter 70, train loss 152.47593688964844, val loss None
iter 80, train loss 152.46798706054688, val loss None
iter 90, train loss 152.41580200195312, val loss None
best loss 150.98695373535156
not here
quantized in 86.00498962402344 seconds
36726 MiB free out of 48676 MiB total
11 mlp.down_proj
Pruning ...
256
iter 0, train loss 3.6008028984069824, val loss None
iter 10, train loss 3.59928560256958, val loss None
iter 20, train loss 3.593940496444702, val loss None
iter 30, train loss 3.5816869735717773, val loss None
iter 40, train loss 3.5727524757385254, val loss None
iter 50, train loss 3.565194606781006, val loss None
iter 60, train loss 3.5615689754486084, val loss None
iter 70, train loss 3.5559728145599365, val loss None
iter 80, train loss 3.557652711868286, val loss None
iter 90, train loss 3.554858684539795, val loss None
best loss 3.552838087081909
not here
quantized in 90.42426776885986 seconds
36446 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.codebook', 'self_attn.q_proj.quantizer.norms_1', 'self_attn.q_proj.quantizer.norms_0', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.codebook', 'self_attn.k_proj.quantizer.norms_1', 'self_attn.k_proj.quantizer.norms_0', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.codebook', 'self_attn.v_proj.quantizer.norms_1', 'self_attn.v_proj.quantizer.norms_0', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.codebook', 'self_attn.o_proj.quantizer.norms_1', 'self_attn.o_proj.quantizer.norms_0', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.codebook', 'mlp.gate_proj.quantizer.norms_1', 'mlp.gate_proj.quantizer.norms_0', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.codebook', 'mlp.up_proj.quantizer.norms_1', 'mlp.up_proj.quantizer.norms_0', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.codebook', 'mlp.down_proj.quantizer.norms_1', 'mlp.down_proj.quantizer.norms_0', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135936
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.003956299786295858
10684 MiB free out of 48676 MiB total
epoch 1 loss: 0.0037925708365946775
10684 MiB free out of 48676 MiB total
epoch 2 loss: 0.0037326720830606064
10684 MiB free out of 48676 MiB total
epoch 3 loss: 0.003696537898576935
10684 MiB free out of 48676 MiB total
epoch 4 loss: 0.0036717739149025874
10684 MiB free out of 48676 MiB total
36446 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
10684 MiB free out of 48676 MiB total
after cast to cpu
39402 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
12 self_attn.q_proj
Pruning ...
256
iter 0, train loss 294.08612060546875, val loss None
iter 10, train loss 308.9424743652344, val loss None
iter 20, train loss 302.76513671875, val loss None
iter 30, train loss 298.5130920410156, val loss None
iter 40, train loss 297.1448974609375, val loss None
iter 50, train loss 295.6011657714844, val loss None
iter 60, train loss 295.01220703125, val loss None
iter 70, train loss 294.77288818359375, val loss None
iter 80, train loss 294.7789611816406, val loss None
iter 90, train loss 294.9052429199219, val loss None
best loss 280.22515869140625
not here
quantized in 34.69581580162048 seconds
37446 MiB free out of 48676 MiB total
12 self_attn.k_proj
Pruning ...
256
iter 0, train loss 329.68145751953125, val loss None
iter 10, train loss 344.3951110839844, val loss None
iter 20, train loss 346.0260009765625, val loss None
iter 30, train loss 338.49908447265625, val loss None
iter 40, train loss 334.4988098144531, val loss None
iter 50, train loss 332.7562561035156, val loss None
iter 60, train loss 331.96624755859375, val loss None
iter 70, train loss 331.5070495605469, val loss None
iter 80, train loss 331.29669189453125, val loss None
iter 90, train loss 330.900634765625, val loss None
best loss 310.62884521484375
not here
quantized in 33.553996086120605 seconds
37404 MiB free out of 48676 MiB total
12 self_attn.v_proj
Pruning ...
256
iter 0, train loss 96.4186019897461, val loss None
iter 10, train loss 96.43986511230469, val loss None
iter 20, train loss 96.27289581298828, val loss None
iter 30, train loss 95.99575805664062, val loss None
iter 40, train loss 95.96697998046875, val loss None
iter 50, train loss 95.74552917480469, val loss None
iter 60, train loss 95.79219055175781, val loss None
iter 70, train loss 95.77798461914062, val loss None
iter 80, train loss 95.71913146972656, val loss None
iter 90, train loss 95.69326782226562, val loss None
best loss 95.67267608642578
not here
quantized in 32.3477098941803 seconds
37426 MiB free out of 48676 MiB total
12 self_attn.o_proj
Pruning ...
256
iter 0, train loss 8.982551574707031, val loss None
iter 10, train loss 8.874621391296387, val loss None
iter 20, train loss 8.826776504516602, val loss None
iter 30, train loss 8.770297050476074, val loss None
iter 40, train loss 8.727180480957031, val loss None
iter 50, train loss 8.659632682800293, val loss None
iter 60, train loss 8.592227935791016, val loss None
iter 70, train loss 8.59712028503418, val loss None
iter 80, train loss 8.595253944396973, val loss None
iter 90, train loss 8.581865310668945, val loss None
best loss 8.563644409179688
not here
quantized in 31.65191102027893 seconds
37362 MiB free out of 48676 MiB total
12 mlp.gate_proj
Pruning ...
256
iter 0, train loss 186.08590698242188, val loss None
iter 10, train loss 196.17758178710938, val loss None
iter 20, train loss 191.7614288330078, val loss None
iter 30, train loss 191.75848388671875, val loss None
iter 40, train loss 191.2430877685547, val loss None
iter 50, train loss 190.9816436767578, val loss None
iter 60, train loss 190.8212890625, val loss None
iter 70, train loss 190.8155517578125, val loss None
iter 80, train loss 190.71249389648438, val loss None
iter 90, train loss 190.69102478027344, val loss None
best loss 183.38226318359375
not here
quantized in 87.78331303596497 seconds
36974 MiB free out of 48676 MiB total
12 mlp.up_proj
Pruning ...
256
iter 0, train loss 164.87757873535156, val loss None
iter 10, train loss 166.48910522460938, val loss None
iter 20, train loss 166.40567016601562, val loss None
iter 30, train loss 166.21524047851562, val loss None
iter 40, train loss 166.0867156982422, val loss None
iter 50, train loss 165.94815063476562, val loss None
iter 60, train loss 165.93603515625, val loss None
iter 70, train loss 165.9368896484375, val loss None
iter 80, train loss 165.95384216308594, val loss None
iter 90, train loss 165.90228271484375, val loss None
best loss 164.87757873535156
not here
quantized in 85.55337953567505 seconds
36694 MiB free out of 48676 MiB total
12 mlp.down_proj
Pruning ...
256
iter 0, train loss 4.04105281829834, val loss None
iter 10, train loss 4.050734043121338, val loss None
iter 20, train loss 4.043874740600586, val loss None
iter 30, train loss 4.047091007232666, val loss None
iter 40, train loss 4.044003486633301, val loss None
iter 50, train loss 4.0404744148254395, val loss None
iter 60, train loss 4.0347442626953125, val loss None
iter 70, train loss 4.035654067993164, val loss None
iter 80, train loss 4.036746978759766, val loss None
iter 90, train loss 4.037686824798584, val loss None
best loss 4.034374713897705
not here
quantized in 90.03714632987976 seconds
36414 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.codebook', 'self_attn.q_proj.quantizer.norms_1', 'self_attn.q_proj.quantizer.norms_0', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.codebook', 'self_attn.k_proj.quantizer.norms_1', 'self_attn.k_proj.quantizer.norms_0', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.codebook', 'self_attn.v_proj.quantizer.norms_1', 'self_attn.v_proj.quantizer.norms_0', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.codebook', 'self_attn.o_proj.quantizer.norms_1', 'self_attn.o_proj.quantizer.norms_0', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.codebook', 'mlp.gate_proj.quantizer.norms_1', 'mlp.gate_proj.quantizer.norms_0', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.codebook', 'mlp.up_proj.quantizer.norms_1', 'mlp.up_proj.quantizer.norms_0', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.codebook', 'mlp.down_proj.quantizer.norms_1', 'mlp.down_proj.quantizer.norms_0', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135936
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.004602529010298895
9692 MiB free out of 48676 MiB total
epoch 1 loss: 0.004382270530186361
9692 MiB free out of 48676 MiB total
epoch 2 loss: 0.004308663455958595
9692 MiB free out of 48676 MiB total
epoch 3 loss: 0.0042680847545852885
9692 MiB free out of 48676 MiB total
epoch 4 loss: 0.004241129568981705
9692 MiB free out of 48676 MiB total
36414 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
9692 MiB free out of 48676 MiB total
after cast to cpu
39370 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
13 self_attn.q_proj
Pruning ...
256
iter 0, train loss 301.6249694824219, val loss None
iter 10, train loss 317.9518737792969, val loss None
iter 20, train loss 308.4739990234375, val loss None
iter 30, train loss 304.34075927734375, val loss None
iter 40, train loss 303.00213623046875, val loss None
iter 50, train loss 302.0746154785156, val loss None
iter 60, train loss 301.5335998535156, val loss None
iter 70, train loss 301.17108154296875, val loss None
iter 80, train loss 301.0914306640625, val loss None
iter 90, train loss 300.9544982910156, val loss None
best loss 286.31781005859375
not here
quantized in 34.19938564300537 seconds
37478 MiB free out of 48676 MiB total
13 self_attn.k_proj
Pruning ...
256
iter 0, train loss 330.2586364746094, val loss None
iter 10, train loss 343.79205322265625, val loss None
iter 20, train loss 343.8124084472656, val loss None
iter 30, train loss 338.02398681640625, val loss None
iter 40, train loss 335.6625061035156, val loss None
iter 50, train loss 333.6800537109375, val loss None
iter 60, train loss 332.2983093261719, val loss None
iter 70, train loss 331.27484130859375, val loss None
iter 80, train loss 330.7132568359375, val loss None
iter 90, train loss 330.46112060546875, val loss None
best loss 308.1905517578125
not here
quantized in 33.514315605163574 seconds
37468 MiB free out of 48676 MiB total
13 self_attn.v_proj
Pruning ...
256
iter 0, train loss 107.11003875732422, val loss None
iter 10, train loss 106.98056030273438, val loss None
iter 20, train loss 106.7584228515625, val loss None
iter 30, train loss 106.53690338134766, val loss None
iter 40, train loss 106.2638931274414, val loss None
iter 50, train loss 106.10105895996094, val loss None
iter 60, train loss 106.03341674804688, val loss None
iter 70, train loss 105.95525360107422, val loss None
iter 80, train loss 105.91758728027344, val loss None
iter 90, train loss 105.90032958984375, val loss None
best loss 105.86222839355469
not here
quantized in 32.285017251968384 seconds
37490 MiB free out of 48676 MiB total
13 self_attn.o_proj
Pruning ...
256
iter 0, train loss 9.461810111999512, val loss None
iter 10, train loss 9.175138473510742, val loss None
iter 20, train loss 9.170892715454102, val loss None
iter 30, train loss 9.031392097473145, val loss None
iter 40, train loss 8.986089706420898, val loss None
iter 50, train loss 8.954931259155273, val loss None
iter 60, train loss 8.944490432739258, val loss None
iter 70, train loss 8.914229393005371, val loss None
iter 80, train loss 8.893017768859863, val loss None
iter 90, train loss 8.860734939575195, val loss None
best loss 8.850004196166992
not here
quantized in 31.813143014907837 seconds
37426 MiB free out of 48676 MiB total
13 mlp.gate_proj
Pruning ...
256
iter 0, train loss 198.35720825195312, val loss None
iter 10, train loss 209.0474853515625, val loss None
iter 20, train loss 205.04429626464844, val loss None
iter 30, train loss 205.28785705566406, val loss None
iter 40, train loss 204.72360229492188, val loss None
iter 50, train loss 204.0289306640625, val loss None
iter 60, train loss 203.83172607421875, val loss None
iter 70, train loss 203.67343139648438, val loss None
iter 80, train loss 203.55606079101562, val loss None
iter 90, train loss 203.55929565429688, val loss None
best loss 195.23951721191406
not here
quantized in 87.53314661979675 seconds
37038 MiB free out of 48676 MiB total
13 mlp.up_proj
Pruning ...
256
iter 0, train loss 180.20401000976562, val loss None
iter 10, train loss 181.74441528320312, val loss None
iter 20, train loss 182.01583862304688, val loss None
iter 30, train loss 181.7061309814453, val loss None
iter 40, train loss 181.6946258544922, val loss None
iter 50, train loss 181.54466247558594, val loss None
iter 60, train loss 181.45252990722656, val loss None
iter 70, train loss 181.42294311523438, val loss None
iter 80, train loss 181.42434692382812, val loss None
iter 90, train loss 181.4246368408203, val loss None
best loss 179.7799835205078
not here
quantized in 85.85554480552673 seconds
36758 MiB free out of 48676 MiB total
13 mlp.down_proj
Pruning ...
256
iter 0, train loss 4.870853424072266, val loss None
iter 10, train loss 4.876711845397949, val loss None
iter 20, train loss 4.852268695831299, val loss None
iter 30, train loss 4.834890365600586, val loss None
iter 40, train loss 4.829856872558594, val loss None
iter 50, train loss 4.828313827514648, val loss None
iter 60, train loss 4.822322845458984, val loss None
iter 70, train loss 4.821844100952148, val loss None
iter 80, train loss 4.819151878356934, val loss None
iter 90, train loss 4.817648410797119, val loss None
best loss 4.817185878753662
not here
quantized in 90.1058235168457 seconds
36478 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.codebook', 'self_attn.q_proj.quantizer.norms_1', 'self_attn.q_proj.quantizer.norms_0', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.codebook', 'self_attn.k_proj.quantizer.norms_1', 'self_attn.k_proj.quantizer.norms_0', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.codebook', 'self_attn.v_proj.quantizer.norms_1', 'self_attn.v_proj.quantizer.norms_0', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.codebook', 'self_attn.o_proj.quantizer.norms_1', 'self_attn.o_proj.quantizer.norms_0', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.codebook', 'mlp.gate_proj.quantizer.norms_1', 'mlp.gate_proj.quantizer.norms_0', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.codebook', 'mlp.up_proj.quantizer.norms_1', 'mlp.up_proj.quantizer.norms_0', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.codebook', 'mlp.down_proj.quantizer.norms_1', 'mlp.down_proj.quantizer.norms_0', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135936
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.0051881154722650535
9692 MiB free out of 48676 MiB total
epoch 1 loss: 0.004936319401167566
9692 MiB free out of 48676 MiB total
epoch 2 loss: 0.004855389775912045
9692 MiB free out of 48676 MiB total
epoch 3 loss: 0.004809057933016447
9692 MiB free out of 48676 MiB total
epoch 4 loss: 0.004777354381076293
9692 MiB free out of 48676 MiB total
36478 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
9692 MiB free out of 48676 MiB total
after cast to cpu
39402 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
14 self_attn.q_proj
Pruning ...
256
iter 0, train loss 319.9600830078125, val loss None
iter 10, train loss 340.0465087890625, val loss None
iter 20, train loss 329.0800476074219, val loss None
iter 30, train loss 325.19921875, val loss None
iter 40, train loss 322.2723693847656, val loss None
iter 50, train loss 321.41046142578125, val loss None
iter 60, train loss 320.671630859375, val loss None
iter 70, train loss 319.9654541015625, val loss None
iter 80, train loss 319.4638977050781, val loss None
iter 90, train loss 319.23583984375, val loss None
best loss 298.39935302734375
not here
quantized in 34.943440198898315 seconds
37446 MiB free out of 48676 MiB total
14 self_attn.k_proj
Pruning ...
256
iter 0, train loss 352.51080322265625, val loss None
iter 10, train loss 367.5344543457031, val loss None
iter 20, train loss 368.5648193359375, val loss None
iter 30, train loss 361.04559326171875, val loss None
iter 40, train loss 357.3367919921875, val loss None
iter 50, train loss 354.19793701171875, val loss None
iter 60, train loss 353.8001403808594, val loss None
iter 70, train loss 353.0999450683594, val loss None
iter 80, train loss 353.07513427734375, val loss None
iter 90, train loss 352.72052001953125, val loss None
best loss 324.38055419921875
not here
quantized in 33.82408332824707 seconds
37404 MiB free out of 48676 MiB total
14 self_attn.v_proj
Pruning ...
256
iter 0, train loss 108.33213806152344, val loss None
iter 10, train loss 107.96092224121094, val loss None
iter 20, train loss 107.66687774658203, val loss None
iter 30, train loss 107.43293762207031, val loss None
iter 40, train loss 107.13986206054688, val loss None
iter 50, train loss 107.15457153320312, val loss None
iter 60, train loss 106.96237182617188, val loss None
iter 70, train loss 106.91312408447266, val loss None
iter 80, train loss 106.9497299194336, val loss None
iter 90, train loss 106.77178955078125, val loss None
best loss 106.77005767822266
not here
quantized in 32.25467586517334 seconds
37426 MiB free out of 48676 MiB total
14 self_attn.o_proj
Pruning ...
256
iter 0, train loss 12.156216621398926, val loss None
iter 10, train loss 11.970274925231934, val loss None
iter 20, train loss 11.81056022644043, val loss None
iter 30, train loss 11.737261772155762, val loss None
iter 40, train loss 11.641650199890137, val loss None
iter 50, train loss 11.557191848754883, val loss None
iter 60, train loss 11.496908187866211, val loss None
iter 70, train loss 11.453620910644531, val loss None
iter 80, train loss 11.423834800720215, val loss None
iter 90, train loss 11.35925579071045, val loss None
best loss 11.339680671691895
not here
quantized in 31.723620176315308 seconds
37426 MiB free out of 48676 MiB total
14 mlp.gate_proj
Pruning ...
256
iter 0, train loss 215.39927673339844, val loss None
iter 10, train loss 227.68621826171875, val loss None
iter 20, train loss 221.579345703125, val loss None
iter 30, train loss 221.41749572753906, val loss None
iter 40, train loss 220.94403076171875, val loss None
iter 50, train loss 220.77337646484375, val loss None
iter 60, train loss 220.6776123046875, val loss None
iter 70, train loss 220.51304626464844, val loss None
iter 80, train loss 220.59381103515625, val loss None
iter 90, train loss 220.58494567871094, val loss None
best loss 211.11952209472656
not here
quantized in 87.31309199333191 seconds
37038 MiB free out of 48676 MiB total
14 mlp.up_proj
Pruning ...
256
iter 0, train loss 195.88140869140625, val loss None
iter 10, train loss 197.32748413085938, val loss None
iter 20, train loss 197.68307495117188, val loss None
iter 30, train loss 197.340576171875, val loss None
iter 40, train loss 197.22280883789062, val loss None
iter 50, train loss 197.3105926513672, val loss None
iter 60, train loss 197.3725128173828, val loss None
iter 70, train loss 197.22003173828125, val loss None
iter 80, train loss 197.16029357910156, val loss None
iter 90, train loss 197.1974334716797, val loss None
best loss 195.44320678710938
not here
quantized in 85.62529420852661 seconds
36758 MiB free out of 48676 MiB total
14 mlp.down_proj
Pruning ...
256
iter 0, train loss 5.490476131439209, val loss None
iter 10, train loss 5.494775772094727, val loss None
iter 20, train loss 5.480615615844727, val loss None
iter 30, train loss 5.4765543937683105, val loss None
iter 40, train loss 5.476624488830566, val loss None
iter 50, train loss 5.46903657913208, val loss None
iter 60, train loss 5.472886562347412, val loss None
iter 70, train loss 5.470076084136963, val loss None
iter 80, train loss 5.467616081237793, val loss None
iter 90, train loss 5.465585708618164, val loss None
best loss 5.463650226593018
not here
quantized in 89.54037928581238 seconds
36650 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.codebook', 'self_attn.q_proj.quantizer.norms_1', 'self_attn.q_proj.quantizer.norms_0', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.codebook', 'self_attn.k_proj.quantizer.norms_1', 'self_attn.k_proj.quantizer.norms_0', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.codebook', 'self_attn.v_proj.quantizer.norms_1', 'self_attn.v_proj.quantizer.norms_0', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.codebook', 'self_attn.o_proj.quantizer.norms_1', 'self_attn.o_proj.quantizer.norms_0', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.codebook', 'mlp.gate_proj.quantizer.norms_1', 'mlp.gate_proj.quantizer.norms_0', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.codebook', 'mlp.up_proj.quantizer.norms_1', 'mlp.up_proj.quantizer.norms_0', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.codebook', 'mlp.down_proj.quantizer.norms_1', 'mlp.down_proj.quantizer.norms_0', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135936
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.006095555778301787
10716 MiB free out of 48676 MiB total
epoch 1 loss: 0.005881286146177445
10716 MiB free out of 48676 MiB total
epoch 2 loss: 0.005796693167212652
10716 MiB free out of 48676 MiB total
epoch 3 loss: 0.0057459810268483125
10716 MiB free out of 48676 MiB total
epoch 4 loss: 0.005710376000934048
10716 MiB free out of 48676 MiB total
36650 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
10716 MiB free out of 48676 MiB total
after cast to cpu
39370 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
15 self_attn.q_proj
Pruning ...
256
iter 0, train loss 303.71856689453125, val loss None
iter 10, train loss 317.73712158203125, val loss None
iter 20, train loss 307.4290466308594, val loss None
iter 30, train loss 304.9294128417969, val loss None
iter 40, train loss 301.7001037597656, val loss None
iter 50, train loss 301.1991271972656, val loss None
iter 60, train loss 300.572509765625, val loss None
iter 70, train loss 299.93365478515625, val loss None
iter 80, train loss 299.00555419921875, val loss None
iter 90, train loss 298.5979919433594, val loss None
best loss 282.8351135253906
not here
quantized in 34.9115207195282 seconds
37478 MiB free out of 48676 MiB total
15 self_attn.k_proj
Pruning ...
256
iter 0, train loss 341.01751708984375, val loss None
iter 10, train loss 356.5840148925781, val loss None
iter 20, train loss 360.42413330078125, val loss None
iter 30, train loss 353.7808532714844, val loss None
iter 40, train loss 350.43798828125, val loss None
iter 50, train loss 348.7835693359375, val loss None
iter 60, train loss 346.7295837402344, val loss None
iter 70, train loss 345.32861328125, val loss None
iter 80, train loss 344.41943359375, val loss None
iter 90, train loss 343.91314697265625, val loss None
best loss 311.34423828125
not here
quantized in 33.548645973205566 seconds
37468 MiB free out of 48676 MiB total
15 self_attn.v_proj
Pruning ...
256
iter 0, train loss 112.98188018798828, val loss None
iter 10, train loss 112.7452392578125, val loss None
iter 20, train loss 112.45187377929688, val loss None
iter 30, train loss 111.8803482055664, val loss None
iter 40, train loss 111.97164154052734, val loss None
iter 50, train loss 111.75041961669922, val loss None
iter 60, train loss 111.73283386230469, val loss None
iter 70, train loss 111.63336944580078, val loss None
iter 80, train loss 111.67024230957031, val loss None
iter 90, train loss 111.523193359375, val loss None
best loss 111.49819946289062
not here
quantized in 32.20407032966614 seconds
37458 MiB free out of 48676 MiB total
15 self_attn.o_proj
Pruning ...
256
iter 0, train loss 11.98855972290039, val loss None
iter 10, train loss 11.930827140808105, val loss None
iter 20, train loss 11.830720901489258, val loss None
iter 30, train loss 11.817728042602539, val loss None
iter 40, train loss 11.753729820251465, val loss None
iter 50, train loss 11.730823516845703, val loss None
iter 60, train loss 11.704587936401367, val loss None
iter 70, train loss 11.696004867553711, val loss None
iter 80, train loss 11.693090438842773, val loss None
iter 90, train loss 11.692770004272461, val loss None
best loss 11.665412902832031
not here
quantized in 31.689257621765137 seconds
37426 MiB free out of 48676 MiB total
15 mlp.gate_proj
Pruning ...
256
iter 0, train loss 237.36859130859375, val loss None
iter 10, train loss 250.91265869140625, val loss None
iter 20, train loss 246.11410522460938, val loss None
iter 30, train loss 246.20726013183594, val loss None
iter 40, train loss 245.28277587890625, val loss None
iter 50, train loss 244.54010009765625, val loss None
iter 60, train loss 244.33053588867188, val loss None
iter 70, train loss 244.04359436035156, val loss None
iter 80, train loss 243.9190673828125, val loss None
iter 90, train loss 243.79266357421875, val loss None
best loss 232.6967315673828
not here
quantized in 87.88358330726624 seconds
37038 MiB free out of 48676 MiB total
15 mlp.up_proj
Pruning ...
256
iter 0, train loss 216.43142700195312, val loss None
iter 10, train loss 218.72120666503906, val loss None
iter 20, train loss 218.86798095703125, val loss None
iter 30, train loss 218.28907775878906, val loss None
iter 40, train loss 218.39480590820312, val loss None
iter 50, train loss 218.27542114257812, val loss None
iter 60, train loss 218.2191162109375, val loss None
iter 70, train loss 218.142822265625, val loss None
iter 80, train loss 218.11216735839844, val loss None
iter 90, train loss 218.08929443359375, val loss None
best loss 216.09295654296875
not here
quantized in 85.9346215724945 seconds
36758 MiB free out of 48676 MiB total
15 mlp.down_proj
Pruning ...
256
iter 0, train loss 6.914093017578125, val loss None
iter 10, train loss 6.917698383331299, val loss None
iter 20, train loss 6.882457733154297, val loss None
iter 30, train loss 6.855845928192139, val loss None
iter 40, train loss 6.843901634216309, val loss None
iter 50, train loss 6.828784465789795, val loss None
iter 60, train loss 6.823459625244141, val loss None
iter 70, train loss 6.81494140625, val loss None
iter 80, train loss 6.811737537384033, val loss None
iter 90, train loss 6.806379318237305, val loss None
best loss 6.801538467407227
not here
quantized in 89.64306020736694 seconds
36478 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.codebook', 'self_attn.q_proj.quantizer.norms_1', 'self_attn.q_proj.quantizer.norms_0', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.codebook', 'self_attn.k_proj.quantizer.norms_1', 'self_attn.k_proj.quantizer.norms_0', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.codebook', 'self_attn.v_proj.quantizer.norms_1', 'self_attn.v_proj.quantizer.norms_0', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.codebook', 'self_attn.o_proj.quantizer.norms_1', 'self_attn.o_proj.quantizer.norms_0', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.codebook', 'mlp.gate_proj.quantizer.norms_1', 'mlp.gate_proj.quantizer.norms_0', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.codebook', 'mlp.up_proj.quantizer.norms_1', 'mlp.up_proj.quantizer.norms_0', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.codebook', 'mlp.down_proj.quantizer.norms_1', 'mlp.down_proj.quantizer.norms_0', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135936
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.007315164577448741
9692 MiB free out of 48676 MiB total
epoch 1 loss: 0.006919117709912825
9692 MiB free out of 48676 MiB total
epoch 2 loss: 0.0068049483161303215
9692 MiB free out of 48676 MiB total
epoch 3 loss: 0.006742600347934058
9692 MiB free out of 48676 MiB total
epoch 4 loss: 0.006698982211673865
9692 MiB free out of 48676 MiB total
36478 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
9692 MiB free out of 48676 MiB total
after cast to cpu
39402 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
16 self_attn.q_proj
Pruning ...
256
iter 0, train loss 317.05609130859375, val loss None
iter 10, train loss 332.52972412109375, val loss None
iter 20, train loss 325.0391845703125, val loss None
iter 30, train loss 320.2430419921875, val loss None
iter 40, train loss 316.253662109375, val loss None
iter 50, train loss 314.95001220703125, val loss None
iter 60, train loss 315.1021728515625, val loss None
iter 70, train loss 313.8849182128906, val loss None
iter 80, train loss 313.28350830078125, val loss None
iter 90, train loss 313.36468505859375, val loss None
best loss 295.1551513671875
not here
quantized in 34.67782735824585 seconds
37446 MiB free out of 48676 MiB total
16 self_attn.k_proj
Pruning ...
256
iter 0, train loss 355.81719970703125, val loss None
iter 10, train loss 370.35504150390625, val loss None
iter 20, train loss 370.1313171386719, val loss None
iter 30, train loss 365.0400085449219, val loss None
iter 40, train loss 361.1106872558594, val loss None
iter 50, train loss 358.46954345703125, val loss None
iter 60, train loss 357.3914794921875, val loss None
iter 70, train loss 356.54058837890625, val loss None
iter 80, train loss 355.9635925292969, val loss None
iter 90, train loss 355.5744934082031, val loss None
best loss 323.3684387207031
not here
quantized in 33.443663358688354 seconds
37404 MiB free out of 48676 MiB total
16 self_attn.v_proj
Pruning ...
256
iter 0, train loss 129.2456817626953, val loss None
iter 10, train loss 128.6877899169922, val loss None
iter 20, train loss 128.34225463867188, val loss None
iter 30, train loss 127.57683563232422, val loss None
iter 40, train loss 127.70600128173828, val loss None
iter 50, train loss 127.17291259765625, val loss None
iter 60, train loss 127.09246826171875, val loss None
iter 70, train loss 127.1270751953125, val loss None
iter 80, train loss 126.9256820678711, val loss None
iter 90, train loss 127.02841186523438, val loss None
best loss 126.9256820678711
not here
quantized in 31.972115755081177 seconds
37426 MiB free out of 48676 MiB total
16 self_attn.o_proj
Pruning ...
256
iter 0, train loss 14.832110404968262, val loss None
iter 10, train loss 14.690179824829102, val loss None
iter 20, train loss 14.633346557617188, val loss None
iter 30, train loss 14.573599815368652, val loss None
iter 40, train loss 14.537343978881836, val loss None
iter 50, train loss 14.531675338745117, val loss None
iter 60, train loss 14.478285789489746, val loss None
iter 70, train loss 14.424614906311035, val loss None
iter 80, train loss 14.431100845336914, val loss None
iter 90, train loss 14.40566635131836, val loss None
best loss 14.379764556884766
not here
quantized in 31.753092765808105 seconds
37362 MiB free out of 48676 MiB total
16 mlp.gate_proj
Pruning ...
256
iter 0, train loss 278.5450439453125, val loss None
iter 10, train loss 296.4377746582031, val loss None
iter 20, train loss 287.39520263671875, val loss None
iter 30, train loss 289.0830383300781, val loss None
iter 40, train loss 289.0032653808594, val loss None
iter 50, train loss 288.2879638671875, val loss None
iter 60, train loss 287.74267578125, val loss None
iter 70, train loss 287.6048889160156, val loss None
iter 80, train loss 287.6739196777344, val loss None
iter 90, train loss 287.68408203125, val loss None
best loss 268.51983642578125
not here
quantized in 87.56286787986755 seconds
36974 MiB free out of 48676 MiB total
16 mlp.up_proj
Pruning ...
256
iter 0, train loss 247.5058135986328, val loss None
iter 10, train loss 249.78067016601562, val loss None
iter 20, train loss 249.62887573242188, val loss None
iter 30, train loss 249.30250549316406, val loss None
iter 40, train loss 249.38265991210938, val loss None
iter 50, train loss 249.22181701660156, val loss None
iter 60, train loss 249.3917999267578, val loss None
iter 70, train loss 249.3098907470703, val loss None
iter 80, train loss 249.35826110839844, val loss None
iter 90, train loss 249.28909301757812, val loss None
best loss 246.43954467773438
not here
quantized in 85.68955183029175 seconds
36694 MiB free out of 48676 MiB total
16 mlp.down_proj
Pruning ...
256
iter 0, train loss 9.113537788391113, val loss None
iter 10, train loss 9.118767738342285, val loss None
iter 20, train loss 9.1019287109375, val loss None
iter 30, train loss 9.081016540527344, val loss None
iter 40, train loss 9.081330299377441, val loss None
iter 50, train loss 9.076337814331055, val loss None
iter 60, train loss 9.07249927520752, val loss None
iter 70, train loss 9.071979522705078, val loss None
iter 80, train loss 9.069320678710938, val loss None
iter 90, train loss 9.063146591186523, val loss None
best loss 9.058527946472168
not here
quantized in 89.06846523284912 seconds
36586 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.codebook', 'self_attn.q_proj.quantizer.norms_1', 'self_attn.q_proj.quantizer.norms_0', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.codebook', 'self_attn.k_proj.quantizer.norms_1', 'self_attn.k_proj.quantizer.norms_0', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.codebook', 'self_attn.v_proj.quantizer.norms_1', 'self_attn.v_proj.quantizer.norms_0', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.codebook', 'self_attn.o_proj.quantizer.norms_1', 'self_attn.o_proj.quantizer.norms_0', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.codebook', 'mlp.gate_proj.quantizer.norms_1', 'mlp.gate_proj.quantizer.norms_0', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.codebook', 'mlp.up_proj.quantizer.norms_1', 'mlp.up_proj.quantizer.norms_0', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.codebook', 'mlp.down_proj.quantizer.norms_1', 'mlp.down_proj.quantizer.norms_0', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135936
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.009560067839629482
9714 MiB free out of 48676 MiB total
epoch 1 loss: 0.009162434020254295
9714 MiB free out of 48676 MiB total
epoch 2 loss: 0.009032703856064472
9714 MiB free out of 48676 MiB total
epoch 3 loss: 0.008952481381129473
9714 MiB free out of 48676 MiB total
epoch 4 loss: 0.008892728430510033
9714 MiB free out of 48676 MiB total
36586 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
9714 MiB free out of 48676 MiB total
after cast to cpu
39370 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
17 self_attn.q_proj
Pruning ...
256
iter 0, train loss 333.8738098144531, val loss None
iter 10, train loss 354.1805114746094, val loss None
iter 20, train loss 344.9725341796875, val loss None
iter 30, train loss 339.18902587890625, val loss None
iter 40, train loss 335.6925964355469, val loss None
iter 50, train loss 333.40167236328125, val loss None
iter 60, train loss 333.0827941894531, val loss None
iter 70, train loss 333.2130432128906, val loss None
iter 80, train loss 333.1248779296875, val loss None
iter 90, train loss 332.8024597167969, val loss None
best loss 309.60137939453125
not here
quantized in 35.03698015213013 seconds
37478 MiB free out of 48676 MiB total
17 self_attn.k_proj
Pruning ...
256
iter 0, train loss 374.0815734863281, val loss None
iter 10, train loss 388.3494873046875, val loss None
iter 20, train loss 384.3209228515625, val loss None
iter 30, train loss 377.3783874511719, val loss None
iter 40, train loss 375.5392761230469, val loss None
iter 50, train loss 372.8914794921875, val loss None
iter 60, train loss 371.07586669921875, val loss None
iter 70, train loss 369.9704284667969, val loss None
iter 80, train loss 369.58447265625, val loss None
iter 90, train loss 369.31494140625, val loss None
best loss 336.79217529296875
not here
quantized in 34.07742476463318 seconds
37468 MiB free out of 48676 MiB total
17 self_attn.v_proj
Pruning ...
256
iter 0, train loss 137.5789794921875, val loss None
iter 10, train loss 137.7974853515625, val loss None
iter 20, train loss 137.52035522460938, val loss None
iter 30, train loss 137.1397705078125, val loss None
iter 40, train loss 136.9579620361328, val loss None
iter 50, train loss 137.10755920410156, val loss None
iter 60, train loss 136.67459106445312, val loss None
iter 70, train loss 136.71041870117188, val loss None
iter 80, train loss 136.80032348632812, val loss None
iter 90, train loss 136.71798706054688, val loss None
best loss 136.6520538330078
not here
quantized in 32.0567352771759 seconds
37490 MiB free out of 48676 MiB total
17 self_attn.o_proj
Pruning ...
256
iter 0, train loss 11.249662399291992, val loss None
iter 10, train loss 11.141637802124023, val loss None
iter 20, train loss 11.069770812988281, val loss None
iter 30, train loss 11.015497207641602, val loss None
iter 40, train loss 10.887320518493652, val loss None
iter 50, train loss 10.913074493408203, val loss None
iter 60, train loss 10.88987922668457, val loss None
iter 70, train loss 10.844249725341797, val loss None
iter 80, train loss 10.821798324584961, val loss None
iter 90, train loss 10.83535385131836, val loss None
best loss 10.810819625854492
not here
quantized in 31.968641757965088 seconds
37426 MiB free out of 48676 MiB total
17 mlp.gate_proj
Pruning ...
256
iter 0, train loss 316.890380859375, val loss None
iter 10, train loss 335.4635314941406, val loss None
iter 20, train loss 327.5098876953125, val loss None
iter 30, train loss 330.0987548828125, val loss None
iter 40, train loss 330.6961669921875, val loss None
iter 50, train loss 330.4111022949219, val loss None
iter 60, train loss 330.1442565917969, val loss None
iter 70, train loss 329.57861328125, val loss None
iter 80, train loss 329.3978271484375, val loss None
iter 90, train loss 329.41925048828125, val loss None
best loss 306.49700927734375
not here
quantized in 87.29292511940002 seconds
37038 MiB free out of 48676 MiB total
17 mlp.up_proj
Pruning ...
256
iter 0, train loss 273.463134765625, val loss None
iter 10, train loss 275.1739501953125, val loss None
iter 20, train loss 275.7174072265625, val loss None
iter 30, train loss 275.34674072265625, val loss None
iter 40, train loss 275.55450439453125, val loss None
iter 50, train loss 275.7878723144531, val loss None
iter 60, train loss 275.7520446777344, val loss None
iter 70, train loss 275.4971923828125, val loss None
iter 80, train loss 275.4845886230469, val loss None
iter 90, train loss 275.55059814453125, val loss None
best loss 272.7243347167969
not here
quantized in 85.85173439979553 seconds
36758 MiB free out of 48676 MiB total
17 mlp.down_proj
Pruning ...
256
iter 0, train loss 9.86650562286377, val loss None
iter 10, train loss 9.856768608093262, val loss None
iter 20, train loss 9.848145484924316, val loss None
iter 30, train loss 9.827543258666992, val loss None
iter 40, train loss 9.820396423339844, val loss None
iter 50, train loss 9.804697036743164, val loss None
iter 60, train loss 9.79686164855957, val loss None
iter 70, train loss 9.798927307128906, val loss None
iter 80, train loss 9.794235229492188, val loss None
iter 90, train loss 9.793784141540527, val loss None
best loss 9.789844512939453
not here
quantized in 89.32830858230591 seconds
36478 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.codebook', 'self_attn.q_proj.quantizer.norms_1', 'self_attn.q_proj.quantizer.norms_0', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.codebook', 'self_attn.k_proj.quantizer.norms_1', 'self_attn.k_proj.quantizer.norms_0', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.codebook', 'self_attn.v_proj.quantizer.norms_1', 'self_attn.v_proj.quantizer.norms_0', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.codebook', 'self_attn.o_proj.quantizer.norms_1', 'self_attn.o_proj.quantizer.norms_0', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.codebook', 'mlp.gate_proj.quantizer.norms_1', 'mlp.gate_proj.quantizer.norms_0', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.codebook', 'mlp.up_proj.quantizer.norms_1', 'mlp.up_proj.quantizer.norms_0', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.codebook', 'mlp.down_proj.quantizer.norms_1', 'mlp.down_proj.quantizer.norms_0', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135936
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.009752890946401749
9692 MiB free out of 48676 MiB total
epoch 1 loss: 0.009198909458064009
9692 MiB free out of 48676 MiB total
epoch 2 loss: 0.009050272121385206
9692 MiB free out of 48676 MiB total
epoch 3 loss: 0.008972851806902327
9692 MiB free out of 48676 MiB total
epoch 4 loss: 0.008918598694435786
9692 MiB free out of 48676 MiB total
36478 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
9692 MiB free out of 48676 MiB total
after cast to cpu
39402 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
18 self_attn.q_proj
Pruning ...
256
iter 0, train loss 363.0413818359375, val loss None
iter 10, train loss 382.6893310546875, val loss None
iter 20, train loss 369.43487548828125, val loss None
iter 30, train loss 361.449951171875, val loss None
iter 40, train loss 357.59002685546875, val loss None
iter 50, train loss 356.57415771484375, val loss None
iter 60, train loss 355.66461181640625, val loss None
iter 70, train loss 355.51593017578125, val loss None
iter 80, train loss 355.260986328125, val loss None
iter 90, train loss 355.018798828125, val loss None
best loss 334.1535339355469
not here
quantized in 34.910240650177 seconds
37446 MiB free out of 48676 MiB total
18 self_attn.k_proj
Pruning ...
256
iter 0, train loss 392.6371154785156, val loss None
iter 10, train loss 411.24908447265625, val loss None
iter 20, train loss 402.511474609375, val loss None
iter 30, train loss 394.77392578125, val loss None
iter 40, train loss 389.89483642578125, val loss None
iter 50, train loss 387.0556945800781, val loss None
iter 60, train loss 386.03594970703125, val loss None
iter 70, train loss 385.59173583984375, val loss None
iter 80, train loss 384.703369140625, val loss None
iter 90, train loss 384.4671325683594, val loss None
best loss 355.5635681152344
not here
quantized in 33.77445697784424 seconds
37404 MiB free out of 48676 MiB total
18 self_attn.v_proj
Pruning ...
256
iter 0, train loss 167.2626953125, val loss None
iter 10, train loss 167.04957580566406, val loss None
iter 20, train loss 166.46014404296875, val loss None
iter 30, train loss 165.97645568847656, val loss None
iter 40, train loss 166.00022888183594, val loss None
iter 50, train loss 166.00111389160156, val loss None
iter 60, train loss 165.68890380859375, val loss None
iter 70, train loss 165.44300842285156, val loss None
iter 80, train loss 165.45932006835938, val loss None
iter 90, train loss 165.5188446044922, val loss None
best loss 165.44300842285156
not here
quantized in 31.904183864593506 seconds
37426 MiB free out of 48676 MiB total
18 self_attn.o_proj
Pruning ...
256
iter 0, train loss 10.357111930847168, val loss None
iter 10, train loss 10.379417419433594, val loss None
iter 20, train loss 10.368460655212402, val loss None
iter 30, train loss 10.324482917785645, val loss None
iter 40, train loss 10.335023880004883, val loss None
iter 50, train loss 10.306232452392578, val loss None
iter 60, train loss 10.281840324401855, val loss None
iter 70, train loss 10.284677505493164, val loss None
iter 80, train loss 10.284908294677734, val loss None
iter 90, train loss 10.278705596923828, val loss None
best loss 10.271477699279785
not here
quantized in 31.975881338119507 seconds
37362 MiB free out of 48676 MiB total
18 mlp.gate_proj
Pruning ...
256
iter 0, train loss 359.13909912109375, val loss None
iter 10, train loss 378.9129943847656, val loss None
iter 20, train loss 369.8213806152344, val loss None
iter 30, train loss 372.52001953125, val loss None
iter 40, train loss 372.9381103515625, val loss None
iter 50, train loss 372.3421936035156, val loss None
iter 60, train loss 371.8550720214844, val loss None
iter 70, train loss 371.68023681640625, val loss None
iter 80, train loss 371.7613220214844, val loss None
iter 90, train loss 371.64910888671875, val loss None
best loss 349.423828125
not here
quantized in 87.58330249786377 seconds
36974 MiB free out of 48676 MiB total
18 mlp.up_proj
Pruning ...
256
iter 0, train loss 302.70843505859375, val loss None
iter 10, train loss 303.90948486328125, val loss None
iter 20, train loss 304.3948669433594, val loss None
iter 30, train loss 303.8977966308594, val loss None
iter 40, train loss 304.32073974609375, val loss None
iter 50, train loss 304.4388122558594, val loss None
iter 60, train loss 304.4154968261719, val loss None
iter 70, train loss 304.5032958984375, val loss None
iter 80, train loss 304.633056640625, val loss None
iter 90, train loss 304.5254821777344, val loss None
best loss 302.70843505859375
not here
quantized in 84.66888475418091 seconds
36694 MiB free out of 48676 MiB total
18 mlp.down_proj
Pruning ...
256
iter 0, train loss 11.64461612701416, val loss None
iter 10, train loss 11.650102615356445, val loss None
iter 20, train loss 11.598373413085938, val loss None
iter 30, train loss 11.578086853027344, val loss None
iter 40, train loss 11.546209335327148, val loss None
iter 50, train loss 11.536901473999023, val loss None
iter 60, train loss 11.518552780151367, val loss None
iter 70, train loss 11.511821746826172, val loss None
iter 80, train loss 11.511028289794922, val loss None
iter 90, train loss 11.4982271194458, val loss None
best loss 11.486056327819824
not here
quantized in 89.16969847679138 seconds
36414 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.codebook', 'self_attn.q_proj.quantizer.norms_1', 'self_attn.q_proj.quantizer.norms_0', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.codebook', 'self_attn.k_proj.quantizer.norms_1', 'self_attn.k_proj.quantizer.norms_0', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.codebook', 'self_attn.v_proj.quantizer.norms_1', 'self_attn.v_proj.quantizer.norms_0', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.codebook', 'self_attn.o_proj.quantizer.norms_1', 'self_attn.o_proj.quantizer.norms_0', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.codebook', 'mlp.gate_proj.quantizer.norms_1', 'mlp.gate_proj.quantizer.norms_0', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.codebook', 'mlp.up_proj.quantizer.norms_1', 'mlp.up_proj.quantizer.norms_0', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.codebook', 'mlp.down_proj.quantizer.norms_1', 'mlp.down_proj.quantizer.norms_0', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135936
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.011039511984563433
9692 MiB free out of 48676 MiB total
epoch 1 loss: 0.010472762573044747
9692 MiB free out of 48676 MiB total
epoch 2 loss: 0.010302205526386388
9692 MiB free out of 48676 MiB total
epoch 3 loss: 0.010206536040641367
9692 MiB free out of 48676 MiB total
epoch 4 loss: 0.010139237485418562
9692 MiB free out of 48676 MiB total
36414 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
9692 MiB free out of 48676 MiB total
after cast to cpu
39370 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
19 self_attn.q_proj
Pruning ...
256
iter 0, train loss 346.70281982421875, val loss None
iter 10, train loss 366.0985107421875, val loss None
iter 20, train loss 354.6368713378906, val loss None
iter 30, train loss 349.75079345703125, val loss None
iter 40, train loss 346.8895263671875, val loss None
iter 50, train loss 346.85760498046875, val loss None
iter 60, train loss 346.92108154296875, val loss None
iter 70, train loss 346.359375, val loss None
iter 80, train loss 346.2457580566406, val loss None
iter 90, train loss 346.0445251464844, val loss None
best loss 323.0240783691406
not here
quantized in 34.79704666137695 seconds
37478 MiB free out of 48676 MiB total
19 self_attn.k_proj
Pruning ...
256
iter 0, train loss 377.26422119140625, val loss None
iter 10, train loss 397.49737548828125, val loss None
iter 20, train loss 393.6304016113281, val loss None
iter 30, train loss 387.35650634765625, val loss None
iter 40, train loss 381.20111083984375, val loss None
iter 50, train loss 378.7120056152344, val loss None
iter 60, train loss 377.4303894042969, val loss None
iter 70, train loss 376.88507080078125, val loss None
iter 80, train loss 376.00848388671875, val loss None
iter 90, train loss 375.44561767578125, val loss None
best loss 343.7943420410156
not here
quantized in 33.70648717880249 seconds
37468 MiB free out of 48676 MiB total
19 self_attn.v_proj
Pruning ...
256
iter 0, train loss 170.034423828125, val loss None
iter 10, train loss 169.82138061523438, val loss None
iter 20, train loss 169.10861206054688, val loss None
iter 30, train loss 168.9768524169922, val loss None
iter 40, train loss 168.63546752929688, val loss None
iter 50, train loss 168.48458862304688, val loss None
iter 60, train loss 168.3220672607422, val loss None
iter 70, train loss 168.13186645507812, val loss None
iter 80, train loss 168.00119018554688, val loss None
iter 90, train loss 167.93116760253906, val loss None
best loss 167.79559326171875
not here
quantized in 31.856531858444214 seconds
37458 MiB free out of 48676 MiB total
19 self_attn.o_proj
Pruning ...
256
iter 0, train loss 11.04461669921875, val loss None
iter 10, train loss 10.84034538269043, val loss None
iter 20, train loss 10.782837867736816, val loss None
iter 30, train loss 10.712263107299805, val loss None
iter 40, train loss 10.668773651123047, val loss None
iter 50, train loss 10.686943054199219, val loss None
iter 60, train loss 10.682226181030273, val loss None
iter 70, train loss 10.67526912689209, val loss None
iter 80, train loss 10.660538673400879, val loss None
iter 90, train loss 10.664834976196289, val loss None
best loss 10.640548706054688
not here
quantized in 31.952049016952515 seconds
37426 MiB free out of 48676 MiB total
19 mlp.gate_proj
Pruning ...
256
iter 0, train loss 380.8875732421875, val loss None
iter 10, train loss 399.75921630859375, val loss None
iter 20, train loss 394.9671325683594, val loss None
iter 30, train loss 396.6512451171875, val loss None
iter 40, train loss 396.6954345703125, val loss None
iter 50, train loss 396.4126892089844, val loss None
iter 60, train loss 395.9090576171875, val loss None
iter 70, train loss 395.4595642089844, val loss None
iter 80, train loss 395.3172302246094, val loss None
iter 90, train loss 395.3537292480469, val loss None
best loss 374.640380859375
not here
quantized in 87.30156373977661 seconds
37038 MiB free out of 48676 MiB total
19 mlp.up_proj
Pruning ...
256
iter 0, train loss 325.1534729003906, val loss None
iter 10, train loss 325.5762023925781, val loss None
iter 20, train loss 326.5780029296875, val loss None
iter 30, train loss 326.09375, val loss None
iter 40, train loss 326.5692138671875, val loss None
iter 50, train loss 326.476318359375, val loss None
iter 60, train loss 326.63916015625, val loss None
iter 70, train loss 326.4449462890625, val loss None
iter 80, train loss 326.3629150390625, val loss None
iter 90, train loss 326.322021484375, val loss None
best loss 324.8663635253906
not here
quantized in 84.98269939422607 seconds
36758 MiB free out of 48676 MiB total
19 mlp.down_proj
Pruning ...
256
iter 0, train loss 12.640193939208984, val loss None
iter 10, train loss 12.643308639526367, val loss None
iter 20, train loss 12.598121643066406, val loss None
iter 30, train loss 12.580809593200684, val loss None
iter 40, train loss 12.578497886657715, val loss None
iter 50, train loss 12.543210983276367, val loss None
iter 60, train loss 12.533252716064453, val loss None
iter 70, train loss 12.529870986938477, val loss None
iter 80, train loss 12.50727367401123, val loss None
iter 90, train loss 12.50227165222168, val loss None
best loss 12.491677284240723
not here
quantized in 88.97815990447998 seconds
36650 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.codebook', 'self_attn.q_proj.quantizer.norms_1', 'self_attn.q_proj.quantizer.norms_0', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.codebook', 'self_attn.k_proj.quantizer.norms_1', 'self_attn.k_proj.quantizer.norms_0', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.codebook', 'self_attn.v_proj.quantizer.norms_1', 'self_attn.v_proj.quantizer.norms_0', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.codebook', 'self_attn.o_proj.quantizer.norms_1', 'self_attn.o_proj.quantizer.norms_0', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.codebook', 'mlp.gate_proj.quantizer.norms_1', 'mlp.gate_proj.quantizer.norms_0', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.codebook', 'mlp.up_proj.quantizer.norms_1', 'mlp.up_proj.quantizer.norms_0', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.codebook', 'mlp.down_proj.quantizer.norms_1', 'mlp.down_proj.quantizer.norms_0', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135936
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.012145883847551886
10716 MiB free out of 48676 MiB total
epoch 1 loss: 0.011524705922056455
10716 MiB free out of 48676 MiB total
epoch 2 loss: 0.011372922039299738
10716 MiB free out of 48676 MiB total
epoch 3 loss: 0.011285095955827273
10716 MiB free out of 48676 MiB total
epoch 4 loss: 0.011222197084862273
10716 MiB free out of 48676 MiB total
36650 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
10716 MiB free out of 48676 MiB total
after cast to cpu
39402 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
20 self_attn.q_proj
Pruning ...
256
iter 0, train loss 360.7368469238281, val loss None
iter 10, train loss 379.846435546875, val loss None
iter 20, train loss 369.4476623535156, val loss None
iter 30, train loss 361.0030517578125, val loss None
iter 40, train loss 357.72802734375, val loss None
iter 50, train loss 358.0723571777344, val loss None
iter 60, train loss 357.9047546386719, val loss None
iter 70, train loss 358.14495849609375, val loss None
iter 80, train loss 357.7523193359375, val loss None
iter 90, train loss 357.7275695800781, val loss None
best loss 334.34149169921875
not here
quantized in 34.73379445075989 seconds
37446 MiB free out of 48676 MiB total
20 self_attn.k_proj
Pruning ...
256
iter 0, train loss 389.64019775390625, val loss None
iter 10, train loss 407.39251708984375, val loss None
iter 20, train loss 402.70831298828125, val loss None
iter 30, train loss 395.40411376953125, val loss None
iter 40, train loss 389.304931640625, val loss None
iter 50, train loss 386.92041015625, val loss None
iter 60, train loss 384.84283447265625, val loss None
iter 70, train loss 384.50433349609375, val loss None
iter 80, train loss 383.79095458984375, val loss None
iter 90, train loss 383.3148498535156, val loss None
best loss 357.1254577636719
not here
quantized in 33.40168023109436 seconds
37404 MiB free out of 48676 MiB total
20 self_attn.v_proj
Pruning ...
256
iter 0, train loss 175.65585327148438, val loss None
iter 10, train loss 175.2905731201172, val loss None
iter 20, train loss 174.5741424560547, val loss None
iter 30, train loss 174.58291625976562, val loss None
iter 40, train loss 174.22967529296875, val loss None
iter 50, train loss 173.98976135253906, val loss None
iter 60, train loss 173.78602600097656, val loss None
iter 70, train loss 173.64373779296875, val loss None
iter 80, train loss 173.58908081054688, val loss None
iter 90, train loss 173.64566040039062, val loss None
best loss 173.5246124267578
not here
quantized in 31.940316677093506 seconds
37426 MiB free out of 48676 MiB total
20 self_attn.o_proj
Pruning ...
256
iter 0, train loss 16.60747718811035, val loss None
iter 10, train loss 14.98115348815918, val loss None
iter 20, train loss 13.84693717956543, val loss None
iter 30, train loss 13.262344360351562, val loss None
iter 40, train loss 12.98474407196045, val loss None
iter 50, train loss 12.787849426269531, val loss None
iter 60, train loss 12.736248970031738, val loss None
iter 70, train loss 12.689727783203125, val loss None
iter 80, train loss 12.635825157165527, val loss None
iter 90, train loss 12.578475952148438, val loss None
best loss 12.55908203125
not here
quantized in 32.66476607322693 seconds
37362 MiB free out of 48676 MiB total
20 mlp.gate_proj
Pruning ...
256
iter 0, train loss 412.3037109375, val loss None
iter 10, train loss 433.04150390625, val loss None
iter 20, train loss 426.58380126953125, val loss None
iter 30, train loss 429.5607604980469, val loss None
iter 40, train loss 429.198486328125, val loss None
iter 50, train loss 428.45391845703125, val loss None
iter 60, train loss 428.3184814453125, val loss None
iter 70, train loss 428.11761474609375, val loss None
iter 80, train loss 427.6238708496094, val loss None
iter 90, train loss 427.53302001953125, val loss None
best loss 403.25042724609375
not here
quantized in 87.3452742099762 seconds
36974 MiB free out of 48676 MiB total
20 mlp.up_proj
Pruning ...
256
iter 0, train loss 346.0402526855469, val loss None
iter 10, train loss 347.84808349609375, val loss None
iter 20, train loss 348.9157409667969, val loss None
iter 30, train loss 348.47576904296875, val loss None
iter 40, train loss 348.9922180175781, val loss None
iter 50, train loss 349.0286560058594, val loss None
iter 60, train loss 349.06005859375, val loss None
iter 70, train loss 349.23016357421875, val loss None
iter 80, train loss 349.09210205078125, val loss None
iter 90, train loss 349.0509948730469, val loss None
best loss 345.7688293457031
not here
quantized in 85.46623229980469 seconds
36694 MiB free out of 48676 MiB total
20 mlp.down_proj
Pruning ...
256
iter 0, train loss 15.499125480651855, val loss None
iter 10, train loss 15.463396072387695, val loss None
iter 20, train loss 15.412969589233398, val loss None
iter 30, train loss 15.375816345214844, val loss None
iter 40, train loss 15.363773345947266, val loss None
iter 50, train loss 15.35861587524414, val loss None
iter 60, train loss 15.337574005126953, val loss None
iter 70, train loss 15.33475399017334, val loss None
iter 80, train loss 15.329941749572754, val loss None
iter 90, train loss 15.321996688842773, val loss None
best loss 15.316621780395508
not here
quantized in 89.34317278862 seconds
36414 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.codebook', 'self_attn.q_proj.quantizer.norms_1', 'self_attn.q_proj.quantizer.norms_0', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.codebook', 'self_attn.k_proj.quantizer.norms_1', 'self_attn.k_proj.quantizer.norms_0', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.codebook', 'self_attn.v_proj.quantizer.norms_1', 'self_attn.v_proj.quantizer.norms_0', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.codebook', 'self_attn.o_proj.quantizer.norms_1', 'self_attn.o_proj.quantizer.norms_0', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.codebook', 'mlp.gate_proj.quantizer.norms_1', 'mlp.gate_proj.quantizer.norms_0', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.codebook', 'mlp.up_proj.quantizer.norms_1', 'mlp.up_proj.quantizer.norms_0', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.codebook', 'mlp.down_proj.quantizer.norms_1', 'mlp.down_proj.quantizer.norms_0', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135936
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.014868730519083329
9692 MiB free out of 48676 MiB total
epoch 1 loss: 0.014087574352743104
9692 MiB free out of 48676 MiB total
epoch 2 loss: 0.0138239195002825
9692 MiB free out of 48676 MiB total
epoch 3 loss: 0.01368583041767124
9692 MiB free out of 48676 MiB total
epoch 4 loss: 0.01359272125409916
9692 MiB free out of 48676 MiB total
36414 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
9692 MiB free out of 48676 MiB total
after cast to cpu
39370 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
21 self_attn.q_proj
Pruning ...
256
iter 0, train loss 373.67279052734375, val loss None
iter 10, train loss 392.0364990234375, val loss None
iter 20, train loss 383.2566833496094, val loss None
iter 30, train loss 377.3358154296875, val loss None
iter 40, train loss 374.3986511230469, val loss None
iter 50, train loss 372.1626892089844, val loss None
iter 60, train loss 371.1812744140625, val loss None
iter 70, train loss 370.8105163574219, val loss None
iter 80, train loss 369.91357421875, val loss None
iter 90, train loss 370.08868408203125, val loss None
best loss 354.10162353515625
not here
quantized in 34.875001192092896 seconds
37478 MiB free out of 48676 MiB total
21 self_attn.k_proj
Pruning ...
256
iter 0, train loss 394.6352233886719, val loss None
iter 10, train loss 410.7370300292969, val loss None
iter 20, train loss 401.89337158203125, val loss None
iter 30, train loss 398.22467041015625, val loss None
iter 40, train loss 395.3966979980469, val loss None
iter 50, train loss 394.0007019042969, val loss None
iter 60, train loss 392.5425109863281, val loss None
iter 70, train loss 391.8210144042969, val loss None
iter 80, train loss 390.938720703125, val loss None
iter 90, train loss 390.3379211425781, val loss None
best loss 371.0352783203125
not here
quantized in 33.26720070838928 seconds
37468 MiB free out of 48676 MiB total
21 self_attn.v_proj
Pruning ...
256
iter 0, train loss 208.42605590820312, val loss None
iter 10, train loss 207.7239990234375, val loss None
iter 20, train loss 206.68690490722656, val loss None
iter 30, train loss 206.8599090576172, val loss None
iter 40, train loss 206.7584686279297, val loss None
iter 50, train loss 206.60606384277344, val loss None
iter 60, train loss 206.1712646484375, val loss None
iter 70, train loss 206.19679260253906, val loss None
iter 80, train loss 206.086669921875, val loss None
iter 90, train loss 206.23431396484375, val loss None
best loss 206.03997802734375
not here
quantized in 31.766080141067505 seconds
37490 MiB free out of 48676 MiB total
21 self_attn.o_proj
Pruning ...
256
iter 0, train loss 11.992280960083008, val loss None
iter 10, train loss 11.047676086425781, val loss None
iter 20, train loss 10.885370254516602, val loss None
iter 30, train loss 10.631912231445312, val loss None
iter 40, train loss 10.488632202148438, val loss None
iter 50, train loss 10.432517051696777, val loss None
iter 60, train loss 10.33989143371582, val loss None
iter 70, train loss 10.32158374786377, val loss None
iter 80, train loss 10.318897247314453, val loss None
iter 90, train loss 10.314689636230469, val loss None
best loss 10.292776107788086
not here
quantized in 32.37622356414795 seconds
37458 MiB free out of 48676 MiB total
21 mlp.gate_proj
Pruning ...
256
iter 0, train loss 439.18701171875, val loss None
iter 10, train loss 459.72314453125, val loss None
iter 20, train loss 454.87939453125, val loss None
iter 30, train loss 456.13861083984375, val loss None
iter 40, train loss 455.836669921875, val loss None
iter 50, train loss 455.99163818359375, val loss None
iter 60, train loss 455.58673095703125, val loss None
iter 70, train loss 455.1981506347656, val loss None
iter 80, train loss 455.3726806640625, val loss None
iter 90, train loss 455.28582763671875, val loss None
best loss 433.0014953613281
not here
quantized in 86.64376902580261 seconds
37070 MiB free out of 48676 MiB total
21 mlp.up_proj
Pruning ...
256
iter 0, train loss 366.0240783691406, val loss None
iter 10, train loss 367.1929931640625, val loss None
iter 20, train loss 368.2927551269531, val loss None
iter 30, train loss 368.1744689941406, val loss None
iter 40, train loss 368.4219970703125, val loss None
iter 50, train loss 368.7254638671875, val loss None
iter 60, train loss 369.1005859375, val loss None
iter 70, train loss 368.9873046875, val loss None
iter 80, train loss 368.7757873535156, val loss None
iter 90, train loss 368.71209716796875, val loss None
best loss 365.8800964355469
not here
quantized in 85.76560473442078 seconds
36790 MiB free out of 48676 MiB total
21 mlp.down_proj
Pruning ...
256
iter 0, train loss 16.196264266967773, val loss None
iter 10, train loss 16.188678741455078, val loss None
iter 20, train loss 16.148983001708984, val loss None
iter 30, train loss 16.10641098022461, val loss None
iter 40, train loss 16.075515747070312, val loss None
iter 50, train loss 16.05472183227539, val loss None
iter 60, train loss 16.026639938354492, val loss None
iter 70, train loss 16.012502670288086, val loss None
iter 80, train loss 16.000347137451172, val loss None
iter 90, train loss 15.978391647338867, val loss None
best loss 15.974655151367188
not here
quantized in 89.0665168762207 seconds
36510 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.codebook', 'self_attn.q_proj.quantizer.norms_1', 'self_attn.q_proj.quantizer.norms_0', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.codebook', 'self_attn.k_proj.quantizer.norms_1', 'self_attn.k_proj.quantizer.norms_0', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.codebook', 'self_attn.v_proj.quantizer.norms_1', 'self_attn.v_proj.quantizer.norms_0', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.codebook', 'self_attn.o_proj.quantizer.norms_1', 'self_attn.o_proj.quantizer.norms_0', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.codebook', 'mlp.gate_proj.quantizer.norms_1', 'mlp.gate_proj.quantizer.norms_0', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.codebook', 'mlp.up_proj.quantizer.norms_1', 'mlp.up_proj.quantizer.norms_0', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.codebook', 'mlp.down_proj.quantizer.norms_1', 'mlp.down_proj.quantizer.norms_0', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135936
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.014691940777993295
9702 MiB free out of 48676 MiB total
epoch 1 loss: 0.014159715457935818
9702 MiB free out of 48676 MiB total
epoch 2 loss: 0.01399719889741391
9702 MiB free out of 48676 MiB total
epoch 3 loss: 0.013901406491640955
9702 MiB free out of 48676 MiB total
epoch 4 loss: 0.013831832802679855
9702 MiB free out of 48676 MiB total
36510 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
9702 MiB free out of 48676 MiB total
after cast to cpu
39402 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
22 self_attn.q_proj
Pruning ...
256
iter 0, train loss 390.90521240234375, val loss None
iter 10, train loss 422.81494140625, val loss None
iter 20, train loss 404.8750915527344, val loss None
iter 30, train loss 399.98712158203125, val loss None
iter 40, train loss 396.37506103515625, val loss None
iter 50, train loss 393.4321594238281, val loss None
iter 60, train loss 391.14361572265625, val loss None
iter 70, train loss 390.76104736328125, val loss None
iter 80, train loss 390.3600158691406, val loss None
iter 90, train loss 390.2918701171875, val loss None
best loss 374.8125305175781
not here
quantized in 34.48544359207153 seconds
37446 MiB free out of 48676 MiB total
22 self_attn.k_proj
Pruning ...
256
iter 0, train loss 427.18572998046875, val loss None
iter 10, train loss 445.5904235839844, val loss None
iter 20, train loss 438.13299560546875, val loss None
iter 30, train loss 429.39984130859375, val loss None
iter 40, train loss 426.8116149902344, val loss None
iter 50, train loss 426.899169921875, val loss None
iter 60, train loss 426.3162841796875, val loss None
iter 70, train loss 424.7862548828125, val loss None
iter 80, train loss 424.0486145019531, val loss None
iter 90, train loss 423.39697265625, val loss None
best loss 399.67108154296875
not here
quantized in 33.403809547424316 seconds
37404 MiB free out of 48676 MiB total
22 self_attn.v_proj
Pruning ...
256
iter 0, train loss 216.0771484375, val loss None
iter 10, train loss 215.52938842773438, val loss None
iter 20, train loss 214.9180908203125, val loss None
iter 30, train loss 214.64439392089844, val loss None
iter 40, train loss 214.46112060546875, val loss None
iter 50, train loss 214.35903930664062, val loss None
iter 60, train loss 213.85989379882812, val loss None
iter 70, train loss 214.04383850097656, val loss None
iter 80, train loss 213.818115234375, val loss None
iter 90, train loss 213.7766876220703, val loss None
best loss 213.68923950195312
not here
quantized in 32.008232831954956 seconds
37426 MiB free out of 48676 MiB total
22 self_attn.o_proj
Pruning ...
256
iter 0, train loss 42.73101043701172, val loss None
iter 10, train loss 31.961681365966797, val loss None
iter 20, train loss 26.474136352539062, val loss None
iter 30, train loss 21.494522094726562, val loss None
iter 40, train loss 18.49555778503418, val loss None
iter 50, train loss 16.929256439208984, val loss None
iter 60, train loss 16.623788833618164, val loss None
iter 70, train loss 16.325580596923828, val loss None
iter 80, train loss 16.202247619628906, val loss None
iter 90, train loss 16.087406158447266, val loss None
best loss 16.058940887451172
not here
quantized in 33.24830937385559 seconds
37362 MiB free out of 48676 MiB total
22 mlp.gate_proj
Pruning ...
256
iter 0, train loss 463.3237609863281, val loss None
iter 10, train loss 482.5816955566406, val loss None
iter 20, train loss 476.4119873046875, val loss None
iter 30, train loss 478.32220458984375, val loss None
iter 40, train loss 478.5367126464844, val loss None
iter 50, train loss 478.2718811035156, val loss None
iter 60, train loss 477.9868469238281, val loss None
iter 70, train loss 477.99700927734375, val loss None
iter 80, train loss 478.0038146972656, val loss None
iter 90, train loss 477.8558654785156, val loss None
best loss 457.71820068359375
not here
quantized in 87.04104447364807 seconds
36974 MiB free out of 48676 MiB total
22 mlp.up_proj
Pruning ...
256
iter 0, train loss 380.739990234375, val loss None
iter 10, train loss 382.5093078613281, val loss None
iter 20, train loss 383.72308349609375, val loss None
iter 30, train loss 383.6590270996094, val loss None
iter 40, train loss 383.583251953125, val loss None
iter 50, train loss 383.6698303222656, val loss None
iter 60, train loss 383.8329162597656, val loss None
iter 70, train loss 383.9635009765625, val loss None
iter 80, train loss 383.98284912109375, val loss None
iter 90, train loss 383.9875793457031, val loss None
best loss 380.739990234375
not here
quantized in 85.35715818405151 seconds
36694 MiB free out of 48676 MiB total
22 mlp.down_proj
Pruning ...
256
iter 0, train loss 17.775854110717773, val loss None
iter 10, train loss 17.80158042907715, val loss None
iter 20, train loss 17.742977142333984, val loss None
iter 30, train loss 17.730802536010742, val loss None
iter 40, train loss 17.7235050201416, val loss None
iter 50, train loss 17.72114372253418, val loss None
iter 60, train loss 17.732036590576172, val loss None
iter 70, train loss 17.714946746826172, val loss None
iter 80, train loss 17.708227157592773, val loss None
iter 90, train loss 17.708908081054688, val loss None
best loss 17.704801559448242
not here
quantized in 89.39993715286255 seconds
36414 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.codebook', 'self_attn.q_proj.quantizer.norms_1', 'self_attn.q_proj.quantizer.norms_0', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.codebook', 'self_attn.k_proj.quantizer.norms_1', 'self_attn.k_proj.quantizer.norms_0', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.codebook', 'self_attn.v_proj.quantizer.norms_1', 'self_attn.v_proj.quantizer.norms_0', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.codebook', 'self_attn.o_proj.quantizer.norms_1', 'self_attn.o_proj.quantizer.norms_0', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.codebook', 'mlp.gate_proj.quantizer.norms_1', 'mlp.gate_proj.quantizer.norms_0', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.codebook', 'mlp.up_proj.quantizer.norms_1', 'mlp.up_proj.quantizer.norms_0', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.codebook', 'mlp.down_proj.quantizer.norms_1', 'mlp.down_proj.quantizer.norms_0', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135936
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.018446051282808185
9692 MiB free out of 48676 MiB total
epoch 1 loss: 0.01705293894337956
9692 MiB free out of 48676 MiB total
epoch 2 loss: 0.016726875022868626
9692 MiB free out of 48676 MiB total
epoch 3 loss: 0.01657382374833105
9692 MiB free out of 48676 MiB total
epoch 4 loss: 0.01647558162221685
9692 MiB free out of 48676 MiB total
36414 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
9692 MiB free out of 48676 MiB total
after cast to cpu
39370 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
23 self_attn.q_proj
Pruning ...
256
iter 0, train loss 420.4758605957031, val loss None
iter 10, train loss 436.3292236328125, val loss None
iter 20, train loss 421.4659423828125, val loss None
iter 30, train loss 423.28985595703125, val loss None
iter 40, train loss 422.4144287109375, val loss None
iter 50, train loss 420.64813232421875, val loss None
iter 60, train loss 419.6852722167969, val loss None
iter 70, train loss 419.2700500488281, val loss None
iter 80, train loss 418.5965576171875, val loss None
iter 90, train loss 418.57952880859375, val loss None
best loss 405.874267578125
not here
quantized in 34.49395728111267 seconds
37478 MiB free out of 48676 MiB total
23 self_attn.k_proj
Pruning ...
256
iter 0, train loss 442.4021911621094, val loss None
iter 10, train loss 468.1643981933594, val loss None
iter 20, train loss 455.53631591796875, val loss None
iter 30, train loss 446.319580078125, val loss None
iter 40, train loss 440.931884765625, val loss None
iter 50, train loss 440.24505615234375, val loss None
iter 60, train loss 439.5135803222656, val loss None
iter 70, train loss 438.24920654296875, val loss None
iter 80, train loss 437.85009765625, val loss None
iter 90, train loss 437.1513977050781, val loss None
best loss 423.28961181640625
not here
quantized in 32.87424349784851 seconds
37468 MiB free out of 48676 MiB total
23 self_attn.v_proj
Pruning ...
256
iter 0, train loss 260.91650390625, val loss None
iter 10, train loss 260.6386413574219, val loss None
iter 20, train loss 259.22943115234375, val loss None
iter 30, train loss 259.34765625, val loss None
iter 40, train loss 259.33587646484375, val loss None
iter 50, train loss 259.18670654296875, val loss None
iter 60, train loss 258.7339782714844, val loss None
iter 70, train loss 258.7479248046875, val loss None
iter 80, train loss 259.0605773925781, val loss None
iter 90, train loss 259.04742431640625, val loss None
best loss 258.63885498046875
not here
quantized in 31.725341796875 seconds
37458 MiB free out of 48676 MiB total
23 self_attn.o_proj
Pruning ...
256
iter 0, train loss 15.158356666564941, val loss None
iter 10, train loss 14.740923881530762, val loss None
iter 20, train loss 14.506425857543945, val loss None
iter 30, train loss 14.430295944213867, val loss None
iter 40, train loss 14.248395919799805, val loss None
iter 50, train loss 14.119443893432617, val loss None
iter 60, train loss 14.038640022277832, val loss None
iter 70, train loss 14.031840324401855, val loss None
iter 80, train loss 13.99896240234375, val loss None
iter 90, train loss 13.972321510314941, val loss None
best loss 13.9608154296875
not here
quantized in 32.09882855415344 seconds
37394 MiB free out of 48676 MiB total
23 mlp.gate_proj
Pruning ...
256
iter 0, train loss 486.1101379394531, val loss None
iter 10, train loss 501.50616455078125, val loss None
iter 20, train loss 499.93499755859375, val loss None
iter 30, train loss 501.423583984375, val loss None
iter 40, train loss 500.639892578125, val loss None
iter 50, train loss 499.98193359375, val loss None
iter 60, train loss 499.7213134765625, val loss None
iter 70, train loss 499.59429931640625, val loss None
iter 80, train loss 499.3951416015625, val loss None
iter 90, train loss 499.4783020019531, val loss None
best loss 482.47216796875
not here
quantized in 85.84495139122009 seconds
37006 MiB free out of 48676 MiB total
23 mlp.up_proj
Pruning ...
256
iter 0, train loss 405.0959777832031, val loss None
iter 10, train loss 405.46484375, val loss None
iter 20, train loss 405.21282958984375, val loss None
iter 30, train loss 405.7214660644531, val loss None
iter 40, train loss 405.767822265625, val loss None
iter 50, train loss 405.41064453125, val loss None
iter 60, train loss 405.62652587890625, val loss None
iter 70, train loss 405.710693359375, val loss None
iter 80, train loss 405.68310546875, val loss None
iter 90, train loss 405.6175231933594, val loss None
best loss 404.9131164550781
not here
quantized in 84.6992859840393 seconds
36726 MiB free out of 48676 MiB total
23 mlp.down_proj
Pruning ...
256
iter 0, train loss 18.841575622558594, val loss None
iter 10, train loss 18.864215850830078, val loss None
iter 20, train loss 18.830045700073242, val loss None
iter 30, train loss 18.822206497192383, val loss None
iter 40, train loss 18.82135009765625, val loss None
iter 50, train loss 18.803634643554688, val loss None
iter 60, train loss 18.797515869140625, val loss None
iter 70, train loss 18.781553268432617, val loss None
iter 80, train loss 18.782833099365234, val loss None
iter 90, train loss 18.780719757080078, val loss None
best loss 18.777402877807617
not here
quantized in 89.38403344154358 seconds
36446 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.codebook', 'self_attn.q_proj.quantizer.norms_1', 'self_attn.q_proj.quantizer.norms_0', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.codebook', 'self_attn.k_proj.quantizer.norms_1', 'self_attn.k_proj.quantizer.norms_0', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.codebook', 'self_attn.v_proj.quantizer.norms_1', 'self_attn.v_proj.quantizer.norms_0', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.codebook', 'self_attn.o_proj.quantizer.norms_1', 'self_attn.o_proj.quantizer.norms_0', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.codebook', 'mlp.gate_proj.quantizer.norms_1', 'mlp.gate_proj.quantizer.norms_0', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.codebook', 'mlp.up_proj.quantizer.norms_1', 'mlp.up_proj.quantizer.norms_0', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.codebook', 'mlp.down_proj.quantizer.norms_1', 'mlp.down_proj.quantizer.norms_0', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135936
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.018069995261612348
10684 MiB free out of 48676 MiB total
epoch 1 loss: 0.017602555075427517
10684 MiB free out of 48676 MiB total
epoch 2 loss: 0.017445917132135946
10684 MiB free out of 48676 MiB total
epoch 3 loss: 0.01735280600405531
10684 MiB free out of 48676 MiB total
epoch 4 loss: 0.01728414987155702
10684 MiB free out of 48676 MiB total
36446 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
10684 MiB free out of 48676 MiB total
after cast to cpu
39402 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
24 self_attn.q_proj
Pruning ...
256
iter 0, train loss 389.0245361328125, val loss None
iter 10, train loss 403.5931091308594, val loss None
iter 20, train loss 387.794189453125, val loss None
iter 30, train loss 387.27557373046875, val loss None
iter 40, train loss 387.0272521972656, val loss None
iter 50, train loss 386.65533447265625, val loss None
iter 60, train loss 385.4006652832031, val loss None
iter 70, train loss 384.8177490234375, val loss None
iter 80, train loss 384.6705322265625, val loss None
iter 90, train loss 384.86407470703125, val loss None
best loss 373.0915222167969
not here
quantized in 34.122666120529175 seconds
37446 MiB free out of 48676 MiB total
24 self_attn.k_proj
Pruning ...
256
iter 0, train loss 415.5560302734375, val loss None
iter 10, train loss 422.6010437011719, val loss None
iter 20, train loss 414.0224609375, val loss None
iter 30, train loss 410.53814697265625, val loss None
iter 40, train loss 407.677490234375, val loss None
iter 50, train loss 406.9989318847656, val loss None
iter 60, train loss 406.8160400390625, val loss None
iter 70, train loss 405.70648193359375, val loss None
iter 80, train loss 404.86187744140625, val loss None
iter 90, train loss 404.6376953125, val loss None
best loss 392.6969909667969
not here
quantized in 33.09048914909363 seconds
37404 MiB free out of 48676 MiB total
24 self_attn.v_proj
Pruning ...
256
iter 0, train loss 248.49758911132812, val loss None
iter 10, train loss 247.55892944335938, val loss None
iter 20, train loss 246.94290161132812, val loss None
iter 30, train loss 246.4327392578125, val loss None
iter 40, train loss 246.29037475585938, val loss None
iter 50, train loss 246.46717834472656, val loss None
iter 60, train loss 246.3504638671875, val loss None
iter 70, train loss 246.27883911132812, val loss None
iter 80, train loss 246.0774383544922, val loss None
iter 90, train loss 245.7877197265625, val loss None
best loss 245.7568359375
not here
quantized in 31.910258531570435 seconds
37426 MiB free out of 48676 MiB total
24 self_attn.o_proj
Pruning ...
256
iter 0, train loss 30.011463165283203, val loss None
iter 10, train loss 23.5129451751709, val loss None
iter 20, train loss 20.7232608795166, val loss None
iter 30, train loss 18.560596466064453, val loss None
iter 40, train loss 17.94301986694336, val loss None
iter 50, train loss 17.73244285583496, val loss None
iter 60, train loss 17.607749938964844, val loss None
iter 70, train loss 17.459989547729492, val loss None
iter 80, train loss 17.46145248413086, val loss None
iter 90, train loss 17.35927391052246, val loss None
best loss 17.322669982910156
not here
quantized in 32.52108550071716 seconds
37426 MiB free out of 48676 MiB total
24 mlp.gate_proj
Pruning ...
256
iter 0, train loss 508.8639831542969, val loss None
iter 10, train loss 525.0858764648438, val loss None
iter 20, train loss 522.3551025390625, val loss None
iter 30, train loss 522.66064453125, val loss None
iter 40, train loss 522.6211547851562, val loss None
iter 50, train loss 522.11962890625, val loss None
iter 60, train loss 521.9699096679688, val loss None
iter 70, train loss 521.7476196289062, val loss None
iter 80, train loss 521.7457275390625, val loss None
iter 90, train loss 521.6534423828125, val loss None
best loss 504.4286193847656
not here
quantized in 86.35045123100281 seconds
37038 MiB free out of 48676 MiB total
24 mlp.up_proj
Pruning ...
256
iter 0, train loss 424.70953369140625, val loss None
iter 10, train loss 425.5928039550781, val loss None
iter 20, train loss 426.517333984375, val loss None
iter 30, train loss 426.3851318359375, val loss None
iter 40, train loss 426.857421875, val loss None
iter 50, train loss 426.7959289550781, val loss None
iter 60, train loss 426.9261474609375, val loss None
iter 70, train loss 427.0359191894531, val loss None
iter 80, train loss 426.99462890625, val loss None
iter 90, train loss 426.9456787109375, val loss None
best loss 424.70953369140625
not here
quantized in 84.5580222606659 seconds
36758 MiB free out of 48676 MiB total
24 mlp.down_proj
Pruning ...
256
iter 0, train loss 19.86288070678711, val loss None
iter 10, train loss 19.864316940307617, val loss None
iter 20, train loss 19.813243865966797, val loss None
iter 30, train loss 19.803112030029297, val loss None
iter 40, train loss 19.797992706298828, val loss None
iter 50, train loss 19.79317855834961, val loss None
iter 60, train loss 19.779747009277344, val loss None
iter 70, train loss 19.776065826416016, val loss None
iter 80, train loss 19.776077270507812, val loss None
iter 90, train loss 19.765705108642578, val loss None
best loss 19.763580322265625
not here
quantized in 88.91213274002075 seconds
36650 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.codebook', 'self_attn.q_proj.quantizer.norms_1', 'self_attn.q_proj.quantizer.norms_0', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.codebook', 'self_attn.k_proj.quantizer.norms_1', 'self_attn.k_proj.quantizer.norms_0', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.codebook', 'self_attn.v_proj.quantizer.norms_1', 'self_attn.v_proj.quantizer.norms_0', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.codebook', 'self_attn.o_proj.quantizer.norms_1', 'self_attn.o_proj.quantizer.norms_0', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.codebook', 'mlp.gate_proj.quantizer.norms_1', 'mlp.gate_proj.quantizer.norms_0', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.codebook', 'mlp.up_proj.quantizer.norms_1', 'mlp.up_proj.quantizer.norms_0', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.codebook', 'mlp.down_proj.quantizer.norms_1', 'mlp.down_proj.quantizer.norms_0', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135936
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.01984087516029831
10716 MiB free out of 48676 MiB total
epoch 1 loss: 0.01935398114437703
10716 MiB free out of 48676 MiB total
epoch 2 loss: 0.01915973555878736
10716 MiB free out of 48676 MiB total
epoch 3 loss: 0.01904052888858132
10716 MiB free out of 48676 MiB total
epoch 4 loss: 0.01895064649579581
10716 MiB free out of 48676 MiB total
36650 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
10716 MiB free out of 48676 MiB total
after cast to cpu
39370 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
25 self_attn.q_proj
Pruning ...
256
iter 0, train loss 449.7227783203125, val loss None
iter 10, train loss 465.4134216308594, val loss None
iter 20, train loss 448.01312255859375, val loss None
iter 30, train loss 446.93310546875, val loss None
iter 40, train loss 446.32781982421875, val loss None
iter 50, train loss 445.7771301269531, val loss None
iter 60, train loss 445.5880126953125, val loss None
iter 70, train loss 445.66387939453125, val loss None
iter 80, train loss 446.16058349609375, val loss None
iter 90, train loss 446.5484924316406, val loss None
best loss 434.6712646484375
not here
quantized in 34.176031827926636 seconds
37478 MiB free out of 48676 MiB total
25 self_attn.k_proj
Pruning ...
256
iter 0, train loss 468.9400634765625, val loss None
iter 10, train loss 490.78570556640625, val loss None
iter 20, train loss 473.8275451660156, val loss None
iter 30, train loss 464.1171875, val loss None
iter 40, train loss 459.1496887207031, val loss None
iter 50, train loss 458.07305908203125, val loss None
iter 60, train loss 458.1336364746094, val loss None
iter 70, train loss 458.16668701171875, val loss None
iter 80, train loss 458.94342041015625, val loss None
iter 90, train loss 458.568115234375, val loss None
best loss 449.50067138671875
not here
quantized in 33.019447565078735 seconds
37468 MiB free out of 48676 MiB total
25 self_attn.v_proj
Pruning ...
256
iter 0, train loss 306.20819091796875, val loss None
iter 10, train loss 305.3662109375, val loss None
iter 20, train loss 304.943359375, val loss None
iter 30, train loss 304.65087890625, val loss None
iter 40, train loss 304.9367370605469, val loss None
iter 50, train loss 304.2932434082031, val loss None
iter 60, train loss 304.2945556640625, val loss None
iter 70, train loss 304.345458984375, val loss None
iter 80, train loss 304.12884521484375, val loss None
iter 90, train loss 303.9917297363281, val loss None
best loss 303.9457702636719
not here
quantized in 31.862643480300903 seconds
37426 MiB free out of 48676 MiB total
25 self_attn.o_proj
Pruning ...
256
iter 0, train loss 17.711021423339844, val loss None
iter 10, train loss 17.962032318115234, val loss None
iter 20, train loss 18.251869201660156, val loss None
iter 30, train loss 18.079891204833984, val loss None
iter 40, train loss 18.039993286132812, val loss None
iter 50, train loss 17.910945892333984, val loss None
iter 60, train loss 17.847431182861328, val loss None
iter 70, train loss 17.825294494628906, val loss None
iter 80, train loss 17.83086395263672, val loss None
iter 90, train loss 17.82708740234375, val loss None
best loss 17.516399383544922
not here
quantized in 32.09510827064514 seconds
37362 MiB free out of 48676 MiB total
25 mlp.gate_proj
Pruning ...
256
iter 0, train loss 548.612548828125, val loss None
iter 10, train loss 567.558349609375, val loss None
iter 20, train loss 567.1060791015625, val loss None
iter 30, train loss 567.6187744140625, val loss None
iter 40, train loss 566.312744140625, val loss None
iter 50, train loss 565.908935546875, val loss None
iter 60, train loss 565.4437866210938, val loss None
iter 70, train loss 565.0510864257812, val loss None
iter 80, train loss 564.90087890625, val loss None
iter 90, train loss 564.6026000976562, val loss None
best loss 543.947021484375
not here
quantized in 86.098788022995 seconds
36974 MiB free out of 48676 MiB total
25 mlp.up_proj
Pruning ...
256
iter 0, train loss 457.323486328125, val loss None
iter 10, train loss 457.3832702636719, val loss None
iter 20, train loss 457.7554931640625, val loss None
iter 30, train loss 457.1668395996094, val loss None
iter 40, train loss 458.23663330078125, val loss None
iter 50, train loss 458.0003662109375, val loss None
iter 60, train loss 458.0721130371094, val loss None
iter 70, train loss 457.970703125, val loss None
iter 80, train loss 457.7599792480469, val loss None
iter 90, train loss 457.7154541015625, val loss None
best loss 457.1668395996094
not here
quantized in 85.05679845809937 seconds
36866 MiB free out of 48676 MiB total
25 mlp.down_proj
Pruning ...
256
iter 0, train loss 20.915878295898438, val loss None
iter 10, train loss 20.924957275390625, val loss None
iter 20, train loss 20.83429718017578, val loss None
iter 30, train loss 20.804533004760742, val loss None
iter 40, train loss 20.793148040771484, val loss None
iter 50, train loss 20.790990829467773, val loss None
iter 60, train loss 20.780750274658203, val loss None
iter 70, train loss 20.764060974121094, val loss None
iter 80, train loss 20.76126480102539, val loss None
iter 90, train loss 20.75586700439453, val loss None
best loss 20.738866806030273
not here
quantized in 89.05105352401733 seconds
36586 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.codebook', 'self_attn.q_proj.quantizer.norms_1', 'self_attn.q_proj.quantizer.norms_0', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.codebook', 'self_attn.k_proj.quantizer.norms_1', 'self_attn.k_proj.quantizer.norms_0', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.codebook', 'self_attn.v_proj.quantizer.norms_1', 'self_attn.v_proj.quantizer.norms_0', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.codebook', 'self_attn.o_proj.quantizer.norms_1', 'self_attn.o_proj.quantizer.norms_0', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.codebook', 'mlp.gate_proj.quantizer.norms_1', 'mlp.gate_proj.quantizer.norms_0', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.codebook', 'mlp.up_proj.quantizer.norms_1', 'mlp.up_proj.quantizer.norms_0', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.codebook', 'mlp.down_proj.quantizer.norms_1', 'mlp.down_proj.quantizer.norms_0', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135936
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.02271955097967293
9714 MiB free out of 48676 MiB total
epoch 1 loss: 0.021564555485383607
9714 MiB free out of 48676 MiB total
epoch 2 loss: 0.021096353797474876
9714 MiB free out of 48676 MiB total
epoch 3 loss: 0.020828965381952003
9714 MiB free out of 48676 MiB total
epoch 4 loss: 0.020643991010729223
9714 MiB free out of 48676 MiB total
36586 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
9714 MiB free out of 48676 MiB total
after cast to cpu
39402 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
26 self_attn.q_proj
Pruning ...
256
iter 0, train loss 430.71826171875, val loss None
iter 10, train loss 452.6622619628906, val loss None
iter 20, train loss 437.8986511230469, val loss None
iter 30, train loss 431.171142578125, val loss None
iter 40, train loss 427.1778564453125, val loss None
iter 50, train loss 424.199462890625, val loss None
iter 60, train loss 423.80364990234375, val loss None
iter 70, train loss 423.62091064453125, val loss None
iter 80, train loss 423.086181640625, val loss None
iter 90, train loss 422.70867919921875, val loss None
best loss 410.44775390625
not here
quantized in 34.32778716087341 seconds
37446 MiB free out of 48676 MiB total
26 self_attn.k_proj
Pruning ...
256
iter 0, train loss 458.6585693359375, val loss None
iter 10, train loss 484.3095703125, val loss None
iter 20, train loss 469.5054931640625, val loss None
iter 30, train loss 460.38397216796875, val loss None
iter 40, train loss 456.87078857421875, val loss None
iter 50, train loss 455.3905029296875, val loss None
iter 60, train loss 453.32696533203125, val loss None
iter 70, train loss 452.33935546875, val loss None
iter 80, train loss 452.2801513671875, val loss None
iter 90, train loss 452.4083251953125, val loss None
best loss 429.8724060058594
not here
quantized in 33.49335026741028 seconds
37404 MiB free out of 48676 MiB total
26 self_attn.v_proj
Pruning ...
256
iter 0, train loss 307.63714599609375, val loss None
iter 10, train loss 306.2596435546875, val loss None
iter 20, train loss 305.4762878417969, val loss None
iter 30, train loss 305.3245544433594, val loss None
iter 40, train loss 304.85498046875, val loss None
iter 50, train loss 304.7970275878906, val loss None
iter 60, train loss 305.03033447265625, val loss None
iter 70, train loss 304.71368408203125, val loss None
iter 80, train loss 304.7973937988281, val loss None
iter 90, train loss 304.86602783203125, val loss None
best loss 304.63201904296875
not here
quantized in 31.786463499069214 seconds
37426 MiB free out of 48676 MiB total
26 self_attn.o_proj
Pruning ...
256
iter 0, train loss 56.0443000793457, val loss None
iter 10, train loss 39.553306579589844, val loss None
iter 20, train loss 31.309324264526367, val loss None
iter 30, train loss 25.138534545898438, val loss None
iter 40, train loss 23.286666870117188, val loss None
iter 50, train loss 22.678295135498047, val loss None
iter 60, train loss 22.505123138427734, val loss None
iter 70, train loss 22.315223693847656, val loss None
iter 80, train loss 22.12598419189453, val loss None
iter 90, train loss 22.176816940307617, val loss None
best loss 22.006080627441406
not here
quantized in 32.72398090362549 seconds
37362 MiB free out of 48676 MiB total
26 mlp.gate_proj
Pruning ...
256
iter 0, train loss 577.0479736328125, val loss None
iter 10, train loss 601.4619140625, val loss None
iter 20, train loss 600.2760009765625, val loss None
iter 30, train loss 599.3920288085938, val loss None
iter 40, train loss 598.863525390625, val loss None
iter 50, train loss 598.396728515625, val loss None
iter 60, train loss 598.112548828125, val loss None
iter 70, train loss 598.571533203125, val loss None
iter 80, train loss 598.3245239257812, val loss None
iter 90, train loss 598.4921875, val loss None
best loss 570.3456420898438
not here
quantized in 86.61104083061218 seconds
36974 MiB free out of 48676 MiB total
26 mlp.up_proj
Pruning ...
256
iter 0, train loss 484.83282470703125, val loss None
iter 10, train loss 485.41619873046875, val loss None
iter 20, train loss 486.36236572265625, val loss None
iter 30, train loss 485.889404296875, val loss None
iter 40, train loss 486.233154296875, val loss None
iter 50, train loss 486.44097900390625, val loss None
iter 60, train loss 486.29437255859375, val loss None
iter 70, train loss 486.1720275878906, val loss None
iter 80, train loss 486.30523681640625, val loss None
iter 90, train loss 486.24163818359375, val loss None
best loss 484.83282470703125
not here
quantized in 85.03056287765503 seconds
36694 MiB free out of 48676 MiB total
26 mlp.down_proj
Pruning ...
256
iter 0, train loss 22.44686508178711, val loss None
iter 10, train loss 22.465469360351562, val loss None
iter 20, train loss 22.398330688476562, val loss None
iter 30, train loss 22.39457130432129, val loss None
iter 40, train loss 22.40030860900879, val loss None
iter 50, train loss 22.373382568359375, val loss None
iter 60, train loss 22.387678146362305, val loss None
iter 70, train loss 22.378421783447266, val loss None
iter 80, train loss 22.38123321533203, val loss None
iter 90, train loss 22.38199806213379, val loss None
best loss 22.36748504638672
not here
quantized in 88.6825852394104 seconds
36414 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.codebook', 'self_attn.q_proj.quantizer.norms_1', 'self_attn.q_proj.quantizer.norms_0', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.codebook', 'self_attn.k_proj.quantizer.norms_1', 'self_attn.k_proj.quantizer.norms_0', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.codebook', 'self_attn.v_proj.quantizer.norms_1', 'self_attn.v_proj.quantizer.norms_0', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.codebook', 'self_attn.o_proj.quantizer.norms_1', 'self_attn.o_proj.quantizer.norms_0', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.codebook', 'mlp.gate_proj.quantizer.norms_1', 'mlp.gate_proj.quantizer.norms_0', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.codebook', 'mlp.up_proj.quantizer.norms_1', 'mlp.up_proj.quantizer.norms_0', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.codebook', 'mlp.down_proj.quantizer.norms_1', 'mlp.down_proj.quantizer.norms_0', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135936
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.024172236691811122
9692 MiB free out of 48676 MiB total
epoch 1 loss: 0.023499195085605606
9692 MiB free out of 48676 MiB total
epoch 2 loss: 0.02323677172535099
9692 MiB free out of 48676 MiB total
epoch 3 loss: 0.023073344273143448
9692 MiB free out of 48676 MiB total
epoch 4 loss: 0.02295019464509096
9692 MiB free out of 48676 MiB total
36414 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
9692 MiB free out of 48676 MiB total
after cast to cpu
39370 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
27 self_attn.q_proj
Pruning ...
256
iter 0, train loss 472.720947265625, val loss None
iter 10, train loss 497.2296142578125, val loss None
iter 20, train loss 478.23333740234375, val loss None
iter 30, train loss 469.15142822265625, val loss None
iter 40, train loss 467.5164794921875, val loss None
iter 50, train loss 468.47613525390625, val loss None
iter 60, train loss 467.9810485839844, val loss None
iter 70, train loss 466.5779113769531, val loss None
iter 80, train loss 466.6741638183594, val loss None
iter 90, train loss 466.75323486328125, val loss None
best loss 446.3227844238281
not here
quantized in 34.197340965270996 seconds
37478 MiB free out of 48676 MiB total
27 self_attn.k_proj
Pruning ...
256
iter 0, train loss 509.4869079589844, val loss None
iter 10, train loss 533.458740234375, val loss None
iter 20, train loss 512.789794921875, val loss None
iter 30, train loss 502.8766784667969, val loss None
iter 40, train loss 495.5391540527344, val loss None
iter 50, train loss 497.0392150878906, val loss None
iter 60, train loss 496.30364990234375, val loss None
iter 70, train loss 494.80548095703125, val loss None
iter 80, train loss 494.16009521484375, val loss None
iter 90, train loss 493.1484375, val loss None
best loss 464.8209533691406
not here
quantized in 33.49079656600952 seconds
37468 MiB free out of 48676 MiB total
27 self_attn.v_proj
Pruning ...
256
iter 0, train loss 315.3658447265625, val loss None
iter 10, train loss 315.4561767578125, val loss None
iter 20, train loss 315.2283935546875, val loss None
iter 30, train loss 314.3529357910156, val loss None
iter 40, train loss 313.4342346191406, val loss None
iter 50, train loss 313.88037109375, val loss None
iter 60, train loss 313.67999267578125, val loss None
iter 70, train loss 313.7773742675781, val loss None
iter 80, train loss 313.78643798828125, val loss None
iter 90, train loss 313.699462890625, val loss None
best loss 313.4342346191406
not here
quantized in 31.99947953224182 seconds
37458 MiB free out of 48676 MiB total
27 self_attn.o_proj
Pruning ...
256
iter 0, train loss 20.867645263671875, val loss None
iter 10, train loss 20.114604949951172, val loss None
iter 20, train loss 19.543636322021484, val loss None
iter 30, train loss 19.28666114807129, val loss None
iter 40, train loss 18.79549217224121, val loss None
iter 50, train loss 18.593006134033203, val loss None
iter 60, train loss 18.41476821899414, val loss None
iter 70, train loss 18.460346221923828, val loss None
iter 80, train loss 18.353439331054688, val loss None
iter 90, train loss 18.316211700439453, val loss None
best loss 18.269010543823242
not here
quantized in 32.433239698410034 seconds
37394 MiB free out of 48676 MiB total
27 mlp.gate_proj
Pruning ...
256
iter 0, train loss 618.6472778320312, val loss None
iter 10, train loss 645.8668212890625, val loss None
iter 20, train loss 641.3432006835938, val loss None
iter 30, train loss 644.6925048828125, val loss None
iter 40, train loss 644.28271484375, val loss None
iter 50, train loss 644.0382080078125, val loss None
iter 60, train loss 644.3162841796875, val loss None
iter 70, train loss 644.1088256835938, val loss None
iter 80, train loss 643.768310546875, val loss None
iter 90, train loss 644.0712890625, val loss None
best loss 608.56005859375
not here
quantized in 86.99595856666565 seconds
37006 MiB free out of 48676 MiB total
27 mlp.up_proj
Pruning ...
256
iter 0, train loss 522.3758544921875, val loss None
iter 10, train loss 524.4764404296875, val loss None
iter 20, train loss 524.384521484375, val loss None
iter 30, train loss 524.3365478515625, val loss None
iter 40, train loss 524.1367797851562, val loss None
iter 50, train loss 524.199462890625, val loss None
iter 60, train loss 524.229736328125, val loss None
iter 70, train loss 524.2910766601562, val loss None
iter 80, train loss 524.0193481445312, val loss None
iter 90, train loss 523.8186645507812, val loss None
best loss 520.4696655273438
not here
quantized in 86.05413794517517 seconds
36726 MiB free out of 48676 MiB total
27 mlp.down_proj
Pruning ...
256
iter 0, train loss 25.527690887451172, val loss None
iter 10, train loss 25.518898010253906, val loss None
iter 20, train loss 25.46017837524414, val loss None
iter 30, train loss 25.45911407470703, val loss None
iter 40, train loss 25.406675338745117, val loss None
iter 50, train loss 25.430931091308594, val loss None
iter 60, train loss 25.409879684448242, val loss None
iter 70, train loss 25.389841079711914, val loss None
iter 80, train loss 25.401714324951172, val loss None
iter 90, train loss 25.391338348388672, val loss None
best loss 25.38793182373047
not here
quantized in 88.89716053009033 seconds
36446 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.codebook', 'self_attn.q_proj.quantizer.norms_1', 'self_attn.q_proj.quantizer.norms_0', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.codebook', 'self_attn.k_proj.quantizer.norms_1', 'self_attn.k_proj.quantizer.norms_0', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.codebook', 'self_attn.v_proj.quantizer.norms_1', 'self_attn.v_proj.quantizer.norms_0', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.codebook', 'self_attn.o_proj.quantizer.norms_1', 'self_attn.o_proj.quantizer.norms_0', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.codebook', 'mlp.gate_proj.quantizer.norms_1', 'mlp.gate_proj.quantizer.norms_0', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.codebook', 'mlp.up_proj.quantizer.norms_1', 'mlp.up_proj.quantizer.norms_0', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.codebook', 'mlp.down_proj.quantizer.norms_1', 'mlp.down_proj.quantizer.norms_0', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135936
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.025348572642542422
10684 MiB free out of 48676 MiB total
epoch 1 loss: 0.02468964850413613
10684 MiB free out of 48676 MiB total
epoch 2 loss: 0.024418945336947218
10684 MiB free out of 48676 MiB total
epoch 3 loss: 0.024239471546025015
10684 MiB free out of 48676 MiB total
epoch 4 loss: 0.024101932198391296
10684 MiB free out of 48676 MiB total
36446 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
10684 MiB free out of 48676 MiB total
after cast to cpu
39402 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
28 self_attn.q_proj
Pruning ...
256
iter 0, train loss 471.8770446777344, val loss None
iter 10, train loss 496.0241394042969, val loss None
iter 20, train loss 476.16326904296875, val loss None
iter 30, train loss 467.874267578125, val loss None
iter 40, train loss 464.2602844238281, val loss None
iter 50, train loss 463.12591552734375, val loss None
iter 60, train loss 463.14605712890625, val loss None
iter 70, train loss 463.03497314453125, val loss None
iter 80, train loss 462.5399169921875, val loss None
iter 90, train loss 462.2040100097656, val loss None
best loss 439.62237548828125
not here
quantized in 34.27156972885132 seconds
37446 MiB free out of 48676 MiB total
28 self_attn.k_proj
Pruning ...
256
iter 0, train loss 506.6802062988281, val loss None
iter 10, train loss 534.1238403320312, val loss None
iter 20, train loss 515.1322021484375, val loss None
iter 30, train loss 508.131103515625, val loss None
iter 40, train loss 502.187744140625, val loss None
iter 50, train loss 497.50701904296875, val loss None
iter 60, train loss 496.75347900390625, val loss None
iter 70, train loss 496.4784851074219, val loss None
iter 80, train loss 495.56640625, val loss None
iter 90, train loss 495.68994140625, val loss None
best loss 458.3085632324219
not here
quantized in 33.73356795310974 seconds
37404 MiB free out of 48676 MiB total
28 self_attn.v_proj
Pruning ...
256
iter 0, train loss 347.58892822265625, val loss None
iter 10, train loss 346.8111877441406, val loss None
iter 20, train loss 347.0574951171875, val loss None
iter 30, train loss 346.01947021484375, val loss None
iter 40, train loss 344.8705139160156, val loss None
iter 50, train loss 344.8333435058594, val loss None
iter 60, train loss 344.6424865722656, val loss None
iter 70, train loss 344.865478515625, val loss None
iter 80, train loss 344.5680236816406, val loss None
iter 90, train loss 344.496337890625, val loss None
best loss 344.1935119628906
not here
quantized in 31.87677788734436 seconds
37426 MiB free out of 48676 MiB total
28 self_attn.o_proj
Pruning ...
256
iter 0, train loss 30.37744903564453, val loss None
iter 10, train loss 29.354351043701172, val loss None
iter 20, train loss 29.280311584472656, val loss None
iter 30, train loss 29.47541046142578, val loss None
iter 40, train loss 29.27279281616211, val loss None
iter 50, train loss 28.97759246826172, val loss None
iter 60, train loss 28.834510803222656, val loss None
iter 70, train loss 28.707561492919922, val loss None
iter 80, train loss 28.630355834960938, val loss None
iter 90, train loss 28.45589256286621, val loss None
best loss 28.334735870361328
not here
quantized in 32.37464618682861 seconds
37362 MiB free out of 48676 MiB total
28 mlp.gate_proj
Pruning ...
256
iter 0, train loss 648.45849609375, val loss None
iter 10, train loss 679.1021728515625, val loss None
iter 20, train loss 668.1778564453125, val loss None
iter 30, train loss 672.2108154296875, val loss None
iter 40, train loss 671.6880493164062, val loss None
iter 50, train loss 671.2771606445312, val loss None
iter 60, train loss 671.3792724609375, val loss None
iter 70, train loss 671.719970703125, val loss None
iter 80, train loss 671.6292724609375, val loss None
iter 90, train loss 671.4972534179688, val loss None
best loss 637.5267333984375
not here
quantized in 86.74145030975342 seconds
36974 MiB free out of 48676 MiB total
28 mlp.up_proj
Pruning ...
256
iter 0, train loss 571.039306640625, val loss None
iter 10, train loss 581.615234375, val loss None
iter 20, train loss 570.9519653320312, val loss None
iter 30, train loss 571.1067504882812, val loss None
iter 40, train loss 570.185546875, val loss None
iter 50, train loss 569.87109375, val loss None
iter 60, train loss 569.7326049804688, val loss None
iter 70, train loss 569.2528076171875, val loss None
iter 80, train loss 569.0198364257812, val loss None
iter 90, train loss 569.1034545898438, val loss None
best loss 566.2132568359375
not here
quantized in 86.52986073493958 seconds
36694 MiB free out of 48676 MiB total
28 mlp.down_proj
Pruning ...
256
iter 0, train loss 30.180622100830078, val loss None
iter 10, train loss 30.19647216796875, val loss None
iter 20, train loss 30.139690399169922, val loss None
iter 30, train loss 30.116912841796875, val loss None
iter 40, train loss 30.08465576171875, val loss None
iter 50, train loss 30.021438598632812, val loss None
iter 60, train loss 29.976261138916016, val loss None
iter 70, train loss 29.98625373840332, val loss None
iter 80, train loss 29.970712661743164, val loss None
iter 90, train loss 29.96731948852539, val loss None
best loss 29.9505558013916
not here
quantized in 88.92034673690796 seconds
36414 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.codebook', 'self_attn.q_proj.quantizer.norms_1', 'self_attn.q_proj.quantizer.norms_0', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.codebook', 'self_attn.k_proj.quantizer.norms_1', 'self_attn.k_proj.quantizer.norms_0', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.codebook', 'self_attn.v_proj.quantizer.norms_1', 'self_attn.v_proj.quantizer.norms_0', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.codebook', 'self_attn.o_proj.quantizer.norms_1', 'self_attn.o_proj.quantizer.norms_0', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.codebook', 'mlp.gate_proj.quantizer.norms_1', 'mlp.gate_proj.quantizer.norms_0', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.codebook', 'mlp.up_proj.quantizer.norms_1', 'mlp.up_proj.quantizer.norms_0', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.codebook', 'mlp.down_proj.quantizer.norms_1', 'mlp.down_proj.quantizer.norms_0', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135936
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.03197298194572795
9692 MiB free out of 48676 MiB total
epoch 1 loss: 0.031072984798811376
9692 MiB free out of 48676 MiB total
epoch 2 loss: 0.030702685617143288
9692 MiB free out of 48676 MiB total
epoch 3 loss: 0.030455515006906353
9692 MiB free out of 48676 MiB total
epoch 4 loss: 0.03026444054557942
9692 MiB free out of 48676 MiB total
36414 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
9692 MiB free out of 48676 MiB total
after cast to cpu
39370 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
29 self_attn.q_proj
Pruning ...
256
iter 0, train loss 425.8703918457031, val loss None
iter 10, train loss 451.2810974121094, val loss None
iter 20, train loss 437.72344970703125, val loss None
iter 30, train loss 426.07086181640625, val loss None
iter 40, train loss 421.16461181640625, val loss None
iter 50, train loss 419.1513671875, val loss None
iter 60, train loss 417.67193603515625, val loss None
iter 70, train loss 416.31341552734375, val loss None
iter 80, train loss 415.21453857421875, val loss None
iter 90, train loss 414.95330810546875, val loss None
best loss 389.9200744628906
not here
quantized in 34.68535494804382 seconds
37478 MiB free out of 48676 MiB total
29 self_attn.k_proj
Pruning ...
256
iter 0, train loss 459.55279541015625, val loss None
iter 10, train loss 482.89764404296875, val loss None
iter 20, train loss 469.5985412597656, val loss None
iter 30, train loss 458.77447509765625, val loss None
iter 40, train loss 450.6578674316406, val loss None
iter 50, train loss 446.6700439453125, val loss None
iter 60, train loss 444.95184326171875, val loss None
iter 70, train loss 444.2770080566406, val loss None
iter 80, train loss 443.69842529296875, val loss None
iter 90, train loss 443.45989990234375, val loss None
best loss 408.1316833496094
not here
quantized in 34.047542333602905 seconds
37468 MiB free out of 48676 MiB total
29 self_attn.v_proj
Pruning ...
256
iter 0, train loss 325.9717102050781, val loss None
iter 10, train loss 325.33984375, val loss None
iter 20, train loss 325.26837158203125, val loss None
iter 30, train loss 324.7078857421875, val loss None
iter 40, train loss 323.66748046875, val loss None
iter 50, train loss 323.50018310546875, val loss None
iter 60, train loss 323.5647888183594, val loss None
iter 70, train loss 323.08709716796875, val loss None
iter 80, train loss 323.3785705566406, val loss None
iter 90, train loss 323.4454040527344, val loss None
best loss 322.971923828125
not here
quantized in 31.84621548652649 seconds
37426 MiB free out of 48676 MiB total
29 self_attn.o_proj
Pruning ...
256
iter 0, train loss 37.62657928466797, val loss None
iter 10, train loss 32.33876037597656, val loss None
iter 20, train loss 30.84066390991211, val loss None
iter 30, train loss 30.24317169189453, val loss None
iter 40, train loss 29.37442398071289, val loss None
iter 50, train loss 28.921056747436523, val loss None
iter 60, train loss 28.801067352294922, val loss None
iter 70, train loss 28.515892028808594, val loss None
iter 80, train loss 28.392601013183594, val loss None
iter 90, train loss 28.370763778686523, val loss None
best loss 28.10184097290039
not here
quantized in 32.63400077819824 seconds
37426 MiB free out of 48676 MiB total
29 mlp.gate_proj
Pruning ...
256
iter 0, train loss 673.765380859375, val loss None
iter 10, train loss 707.0439453125, val loss None
iter 20, train loss 692.528564453125, val loss None
iter 30, train loss 693.276123046875, val loss None
iter 40, train loss 688.6334228515625, val loss None
iter 50, train loss 686.7479248046875, val loss None
iter 60, train loss 686.231201171875, val loss None
iter 70, train loss 686.24560546875, val loss None
iter 80, train loss 685.8552856445312, val loss None
iter 90, train loss 685.6402587890625, val loss None
best loss 654.90234375
not here
quantized in 87.17128467559814 seconds
37038 MiB free out of 48676 MiB total
29 mlp.up_proj
Pruning ...
256
iter 0, train loss 602.802490234375, val loss None
iter 10, train loss 623.5108642578125, val loss None
iter 20, train loss 598.5491943359375, val loss None
iter 30, train loss 598.04248046875, val loss None
iter 40, train loss 594.5372314453125, val loss None
iter 50, train loss 593.8283081054688, val loss None
iter 60, train loss 593.8406982421875, val loss None
iter 70, train loss 592.8286743164062, val loss None
iter 80, train loss 592.6201171875, val loss None
iter 90, train loss 592.44287109375, val loss None
best loss 590.3519287109375
not here
quantized in 86.7982120513916 seconds
36758 MiB free out of 48676 MiB total
29 mlp.down_proj
Pruning ...
256
iter 0, train loss 35.564964294433594, val loss None
iter 10, train loss 35.657012939453125, val loss None
iter 20, train loss 35.53369140625, val loss None
iter 30, train loss 35.46453094482422, val loss None
iter 40, train loss 35.44853210449219, val loss None
iter 50, train loss 35.389102935791016, val loss None
iter 60, train loss 35.39646911621094, val loss None
iter 70, train loss 35.36221694946289, val loss None
iter 80, train loss 35.33860778808594, val loss None
iter 90, train loss 35.28421401977539, val loss None
best loss 35.26911163330078
not here
quantized in 89.32774639129639 seconds
36478 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.codebook', 'self_attn.q_proj.quantizer.norms_1', 'self_attn.q_proj.quantizer.norms_0', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.codebook', 'self_attn.k_proj.quantizer.norms_1', 'self_attn.k_proj.quantizer.norms_0', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.codebook', 'self_attn.v_proj.quantizer.norms_1', 'self_attn.v_proj.quantizer.norms_0', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.codebook', 'self_attn.o_proj.quantizer.norms_1', 'self_attn.o_proj.quantizer.norms_0', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.codebook', 'mlp.gate_proj.quantizer.norms_1', 'mlp.gate_proj.quantizer.norms_0', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.codebook', 'mlp.up_proj.quantizer.norms_1', 'mlp.up_proj.quantizer.norms_0', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.codebook', 'mlp.down_proj.quantizer.norms_1', 'mlp.down_proj.quantizer.norms_0', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135936
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.037000835698563606
9692 MiB free out of 48676 MiB total
epoch 1 loss: 0.03572705187252723
9692 MiB free out of 48676 MiB total
epoch 2 loss: 0.03517948542139493
9692 MiB free out of 48676 MiB total
epoch 3 loss: 0.034804627764970064
9692 MiB free out of 48676 MiB total
epoch 4 loss: 0.03451048643910326
9692 MiB free out of 48676 MiB total
36478 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
9692 MiB free out of 48676 MiB total
after cast to cpu
39402 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
30 self_attn.q_proj
Pruning ...
256
iter 0, train loss 453.66619873046875, val loss None
iter 10, train loss 480.6396484375, val loss None
iter 20, train loss 463.892822265625, val loss None
iter 30, train loss 455.3954162597656, val loss None
iter 40, train loss 450.3518981933594, val loss None
iter 50, train loss 447.4900207519531, val loss None
iter 60, train loss 444.9269104003906, val loss None
iter 70, train loss 443.9340515136719, val loss None
iter 80, train loss 442.80059814453125, val loss None
iter 90, train loss 442.51202392578125, val loss None
best loss 413.1687927246094
not here
quantized in 34.880305767059326 seconds
37446 MiB free out of 48676 MiB total
30 self_attn.k_proj
Pruning ...
256
iter 0, train loss 488.2746276855469, val loss None
iter 10, train loss 514.794921875, val loss None
iter 20, train loss 498.4781188964844, val loss None
iter 30, train loss 486.4137268066406, val loss None
iter 40, train loss 477.3060302734375, val loss None
iter 50, train loss 469.4805603027344, val loss None
iter 60, train loss 466.45147705078125, val loss None
iter 70, train loss 464.6869201660156, val loss None
iter 80, train loss 464.42852783203125, val loss None
iter 90, train loss 464.1523132324219, val loss None
best loss 429.6882629394531
not here
quantized in 33.939632177352905 seconds
37404 MiB free out of 48676 MiB total
30 self_attn.v_proj
Pruning ...
256
iter 0, train loss 363.41021728515625, val loss None
iter 10, train loss 364.77484130859375, val loss None
iter 20, train loss 363.6287536621094, val loss None
iter 30, train loss 361.3370666503906, val loss None
iter 40, train loss 362.6753845214844, val loss None
iter 50, train loss 362.56207275390625, val loss None
iter 60, train loss 362.5682373046875, val loss None
iter 70, train loss 362.092529296875, val loss None
iter 80, train loss 361.939453125, val loss None
iter 90, train loss 361.87042236328125, val loss None
best loss 361.08819580078125
not here
quantized in 32.03813099861145 seconds
37426 MiB free out of 48676 MiB total
30 self_attn.o_proj
Pruning ...
256
iter 0, train loss 37.76480484008789, val loss None
iter 10, train loss 32.79383087158203, val loss None
iter 20, train loss 31.683874130249023, val loss None
iter 30, train loss 30.733402252197266, val loss None
iter 40, train loss 30.311256408691406, val loss None
iter 50, train loss 29.750534057617188, val loss None
iter 60, train loss 29.843280792236328, val loss None
iter 70, train loss 29.49265480041504, val loss None
iter 80, train loss 29.18924331665039, val loss None
iter 90, train loss 29.122100830078125, val loss None
best loss 29.1197509765625
not here
quantized in 32.43834686279297 seconds
37426 MiB free out of 48676 MiB total
30 mlp.gate_proj
Pruning ...
256
iter 0, train loss 762.2426147460938, val loss None
iter 10, train loss 806.7214965820312, val loss None
iter 20, train loss 770.8178100585938, val loss None
iter 30, train loss 755.1575317382812, val loss None
iter 40, train loss 748.766845703125, val loss None
iter 50, train loss 750.9224853515625, val loss None
iter 60, train loss 752.1406860351562, val loss None
iter 70, train loss 753.3829345703125, val loss None
iter 80, train loss 753.5777587890625, val loss None
iter 90, train loss 753.3042602539062, val loss None
best loss 700.9290771484375
not here
quantized in 89.32685136795044 seconds
37038 MiB free out of 48676 MiB total
30 mlp.up_proj
Pruning ...
256
iter 0, train loss 676.0194091796875, val loss None
iter 10, train loss 696.786376953125, val loss None
iter 20, train loss 659.527587890625, val loss None
iter 30, train loss 640.619140625, val loss None
iter 40, train loss 639.4888916015625, val loss None
iter 50, train loss 636.3560180664062, val loss None
iter 60, train loss 635.0835571289062, val loss None
iter 70, train loss 634.2164916992188, val loss None
iter 80, train loss 634.4505615234375, val loss None
iter 90, train loss 634.5057373046875, val loss None
best loss 618.9651489257812
not here
quantized in 88.15219855308533 seconds
36758 MiB free out of 48676 MiB total
30 mlp.down_proj
Pruning ...
256
iter 0, train loss 50.12637710571289, val loss None
iter 10, train loss 50.93083953857422, val loss None
iter 20, train loss 50.12260437011719, val loss None
iter 30, train loss 49.94315719604492, val loss None
iter 40, train loss 49.79566192626953, val loss None
iter 50, train loss 49.81970977783203, val loss None
iter 60, train loss 49.6214714050293, val loss None
iter 70, train loss 49.613433837890625, val loss None
iter 80, train loss 49.472774505615234, val loss None
iter 90, train loss 49.3469352722168, val loss None
best loss 49.27214813232422
not here
quantized in 89.81290245056152 seconds
36478 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.codebook', 'self_attn.q_proj.quantizer.norms_1', 'self_attn.q_proj.quantizer.norms_0', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.codebook', 'self_attn.k_proj.quantizer.norms_1', 'self_attn.k_proj.quantizer.norms_0', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.codebook', 'self_attn.v_proj.quantizer.norms_1', 'self_attn.v_proj.quantizer.norms_0', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.codebook', 'self_attn.o_proj.quantizer.norms_1', 'self_attn.o_proj.quantizer.norms_0', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.codebook', 'mlp.gate_proj.quantizer.norms_1', 'mlp.gate_proj.quantizer.norms_0', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.codebook', 'mlp.up_proj.quantizer.norms_1', 'mlp.up_proj.quantizer.norms_0', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.codebook', 'mlp.down_proj.quantizer.norms_1', 'mlp.down_proj.quantizer.norms_0', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135936
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.05210336745949462
9692 MiB free out of 48676 MiB total
epoch 1 loss: 0.04827805102104321
9692 MiB free out of 48676 MiB total
epoch 2 loss: 0.0472047702933196
9692 MiB free out of 48676 MiB total
epoch 3 loss: 0.04655738818109967
9692 MiB free out of 48676 MiB total
epoch 4 loss: 0.046066249458817765
9692 MiB free out of 48676 MiB total
36478 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
9692 MiB free out of 48676 MiB total
after cast to cpu
39370 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
finished adding batches
subset {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
31 self_attn.q_proj
Pruning ...
256
iter 0, train loss 344.1916809082031, val loss None
iter 10, train loss 360.1473693847656, val loss None
iter 20, train loss 349.0821533203125, val loss None
iter 30, train loss 337.83154296875, val loss None
iter 40, train loss 333.4259338378906, val loss None
iter 50, train loss 331.4872131347656, val loss None
iter 60, train loss 328.8477783203125, val loss None
iter 70, train loss 328.85418701171875, val loss None
iter 80, train loss 328.6047058105469, val loss None
iter 90, train loss 327.98187255859375, val loss None
best loss 295.8394775390625
not here
quantized in 35.2529239654541 seconds
37478 MiB free out of 48676 MiB total
31 self_attn.k_proj
Pruning ...
256
iter 0, train loss 404.78070068359375, val loss None
iter 10, train loss 410.7128601074219, val loss None
iter 20, train loss 409.532470703125, val loss None
iter 30, train loss 391.4014587402344, val loss None
iter 40, train loss 386.7099914550781, val loss None
iter 50, train loss 382.1122131347656, val loss None
iter 60, train loss 380.3915100097656, val loss None
iter 70, train loss 379.1884765625, val loss None
iter 80, train loss 377.59625244140625, val loss None
iter 90, train loss 376.4196472167969, val loss None
best loss 325.7835388183594
not here
quantized in 35.19638514518738 seconds
37468 MiB free out of 48676 MiB total
31 self_attn.v_proj
Pruning ...
256
iter 0, train loss 208.26226806640625, val loss None
iter 10, train loss 208.2210693359375, val loss None
iter 20, train loss 208.14212036132812, val loss None
iter 30, train loss 208.11740112304688, val loss None
iter 40, train loss 207.80703735351562, val loss None
iter 50, train loss 207.54554748535156, val loss None
iter 60, train loss 207.19509887695312, val loss None
iter 70, train loss 206.9335174560547, val loss None
iter 80, train loss 207.3052520751953, val loss None
iter 90, train loss 207.09597778320312, val loss None
best loss 206.87594604492188
not here
quantized in 32.03501486778259 seconds
37458 MiB free out of 48676 MiB total
31 self_attn.o_proj
Pruning ...
256
iter 0, train loss 313.2906494140625, val loss None
iter 10, train loss 228.1402587890625, val loss None
iter 20, train loss 173.45736694335938, val loss None
iter 30, train loss 139.31460571289062, val loss None
iter 40, train loss 124.7166748046875, val loss None
iter 50, train loss 101.63190460205078, val loss None
iter 60, train loss 91.69833374023438, val loss None
iter 70, train loss 87.54409790039062, val loss None
iter 80, train loss 85.21739959716797, val loss None
iter 90, train loss 83.84040832519531, val loss None
best loss 83.07015991210938
not here
quantized in 34.147114515304565 seconds
37394 MiB free out of 48676 MiB total
31 mlp.gate_proj
Pruning ...
256
iter 0, train loss 704.9412231445312, val loss None
iter 10, train loss 707.390625, val loss None
iter 20, train loss 682.2639770507812, val loss None
iter 30, train loss 663.195068359375, val loss None
iter 40, train loss 658.4022216796875, val loss None
iter 50, train loss 659.5238037109375, val loss None
iter 60, train loss 660.4874877929688, val loss None
iter 70, train loss 659.3248291015625, val loss None
iter 80, train loss 658.6312866210938, val loss None
iter 90, train loss 658.5010375976562, val loss None
best loss 601.5210571289062
not here
quantized in 89.56582975387573 seconds
37006 MiB free out of 48676 MiB total
31 mlp.up_proj
Pruning ...
256
iter 0, train loss 679.234375, val loss None
iter 10, train loss 672.8072509765625, val loss None
iter 20, train loss 658.6502685546875, val loss None
iter 30, train loss 641.2898559570312, val loss None
iter 40, train loss 625.1282348632812, val loss None
iter 50, train loss 623.5643310546875, val loss None
iter 60, train loss 620.3014526367188, val loss None
iter 70, train loss 618.2711181640625, val loss None
iter 80, train loss 616.39111328125, val loss None
iter 90, train loss 615.6865844726562, val loss None
best loss 553.1023559570312
not here
quantized in 92.96846222877502 seconds
36726 MiB free out of 48676 MiB total
31 mlp.down_proj
Pruning ...
256
iter 0, train loss 110.84677124023438, val loss None
iter 10, train loss 116.59576416015625, val loss None
iter 20, train loss 106.83686065673828, val loss None
iter 30, train loss 102.89666748046875, val loss None
iter 40, train loss 100.29383850097656, val loss None
iter 50, train loss 98.44143676757812, val loss None
iter 60, train loss 96.15965270996094, val loss None
iter 70, train loss 95.22945404052734, val loss None
iter 80, train loss 94.26537322998047, val loss None
iter 90, train loss 92.90058135986328, val loss None
best loss 91.81330108642578
not here
quantized in 93.6945412158966 seconds
36446 MiB free out of 48676 MiB total
finished: {'self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False), 'self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False), 'mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False), 'mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False)}
Fine tuning ...
the following parameters will not be optimized: dict_keys([])
number of parameters to not optimize: 0
optimizing the following parameters: dict_keys(['self_attn.q_proj.bias', 'self_attn.q_proj.quantizer.codebook', 'self_attn.q_proj.quantizer.norms_1', 'self_attn.q_proj.quantizer.norms_0', 'self_attn.k_proj.bias', 'self_attn.k_proj.quantizer.codebook', 'self_attn.k_proj.quantizer.norms_1', 'self_attn.k_proj.quantizer.norms_0', 'self_attn.v_proj.bias', 'self_attn.v_proj.quantizer.codebook', 'self_attn.v_proj.quantizer.norms_1', 'self_attn.v_proj.quantizer.norms_0', 'self_attn.o_proj.bias', 'self_attn.o_proj.quantizer.codebook', 'self_attn.o_proj.quantizer.norms_1', 'self_attn.o_proj.quantizer.norms_0', 'mlp.gate_proj.bias', 'mlp.gate_proj.quantizer.codebook', 'mlp.gate_proj.quantizer.norms_1', 'mlp.gate_proj.quantizer.norms_0', 'mlp.up_proj.bias', 'mlp.up_proj.quantizer.codebook', 'mlp.up_proj.quantizer.norms_1', 'mlp.up_proj.quantizer.norms_0', 'mlp.down_proj.bias', 'mlp.down_proj.quantizer.codebook', 'mlp.down_proj.quantizer.norms_1', 'mlp.down_proj.quantizer.norms_0', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
total number of parameters to optimize: 135936
n accumulation steps: 1
n batches: 128
epoch 0 loss: 0.15158381476067007
10684 MiB free out of 48676 MiB total
epoch 1 loss: 0.1305726011050865
10684 MiB free out of 48676 MiB total
epoch 2 loss: 0.12476296577369794
10684 MiB free out of 48676 MiB total
epoch 3 loss: 0.12159924290608615
10684 MiB free out of 48676 MiB total
epoch 4 loss: 0.11941384081728756
10684 MiB free out of 48676 MiB total
36446 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
10684 MiB free out of 48676 MiB total
after cast to cpu
39402 MiB free out of 48676 MiB total
Total bits: 13017415680.0 Total params: 6476005376
average bits per value: 2.0100995790155443
total time taken: 21023.43182182312
Loading tokenizer for meta-llama/Llama-2-7b-hf
/home/lliu/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Dataset: wikitext2
Evaluating ...
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
Perplexity: 6.728445
