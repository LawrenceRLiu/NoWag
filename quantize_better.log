wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: m6481. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /data/lliu/huffman/wandb/run-20250103_021550-29e1fiyr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run amber-serenity-59
wandb: ‚≠êÔ∏è View project at https://wandb.ai/m6481/compression_no_finetune
wandb: üöÄ View run at https://wandb.ai/m6481/compression_no_finetune/runs/29e1fiyr
Namespace(models_to_compress=['meta-llama/Llama-2-7b-hf', 'meta-llama/Llama-2-13b-hf', 'meta-llama/Llama-2-70b-hf', 'meta-llama/Meta-Llama-3-8B', 'meta-llama/Meta-Llama-3-70B'], seqlens=[4096, 4096, 4096, 8192, 8192], batch_size=1, hessian_path='/data/lliu/huffman/models/{model_name}/hessians_new/pajama/128/', save_path='/data/lliu/huffman/models/{model_name}/compressed', self_attn_compression_algorithm='quantize', mlp_compression_algorithm='quantize', devices=['cuda:5', 'cuda:6', 'cuda:4', 'cuda:7'], yaml_path='/data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml', self_attn_yaml_path=None, mlp_yaml_path=None, use_already_done=False, use_wandb=True, wandb_project='compression_no_finetune')
  0%|          | 0/1848 [00:00<?, ?it/s]  0%|          | 1/1848 [02:50<87:13:58, 170.03s/it]  0%|          | 3/1848 [05:30<52:57:54, 103.35s/it]  0%|          | 4/1848 [06:25<44:22:07, 86.62s/it]   0%|          | 5/1848 [06:40<32:13:00, 62.93s/it]  0%|          | 6/1848 [08:05<35:49:29, 70.02s/it]n_commands 1848
sample command python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_3/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_3/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_3/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_3/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_3/mlp.gate_proj/compressed.log 2>&1 &
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_3/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_3/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_3/mlp.up_proj/compressed.log 2>&1 &
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_3/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_3/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_3/self_attn.q_proj/compressed.log 2>&1 &
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_3/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_3/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_3/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf/layer_3/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_3/self_attn.q_proj/compressed.log
best_loss 69.29193115234375 running bpv: 2.014652
COMMANDS_FINISHED 1 n_commands 1848
meta-llama/Llama-2-7b-hf/layer_3/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_3/self_attn.k_proj/compressed.log
best_loss 75.79568481445312 running bpv: 2.014652
COMMANDS_FINISHED 2 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_3/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_3/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_3/self_attn.o_proj/compressed.log 2>&1 &
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_3/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_3/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_3/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_3/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_3/self_attn.o_proj/compressed.log
best_loss 0.15544834733009338 running bpv: 2.014652
COMMANDS_FINISHED 3 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_3/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_3/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_3/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_3/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_3/mlp.gate_proj/compressed.log
best_loss 31.99759292602539 running bpv: 2.012043
COMMANDS_FINISHED 4 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_5/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_5/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_5/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_3/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_3/mlp.up_proj/compressed.log
best_loss 27.284832000732422 running bpv: 2.011109
COMMANDS_FINISHED 5 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_5/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_5/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_5/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_3/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_3/self_attn.v_proj/compressed.log
best_loss 19.549575805664062 running bpv: 2.011487
COMMANDS_FINISHED 6 n_commands 1848
  0%|          | 7/1848 [10:10<44:38:45, 87.30s/it]  0%|          | 8/1848 [10:45<36:20:17, 71.10s/it]  0%|          | 9/1848 [12:50<44:46:12, 87.64s/it]  1%|          | 11/1848 [13:10<26:12:50, 51.37s/it]  1%|          | 12/1848 [13:25<21:33:34, 42.27s/it]  1%|          | 13/1848 [15:30<32:39:11, 64.06s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_5/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_5/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_5/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_3/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_3/mlp.down_proj/compressed.log
best_loss 0.6368050575256348 running bpv: 2.010608
COMMANDS_FINISHED 7 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_5/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_5/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_5/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_5/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_5/self_attn.q_proj/compressed.log
best_loss 79.58731079101562 running bpv: 2.010917
COMMANDS_FINISHED 8 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_5/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_5/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_5/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_5/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_5/mlp.gate_proj/compressed.log
best_loss 58.073524475097656 running bpv: 2.010612
COMMANDS_FINISHED 9 n_commands 1848
meta-llama/Llama-2-7b-hf/layer_5/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_5/self_attn.k_proj/compressed.log
best_loss 86.75576782226562 running bpv: 2.010854
COMMANDS_FINISHED 10 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_5/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_5/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_5/mlp.down_proj/compressed.log 2>&1 &
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_5/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_5/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_5/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_5/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_5/mlp.up_proj/compressed.log
best_loss 46.29872512817383 running bpv: 2.010616
COMMANDS_FINISHED 11 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_8/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_8/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_8/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_5/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_5/self_attn.o_proj/compressed.log
best_loss 0.5195887088775635 running bpv: 2.010813
COMMANDS_FINISHED 12 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_8/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_8/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_8/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_5/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_5/self_attn.v_proj/compressed.log
best_loss 23.921884536743164 running bpv: 2.010992
COMMANDS_FINISHED 13 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_8/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_8/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_8/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
  1%|          | 14/1848 [18:15<46:41:37, 91.66s/it]  1%|          | 15/1848 [19:40<45:43:05, 89.79s/it]  1%|          | 16/1848 [19:55<34:47:49, 68.38s/it]  1%|          | 18/1848 [20:55<25:58:56, 51.11s/it]  1%|          | 19/1848 [22:20<30:09:26, 59.36s/it]  1%|          | 20/1848 [22:35<24:20:00, 47.92s/it]  1%|          | 21/1848 [25:10<38:57:09, 76.75s/it]meta-llama/Llama-2-7b-hf/layer_8/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_8/self_attn.q_proj/compressed.log
best_loss 126.93960571289062 running bpv: 2.011155
COMMANDS_FINISHED 14 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_8/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_8/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_8/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_8/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_8/mlp.gate_proj/compressed.log
best_loss 88.64012145996094 running bpv: 2.010939
COMMANDS_FINISHED 15 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_8/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_8/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_8/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_5/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_5/mlp.down_proj/compressed.log
best_loss 1.7330942153930664 running bpv: 2.01061
COMMANDS_FINISHED 16 n_commands 1848
meta-llama/Llama-2-7b-hf/layer_8/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_8/mlp.up_proj/compressed.log
best_loss 72.50320434570312 running bpv: 2.01048
COMMANDS_FINISHED 17 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_8/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_8/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_8/mlp.down_proj/compressed.log 2>&1 &
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_8/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_8/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_8/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_8/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_8/self_attn.k_proj/compressed.log
best_loss 128.60784912109375 running bpv: 2.010612
COMMANDS_FINISHED 18 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_12/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_12/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_12/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_8/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_8/self_attn.o_proj/compressed.log
best_loss 1.6807531118392944 running bpv: 2.010737
COMMANDS_FINISHED 19 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_12/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_12/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_12/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_8/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_8/self_attn.v_proj/compressed.log
best_loss 39.3253059387207 running bpv: 2.010854
COMMANDS_FINISHED 20 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_12/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_12/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_12/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_12/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_12/self_attn.q_proj/compressed.log
best_loss 165.55532836914062 running bpv: 2.010964
COMMANDS_FINISHED 21 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_12/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_12/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_12/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
  1%|          | 22/1848 [27:05<44:19:19, 87.38s/it]  1%|          | 23/1848 [27:30<35:18:54, 69.66s/it]  1%|‚ñè         | 24/1848 [27:45<27:18:05, 53.88s/it]  1%|‚ñè         | 25/1848 [28:50<28:55:54, 57.13s/it]  1%|‚ñè         | 26/1848 [29:45<28:35:59, 56.51s/it]  1%|‚ñè         | 27/1848 [30:20<25:21:53, 50.14s/it]  2%|‚ñè         | 28/1848 [32:55<41:06:30, 81.31s/it]  2%|‚ñè         | 29/1848 [34:50<46:09:40, 91.36s/it]meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_8/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_8/mlp.down_proj/compressed.log
best_loss 4.111258506774902 running bpv: 2.010716
COMMANDS_FINISHED 22 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_12/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_12/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_12/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_12/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_12/mlp.gate_proj/compressed.log
best_loss 112.66766357421875 running bpv: 2.01061
COMMANDS_FINISHED 23 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_12/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_12/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_12/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_12/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_12/self_attn.k_proj/compressed.log
best_loss 176.85177612304688 running bpv: 2.010708
COMMANDS_FINISHED 24 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_12/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_12/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_12/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_12/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_12/mlp.up_proj/compressed.log
best_loss 103.37118530273438 running bpv: 2.010611
COMMANDS_FINISHED 25 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_1/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_1/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_1/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_12/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_12/self_attn.o_proj/compressed.log
best_loss 3.8630757331848145 running bpv: 2.010702
COMMANDS_FINISHED 26 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_1/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_1/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_1/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_12/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_12/self_attn.v_proj/compressed.log
best_loss 59.27488708496094 running bpv: 2.010789
COMMANDS_FINISHED 27 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_1/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_1/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_1/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_1/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_1/self_attn.q_proj/compressed.log
best_loss 2.5829238891601562 running bpv: 2.010872
COMMANDS_FINISHED 28 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_1/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_1/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_1/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_12/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_12/mlp.down_proj/compressed.log
best_loss 7.5037102699279785 running bpv: 2.01069
COMMANDS_FINISHED 29 n_commands 1848
  2%|‚ñè         | 30/1848 [35:25<37:38:17, 74.53s/it]  2%|‚ñè         | 31/1848 [35:40<28:38:00, 56.73s/it]  2%|‚ñè         | 32/1848 [36:05<23:49:39, 47.24s/it]  2%|‚ñè         | 33/1848 [37:30<29:31:10, 58.55s/it]  2%|‚ñè         | 34/1848 [38:15<27:27:30, 54.49s/it]  2%|‚ñè         | 35/1848 [40:50<42:37:10, 84.63s/it]  2%|‚ñè         | 36/1848 [42:25<44:09:47, 87.74s/it]  2%|‚ñè         | 37/1848 [42:40<33:09:58, 65.93s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_1/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_1/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_1/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_1/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_1/mlp.gate_proj/compressed.log
best_loss 6.716279029846191 running bpv: 2.010609
COMMANDS_FINISHED 30 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_1/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_1/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_1/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_1/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_1/self_attn.k_proj/compressed.log
best_loss 2.5841526985168457 running bpv: 2.010685
COMMANDS_FINISHED 31 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_1/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_1/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_1/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_1/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_1/mlp.up_proj/compressed.log
best_loss 5.902160167694092 running bpv: 2.01061
COMMANDS_FINISHED 32 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_7/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_7/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_7/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_1/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_1/self_attn.o_proj/compressed.log
best_loss 0.035955555737018585 running bpv: 2.010682
COMMANDS_FINISHED 33 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_7/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_7/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_7/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_1/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_1/self_attn.v_proj/compressed.log
best_loss 0.2984074652194977 running bpv: 2.010751
COMMANDS_FINISHED 34 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_7/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_7/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_7/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_7/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_7/self_attn.q_proj/compressed.log
best_loss 129.08767700195312 running bpv: 2.010817
COMMANDS_FINISHED 35 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_7/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_7/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_7/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_7/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_7/mlp.gate_proj/compressed.log
best_loss 85.44684600830078 running bpv: 2.010743
COMMANDS_FINISHED 36 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_7/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_7/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_7/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_1/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_1/mlp.down_proj/compressed.log
best_loss 0.12558403611183167 running bpv: 2.010609
COMMANDS_FINISHED 37 n_commands 1848
  2%|‚ñè         | 38/1848 [43:25<29:59:37, 59.66s/it]  2%|‚ñè         | 39/1848 [44:00<26:15:44, 52.26s/it]  2%|‚ñè         | 40/1848 [45:05<28:10:05, 56.09s/it]  2%|‚ñè         | 41/1848 [46:00<27:59:26, 55.76s/it]  2%|‚ñè         | 42/1848 [48:35<42:54:43, 85.54s/it]  2%|‚ñè         | 43/1848 [50:00<42:48:31, 85.38s/it]  2%|‚ñè         | 44/1848 [50:25<33:42:33, 67.27s/it]  2%|‚ñè         | 45/1848 [51:10<30:20:46, 60.59s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_7/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_7/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_7/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_7/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_7/self_attn.k_proj/compressed.log
best_loss 130.5091552734375 running bpv: 2.010671
COMMANDS_FINISHED 38 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_7/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_7/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_7/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_7/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_7/mlp.up_proj/compressed.log
best_loss 65.93283081054688 running bpv: 2.01061
COMMANDS_FINISHED 39 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_0/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_0/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_0/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_7/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_7/self_attn.o_proj/compressed.log
best_loss 1.1046137809753418 running bpv: 2.010669
COMMANDS_FINISHED 40 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_0/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_0/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_0/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_7/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_7/self_attn.v_proj/compressed.log
best_loss 38.37043762207031 running bpv: 2.010726
COMMANDS_FINISHED 41 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_0/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_0/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_0/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_0/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_0/self_attn.q_proj/compressed.log
best_loss 0.052734360098838806 running bpv: 2.010781
COMMANDS_FINISHED 42 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_0/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_0/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_0/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_7/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_7/mlp.down_proj/compressed.log
best_loss 3.3449251651763916 running bpv: 2.010663
COMMANDS_FINISHED 43 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_0/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_0/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_0/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_0/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_0/mlp.gate_proj/compressed.log
best_loss 1.5463067293167114 running bpv: 2.010609
COMMANDS_FINISHED 44 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_0/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_0/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_0/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_0/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_0/self_attn.k_proj/compressed.log
best_loss 0.0555756501853466 running bpv: 2.010661
  2%|‚ñè         | 46/1848 [51:25<23:29:02, 46.92s/it]  3%|‚ñé         | 47/1848 [52:40<27:41:16, 55.34s/it]  3%|‚ñé         | 48/1848 [53:45<29:07:21, 58.25s/it]  3%|‚ñé         | 49/1848 [56:20<43:36:50, 87.28s/it]  3%|‚ñé         | 50/1848 [57:35<41:45:07, 83.60s/it]  3%|‚ñé         | 51/1848 [57:50<31:27:26, 63.02s/it]  3%|‚ñé         | 52/1848 [58:55<31:44:16, 63.62s/it]  3%|‚ñé         | 53/1848 [59:10<24:26:56, 49.03s/it]COMMANDS_FINISHED 45 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_0/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_0/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_0/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_0/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_0/mlp.up_proj/compressed.log
best_loss 1.4914021492004395 running bpv: 2.01061
COMMANDS_FINISHED 46 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_31/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_31/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_31/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_0/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_0/self_attn.o_proj/compressed.log
best_loss 0.001604154473170638 running bpv: 2.01066
COMMANDS_FINISHED 47 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_31/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_31/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_31/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_0/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_0/self_attn.v_proj/compressed.log
best_loss 0.02216360904276371 running bpv: 2.010708
COMMANDS_FINISHED 48 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_31/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_31/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_31/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_31/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_31/self_attn.q_proj/compressed.log
best_loss 173.81451416015625 running bpv: 2.010756
COMMANDS_FINISHED 49 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_31/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_31/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_31/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_0/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_0/mlp.down_proj/compressed.log
best_loss 0.011959925293922424 running bpv: 2.010655
COMMANDS_FINISHED 50 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_31/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_31/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_31/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_31/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_31/mlp.gate_proj/compressed.log
best_loss 371.8102722167969 running bpv: 2.010608
COMMANDS_FINISHED 51 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_31/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_31/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_31/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_31/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_31/self_attn.k_proj/compressed.log
best_loss 188.3883056640625 running bpv: 2.010654
COMMANDS_FINISHED 52 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_31/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_31/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_31/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_31/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_31/mlp.up_proj/compressed.log
best_loss 331.427978515625 running bpv: 2.010609
COMMANDS_FINISHED  3%|‚ñé         | 54/1848 [1:00:15<26:49:27, 53.83s/it]  3%|‚ñé         | 55/1848 [1:01:30<29:58:28, 60.18s/it]  3%|‚ñé         | 56/1848 [1:04:05<44:07:11, 88.63s/it]  3%|‚ñé         | 57/1848 [1:05:00<39:04:35, 78.55s/it]  3%|‚ñé         | 58/1848 [1:05:45<34:03:08, 68.49s/it]  3%|‚ñé         | 59/1848 [1:06:40<32:01:27, 64.44s/it]  3%|‚ñé         | 60/1848 [1:06:55<24:38:25, 49.61s/it] 53 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_21/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_21/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_21/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_31/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_31/self_attn.o_proj/compressed.log
best_loss 21.305946350097656 running bpv: 2.010653
COMMANDS_FINISHED 54 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_21/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_21/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_21/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_31/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_31/self_attn.v_proj/compressed.log
best_loss 132.89285278320312 running bpv: 2.010695
COMMANDS_FINISHED 55 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_21/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_21/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_21/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_21/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_21/self_attn.q_proj/compressed.log
best_loss 207.49642944335938 running bpv: 2.010737
COMMANDS_FINISHED 56 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_21/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_21/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_21/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_31/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_31/mlp.down_proj/compressed.log
best_loss 157.01510620117188 running bpv: 2.010649
COMMANDS_FINISHED 57 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_21/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_21/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_21/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_21/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_21/mlp.gate_proj/compressed.log
best_loss 270.08795166015625 running bpv: 2.010608
COMMANDS_FINISHED 58 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_21/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_21/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_21/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_21/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_21/self_attn.k_proj/compressed.log
best_loss 212.67990112304688 running bpv: 2.010648
COMMANDS_FINISHED 59 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_21/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_21/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_21/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_21/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_21/mlp.up_proj/compressed.log
best_loss 231.74664306640625 running bpv: 2.010609
COMMANDS_FINISHED 60 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_26/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_26/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_26/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_21/self_attn.o_proj is done
reading log   3%|‚ñé         | 61/1848 [1:07:40<23:56:29, 48.23s/it]  3%|‚ñé         | 62/1848 [1:09:15<30:53:27, 62.27s/it]  3%|‚ñé         | 63/1848 [1:11:50<44:40:14, 90.09s/it]  3%|‚ñé         | 64/1848 [1:13:05<42:24:13, 85.57s/it]  4%|‚ñé         | 65/1848 [1:13:20<31:53:45, 64.40s/it]  4%|‚ñé         | 66/1848 [1:14:05<28:59:55, 58.58s/it]  4%|‚ñé         | 67/1848 [1:14:30<23:59:57, 48.51s/it]  4%|‚ñé         | 68/1848 [1:15:45<27:55:01, 56.46s/it]/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_21/self_attn.o_proj/compressed.log
best_loss 7.050443649291992 running bpv: 2.010648
COMMANDS_FINISHED 61 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_26/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_26/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_26/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_21/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_21/self_attn.v_proj/compressed.log
best_loss 132.16412353515625 running bpv: 2.010685
COMMANDS_FINISHED 62 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_26/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_26/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_26/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_26/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_26/self_attn.q_proj/compressed.log
best_loss 253.88302612304688 running bpv: 2.010723
COMMANDS_FINISHED 63 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_26/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_26/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_26/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_21/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_21/mlp.down_proj/compressed.log
best_loss 33.356658935546875 running bpv: 2.010645
COMMANDS_FINISHED 64 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_26/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_26/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_26/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_26/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_26/mlp.gate_proj/compressed.log
best_loss 376.36395263671875 running bpv: 2.010608
COMMANDS_FINISHED 65 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_26/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_26/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_26/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_26/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_26/mlp.up_proj/compressed.log
best_loss 325.0013122558594 running bpv: 2.010574
COMMANDS_FINISHED 66 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_26/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_26/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_26/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_26/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_26/self_attn.k_proj/compressed.log
best_loss 259.28887939453125 running bpv: 2.010609
COMMANDS_FINISHED 67 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_14/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_14/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_14/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_26/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_26/self_attn.o_proj/compressed.log
best_loss 13.024301528930664 running bpv: 2.010644
COMMANDS_FINISHED 68 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_14/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_14/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_14/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
  4%|‚ñé         | 69/1848 [1:16:40<27:41:09, 56.03s/it]  4%|‚ñç         | 70/1848 [1:19:15<42:20:15, 85.72s/it]  4%|‚ñç         | 71/1848 [1:20:40<42:12:31, 85.51s/it]  4%|‚ñç         | 72/1848 [1:20:55<31:45:01, 64.36s/it]  4%|‚ñç         | 73/1848 [1:21:50<30:20:58, 61.55s/it]  4%|‚ñç         | 74/1848 [1:22:25<26:24:29, 53.59s/it]  4%|‚ñç         | 75/1848 [1:23:20<26:36:11, 54.02s/it]  4%|‚ñç         | 76/1848 [1:24:25<28:12:43, 57.32s/it]meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_26/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_26/self_attn.v_proj/compressed.log
best_loss 204.91920471191406 running bpv: 2.010678
COMMANDS_FINISHED 69 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_14/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_14/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_14/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_14/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_14/self_attn.q_proj/compressed.log
best_loss 175.35794067382812 running bpv: 2.010711
COMMANDS_FINISHED 70 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_14/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_14/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_14/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_26/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_26/mlp.down_proj/compressed.log
best_loss 51.87156295776367 running bpv: 2.010641
COMMANDS_FINISHED 71 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_14/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_14/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_14/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_14/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_14/mlp.gate_proj/compressed.log
best_loss 130.00714111328125 running bpv: 2.010608
COMMANDS_FINISHED 72 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_14/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_14/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_14/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_14/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_14/self_attn.k_proj/compressed.log
best_loss 182.947021484375 running bpv: 2.010641
COMMANDS_FINISHED 73 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_14/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_14/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_14/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_14/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_14/mlp.up_proj/compressed.log
best_loss 123.47047424316406 running bpv: 2.010609
COMMANDS_FINISHED 74 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_15/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_15/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_15/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_14/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_14/self_attn.o_proj/compressed.log
best_loss 5.3282060623168945 running bpv: 2.01064
COMMANDS_FINISHED 75 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_15/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_15/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_15/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_14/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_14/self_attn.v_proj/compressed.log
best_loss 66.44961547851562 running bpv: 2.010671
COMMANDS_FINISHED 76 n_commands 1848
  4%|‚ñç         | 77/1848 [1:27:00<42:36:57, 86.63s/it]  4%|‚ñç         | 78/1848 [1:28:05<39:24:13, 80.14s/it]  4%|‚ñç         | 79/1848 [1:29:00<35:40:36, 72.60s/it]  4%|‚ñç         | 80/1848 [1:29:35<30:07:03, 61.33s/it]  4%|‚ñç         | 81/1848 [1:29:50<23:16:48, 47.43s/it]  4%|‚ñç         | 82/1848 [1:30:45<24:22:58, 49.70s/it]  4%|‚ñç         | 83/1848 [1:32:10<29:33:44, 60.30s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_15/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_15/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_15/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_15/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_15/self_attn.q_proj/compressed.log
best_loss 165.3943634033203 running bpv: 2.010701
COMMANDS_FINISHED 77 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_15/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_15/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_15/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_14/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_14/mlp.down_proj/compressed.log
best_loss 10.85997200012207 running bpv: 2.010638
COMMANDS_FINISHED 78 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_15/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_15/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_15/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_15/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_15/mlp.gate_proj/compressed.log
best_loss 141.84625244140625 running bpv: 2.010608
COMMANDS_FINISHED 79 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_15/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_15/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_15/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_15/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_15/self_attn.k_proj/compressed.log
best_loss 178.5079345703125 running bpv: 2.010638
COMMANDS_FINISHED 80 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_15/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_15/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_15/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_15/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_15/mlp.up_proj/compressed.log
best_loss 134.73837280273438 running bpv: 2.010609
COMMANDS_FINISHED 81 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_4/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_4/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_4/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_15/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_15/self_attn.o_proj/compressed.log
best_loss 5.773621559143066 running bpv: 2.010637
COMMANDS_FINISHED 82 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_4/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_4/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_4/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_15/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_15/self_attn.v_proj/compressed.log
best_loss 69.85176086425781 running bpv: 2.010666
COMMANDS_FINISHED 83 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_4/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_4/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_4/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
  5%|‚ñç         | 84/1848 [1:34:45<43:28:12, 88.71s/it]  5%|‚ñç         | 85/1848 [1:36:20<44:22:16, 90.61s/it]  5%|‚ñç         | 87/1848 [1:37:10<29:30:40, 60.33s/it]  5%|‚ñç         | 88/1848 [1:37:25<24:00:03, 49.09s/it]  5%|‚ñç         | 89/1848 [1:39:01<29:51:21, 61.10s/it]  5%|‚ñç         | 90/1848 [1:39:46<27:42:04, 56.73s/it]  5%|‚ñç         | 91/1848 [1:42:21<41:06:38, 84.23s/it]meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_4/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_4/self_attn.q_proj/compressed.log
best_loss 68.12787628173828 running bpv: 2.010694
COMMANDS_FINISHED 84 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_4/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_4/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_4/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_15/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_15/mlp.down_proj/compressed.log
best_loss 13.571081161499023 running bpv: 2.010635
COMMANDS_FINISHED 85 n_commands 1848
meta-llama/Llama-2-7b-hf/layer_4/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_4/mlp.gate_proj/compressed.log
best_loss 45.80002975463867 running bpv: 2.010608
COMMANDS_FINISHED 86 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_4/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_4/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_4/self_attn.o_proj/compressed.log 2>&1 &
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_4/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_4/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_4/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_4/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_4/mlp.up_proj/compressed.log
best_loss 36.928218841552734 running bpv: 2.010582
COMMANDS_FINISHED 87 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_4/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_4/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_4/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_4/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_4/self_attn.k_proj/compressed.log
best_loss 71.58419799804688 running bpv: 2.010609
COMMANDS_FINISHED 88 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_27/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_27/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_27/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_4/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_4/self_attn.o_proj/compressed.log
best_loss 0.33847036957740784 running bpv: 2.010635
COMMANDS_FINISHED 89 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_27/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_27/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_27/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_4/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_4/self_attn.v_proj/compressed.log
best_loss 19.691755294799805 running bpv: 2.010661
COMMANDS_FINISHED 90 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_27/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_27/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_27/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_27/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_27/self_attn.q_proj/compressed.log
best_loss 282.5318603515625 running bpv: 2.010687
COMMANDS_FINISHED 91 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_27/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_27/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_27/self_attn.k_proj/compressed.log 2>&1 &
  5%|‚ñç         | 92/1848 [1:43:46<41:11:45, 84.46s/it]  5%|‚ñå         | 94/1848 [1:44:56<30:19:25, 62.24s/it]  5%|‚ñå         | 95/1848 [1:45:31<27:05:05, 55.62s/it]  5%|‚ñå         | 96/1848 [1:46:26<26:59:35, 55.47s/it]  5%|‚ñå         | 97/1848 [1:47:31<28:13:36, 58.03s/it]  5%|‚ñå         | 98/1848 [1:50:06<41:18:00, 84.96s/it]  5%|‚ñå         | 99/1848 [1:51:11<38:31:26, 79.29s/it]meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_27/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_27/mlp.gate_proj/compressed.log
best_loss 400.8478698730469 running bpv: 2.01066
COMMANDS_FINISHED 92 n_commands 1848
meta-llama/Llama-2-7b-hf/layer_4/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_4/mlp.down_proj/compressed.log
best_loss 1.1686716079711914 running bpv: 2.010608
COMMANDS_FINISHED 93 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_27/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_27/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_27/self_attn.o_proj/compressed.log 2>&1 &
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_27/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_27/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_27/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_27/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_27/self_attn.k_proj/compressed.log
best_loss 289.3206787109375 running bpv: 2.010633
COMMANDS_FINISHED 94 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_27/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_27/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_27/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_27/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_27/mlp.up_proj/compressed.log
best_loss 348.5748291015625 running bpv: 2.010609
COMMANDS_FINISHED 95 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_2/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_2/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_2/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_27/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_27/self_attn.o_proj/compressed.log
best_loss 12.538642883300781 running bpv: 2.010633
COMMANDS_FINISHED 96 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_2/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_2/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_2/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_27/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_27/self_attn.v_proj/compressed.log
best_loss 213.80877685546875 running bpv: 2.010657
COMMANDS_FINISHED 97 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_2/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_2/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_2/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_2/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_2/self_attn.q_proj/compressed.log
best_loss 22.16316795349121 running bpv: 2.010681
COMMANDS_FINISHED 98 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_2/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_2/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_2/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_27/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_27/mlp.down_proj/compressed.log
best_loss 58.904876708984375 running bpv: 2.010631
COMMANDS_FINISHED 99 n_commands 1848
  5%|‚ñå         | 100/1848 [1:52:06<35:05:55, 72.29s/it]  5%|‚ñå         | 101/1848 [1:52:41<29:47:50, 61.40s/it]  6%|‚ñå         | 102/1848 [1:52:56<23:09:27, 47.75s/it]  6%|‚ñå         | 103/1848 [1:53:51<24:11:10, 49.90s/it]  6%|‚ñå         | 104/1848 [1:55:16<29:13:41, 60.33s/it]  6%|‚ñå         | 105/1848 [1:57:51<42:52:25, 88.55s/it]  6%|‚ñå         | 106/1848 [1:59:16<42:20:15, 87.49s/it]  6%|‚ñå         | 107/1848 [1:59:31<31:49:49, 65.82s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_2/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_2/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_2/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_2/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_2/mlp.gate_proj/compressed.log
best_loss 17.226707458496094 running bpv: 2.010608
COMMANDS_FINISHED 100 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_2/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_2/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_2/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_2/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_2/self_attn.k_proj/compressed.log
best_loss 24.744766235351562 running bpv: 2.010631
COMMANDS_FINISHED 101 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_2/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_2/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_2/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_2/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_2/mlp.up_proj/compressed.log
best_loss 14.866157531738281 running bpv: 2.010608
COMMANDS_FINISHED 102 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_28/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_28/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_28/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_2/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_2/self_attn.o_proj/compressed.log
best_loss 0.0929529145359993 running bpv: 2.010631
COMMANDS_FINISHED 103 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_28/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_28/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_28/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_2/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_2/self_attn.v_proj/compressed.log
best_loss 6.177560329437256 running bpv: 2.010654
COMMANDS_FINISHED 104 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_28/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_28/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_28/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_28/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_28/self_attn.q_proj/compressed.log
best_loss 275.7471008300781 running bpv: 2.010676
COMMANDS_FINISHED 105 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_28/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_28/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_28/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_28/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_28/mlp.gate_proj/compressed.log
best_loss 415.91650390625 running bpv: 2.010653
COMMANDS_FINISHED 106 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_28/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_28/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_28/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_2/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_2/mlp.down_proj/compressed.log
best_loss 0.2989560067653656 running bpv: 2.010608
  6%|‚ñå         | 108/1848 [2:00:16<28:48:05, 59.59s/it]  6%|‚ñå         | 109/1848 [2:00:31<22:20:04, 46.24s/it]  6%|‚ñå         | 110/1848 [2:01:46<26:29:07, 54.86s/it]  6%|‚ñå         | 111/1848 [2:02:51<27:56:17, 57.90s/it]  6%|‚ñå         | 112/1848 [2:05:26<41:57:51, 87.02s/it]  6%|‚ñå         | 113/1848 [2:06:51<41:38:58, 86.42s/it]COMMANDS_FINISHED 107 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_28/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_28/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_28/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_28/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_28/mlp.up_proj/compressed.log
best_loss 372.24920654296875 running bpv: 2.010587
COMMANDS_FINISHED 108 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_28/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_28/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_28/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_28/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_28/self_attn.k_proj/compressed.log
best_loss 283.6520690917969 running bpv: 2.010608
COMMANDS_FINISHED 109 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_18/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_18/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_18/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_28/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_28/self_attn.o_proj/compressed.log
best_loss 16.619781494140625 running bpv: 2.01063
COMMANDS_FINISHED 110 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_18/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_18/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_18/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_28/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_28/self_attn.v_proj/compressed.log
best_loss 234.99044799804688 running bpv: 2.010651
COMMANDS_FINISHED 111 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_18/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_18/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_18/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_18/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_18/self_attn.q_proj/compressed.log
best_loss 195.29544067382812 running bpv: 2.010672
COMMANDS_FINISHED 112 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_18/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_18/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_18/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_28/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_28/mlp.down_proj/compressed.log
best_loss 69.99522399902344 running bpv: 2.010628
COMMANDS_FINISHED 113 n_commands 1848
meta-llama/Llama-2-7b-hf/layer_18/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_18/mlp.gate_proj/compressed.log
best_loss 214.42446899414062 running bpv: 2.010608
COMMANDS_FINISHED 114 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_18/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_18/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_18/self_attn.o_proj/compressed.log 2>&1 &
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_18/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_18/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_18/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_18/self_attn.k_proj is done
reading log   6%|‚ñå         | 115/1848 [2:08:01<30:10:52, 62.70s/it]  6%|‚ñã         | 116/1848 [2:08:16<24:28:33, 50.87s/it]  6%|‚ñã         | 117/1848 [2:09:31<27:29:49, 57.19s/it]  6%|‚ñã         | 118/1848 [2:10:36<28:30:13, 59.31s/it]  6%|‚ñã         | 119/1848 [2:13:11<41:20:58, 86.09s/it]  6%|‚ñã         | 120/1848 [2:14:06<37:03:51, 77.22s/it]  7%|‚ñã         | 121/1848 [2:14:41<31:10:31, 64.99s/it]  7%|‚ñã         | 122/1848 [2:15:46<31:09:39, 64.99s/it]/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_18/self_attn.k_proj/compressed.log
best_loss 204.95590209960938 running bpv: 2.010628
COMMANDS_FINISHED 115 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_18/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_18/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_18/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_18/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_18/mlp.up_proj/compressed.log
best_loss 190.45147705078125 running bpv: 2.010608
COMMANDS_FINISHED 116 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_25/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_25/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_25/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_18/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_18/self_attn.o_proj/compressed.log
best_loss 6.664556503295898 running bpv: 2.010628
COMMANDS_FINISHED 117 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_25/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_25/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_25/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_18/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_18/self_attn.v_proj/compressed.log
best_loss 105.4830093383789 running bpv: 2.010648
COMMANDS_FINISHED 118 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_25/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_25/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_25/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_25/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_25/self_attn.q_proj/compressed.log
best_loss 272.71759033203125 running bpv: 2.010668
COMMANDS_FINISHED 119 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_25/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_25/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_25/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_18/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_18/mlp.down_proj/compressed.log
best_loss 24.173442840576172 running bpv: 2.010627
COMMANDS_FINISHED 120 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_25/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_25/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_25/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_25/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_25/mlp.gate_proj/compressed.log
best_loss 354.19146728515625 running bpv: 2.010608
COMMANDS_FINISHED 121 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_25/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_25/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_25/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_25/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_25/self_attn.k_proj/compressed.log
best_loss 275.3825988769531 running bpv: 2.010627
COMMANDS_FINISHED 122 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_25/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_25/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_25/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_25/mlp.up_proj is done
  7%|‚ñã         | 123/1848 [2:16:01<24:04:46, 50.25s/it]  7%|‚ñã         | 124/1848 [2:16:46<23:19:16, 48.70s/it]  7%|‚ñã         | 125/1848 [2:18:21<29:54:06, 62.48s/it]  7%|‚ñã         | 126/1848 [2:20:56<43:05:09, 90.08s/it]  7%|‚ñã         | 127/1848 [2:21:51<38:03:10, 79.60s/it]  7%|‚ñã         | 128/1848 [2:22:36<33:05:14, 69.25s/it]  7%|‚ñã         | 129/1848 [2:23:11<28:10:20, 59.00s/it]  7%|‚ñã         | 130/1848 [2:23:36<23:17:46, 48.82s/it]reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_25/mlp.up_proj/compressed.log
best_loss 305.45977783203125 running bpv: 2.010608
COMMANDS_FINISHED 123 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_30/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_30/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_30/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_25/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_25/self_attn.o_proj/compressed.log
best_loss 8.311108589172363 running bpv: 2.010627
COMMANDS_FINISHED 124 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_30/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_30/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_30/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_25/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_25/self_attn.v_proj/compressed.log
best_loss 206.60195922851562 running bpv: 2.010646
COMMANDS_FINISHED 125 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_30/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_30/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_30/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_30/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_30/self_attn.q_proj/compressed.log
best_loss 258.5396728515625 running bpv: 2.010665
COMMANDS_FINISHED 126 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_30/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_30/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_30/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_25/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_25/mlp.down_proj/compressed.log
best_loss 46.876976013183594 running bpv: 2.010626
COMMANDS_FINISHED 127 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_30/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_30/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_30/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_30/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_30/mlp.gate_proj/compressed.log
best_loss 442.7906494140625 running bpv: 2.010608
COMMANDS_FINISHED 128 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_30/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_30/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_30/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_30/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_30/mlp.up_proj/compressed.log
best_loss 396.9338684082031 running bpv: 2.01059
COMMANDS_FINISHED 129 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_30/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_30/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_30/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_30/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_30/self_attn.k_proj/compressed.log
best_loss 267.81201171875 running bpv: 2.010608
COMMANDS_FINISHED 130 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_29/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_29/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_29/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
  7%|‚ñã         | 131/1848 [2:24:31<24:10:04, 50.67s/it]  7%|‚ñã         | 132/1848 [2:25:46<27:37:55, 57.97s/it]  7%|‚ñã         | 133/1848 [2:28:21<41:28:45, 87.07s/it]  7%|‚ñã         | 134/1848 [2:29:56<42:35:22, 89.45s/it]  7%|‚ñã         | 135/1848 [2:30:11<31:56:24, 67.12s/it]  7%|‚ñã         | 136/1848 [2:30:56<28:46:01, 60.49s/it]  7%|‚ñã         | 138/1848 [2:32:36<26:26:10, 55.66s/it]meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_30/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_30/self_attn.o_proj/compressed.log
best_loss 19.87570571899414 running bpv: 2.010626
COMMANDS_FINISHED 131 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_29/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_29/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_29/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_30/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_30/self_attn.v_proj/compressed.log
best_loss 243.7506103515625 running bpv: 2.010644
COMMANDS_FINISHED 132 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_29/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_29/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_29/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_29/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_29/self_attn.q_proj/compressed.log
best_loss 241.94735717773438 running bpv: 2.010662
COMMANDS_FINISHED 133 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_29/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_29/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_29/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_30/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_30/mlp.down_proj/compressed.log
best_loss 104.67251586914062 running bpv: 2.010625
COMMANDS_FINISHED 134 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_29/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_29/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_29/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_29/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_29/mlp.gate_proj/compressed.log
best_loss 430.6133728027344 running bpv: 2.010608
COMMANDS_FINISHED 135 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_29/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_29/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_29/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_29/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_29/mlp.up_proj/compressed.log
best_loss 390.3703308105469 running bpv: 2.010591
COMMANDS_FINISHED 136 n_commands 1848
meta-llama/Llama-2-7b-hf/layer_29/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_29/self_attn.k_proj/compressed.log
best_loss 248.48719787597656 running bpv: 2.010608
COMMANDS_FINISHED 137 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_29/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_29/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_29/self_attn.v_proj/compressed.log 2>&1 &
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_16/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_16/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_16/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_29/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_29/self_attn.o_proj/compressed.log
best_loss 15.217544555664062 running bpv: 2.010625
COMMANDS_FINISHED 138 n_commands 1848
  8%|‚ñä         | 139/1848 [2:33:31<26:20:42, 55.50s/it]  8%|‚ñä         | 140/1848 [2:36:06<38:40:45, 81.53s/it]  8%|‚ñä         | 141/1848 [2:37:21<37:49:00, 79.75s/it]  8%|‚ñä         | 142/1848 [2:37:36<29:12:30, 61.64s/it]  8%|‚ñä         | 143/1848 [2:38:41<29:38:53, 62.60s/it]  8%|‚ñä         | 144/1848 [2:39:06<24:28:28, 51.71s/it]  8%|‚ñä         | 145/1848 [2:40:01<24:55:03, 52.67s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_16/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_16/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_16/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_29/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_29/self_attn.v_proj/compressed.log
best_loss 219.16796875 running bpv: 2.010642
COMMANDS_FINISHED 139 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_16/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_16/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_16/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_16/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_16/self_attn.q_proj/compressed.log
best_loss 172.33575439453125 running bpv: 2.010659
COMMANDS_FINISHED 140 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_16/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_16/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_16/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_16/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_16/mlp.gate_proj/compressed.log
best_loss 161.15602111816406 running bpv: 2.010642
COMMANDS_FINISHED 141 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_16/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_16/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_16/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_29/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_29/mlp.down_proj/compressed.log
best_loss 82.12153625488281 running bpv: 2.010608
COMMANDS_FINISHED 142 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_16/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_16/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_16/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_16/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_16/self_attn.k_proj/compressed.log
best_loss 183.03831481933594 running bpv: 2.010624
COMMANDS_FINISHED 143 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_16/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_16/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_16/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_16/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_16/mlp.up_proj/compressed.log
best_loss 150.57467651367188 running bpv: 2.010608
COMMANDS_FINISHED 144 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_13/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_13/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_13/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_16/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_16/self_attn.o_proj/compressed.log
best_loss 7.831895351409912 running bpv: 2.010624
COMMANDS_FINISHED 145 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_13/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_13/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_13/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_16/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_16/self_attn.v_proj/compressed.log  8%|‚ñä         | 146/1848 [2:41:16<28:01:02, 59.26s/it]  8%|‚ñä         | 147/1848 [2:43:51<41:24:42, 87.64s/it]  8%|‚ñä         | 148/1848 [2:44:56<38:12:28, 80.91s/it]  8%|‚ñä         | 149/1848 [2:45:41<33:07:55, 70.20s/it]  8%|‚ñä         | 150/1848 [2:46:26<29:33:43, 62.68s/it]  8%|‚ñä         | 152/1848 [2:47:36<23:31:28, 49.93s/it]  8%|‚ñä         | 153/1848 [2:49:01<27:35:55, 58.62s/it]
best_loss 79.95016479492188 running bpv: 2.010641
COMMANDS_FINISHED 146 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_13/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_13/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_13/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_13/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_13/self_attn.q_proj/compressed.log
best_loss 170.72171020507812 running bpv: 2.010656
COMMANDS_FINISHED 147 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_13/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_13/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_13/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_16/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_16/mlp.down_proj/compressed.log
best_loss 18.08148956298828 running bpv: 2.010624
COMMANDS_FINISHED 148 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_13/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_13/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_13/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_13/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_13/mlp.gate_proj/compressed.log
best_loss 118.44306945800781 running bpv: 2.010608
COMMANDS_FINISHED 149 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_13/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_13/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_13/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_13/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_13/self_attn.k_proj/compressed.log
best_loss 177.33175659179688 running bpv: 2.010624
COMMANDS_FINISHED 150 n_commands 1848
meta-llama/Llama-2-7b-hf/layer_13/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_13/mlp.up_proj/compressed.log
best_loss 111.76387023925781 running bpv: 2.010608
COMMANDS_FINISHED 151 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_13/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_13/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_13/self_attn.v_proj/compressed.log 2>&1 &
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_10/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_10/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_10/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_13/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_13/self_attn.o_proj/compressed.log
best_loss 4.334868907928467 running bpv: 2.010624
COMMANDS_FINISHED 152 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_10/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_10/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_10/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_13/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_13/self_attn.v_proj/compressed.log
best_loss 66.8857421875 running bpv: 2.010639
COMMANDS_FINISHED 153 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_10/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_10/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_10/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
  8%|‚ñä         | 154/1848 [2:51:36<39:25:56, 83.80s/it]  8%|‚ñä         | 155/1848 [2:52:51<38:17:08, 81.41s/it]  8%|‚ñä         | 156/1848 [2:53:06<29:32:05, 62.84s/it]  8%|‚ñä         | 157/1848 [2:54:11<29:48:30, 63.46s/it]  9%|‚ñä         | 159/1848 [2:55:31<24:49:51, 52.93s/it]  9%|‚ñä         | 160/1848 [2:56:46<27:19:55, 58.29s/it]  9%|‚ñä         | 161/1848 [2:59:21<38:59:36, 83.21s/it]meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_10/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_10/self_attn.q_proj/compressed.log
best_loss 140.03009033203125 running bpv: 2.010654
COMMANDS_FINISHED 154 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_10/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_10/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_10/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_10/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_10/mlp.gate_proj/compressed.log
best_loss 97.95689392089844 running bpv: 2.010639
COMMANDS_FINISHED 155 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_10/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_10/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_10/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_13/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_13/mlp.down_proj/compressed.log
best_loss 9.247007369995117 running bpv: 2.010608
COMMANDS_FINISHED 156 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_10/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_10/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_10/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_10/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_10/self_attn.k_proj/compressed.log
best_loss 149.17898559570312 running bpv: 2.010623
COMMANDS_FINISHED 157 n_commands 1848
meta-llama/Llama-2-7b-hf/layer_10/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_10/mlp.up_proj/compressed.log
best_loss 84.971923828125 running bpv: 2.010608
COMMANDS_FINISHED 158 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_10/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_10/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_10/self_attn.v_proj/compressed.log 2>&1 &
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_19/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_19/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_19/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_10/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_10/self_attn.o_proj/compressed.log
best_loss 3.3449947834014893 running bpv: 2.010623
COMMANDS_FINISHED 159 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_19/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_19/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_19/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_10/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_10/self_attn.v_proj/compressed.log
best_loss 44.31135559082031 running bpv: 2.010638
COMMANDS_FINISHED 160 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_19/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_19/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_19/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_19/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_19/self_attn.q_proj/compressed.log
best_loss 188.71734619140625 running bpv: 2.010652
COMMANDS_FINISHED 161 n_commands 1848
  9%|‚ñâ         | 162/1848 [3:00:26<36:40:39, 78.32s/it]  9%|‚ñâ         | 163/1848 [3:00:41<28:25:45, 60.74s/it]  9%|‚ñâ         | 164/1848 [3:01:56<30:18:30, 64.79s/it]  9%|‚ñâ         | 166/1848 [3:03:06<24:02:21, 51.45s/it]  9%|‚ñâ         | 167/1848 [3:04:31<27:49:25, 59.59s/it]  9%|‚ñâ         | 168/1848 [3:07:06<39:15:44, 84.13s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_19/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_19/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_19/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_10/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_10/mlp.down_proj/compressed.log
best_loss 5.663957595825195 running bpv: 2.010622
COMMANDS_FINISHED 162 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_19/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_19/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_19/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_19/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_19/mlp.gate_proj/compressed.log
best_loss 232.3431396484375 running bpv: 2.010608
COMMANDS_FINISHED 163 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_19/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_19/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_19/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_19/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_19/self_attn.k_proj/compressed.log
best_loss 197.11013793945312 running bpv: 2.010622
COMMANDS_FINISHED 164 n_commands 1848
meta-llama/Llama-2-7b-hf/layer_19/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_19/mlp.up_proj/compressed.log
best_loss 205.2159423828125 running bpv: 2.010608
COMMANDS_FINISHED 165 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_19/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_19/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_19/self_attn.v_proj/compressed.log 2>&1 &
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_6/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_6/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_6/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_19/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_19/self_attn.o_proj/compressed.log
best_loss 6.333401203155518 running bpv: 2.010622
COMMANDS_FINISHED 166 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_6/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_6/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_6/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_19/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_19/self_attn.v_proj/compressed.log
best_loss 106.68180847167969 running bpv: 2.010636
COMMANDS_FINISHED 167 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_6/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_6/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_6/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_6/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_6/self_attn.q_proj/compressed.log
best_loss 117.91098022460938 running bpv: 2.01065
COMMANDS_FINISHED 168 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_6/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_6/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_6/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_19/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_19/mlp.down_proj/compressed.log
  9%|‚ñâ         | 169/1848 [3:08:01<35:35:20, 76.31s/it]  9%|‚ñâ         | 170/1848 [3:08:26<28:56:05, 62.08s/it]  9%|‚ñâ         | 171/1848 [3:09:41<30:37:39, 65.75s/it]  9%|‚ñâ         | 173/1848 [3:10:41<23:08:40, 49.74s/it]  9%|‚ñâ         | 174/1848 [3:12:16<28:13:54, 60.71s/it]  9%|‚ñâ         | 175/1848 [3:14:51<39:29:06, 84.97s/it] 10%|‚ñâ         | 176/1848 [3:15:36<34:28:32, 74.23s/it]best_loss 27.096654891967773 running bpv: 2.010622
COMMANDS_FINISHED 169 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_6/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_6/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_6/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_6/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_6/mlp.gate_proj/compressed.log
best_loss 74.12422180175781 running bpv: 2.010608
COMMANDS_FINISHED 170 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_6/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_6/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_6/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_6/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_6/self_attn.k_proj/compressed.log
best_loss 123.02301025390625 running bpv: 2.010622
COMMANDS_FINISHED 171 n_commands 1848
meta-llama/Llama-2-7b-hf/layer_6/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_6/mlp.up_proj/compressed.log
best_loss 56.67805099487305 running bpv: 2.010608
COMMANDS_FINISHED 172 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_6/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_6/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_6/self_attn.v_proj/compressed.log 2>&1 &
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_11/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_11/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_11/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_6/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_6/self_attn.o_proj/compressed.log
best_loss 0.7431215643882751 running bpv: 2.010622
COMMANDS_FINISHED 173 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_11/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_11/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_11/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_6/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_6/self_attn.v_proj/compressed.log
best_loss 34.1881103515625 running bpv: 2.010635
COMMANDS_FINISHED 174 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_11/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_11/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_11/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_11/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_11/self_attn.q_proj/compressed.log
best_loss 157.06134033203125 running bpv: 2.010649
COMMANDS_FINISHED 175 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_11/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_11/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_11/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_6/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_6/mlp.down_proj/compressed.log
best_loss 2.515145778656006 running bpv: 2.010621
COMMANDS_FINISHED 176 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_11/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_11/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_11/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_11/mlp.gate_proj is done
reading log  10%|‚ñâ         | 177/1848 [3:16:21<30:41:35, 66.13s/it] 10%|‚ñâ         | 178/1848 [3:17:07<27:53:42, 60.13s/it] 10%|‚ñâ         | 179/1848 [3:17:32<23:10:50, 50.00s/it] 10%|‚ñâ         | 180/1848 [3:18:17<22:29:30, 48.54s/it] 10%|‚ñâ         | 181/1848 [3:19:42<27:26:48, 59.27s/it] 10%|‚ñâ         | 182/1848 [3:22:27<41:54:45, 90.57s/it] 10%|‚ñâ         | 183/1848 [3:23:42<39:44:58, 85.95s/it] 10%|‚ñâ         | 184/1848 [3:23:57<29:57:16, 64.81s/it]/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_11/mlp.gate_proj/compressed.log
best_loss 106.32974243164062 running bpv: 2.010608
COMMANDS_FINISHED 177 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_11/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_11/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_11/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_11/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_11/mlp.up_proj/compressed.log
best_loss 94.64886474609375 running bpv: 2.010595
COMMANDS_FINISHED 178 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_11/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_11/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_11/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_11/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_11/self_attn.k_proj/compressed.log
best_loss 156.44589233398438 running bpv: 2.010608
COMMANDS_FINISHED 179 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_20/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_20/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_20/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_11/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_11/self_attn.o_proj/compressed.log
best_loss 3.408346176147461 running bpv: 2.010621
COMMANDS_FINISHED 180 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_20/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_20/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_20/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_11/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_11/self_attn.v_proj/compressed.log
best_loss 59.8790397644043 running bpv: 2.010634
COMMANDS_FINISHED 181 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_20/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_20/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_20/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_20/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_20/self_attn.q_proj/compressed.log
best_loss 195.01754760742188 running bpv: 2.010647
COMMANDS_FINISHED 182 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_20/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_20/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_20/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_11/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_11/mlp.down_proj/compressed.log
best_loss 6.452680587768555 running bpv: 2.01062
COMMANDS_FINISHED 183 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_20/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_20/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_20/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_20/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_20/mlp.gate_proj/compressed.log
best_loss 248.21046447753906 running bpv: 2.010608
COMMANDS_FINISHED 184 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_20/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_20/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_20/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
 10%|‚ñà         | 185/1848 [3:24:42<27:12:21, 58.89s/it] 10%|‚ñà         | 186/1848 [3:25:07<22:30:42, 48.76s/it] 10%|‚ñà         | 187/1848 [3:26:22<26:07:22, 56.62s/it] 10%|‚ñà         | 188/1848 [3:27:17<25:53:06, 56.14s/it] 10%|‚ñà         | 189/1848 [3:29:52<39:31:27, 85.77s/it] 10%|‚ñà         | 190/1848 [3:31:07<38:00:56, 82.54s/it] 10%|‚ñà         | 191/1848 [3:31:32<30:03:08, 65.29s/it] 10%|‚ñà         | 192/1848 [3:32:27<28:36:56, 62.21s/it]meta-llama/Llama-2-7b-hf/layer_20/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_20/mlp.up_proj/compressed.log
best_loss 216.62899780273438 running bpv: 2.010595
COMMANDS_FINISHED 185 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_20/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_20/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_20/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_20/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_20/self_attn.k_proj/compressed.log
best_loss 203.06459045410156 running bpv: 2.010608
COMMANDS_FINISHED 186 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_23/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_23/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_23/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_20/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_20/self_attn.o_proj/compressed.log
best_loss 7.705390930175781 running bpv: 2.010621
COMMANDS_FINISHED 187 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_23/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_23/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_23/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_20/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_20/self_attn.v_proj/compressed.log
best_loss 111.33354187011719 running bpv: 2.010633
COMMANDS_FINISHED 188 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_23/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_23/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_23/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_23/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_23/self_attn.q_proj/compressed.log
best_loss 247.87661743164062 running bpv: 2.010646
COMMANDS_FINISHED 189 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_23/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_23/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_23/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_20/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_20/mlp.down_proj/compressed.log
best_loss 31.86086654663086 running bpv: 2.01062
COMMANDS_FINISHED 190 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_23/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_23/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_23/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_23/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_23/mlp.gate_proj/compressed.log
best_loss 310.1049499511719 running bpv: 2.010608
COMMANDS_FINISHED 191 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_23/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_23/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_23/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_23/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_23/self_attn.k_proj/compressed.log
best_loss 252.89199829101562 running bpv: 2.01062
COMMANDS_FINISHED 192 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_23/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_23/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_23/self_attn.v_proj/compressed.log 2>&1 &
 10%|‚ñà         | 193/1848 [3:32:52<23:28:08, 51.05s/it] 10%|‚ñà         | 194/1848 [3:33:47<24:00:01, 52.24s/it] 11%|‚ñà         | 195/1848 [3:35:02<27:07:20, 59.07s/it] 11%|‚ñà         | 196/1848 [3:37:37<40:18:49, 87.85s/it] 11%|‚ñà         | 197/1848 [3:38:52<38:31:22, 84.00s/it] 11%|‚ñà         | 198/1848 [3:39:27<31:45:48, 69.30s/it] 11%|‚ñà         | 199/1848 [3:40:12<28:24:20, 62.01s/it]meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_23/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_23/mlp.up_proj/compressed.log
best_loss 265.72314453125 running bpv: 2.010608
COMMANDS_FINISHED 193 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_17/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_17/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_17/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_23/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_23/self_attn.o_proj/compressed.log
best_loss 8.825711250305176 running bpv: 2.01062
COMMANDS_FINISHED 194 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_17/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_17/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_17/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_23/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_23/self_attn.v_proj/compressed.log
best_loss 170.78839111328125 running bpv: 2.010632
COMMANDS_FINISHED 195 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_17/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_17/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_17/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_17/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_17/self_attn.q_proj/compressed.log
best_loss 180.06478881835938 running bpv: 2.010644
COMMANDS_FINISHED 196 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_17/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_17/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_17/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_23/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_23/mlp.down_proj/compressed.log
best_loss 40.61639404296875 running bpv: 2.01062
COMMANDS_FINISHED 197 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_17/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_17/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_17/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_17/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_17/mlp.gate_proj/compressed.log
best_loss 187.82781982421875 running bpv: 2.010608
COMMANDS_FINISHED 198 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_17/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_17/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_17/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_17/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_17/mlp.up_proj/compressed.log
best_loss 171.08407592773438 running bpv: 2.010596
COMMANDS_FINISHED 199 n_commands 1848
meta-llama/Llama-2-7b-hf/layer_17/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_17/self_attn.k_proj/compressed.log
best_loss 188.9166717529297 running bpv: 2.010608
COMMANDS_FINISHED 200 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_17/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_17/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_17/self_attn.v_proj/compressed.log 2>&1 &
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_9/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_9/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_9/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
 11%|‚ñà         | 201/1848 [3:41:32<23:43:27, 51.86s/it] 11%|‚ñà         | 202/1848 [3:42:47<26:20:04, 57.60s/it] 11%|‚ñà         | 203/1848 [3:45:22<37:57:40, 83.08s/it] 11%|‚ñà         | 204/1848 [3:46:37<36:56:10, 80.88s/it] 11%|‚ñà         | 205/1848 [3:46:52<28:30:00, 62.45s/it] 11%|‚ñà         | 206/1848 [3:47:57<28:48:59, 63.18s/it] 11%|‚ñà‚ñè        | 208/1848 [3:49:17<24:02:19, 52.77s/it]meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_17/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_17/self_attn.o_proj/compressed.log
best_loss 5.848226070404053 running bpv: 2.01062
COMMANDS_FINISHED 201 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_9/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_9/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_9/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_17/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_17/self_attn.v_proj/compressed.log
best_loss 84.65616607666016 running bpv: 2.010631
COMMANDS_FINISHED 202 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_9/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_9/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_9/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_9/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_9/self_attn.q_proj/compressed.log
best_loss 135.243896484375 running bpv: 2.010643
COMMANDS_FINISHED 203 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_9/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_9/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_9/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_9/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_9/mlp.gate_proj/compressed.log
best_loss 94.2185287475586 running bpv: 2.010631
COMMANDS_FINISHED 204 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_9/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_9/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_9/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_17/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_17/mlp.down_proj/compressed.log
best_loss 20.352445602416992 running bpv: 2.010608
COMMANDS_FINISHED 205 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_9/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_9/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_9/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_9/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_9/self_attn.k_proj/compressed.log
best_loss 142.23751831054688 running bpv: 2.010619
COMMANDS_FINISHED 206 n_commands 1848
meta-llama/Llama-2-7b-hf/layer_9/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_9/mlp.up_proj/compressed.log
best_loss 79.61592102050781 running bpv: 2.010608
COMMANDS_FINISHED 207 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_9/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_9/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_9/self_attn.v_proj/compressed.log 2>&1 &
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_22/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_22/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_22/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_9/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_9/self_attn.o_proj/compressed.log
best_loss 2.3355958461761475 running bpv: 2.010619
COMMANDS_FINISHED 208 n_commands 1848
 11%|‚ñà‚ñè        | 209/1848 [3:50:32<26:29:03, 58.17s/it] 11%|‚ñà‚ñè        | 210/1848 [3:53:07<37:49:18, 83.13s/it] 11%|‚ñà‚ñè        | 211/1848 [3:54:12<35:34:57, 78.25s/it] 11%|‚ñà‚ñè        | 212/1848 [3:54:27<27:34:51, 60.69s/it] 12%|‚ñà‚ñè        | 213/1848 [3:55:42<29:24:38, 64.76s/it] 12%|‚ñà‚ñè        | 215/1848 [3:56:52<23:19:45, 51.43s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_22/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_22/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_22/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_9/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_9/self_attn.v_proj/compressed.log
best_loss 43.53614807128906 running bpv: 2.010631
COMMANDS_FINISHED 209 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_22/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_22/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_22/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_22/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_22/self_attn.q_proj/compressed.log
best_loss 224.0756072998047 running bpv: 2.010642
COMMANDS_FINISHED 210 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_22/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_22/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_22/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_9/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_9/mlp.down_proj/compressed.log
best_loss 4.8822712898254395 running bpv: 2.010619
COMMANDS_FINISHED 211 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_22/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_22/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_22/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_22/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_22/mlp.gate_proj/compressed.log
best_loss 289.09771728515625 running bpv: 2.010608
COMMANDS_FINISHED 212 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_22/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_22/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_22/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_22/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_22/self_attn.k_proj/compressed.log
best_loss 230.647216796875 running bpv: 2.010619
COMMANDS_FINISHED 213 n_commands 1848
meta-llama/Llama-2-7b-hf/layer_22/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_22/mlp.up_proj/compressed.log
best_loss 245.04483032226562 running bpv: 2.010608
COMMANDS_FINISHED 214 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_22/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_22/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_22/self_attn.v_proj/compressed.log 2>&1 &
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_24/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_24/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_24/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_22/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_22/self_attn.o_proj/compressed.log
best_loss 9.133853912353516 running bpv: 2.010619
COMMANDS_FINISHED 215 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_24/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_24/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_24/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_22/self_attn.v_proj is done
reading log  12%|‚ñà‚ñè        | 216/1848 [3:58:17<27:00:18, 59.57s/it] 12%|‚ñà‚ñè        | 217/1848 [4:00:52<38:06:40, 84.12s/it] 12%|‚ñà‚ñè        | 218/1848 [4:01:47<34:32:45, 76.30s/it] 12%|‚ñà‚ñè        | 219/1848 [4:02:12<28:05:12, 62.07s/it] 12%|‚ñà‚ñè        | 220/1848 [4:03:27<29:43:49, 65.74s/it] 12%|‚ñà‚ñè        | 222/1848 [4:04:27<22:28:00, 49.74s/it] 12%|‚ñà‚ñè        | 223/1848 [4:06:02<27:24:17, 60.71s/it]/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_22/self_attn.v_proj/compressed.log
best_loss 138.04763793945312 running bpv: 2.01063
COMMANDS_FINISHED 216 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_24/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_24/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_24/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_24/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_24/self_attn.q_proj/compressed.log
best_loss 229.44468688964844 running bpv: 2.010641
COMMANDS_FINISHED 217 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_24/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_24/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_24/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_22/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_22/mlp.down_proj/compressed.log
best_loss 37.697669982910156 running bpv: 2.010618
COMMANDS_FINISHED 218 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_24/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_24/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_24/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_24/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_24/mlp.gate_proj/compressed.log
best_loss 329.3626708984375 running bpv: 2.010608
COMMANDS_FINISHED 219 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_24/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_24/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_24/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_24/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_24/self_attn.k_proj/compressed.log
best_loss 232.95291137695312 running bpv: 2.010619
COMMANDS_FINISHED 220 n_commands 1848
meta-llama/Llama-2-7b-hf/layer_24/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_24/mlp.up_proj/compressed.log
best_loss 282.54522705078125 running bpv: 2.010608
COMMANDS_FINISHED 221 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/hessians_new/pajama/128/layer_24/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_24/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_24/self_attn.v_proj/compressed.log 2>&1 &
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_3/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_3/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_3/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_24/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_24/self_attn.o_proj/compressed.log
best_loss 9.930296897888184 running bpv: 2.010619
COMMANDS_FINISHED 222 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_3/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_3/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_3/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_24/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_24/self_attn.v_proj/compressed.log
best_loss 163.91827392578125 running bpv: 2.010629
COMMANDS_FINISHED 223 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_3/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_3/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_3/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
 12%|‚ñà‚ñè        | 224/1848 [4:09:17<42:58:07, 95.25s/it]meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-hf/layer_24/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_24/mlp.down_proj/compressed.log
best_loss 43.59532928466797 running bpv: 2.010608
COMMANDS_FINISHED 224 n_commands 1848
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_3/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_3/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_3/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-7b-hf
done with meta-llama/Llama-2-7b-hf
done with {'meta-llama/Llama-2-7b-hf/layer_3/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_3/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_3/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_3/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_3/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_3/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_3/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_3/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_3/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_3/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_3/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_3/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_3/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_3/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_5/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_5/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_5/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_5/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_5/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_5/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_5/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_5/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_5/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_5/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_5/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_5/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_8/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_8/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_8/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_8/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_5/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_5/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_8/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_8/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_8/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_8/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_8/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_8/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_8/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_8/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_12/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_12/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_8/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_8/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_12/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_12/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_12/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_12/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_12/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_12/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_12/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_12/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_12/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_12/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_1/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_1/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_12/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_12/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_1/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_1/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_1/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_1/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_1/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_1/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_1/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_1/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_1/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_1/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_7/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_7/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_7/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_7/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_1/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_1/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_7/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_7/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_7/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_7/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_7/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_7/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_7/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_7/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_0/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_0/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_7/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_7/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_0/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_0/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_0/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_0/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_0/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_0/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_0/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_0/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_0/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_0/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_31/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_31/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_0/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_0/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_31/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_31/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_31/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_31/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_31/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_31/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_31/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_31/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_31/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_31/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_21/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_21/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_31/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_31/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_21/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_21/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_21/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_21/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_21/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_21/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_21/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_21/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_21/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_21/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_26/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_26/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_21/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_21/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_26/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_26/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_26/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_26/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_26/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_26/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_26/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_26/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_26/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_26/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_14/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_14/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_26/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_26/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_14/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_14/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_14/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_14/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_14/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_14/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_14/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_14/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_14/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_14/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_15/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_15/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_14/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_14/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_15/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_15/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_15/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_15/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_15/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_15/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_15/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_15/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_15/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_15/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_4/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_4/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_15/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_15/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_4/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_4/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_4/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_4/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_4/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_4/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_4/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_4/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_4/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_4/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_27/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_27/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_27/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_27/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_4/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_4/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_27/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_27/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_27/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_27/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_27/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_27/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_27/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_27/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_2/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_2/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_27/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_27/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_2/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_2/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_2/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_2/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_2/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_2/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_2/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_2/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_2/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_2/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_28/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_28/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_28/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_28/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_2/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_2/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_28/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_28/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_28/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_28/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_28/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_28/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_28/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_28/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_18/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_18/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_28/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_28/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_18/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_18/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_18/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_18/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_18/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_18/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_18/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_18/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_18/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_18/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_25/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_25/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_18/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_18/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_25/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_25/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_25/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_25/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_25/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_25/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_25/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_25/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_25/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_25/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_30/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_30/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_25/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_25/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_30/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_30/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_30/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_30/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_30/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_30/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_30/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_30/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_30/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_30/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_29/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_29/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_30/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_30/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_29/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_29/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_29/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_29/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_29/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_29/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_29/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_29/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_29/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_29/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_16/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_16/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_16/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_16/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_29/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_29/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_16/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_16/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_16/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_16/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_16/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_16/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_16/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_16/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_13/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_13/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_16/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_16/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_13/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_13/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_13/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_13/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_13/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_13/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_13/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_13/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_13/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_13/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_10/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_10/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_10/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_10/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_13/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_13/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_10/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_10/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_10/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_10/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_10/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_10/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_10/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_10/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_19/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_19/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_10/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_10/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_19/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_19/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_19/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_19/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_19/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_19/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_19/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_19/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_19/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_19/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_6/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_6/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_19/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_19/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_6/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_6/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_6/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_6/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_6/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_6/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_6/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_6/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_6/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_6/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_11/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_11/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_6/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_6/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_11/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_11/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_11/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_11/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_11/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_11/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_11/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_11/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_11/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_11/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_20/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_20/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_11/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_11/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_20/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_20/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_20/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_20/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_20/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_20/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_20/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_20/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_20/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_20/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_23/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_23/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_20/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_20/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_23/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_23/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_23/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_23/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_23/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_23/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_23/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_23/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_23/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_23/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_17/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_17/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_23/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_23/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_17/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_17/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_17/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_17/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_17/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_17/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_17/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_17/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_17/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_17/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_9/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_9/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_9/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_9/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_17/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_17/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_9/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_9/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_9/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_9/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_9/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_9/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_9/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_9/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_22/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_22/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_9/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_9/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_22/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_22/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_22/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_22/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_22/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_22/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_22/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_22/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_22/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_22/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_24/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_24/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_22/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_22/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_24/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_24/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_24/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_24/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_24/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_24/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_24/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_24/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_24/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_24/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-7b-hf/layer_24/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-7b-hf/layer_24/mlp.down_proj/compressed.pt'} 12%|‚ñà‚ñè        | 225/1848 [4:09:52<35:39:02, 79.08s/it] 12%|‚ñà‚ñè        | 226/1848 [4:13:07<50:07:04, 111.24s/it] 12%|‚ñà‚ñè        | 227/1848 [4:13:52<41:37:27, 92.44s/it]  12%|‚ñà‚ñè        | 228/1848 [4:14:37<35:26:29, 78.76s/it] 12%|‚ñà‚ñè        | 229/1848 [4:17:02<44:06:54, 98.09s/it] 12%|‚ñà‚ñè        | 230/1848 [4:18:32<43:01:09, 95.72s/it]
/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/checkpoints.yaml
perplexity_inference_command:
 python -u perplexity_eval.py --base_model meta-llama/Llama-2-7b-hf --seqlen 4096 --checkpoint_list_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/checkpoints.yaml --log_wandb --wandb_project compression_no_finetune --wandb_id 29e1fiyr
meta-llama/Llama-2-13b-hf/layer_3/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_3/self_attn.q_proj/compressed.log
best_loss 29.961624145507812 running bpv: 2.010603
COMMANDS_FINISHED 225 n_commands 1849
running: nohup python -u perplexity_eval.py --base_model meta-llama/Llama-2-7b-hf --seqlen 4096 --checkpoint_list_path /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/checkpoints.yaml --log_wandb --wandb_project compression_no_finetune --wandb_id 29e1fiyr --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/amber-serenity-59/ppl_eval.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_3/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_3/self_attn.k_proj/compressed.log
best_loss 33.958656311035156 running bpv: 2.010598
COMMANDS_FINISHED 226 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_3/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_3/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_3/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_3/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_3/mlp.gate_proj/compressed.log
best_loss 32.708778381347656 running bpv: 2.010542
COMMANDS_FINISHED 227 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_3/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_3/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_3/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_3/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_3/mlp.up_proj/compressed.log
best_loss 27.98086929321289 running bpv: 2.010488
COMMANDS_FINISHED 228 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_3/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_3/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_3/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_3/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_3/self_attn.o_proj/compressed.log
best_loss 0.2677464485168457 running bpv: 2.010484
COMMANDS_FINISHED 229 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_5/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_5/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_5/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
eval is done
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_5/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_5/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_5/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_3/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_3/self_attn.v_proj/compressed.log
best_loss 9.344077110290527 running bpv: 2.01048
COMMANDS_FINISHED 231 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_5/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_5/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_5/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_5/self_attn.q_proj is done
reading log  12%|‚ñà‚ñé        | 231/1848 [4:22:27<61:30:34, 136.94s/it] 13%|‚ñà‚ñé        | 232/1848 [4:25:52<70:33:06, 157.17s/it] 13%|‚ñà‚ñé        | 233/1848 [4:26:27<54:10:36, 120.77s/it] 13%|‚ñà‚ñé        | 234/1848 [4:27:12<44:00:05, 98.14s/it]  13%|‚ñà‚ñé        | 235/1848 [4:28:07<38:11:42, 85.25s/it] 13%|‚ñà‚ñé        | 236/1848 [4:29:52<40:49:13, 91.16s/it] 13%|‚ñà‚ñé        | 237/1848 [4:31:07<38:37:47, 86.32s/it]/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_5/self_attn.q_proj/compressed.log
best_loss 79.51600646972656 running bpv: 2.010475
COMMANDS_FINISHED 232 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_5/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_5/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_5/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_3/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_3/mlp.down_proj/compressed.log
best_loss 0.9831059575080872 running bpv: 2.010425
COMMANDS_FINISHED 233 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_5/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_5/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_5/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_5/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_5/self_attn.k_proj/compressed.log
best_loss 77.92356872558594 running bpv: 2.010421
COMMANDS_FINISHED 234 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_5/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_5/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_5/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_5/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_5/mlp.gate_proj/compressed.log
best_loss 64.21192932128906 running bpv: 2.01037
COMMANDS_FINISHED 235 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_5/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_5/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_5/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_5/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_5/mlp.up_proj/compressed.log
best_loss 53.11825942993164 running bpv: 2.01032
COMMANDS_FINISHED 236 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_8/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_8/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_8/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_5/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_5/self_attn.o_proj/compressed.log
best_loss 0.6995044350624084 running bpv: 2.010316
COMMANDS_FINISHED 237 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_8/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_8/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_8/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_5/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_5/self_attn.v_proj/compressed.log
best_loss 28.086318969726562 running bpv: 2.010313
COMMANDS_FINISHED 238 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_8/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_8/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_8/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_8/self_attn.q_proj is done
reading log  13%|‚ñà‚ñé        | 238/1848 [4:35:02<58:31:59, 130.88s/it] 13%|‚ñà‚ñé        | 239/1848 [4:38:07<65:44:58, 147.11s/it] 13%|‚ñà‚ñé        | 240/1848 [4:38:22<48:01:00, 107.50s/it] 13%|‚ñà‚ñé        | 241/1848 [4:38:57<38:16:57, 85.76s/it]  13%|‚ñà‚ñé        | 242/1848 [4:40:12<36:49:12, 82.54s/it] 13%|‚ñà‚ñé        | 243/1848 [4:41:57<39:48:09, 89.28s/it] 13%|‚ñà‚ñé        | 244/1848 [4:42:52<35:11:52, 79.00s/it] 13%|‚ñà‚ñé        | 245/1848 [4:46:47<56:00:59, 125.80s/it]/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_8/self_attn.q_proj/compressed.log
best_loss 124.46751403808594 running bpv: 2.010309
COMMANDS_FINISHED 239 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_8/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_8/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_8/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_8/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_8/mlp.gate_proj/compressed.log
best_loss 109.74571228027344 running bpv: 2.010261
COMMANDS_FINISHED 240 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_8/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_8/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_8/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_5/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_5/mlp.down_proj/compressed.log
best_loss 2.5739529132843018 running bpv: 2.010215
COMMANDS_FINISHED 241 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_8/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_8/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_8/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_8/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_8/self_attn.k_proj/compressed.log
best_loss 128.59869384765625 running bpv: 2.010212
COMMANDS_FINISHED 242 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_8/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_8/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_8/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_8/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_8/mlp.up_proj/compressed.log
best_loss 90.18995666503906 running bpv: 2.010166
COMMANDS_FINISHED 243 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_39/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_39/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_39/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_8/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_8/self_attn.o_proj/compressed.log
best_loss 2.2578067779541016 running bpv: 2.010163
COMMANDS_FINISHED 244 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_39/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_39/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_39/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_8/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_8/self_attn.v_proj/compressed.log
best_loss 49.63524627685547 running bpv: 2.01016
COMMANDS_FINISHED 245 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_39/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_39/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_39/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_39/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_39/self_attn.q_proj/compressed.log
best_loss 180.03724670410156 running bpv: 2.010157
 13%|‚ñà‚ñé        | 246/1848 [4:50:12<66:33:23, 149.57s/it] 13%|‚ñà‚ñé        | 247/1848 [4:50:37<49:53:51, 112.20s/it] 13%|‚ñà‚ñé        | 249/1848 [4:51:57<35:02:10, 78.88s/it]  14%|‚ñà‚ñé        | 250/1848 [4:54:12<41:11:33, 92.80s/it] 14%|‚ñà‚ñé        | 251/1848 [4:54:37<33:18:04, 75.07s/it] 14%|‚ñà‚ñé        | 252/1848 [4:58:32<52:34:08, 118.58s/it]COMMANDS_FINISHED 246 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_39/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_39/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_39/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_8/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_8/mlp.down_proj/compressed.log
best_loss 6.397808074951172 running bpv: 2.010114
COMMANDS_FINISHED 247 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_39/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_39/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_39/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_39/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_39/mlp.gate_proj/compressed.log
best_loss 431.62164306640625 running bpv: 2.01007
COMMANDS_FINISHED 248 n_commands 1849
meta-llama/Llama-2-13b-hf/layer_39/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_39/self_attn.k_proj/compressed.log
best_loss 184.91729736328125 running bpv: 2.010067
COMMANDS_FINISHED 249 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_39/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_39/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_39/mlp.down_proj/compressed.log 2>&1 &
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_39/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_39/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_39/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_39/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_39/mlp.up_proj/compressed.log
best_loss 368.5518493652344 running bpv: 2.010024
COMMANDS_FINISHED 250 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_12/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_12/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_12/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_39/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_39/self_attn.o_proj/compressed.log
best_loss 44.01219177246094 running bpv: 2.010022
COMMANDS_FINISHED 251 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_12/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_12/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_12/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_39/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_39/self_attn.v_proj/compressed.log
best_loss 193.41571044921875 running bpv: 2.01002
COMMANDS_FINISHED 252 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_12/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_12/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_12/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_12/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_12/self_attn.q_proj/compressed.log
best_loss 165.6610565185547 running bpv: 2.010018
COMMANDS_FINISHED 253 n_commands 1849
 14%|‚ñà‚ñé        | 253/1848 [5:01:57<63:15:13, 142.77s/it] 14%|‚ñà‚ñé        | 254/1848 [5:02:22<48:19:21, 109.14s/it] 14%|‚ñà‚ñç        | 255/1848 [5:02:37<36:13:21, 81.86s/it]  14%|‚ñà‚ñç        | 256/1848 [5:04:32<40:29:33, 91.57s/it] 14%|‚ñà‚ñç        | 257/1848 [5:05:47<38:18:33, 86.68s/it] 14%|‚ñà‚ñç        | 258/1848 [5:06:32<32:49:45, 74.33s/it] 14%|‚ñà‚ñç        | 259/1848 [5:10:27<53:54:28, 122.13s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_12/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_12/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_12/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_12/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_12/mlp.gate_proj/compressed.log
best_loss 134.77255249023438 running bpv: 2.009976
COMMANDS_FINISHED 254 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_12/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_12/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_12/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_12/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_12/self_attn.k_proj/compressed.log
best_loss 171.758544921875 running bpv: 2.009974
COMMANDS_FINISHED 255 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_12/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_12/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_12/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_39/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_39/mlp.down_proj/compressed.log
best_loss 168.07931518554688 running bpv: 2.009935
COMMANDS_FINISHED 256 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_12/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_12/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_12/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_12/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_12/mlp.up_proj/compressed.log
best_loss 126.40052795410156 running bpv: 2.009894
COMMANDS_FINISHED 257 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_1/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_1/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_1/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_12/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_12/self_attn.o_proj/compressed.log
best_loss 6.113953590393066 running bpv: 2.009893
COMMANDS_FINISHED 258 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_1/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_1/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_1/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_12/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_12/self_attn.v_proj/compressed.log
best_loss 71.80314636230469 running bpv: 2.009891
COMMANDS_FINISHED 259 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_1/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_1/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_1/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_1/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_1/self_attn.q_proj/compressed.log
best_loss 2.123755931854248 running bpv: 2.009889
COMMANDS_FINISHED 260 n_commands 1849
 14%|‚ñà‚ñç        | 260/1848 [5:14:12<67:24:32, 152.82s/it] 14%|‚ñà‚ñç        | 261/1848 [5:14:27<49:12:58, 111.64s/it] 14%|‚ñà‚ñç        | 262/1848 [5:14:52<37:46:05, 85.73s/it]  14%|‚ñà‚ñç        | 263/1848 [5:15:47<33:41:41, 76.53s/it] 14%|‚ñà‚ñç        | 264/1848 [5:18:12<42:42:02, 97.05s/it] 14%|‚ñà‚ñç        | 265/1848 [5:18:47<34:29:51, 78.45s/it] 14%|‚ñà‚ñç        | 266/1848 [5:22:43<55:06:08, 125.39s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_1/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_1/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_1/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_12/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_12/mlp.down_proj/compressed.log
best_loss 11.341678619384766 running bpv: 2.009852
COMMANDS_FINISHED 261 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_1/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_1/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_1/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_1/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_1/self_attn.k_proj/compressed.log
best_loss 2.2020392417907715 running bpv: 2.00985
COMMANDS_FINISHED 262 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_1/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_1/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_1/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_1/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_1/mlp.gate_proj/compressed.log
best_loss 5.597511291503906 running bpv: 2.009812
COMMANDS_FINISHED 263 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_1/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_1/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_1/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_1/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_1/mlp.up_proj/compressed.log
best_loss 4.996666431427002 running bpv: 2.009775
COMMANDS_FINISHED 264 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_7/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_7/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_7/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_1/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_1/self_attn.o_proj/compressed.log
best_loss 0.05313887447118759 running bpv: 2.009773
COMMANDS_FINISHED 265 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_7/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_7/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_7/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_1/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_1/self_attn.v_proj/compressed.log
best_loss 0.564031183719635 running bpv: 2.009772
COMMANDS_FINISHED 266 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_7/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_7/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_7/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_7/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_7/self_attn.q_proj/compressed.log
best_loss 102.64671325683594 running bpv: 2.009771
COMMANDS_FINISHED 267 n_commands 1849
 14%|‚ñà‚ñç        | 267/1848 [5:25:48<62:55:10, 143.27s/it] 15%|‚ñà‚ñç        | 268/1848 [5:26:33<49:56:47, 113.80s/it] 15%|‚ñà‚ñç        | 269/1848 [5:26:48<36:55:04, 84.17s/it]  15%|‚ñà‚ñç        | 270/1848 [5:28:13<37:00:18, 84.42s/it] 15%|‚ñà‚ñç        | 271/1848 [5:29:38<37:03:32, 84.60s/it] 15%|‚ñà‚ñç        | 272/1848 [5:30:43<34:27:47, 78.72s/it] 15%|‚ñà‚ñç        | 273/1848 [5:34:38<54:57:17, 125.61s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_7/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_7/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_7/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_7/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_7/mlp.gate_proj/compressed.log
best_loss 96.93896484375 running bpv: 2.009734
COMMANDS_FINISHED 268 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_7/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_7/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_7/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_1/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_1/mlp.down_proj/compressed.log
best_loss 0.17576614022254944 running bpv: 2.0097
COMMANDS_FINISHED 269 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_7/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_7/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_7/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_7/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_7/self_attn.k_proj/compressed.log
best_loss 105.93560791015625 running bpv: 2.009699
COMMANDS_FINISHED 270 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_7/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_7/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_7/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_7/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_7/mlp.up_proj/compressed.log
best_loss 77.76620483398438 running bpv: 2.009664
COMMANDS_FINISHED 271 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_33/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_33/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_33/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_7/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_7/self_attn.o_proj/compressed.log
best_loss 1.749038815498352 running bpv: 2.009663
COMMANDS_FINISHED 272 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_33/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_33/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_33/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_7/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_7/self_attn.v_proj/compressed.log
best_loss 38.3995475769043 running bpv: 2.009662
COMMANDS_FINISHED 273 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_33/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_33/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_33/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_33/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_33/self_attn.q_proj/compressed.log
best_loss 296.15155029296875 running bpv: 2.009661
COMMANDS_FINISHED 274 n_commands 1849
 15%|‚ñà‚ñç        | 274/1848 [5:38:23<67:57:33, 155.43s/it] 15%|‚ñà‚ñç        | 275/1848 [5:38:38<49:30:31, 113.31s/it] 15%|‚ñà‚ñç        | 277/1848 [5:39:48<33:40:32, 77.17s/it]  15%|‚ñà‚ñå        | 278/1848 [5:42:13<40:59:24, 93.99s/it] 15%|‚ñà‚ñå        | 279/1848 [5:42:38<33:06:02, 75.95s/it] 15%|‚ñà‚ñå        | 280/1848 [5:46:33<51:55:33, 119.22s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_33/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_33/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_33/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_33/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_33/mlp.gate_proj/compressed.log
best_loss 497.54205322265625 running bpv: 2.009627
COMMANDS_FINISHED 275 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_33/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_33/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_33/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_7/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_7/mlp.down_proj/compressed.log
best_loss 5.01416015625 running bpv: 2.009594
COMMANDS_FINISHED 276 n_commands 1849
meta-llama/Llama-2-13b-hf/layer_33/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_33/self_attn.k_proj/compressed.log
best_loss 303.9653015136719 running bpv: 2.009593
COMMANDS_FINISHED 277 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_33/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_33/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_33/mlp.down_proj/compressed.log 2>&1 &
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_33/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_33/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_33/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_33/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_33/mlp.up_proj/compressed.log
best_loss 432.03228759765625 running bpv: 2.00956
COMMANDS_FINISHED 278 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_34/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_34/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_34/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_33/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_33/self_attn.o_proj/compressed.log
best_loss 12.692282676696777 running bpv: 2.00956
COMMANDS_FINISHED 279 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_34/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_34/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_34/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_33/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_33/self_attn.v_proj/compressed.log
best_loss 247.36648559570312 running bpv: 2.009559
COMMANDS_FINISHED 280 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_34/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_34/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_34/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_34/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_34/self_attn.q_proj/compressed.log
best_loss 300.33526611328125 running bpv: 2.009559
COMMANDS_FINISHED 281 n_commands 1849
 15%|‚ñà‚ñå        | 281/1848 [5:49:58<62:20:40, 143.23s/it] 15%|‚ñà‚ñå        | 282/1848 [5:50:33<48:51:35, 112.32s/it] 15%|‚ñà‚ñå        | 283/1848 [5:50:48<36:34:11, 84.12s/it]  15%|‚ñà‚ñå        | 284/1848 [5:52:23<37:55:52, 87.31s/it] 15%|‚ñà‚ñå        | 285/1848 [5:53:48<37:36:45, 86.63s/it] 15%|‚ñà‚ñå        | 286/1848 [5:54:43<33:31:17, 77.26s/it] 16%|‚ñà‚ñå        | 287/1848 [5:58:48<55:08:25, 127.17s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_34/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_34/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_34/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_34/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_34/mlp.gate_proj/compressed.log
best_loss 514.2415161132812 running bpv: 2.009527
COMMANDS_FINISHED 282 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_34/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_34/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_34/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_34/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_34/self_attn.k_proj/compressed.log
best_loss 307.4090576171875 running bpv: 2.009526
COMMANDS_FINISHED 283 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_34/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_34/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_34/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_33/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_33/mlp.down_proj/compressed.log
best_loss 73.0367431640625 running bpv: 2.009496
COMMANDS_FINISHED 284 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_34/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_34/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_34/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_34/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_34/mlp.up_proj/compressed.log
best_loss 456.5461120605469 running bpv: 2.009464
COMMANDS_FINISHED 285 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_0/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_0/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_0/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_34/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_34/self_attn.o_proj/compressed.log
best_loss 16.823265075683594 running bpv: 2.009464
COMMANDS_FINISHED 286 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_0/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_0/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_0/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_34/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_34/self_attn.v_proj/compressed.log
best_loss 272.5690612792969 running bpv: 2.009464
COMMANDS_FINISHED 287 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_0/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_0/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_0/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_0/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_0/self_attn.q_proj/compressed.log
best_loss 0.04488839954137802 running bpv: 2.009464
COMMANDS_FINISHED 288 n_commands 1849
 16%|‚ñà‚ñå        | 288/1848 [6:02:23<66:27:30, 153.37s/it] 16%|‚ñà‚ñå        | 289/1848 [6:02:38<48:30:53, 112.03s/it] 16%|‚ñà‚ñå        | 290/1848 [6:02:53<35:55:24, 83.01s/it]  16%|‚ñà‚ñå        | 291/1848 [6:03:48<32:16:29, 74.62s/it] 16%|‚ñà‚ñå        | 292/1848 [6:06:23<42:39:48, 98.71s/it] 16%|‚ñà‚ñå        | 293/1848 [6:06:48<33:05:43, 76.62s/it] 16%|‚ñà‚ñå        | 294/1848 [6:10:43<53:34:22, 124.11s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_0/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_0/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_0/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_34/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_34/mlp.down_proj/compressed.log
best_loss 81.28425598144531 running bpv: 2.009434
COMMANDS_FINISHED 289 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_0/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_0/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_0/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_0/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_0/mlp.gate_proj/compressed.log
best_loss 1.026031255722046 running bpv: 2.009404
COMMANDS_FINISHED 290 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_0/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_0/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_0/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_0/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_0/self_attn.k_proj/compressed.log
best_loss 0.030366256833076477 running bpv: 2.009404
COMMANDS_FINISHED 291 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_0/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_0/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_0/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_0/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_0/mlp.up_proj/compressed.log
best_loss 0.9664129614830017 running bpv: 2.009375
COMMANDS_FINISHED 292 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_32/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_32/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_32/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_0/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_0/self_attn.o_proj/compressed.log
best_loss 0.0016977095510810614 running bpv: 2.009375
COMMANDS_FINISHED 293 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_32/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_32/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_32/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_0/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_0/self_attn.v_proj/compressed.log
best_loss 0.010437104851007462 running bpv: 2.009375
COMMANDS_FINISHED 294 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_32/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_32/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_32/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_32/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_32/self_attn.q_proj/compressed.log
best_loss 320.9918212890625 running bpv: 2.009375
COMMANDS_FINISHED 295 n_commands 1849
 16%|‚ñà‚ñå        | 295/1848 [6:13:48<61:25:02, 142.37s/it] 16%|‚ñà‚ñå        | 296/1848 [6:14:23<47:29:49, 110.17s/it] 16%|‚ñà‚ñå        | 297/1848 [6:14:38<35:10:08, 81.63s/it]  16%|‚ñà‚ñå        | 298/1848 [6:16:43<40:44:57, 94.64s/it] 16%|‚ñà‚ñå        | 299/1848 [6:17:38<35:36:27, 82.75s/it] 16%|‚ñà‚ñå        | 300/1848 [6:18:33<32:00:20, 74.43s/it] 16%|‚ñà‚ñã        | 301/1848 [6:22:28<52:41:12, 122.61s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_32/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_32/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_32/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_32/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_32/mlp.gate_proj/compressed.log
best_loss 480.49072265625 running bpv: 2.009346
COMMANDS_FINISHED 296 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_32/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_32/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_32/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_0/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_0/mlp.down_proj/compressed.log
best_loss 0.016029203310608864 running bpv: 2.009319
COMMANDS_FINISHED 297 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_32/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_32/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_32/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_32/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_32/self_attn.k_proj/compressed.log
best_loss 325.8844909667969 running bpv: 2.009319
COMMANDS_FINISHED 298 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_32/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_32/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_32/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_32/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_32/mlp.up_proj/compressed.log
best_loss 413.33306884765625 running bpv: 2.009291
COMMANDS_FINISHED 299 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_31/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_31/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_31/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_32/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_32/self_attn.o_proj/compressed.log
best_loss 13.39853286743164 running bpv: 2.009291
COMMANDS_FINISHED 300 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_31/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_31/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_31/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_32/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_32/self_attn.v_proj/compressed.log
best_loss 276.3046569824219 running bpv: 2.009291
COMMANDS_FINISHED 301 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_31/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_31/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_31/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_31/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_31/self_attn.q_proj/compressed.log
best_loss 304.27728271484375 running bpv: 2.009292
COMMANDS_FINISHED 302 n_commands 1849
 16%|‚ñà‚ñã        | 302/1848 [6:26:03<64:33:28, 150.33s/it] 16%|‚ñà‚ñã        | 303/1848 [6:26:28<48:22:52, 112.73s/it] 16%|‚ñà‚ñã        | 304/1848 [6:27:03<38:20:58, 89.42s/it]  17%|‚ñà‚ñã        | 305/1848 [6:27:38<31:19:43, 73.09s/it] 17%|‚ñà‚ñã        | 306/1848 [6:29:53<39:15:53, 91.67s/it] 17%|‚ñà‚ñã        | 307/1848 [6:30:58<35:48:57, 83.67s/it] 17%|‚ñà‚ñã        | 308/1848 [6:34:53<55:12:56, 129.08s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_31/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_31/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_31/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_32/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_32/mlp.down_proj/compressed.log
best_loss 68.54423522949219 running bpv: 2.009265
COMMANDS_FINISHED 303 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_31/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_31/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_31/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_31/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_31/self_attn.k_proj/compressed.log
best_loss 315.6231384277344 running bpv: 2.009266
COMMANDS_FINISHED 304 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_31/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_31/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_31/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_31/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_31/mlp.gate_proj/compressed.log
best_loss 461.57305908203125 running bpv: 2.009239
COMMANDS_FINISHED 305 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_31/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_31/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_31/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_31/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_31/mlp.up_proj/compressed.log
best_loss 391.83392333984375 running bpv: 2.009212
COMMANDS_FINISHED 306 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_21/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_21/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_21/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_31/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_31/self_attn.o_proj/compressed.log
best_loss 12.917659759521484 running bpv: 2.009213
COMMANDS_FINISHED 307 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_21/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_21/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_21/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_31/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_31/self_attn.v_proj/compressed.log
best_loss 237.96389770507812 running bpv: 2.009213
COMMANDS_FINISHED 308 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_21/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_21/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_21/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_21/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_21/self_attn.q_proj/compressed.log
best_loss 209.80447387695312 running bpv: 2.009214
COMMANDS_FINISHED 309 n_commands 1849
 17%|‚ñà‚ñã        | 309/1848 [6:37:48<61:04:21, 142.86s/it] 17%|‚ñà‚ñã        | 310/1848 [6:38:33<48:29:29, 113.50s/it] 17%|‚ñà‚ñã        | 311/1848 [6:38:48<35:50:38, 83.95s/it]  17%|‚ñà‚ñã        | 312/1848 [6:40:03<34:40:33, 81.27s/it] 17%|‚ñà‚ñã        | 313/1848 [6:41:38<36:24:39, 85.39s/it] 17%|‚ñà‚ñã        | 314/1848 [6:42:43<33:46:53, 79.28s/it] 17%|‚ñà‚ñã        | 315/1848 [6:46:38<53:39:18, 126.00s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_21/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_21/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_21/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_21/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_21/mlp.gate_proj/compressed.log
best_loss 274.93408203125 running bpv: 2.009188
COMMANDS_FINISHED 310 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_21/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_21/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_21/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_31/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_31/mlp.down_proj/compressed.log
best_loss 64.0650634765625 running bpv: 2.009163
COMMANDS_FINISHED 311 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_21/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_21/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_21/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_21/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_21/self_attn.k_proj/compressed.log
best_loss 216.80078125 running bpv: 2.009164
COMMANDS_FINISHED 312 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_21/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_21/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_21/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_21/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_21/mlp.up_proj/compressed.log
best_loss 244.71630859375 running bpv: 2.009138
COMMANDS_FINISHED 313 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_26/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_26/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_26/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_21/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_21/self_attn.o_proj/compressed.log
best_loss 14.824429512023926 running bpv: 2.009139
COMMANDS_FINISHED 314 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_26/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_26/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_26/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_21/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_21/self_attn.v_proj/compressed.log
best_loss 128.972900390625 running bpv: 2.00914
COMMANDS_FINISHED 315 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_26/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_26/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_26/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_26/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_26/self_attn.q_proj/compressed.log
best_loss 251.649169921875 running bpv: 2.00914
COMMANDS_FINISHED 316 n_commands 1849
 17%|‚ñà‚ñã        | 316/1848 [6:50:03<63:42:28, 149.71s/it] 17%|‚ñà‚ñã        | 317/1848 [6:50:38<49:01:59, 115.30s/it] 17%|‚ñà‚ñã        | 319/1848 [6:51:38<32:15:00, 75.93s/it]  17%|‚ñà‚ñã        | 320/1848 [6:53:53<38:26:47, 90.58s/it] 17%|‚ñà‚ñã        | 321/1848 [6:54:38<33:21:55, 78.66s/it] 17%|‚ñà‚ñã        | 322/1848 [6:58:33<51:22:19, 121.19s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_26/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_26/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_26/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_26/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_26/mlp.gate_proj/compressed.log
best_loss 367.1477355957031 running bpv: 2.009116
COMMANDS_FINISHED 317 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_26/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_26/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_26/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_21/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_21/mlp.down_proj/compressed.log
best_loss 41.481239318847656 running bpv: 2.009092
COMMANDS_FINISHED 318 n_commands 1849
meta-llama/Llama-2-13b-hf/layer_26/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_26/self_attn.k_proj/compressed.log
best_loss 257.4083251953125 running bpv: 2.009093
COMMANDS_FINISHED 319 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_26/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_26/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_26/mlp.down_proj/compressed.log 2>&1 &
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_26/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_26/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_26/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_26/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_26/mlp.up_proj/compressed.log
best_loss 309.3045654296875 running bpv: 2.009069
COMMANDS_FINISHED 320 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_14/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_14/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_14/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_26/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_26/self_attn.o_proj/compressed.log
best_loss 12.20315933227539 running bpv: 2.00907
COMMANDS_FINISHED 321 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_14/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_14/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_14/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_26/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_26/self_attn.v_proj/compressed.log
best_loss 193.14840698242188 running bpv: 2.00907
COMMANDS_FINISHED 322 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_14/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_14/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_14/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_14/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_14/self_attn.q_proj/compressed.log
best_loss 194.48406982421875 running bpv: 2.009071
COMMANDS_FINISHED 323 n_commands 1849
 17%|‚ñà‚ñã        | 323/1848 [7:01:48<60:05:24, 141.85s/it] 18%|‚ñà‚ñä        | 324/1848 [7:02:33<48:20:30, 114.19s/it] 18%|‚ñà‚ñä        | 325/1848 [7:02:48<36:09:03, 85.45s/it]  18%|‚ñà‚ñä        | 326/1848 [7:04:03<34:50:04, 82.39s/it] 18%|‚ñà‚ñä        | 327/1848 [7:05:38<36:23:01, 86.12s/it] 18%|‚ñà‚ñä        | 328/1848 [7:06:43<33:43:05, 79.86s/it] 18%|‚ñà‚ñä        | 329/1848 [7:10:38<53:10:17, 126.02s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_14/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_14/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_14/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_14/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_14/mlp.gate_proj/compressed.log
best_loss 155.40321350097656 running bpv: 2.009048
COMMANDS_FINISHED 324 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_14/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_14/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_14/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_14/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_14/self_attn.k_proj/compressed.log
best_loss 201.694580078125 running bpv: 2.009048
COMMANDS_FINISHED 325 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_14/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_14/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_14/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_26/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_26/mlp.down_proj/compressed.log
best_loss 52.254295349121094 running bpv: 2.009026
COMMANDS_FINISHED 326 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_14/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_14/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_14/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_14/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_14/mlp.up_proj/compressed.log
best_loss 150.941650390625 running bpv: 2.009003
COMMANDS_FINISHED 327 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_15/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_15/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_15/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_14/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_14/self_attn.o_proj/compressed.log
best_loss 8.960054397583008 running bpv: 2.009004
COMMANDS_FINISHED 328 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_15/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_15/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_15/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_14/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_14/self_attn.v_proj/compressed.log
best_loss 86.74590301513672 running bpv: 2.009005
COMMANDS_FINISHED 329 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_15/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_15/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_15/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_15/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_15/self_attn.q_proj/compressed.log
best_loss 183.57217407226562 running bpv: 2.009006
COMMANDS_FINISHED 330 n_commands 1849
 18%|‚ñà‚ñä        | 330/1848 [7:14:03<63:04:16, 149.58s/it] 18%|‚ñà‚ñä        | 331/1848 [7:14:28<47:20:49, 112.36s/it] 18%|‚ñà‚ñä        | 332/1848 [7:14:43<35:03:08, 83.24s/it]  18%|‚ñà‚ñä        | 333/1848 [7:15:38<31:28:21, 74.79s/it] 18%|‚ñà‚ñä        | 334/1848 [7:17:53<39:02:22, 92.83s/it] 18%|‚ñà‚ñä        | 335/1848 [7:18:38<32:59:25, 78.50s/it] 18%|‚ñà‚ñä        | 336/1848 [7:22:33<52:40:36, 125.42s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_15/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_15/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_15/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_15/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_15/mlp.gate_proj/compressed.log
best_loss 167.79452514648438 running bpv: 2.008984
COMMANDS_FINISHED 331 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_15/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_15/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_15/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_14/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_14/mlp.down_proj/compressed.log
best_loss 15.76354694366455 running bpv: 2.008962
COMMANDS_FINISHED 332 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_15/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_15/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_15/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_15/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_15/self_attn.k_proj/compressed.log
best_loss 195.06573486328125 running bpv: 2.008963
COMMANDS_FINISHED 333 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_15/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_15/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_15/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_15/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_15/mlp.up_proj/compressed.log
best_loss 163.78656005859375 running bpv: 2.008941
COMMANDS_FINISHED 334 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_4/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_4/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_4/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_15/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_15/self_attn.o_proj/compressed.log
best_loss 10.169445037841797 running bpv: 2.008942
COMMANDS_FINISHED 335 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_4/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_4/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_4/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_15/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_15/self_attn.v_proj/compressed.log
best_loss 90.72882080078125 running bpv: 2.008943
COMMANDS_FINISHED 336 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_4/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_4/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_4/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_4/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_4/self_attn.q_proj/compressed.log
best_loss 67.34500122070312 running bpv: 2.008944
COMMANDS_FINISHED 337 n_commands 1849
 18%|‚ñà‚ñä        | 337/1848 [7:25:38<60:08:33, 143.29s/it] 18%|‚ñà‚ñä        | 338/1848 [7:26:23<47:44:22, 113.82s/it] 18%|‚ñà‚ñä        | 339/1848 [7:26:38<35:17:08, 84.18s/it]  18%|‚ñà‚ñä        | 340/1848 [7:27:53<34:06:36, 81.43s/it] 18%|‚ñà‚ñä        | 341/1848 [7:29:28<35:47:34, 85.50s/it] 19%|‚ñà‚ñä        | 342/1848 [7:30:33<33:11:51, 79.36s/it] 19%|‚ñà‚ñä        | 343/1848 [7:34:28<52:41:52, 126.05s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_4/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_4/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_4/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_4/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_4/mlp.gate_proj/compressed.log
best_loss 46.53514862060547 running bpv: 2.008923
COMMANDS_FINISHED 338 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_4/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_4/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_4/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_15/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_15/mlp.down_proj/compressed.log
best_loss 18.246658325195312 running bpv: 2.008903
COMMANDS_FINISHED 339 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_4/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_4/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_4/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_4/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_4/self_attn.k_proj/compressed.log
best_loss 68.94725036621094 running bpv: 2.008904
COMMANDS_FINISHED 340 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_4/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_4/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_4/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_4/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_4/mlp.up_proj/compressed.log
best_loss 39.488861083984375 running bpv: 2.008883
COMMANDS_FINISHED 341 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_27/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_27/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_27/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_4/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_4/self_attn.o_proj/compressed.log
best_loss 0.45388704538345337 running bpv: 2.008884
COMMANDS_FINISHED 342 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_27/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_27/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_27/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_4/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_4/self_attn.v_proj/compressed.log
best_loss 22.04465675354004 running bpv: 2.008885
COMMANDS_FINISHED 343 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_27/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_27/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_27/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_27/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_27/self_attn.q_proj/compressed.log
best_loss 287.65606689453125 running bpv: 2.008886
COMMANDS_FINISHED 344 n_commands 1849
 19%|‚ñà‚ñä        | 344/1848 [7:37:53<62:33:32, 149.74s/it] 19%|‚ñà‚ñä        | 345/1848 [7:38:18<46:53:41, 112.32s/it] 19%|‚ñà‚ñä        | 346/1848 [7:38:33<34:40:58, 83.13s/it]  19%|‚ñà‚ñâ        | 347/1848 [7:39:29<31:08:34, 74.69s/it] 19%|‚ñà‚ñâ        | 348/1848 [7:41:44<38:39:44, 92.79s/it] 19%|‚ñà‚ñâ        | 349/1848 [7:42:29<32:40:03, 78.45s/it] 19%|‚ñà‚ñâ        | 350/1848 [7:46:24<52:11:24, 125.42s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_27/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_27/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_27/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_27/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_27/mlp.gate_proj/compressed.log
best_loss 387.1518859863281 running bpv: 2.008866
COMMANDS_FINISHED 345 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_27/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_27/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_27/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_4/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_4/mlp.down_proj/compressed.log
best_loss 1.6036598682403564 running bpv: 2.008846
COMMANDS_FINISHED 346 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_27/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_27/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_27/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_27/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_27/self_attn.k_proj/compressed.log
best_loss 292.6958312988281 running bpv: 2.008847
COMMANDS_FINISHED 347 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_27/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_27/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_27/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_27/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_27/mlp.up_proj/compressed.log
best_loss 324.8235778808594 running bpv: 2.008827
COMMANDS_FINISHED 348 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_2/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_2/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_2/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_27/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_27/self_attn.o_proj/compressed.log
best_loss 10.034591674804688 running bpv: 2.008829
COMMANDS_FINISHED 349 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_2/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_2/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_2/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_27/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_27/self_attn.v_proj/compressed.log
best_loss 217.20404052734375 running bpv: 2.00883
COMMANDS_FINISHED 350 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_2/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_2/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_2/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_2/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_2/self_attn.q_proj/compressed.log
best_loss 17.65926742553711 running bpv: 2.008831
COMMANDS_FINISHED 351 n_commands 1849
 19%|‚ñà‚ñâ        | 351/1848 [7:49:29<59:35:22, 143.30s/it] 19%|‚ñà‚ñâ        | 352/1848 [7:50:14<47:17:46, 113.81s/it] 19%|‚ñà‚ñâ        | 353/1848 [7:50:29<34:57:16, 84.17s/it]  19%|‚ñà‚ñâ        | 354/1848 [7:51:54<35:02:08, 84.42s/it] 19%|‚ñà‚ñâ        | 355/1848 [7:53:19<35:05:07, 84.60s/it] 19%|‚ñà‚ñâ        | 356/1848 [7:54:24<32:37:33, 78.72s/it] 19%|‚ñà‚ñâ        | 357/1848 [7:58:29<53:16:00, 128.61s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_2/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_2/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_2/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_2/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_2/mlp.gate_proj/compressed.log
best_loss 18.00757598876953 running bpv: 2.008811
COMMANDS_FINISHED 352 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_2/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_2/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_2/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_27/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_27/mlp.down_proj/compressed.log
best_loss 53.93946838378906 running bpv: 2.008793
COMMANDS_FINISHED 353 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_2/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_2/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_2/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_2/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_2/self_attn.k_proj/compressed.log
best_loss 18.99352264404297 running bpv: 2.008794
COMMANDS_FINISHED 354 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_2/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_2/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_2/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_2/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_2/mlp.up_proj/compressed.log
best_loss 15.420233726501465 running bpv: 2.008775
COMMANDS_FINISHED 355 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_36/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_36/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_36/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_2/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_2/self_attn.o_proj/compressed.log
best_loss 0.13775233924388885 running bpv: 2.008776
COMMANDS_FINISHED 356 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_36/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_36/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_36/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_2/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_2/self_attn.v_proj/compressed.log
best_loss 5.192710876464844 running bpv: 2.008777
COMMANDS_FINISHED 357 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_36/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_36/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_36/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_36/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_36/self_attn.q_proj/compressed.log
best_loss 263.034912109375 running bpv: 2.008779
COMMANDS_FINISHED 358 n_commands 1849
 19%|‚ñà‚ñâ        | 358/1848 [8:02:04<63:57:37, 154.54s/it] 19%|‚ñà‚ñâ        | 360/1848 [8:02:24<36:18:09, 87.83s/it]  20%|‚ñà‚ñâ        | 361/1848 [8:03:19<32:55:01, 79.69s/it] 20%|‚ñà‚ñâ        | 362/1848 [8:05:54<41:01:37, 99.39s/it] 20%|‚ñà‚ñâ        | 363/1848 [8:06:19<32:39:10, 79.16s/it] 20%|‚ñà‚ñâ        | 364/1848 [8:10:14<50:36:38, 122.78s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_36/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_36/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_36/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_36/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_36/mlp.gate_proj/compressed.log
best_loss 545.7139892578125 running bpv: 2.00876
COMMANDS_FINISHED 359 n_commands 1849
meta-llama/Llama-2-13b-hf/layer_2/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_2/mlp.down_proj/compressed.log
best_loss 0.5013481974601746 running bpv: 2.008742
COMMANDS_FINISHED 360 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_36/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_36/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_36/self_attn.o_proj/compressed.log 2>&1 &
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_36/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_36/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_36/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_36/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_36/self_attn.k_proj/compressed.log
best_loss 272.68621826171875 running bpv: 2.008743
COMMANDS_FINISHED 361 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_36/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_36/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_36/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_36/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_36/mlp.up_proj/compressed.log
best_loss 502.67767333984375 running bpv: 2.008725
COMMANDS_FINISHED 362 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_28/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_28/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_28/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_36/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_36/self_attn.o_proj/compressed.log
best_loss 25.293472290039062 running bpv: 2.008726
COMMANDS_FINISHED 363 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_28/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_28/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_28/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_36/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_36/self_attn.v_proj/compressed.log
best_loss 252.85302734375 running bpv: 2.008727
COMMANDS_FINISHED 364 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_28/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_28/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_28/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_28/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_28/self_attn.q_proj/compressed.log
best_loss 286.8455810546875 running bpv: 2.008729
COMMANDS_FINISHED 365 n_commands 1849
 20%|‚ñà‚ñâ        | 365/1848 [8:13:19<57:53:58, 140.55s/it] 20%|‚ñà‚ñâ        | 366/1848 [8:14:04<46:27:47, 112.87s/it] 20%|‚ñà‚ñâ        | 367/1848 [8:14:19<34:38:43, 84.22s/it]  20%|‚ñà‚ñâ        | 368/1848 [8:16:04<37:08:36, 90.35s/it] 20%|‚ñà‚ñâ        | 369/1848 [8:17:09<34:01:57, 82.84s/it] 20%|‚ñà‚ñà        | 370/1848 [8:18:14<31:49:55, 77.53s/it] 20%|‚ñà‚ñà        | 371/1848 [8:22:09<51:04:48, 124.50s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_28/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_28/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_28/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_28/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_28/mlp.gate_proj/compressed.log
best_loss 402.29656982421875 running bpv: 2.008711
COMMANDS_FINISHED 366 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_28/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_28/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_28/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_36/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_36/mlp.down_proj/compressed.log
best_loss 105.47350311279297 running bpv: 2.008694
COMMANDS_FINISHED 367 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_28/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_28/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_28/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_28/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_28/self_attn.k_proj/compressed.log
best_loss 291.40899658203125 running bpv: 2.008695
COMMANDS_FINISHED 368 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_28/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_28/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_28/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_28/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_28/mlp.up_proj/compressed.log
best_loss 338.2417907714844 running bpv: 2.008677
COMMANDS_FINISHED 369 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_18/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_18/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_18/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_28/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_28/self_attn.o_proj/compressed.log
best_loss 11.716167449951172 running bpv: 2.008679
COMMANDS_FINISHED 370 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_18/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_18/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_18/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_28/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_28/self_attn.v_proj/compressed.log
best_loss 220.57391357421875 running bpv: 2.00868
COMMANDS_FINISHED 371 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_18/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_18/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_18/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_18/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_18/self_attn.q_proj/compressed.log
best_loss 212.48118591308594 running bpv: 2.008681
COMMANDS_FINISHED 372 n_commands 1849
 20%|‚ñà‚ñà        | 372/1848 [8:25:54<63:21:29, 154.53s/it] 20%|‚ñà‚ñà        | 373/1848 [8:26:09<46:12:52, 112.80s/it] 20%|‚ñà‚ñà        | 375/1848 [8:27:09<30:32:32, 74.65s/it]  20%|‚ñà‚ñà        | 376/1848 [8:29:54<39:40:20, 97.02s/it] 20%|‚ñà‚ñà        | 377/1848 [8:30:09<30:53:14, 75.59s/it] 20%|‚ñà‚ñà        | 378/1848 [8:34:04<48:33:47, 118.93s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_18/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_18/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_18/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_28/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_28/mlp.down_proj/compressed.log
best_loss 55.29531478881836 running bpv: 2.008665
COMMANDS_FINISHED 373 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_18/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_18/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_18/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_18/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_18/self_attn.k_proj/compressed.log
best_loss 222.16525268554688 running bpv: 2.008666
COMMANDS_FINISHED 374 n_commands 1849
meta-llama/Llama-2-13b-hf/layer_18/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_18/mlp.gate_proj/compressed.log
best_loss 226.1419677734375 running bpv: 2.008649
COMMANDS_FINISHED 375 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_18/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_18/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_18/mlp.down_proj/compressed.log 2>&1 &
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_18/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_18/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_18/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_18/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_18/mlp.up_proj/compressed.log
best_loss 209.82252502441406 running bpv: 2.008632
COMMANDS_FINISHED 376 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_25/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_25/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_25/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_18/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_18/self_attn.o_proj/compressed.log
best_loss 9.932954788208008 running bpv: 2.008633
COMMANDS_FINISHED 377 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_25/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_25/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_25/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_18/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_18/self_attn.v_proj/compressed.log
best_loss 117.78237915039062 running bpv: 2.008635
COMMANDS_FINISHED 378 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_25/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_25/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_25/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_25/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_25/self_attn.q_proj/compressed.log
best_loss 251.31549072265625 running bpv: 2.008636
COMMANDS_FINISHED 379 n_commands 1849
 21%|‚ñà‚ñà        | 379/1848 [8:37:09<56:04:23, 137.42s/it] 21%|‚ñà‚ñà        | 380/1848 [8:37:54<45:16:35, 111.03s/it] 21%|‚ñà‚ñà        | 381/1848 [8:38:09<33:54:33, 83.21s/it]  21%|‚ñà‚ñà        | 382/1848 [8:40:14<38:52:07, 95.45s/it] 21%|‚ñà‚ñà        | 383/1848 [8:40:59<32:47:23, 80.58s/it] 21%|‚ñà‚ñà        | 384/1848 [8:42:04<30:53:28, 75.96s/it] 21%|‚ñà‚ñà        | 385/1848 [8:45:59<50:05:52, 123.28s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_25/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_25/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_25/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_25/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_25/mlp.gate_proj/compressed.log
best_loss 353.27874755859375 running bpv: 2.008619
COMMANDS_FINISHED 380 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_25/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_25/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_25/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_25/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_25/self_attn.k_proj/compressed.log
best_loss 256.8034973144531 running bpv: 2.008621
COMMANDS_FINISHED 381 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_25/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_25/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_25/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_18/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_18/mlp.down_proj/compressed.log
best_loss 27.225112915039062 running bpv: 2.008605
COMMANDS_FINISHED 382 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_25/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_25/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_25/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_25/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_25/mlp.up_proj/compressed.log
best_loss 298.64727783203125 running bpv: 2.008589
COMMANDS_FINISHED 383 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_37/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_37/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_37/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_25/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_25/self_attn.o_proj/compressed.log
best_loss 11.266336441040039 running bpv: 2.00859
COMMANDS_FINISHED 384 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_37/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_37/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_37/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_25/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_25/self_attn.v_proj/compressed.log
best_loss 187.91299438476562 running bpv: 2.008592
COMMANDS_FINISHED 385 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_37/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_37/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_37/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_37/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_37/self_attn.q_proj/compressed.log
best_loss 232.67086791992188 running bpv: 2.008593
COMMANDS_FINISHED 386 n_commands 1849
 21%|‚ñà‚ñà        | 386/1848 [8:49:44<62:23:09, 153.62s/it] 21%|‚ñà‚ñà        | 387/1848 [8:49:59<45:32:14, 112.21s/it] 21%|‚ñà‚ñà        | 388/1848 [8:50:34<36:08:28, 89.12s/it]  21%|‚ñà‚ñà        | 389/1848 [8:50:59<28:20:17, 69.92s/it] 21%|‚ñà‚ñà        | 390/1848 [8:53:34<38:38:34, 95.41s/it] 21%|‚ñà‚ñà        | 391/1848 [8:54:29<33:42:55, 83.31s/it] 21%|‚ñà‚ñà        | 392/1848 [8:58:24<52:05:14, 128.79s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_37/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_37/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_37/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_25/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_25/mlp.down_proj/compressed.log
best_loss 49.97477722167969 running bpv: 2.008578
COMMANDS_FINISHED 387 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_37/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_37/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_37/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_37/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_37/self_attn.k_proj/compressed.log
best_loss 232.44461059570312 running bpv: 2.008579
COMMANDS_FINISHED 388 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_37/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_37/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_37/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_37/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_37/mlp.gate_proj/compressed.log
best_loss 544.0896606445312 running bpv: 2.008563
COMMANDS_FINISHED 389 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_37/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_37/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_37/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_37/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_37/mlp.up_proj/compressed.log
best_loss 503.90948486328125 running bpv: 2.008547
COMMANDS_FINISHED 390 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_30/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_30/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_30/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_37/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_37/self_attn.o_proj/compressed.log
best_loss 25.079235076904297 running bpv: 2.008549
COMMANDS_FINISHED 391 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_30/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_30/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_30/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_37/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_37/self_attn.v_proj/compressed.log
best_loss 251.369873046875 running bpv: 2.008551
COMMANDS_FINISHED 392 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_30/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_30/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_30/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_30/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_30/self_attn.q_proj/compressed.log
best_loss 333.36163330078125 running bpv: 2.008552
COMMANDS_FINISHED 393 n_commands 1849
 21%|‚ñà‚ñà‚ñè       | 393/1848 [9:01:09<56:26:31, 139.65s/it] 21%|‚ñà‚ñà‚ñè       | 394/1848 [9:02:04<46:09:04, 114.27s/it] 21%|‚ñà‚ñà‚ñè       | 395/1848 [9:02:29<35:18:50, 87.50s/it]  21%|‚ñà‚ñà‚ñè       | 396/1848 [9:03:44<33:46:46, 83.75s/it] 21%|‚ñà‚ñà‚ñè       | 397/1848 [9:04:59<32:41:57, 81.13s/it] 22%|‚ñà‚ñà‚ñè       | 398/1848 [9:06:24<33:08:45, 82.29s/it] 22%|‚ñà‚ñà‚ñè       | 399/1848 [9:10:19<51:33:49, 128.11s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_30/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_30/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_30/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_30/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_30/mlp.gate_proj/compressed.log
best_loss 442.3099060058594 running bpv: 2.008536
COMMANDS_FINISHED 394 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_30/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_30/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_30/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_37/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_37/mlp.down_proj/compressed.log
best_loss 120.42265319824219 running bpv: 2.008522
COMMANDS_FINISHED 395 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_30/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_30/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_30/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_30/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_30/self_attn.k_proj/compressed.log
best_loss 337.7048034667969 running bpv: 2.008523
COMMANDS_FINISHED 396 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_30/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_30/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_30/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_30/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_30/mlp.up_proj/compressed.log
best_loss 374.81622314453125 running bpv: 2.008508
COMMANDS_FINISHED 397 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_29/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_29/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_29/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_30/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_30/self_attn.o_proj/compressed.log
best_loss 12.549088478088379 running bpv: 2.00851
COMMANDS_FINISHED 398 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_29/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_29/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_29/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_30/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_30/self_attn.v_proj/compressed.log
best_loss 280.64013671875 running bpv: 2.008511
COMMANDS_FINISHED 399 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_29/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_29/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_29/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_29/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_29/self_attn.q_proj/compressed.log
best_loss 308.0360107421875 running bpv: 2.008513
COMMANDS_FINISHED 400 n_commands 1849
 22%|‚ñà‚ñà‚ñè       | 400/1848 [9:13:54<62:00:55, 154.18s/it] 22%|‚ñà‚ñà‚ñè       | 401/1848 [9:14:09<45:11:26, 112.43s/it] 22%|‚ñà‚ñà‚ñè       | 402/1848 [9:14:24<33:25:12, 83.20s/it]  22%|‚ñà‚ñà‚ñè       | 403/1848 [9:15:09<28:47:52, 71.75s/it] 22%|‚ñà‚ñà‚ñè       | 404/1848 [9:17:44<38:47:52, 96.73s/it] 22%|‚ñà‚ñà‚ñè       | 405/1848 [9:18:19<31:20:57, 78.21s/it] 22%|‚ñà‚ñà‚ñè       | 406/1848 [9:22:14<50:10:14, 125.25s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_29/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_29/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_29/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_29/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_29/mlp.gate_proj/compressed.log
best_loss 418.3399658203125 running bpv: 2.008498
COMMANDS_FINISHED 401 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_29/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_29/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_29/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_30/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_30/mlp.down_proj/compressed.log
best_loss 61.61026382446289 running bpv: 2.008484
COMMANDS_FINISHED 402 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_29/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_29/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_29/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_29/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_29/self_attn.k_proj/compressed.log
best_loss 316.05718994140625 running bpv: 2.008485
COMMANDS_FINISHED 403 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_29/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_29/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_29/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_29/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_29/mlp.up_proj/compressed.log
best_loss 354.5833740234375 running bpv: 2.00847
COMMANDS_FINISHED 404 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_16/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_16/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_16/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_29/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_29/self_attn.o_proj/compressed.log
best_loss 11.067939758300781 running bpv: 2.008472
COMMANDS_FINISHED 405 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_16/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_16/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_16/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_29/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_29/self_attn.v_proj/compressed.log
best_loss 233.17364501953125 running bpv: 2.008474
COMMANDS_FINISHED 406 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_16/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_16/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_16/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_16/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_16/self_attn.q_proj/compressed.log
best_loss 203.17559814453125 running bpv: 2.008475
COMMANDS_FINISHED 407 n_commands 1849
 22%|‚ñà‚ñà‚ñè       | 407/1848 [9:25:09<56:06:42, 140.18s/it] 22%|‚ñà‚ñà‚ñè       | 408/1848 [9:26:14<47:03:08, 117.63s/it] 22%|‚ñà‚ñà‚ñè       | 410/1848 [9:27:44<33:35:53, 84.11s/it]  22%|‚ñà‚ñà‚ñè       | 411/1848 [9:28:59<32:40:26, 81.86s/it] 22%|‚ñà‚ñà‚ñè       | 412/1848 [9:30:14<31:56:13, 80.07s/it] 22%|‚ñà‚ñà‚ñè       | 413/1848 [9:34:09<48:42:57, 122.21s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_16/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_16/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_16/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_16/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_16/mlp.gate_proj/compressed.log
best_loss 183.737548828125 running bpv: 2.008461
COMMANDS_FINISHED 408 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_16/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_16/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_16/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_29/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_29/mlp.down_proj/compressed.log
best_loss 58.27656555175781 running bpv: 2.008447
COMMANDS_FINISHED 409 n_commands 1849
meta-llama/Llama-2-13b-hf/layer_16/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_16/self_attn.k_proj/compressed.log
best_loss 211.84423828125 running bpv: 2.008449
COMMANDS_FINISHED 410 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_16/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_16/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_16/mlp.down_proj/compressed.log 2>&1 &
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_16/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_16/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_16/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_16/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_16/mlp.up_proj/compressed.log
best_loss 178.53787231445312 running bpv: 2.008434
COMMANDS_FINISHED 411 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_13/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_13/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_13/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_16/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_16/self_attn.o_proj/compressed.log
best_loss 10.359545707702637 running bpv: 2.008436
COMMANDS_FINISHED 412 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_13/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_13/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_13/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_16/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_16/self_attn.v_proj/compressed.log
best_loss 102.49150085449219 running bpv: 2.008438
COMMANDS_FINISHED 413 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_13/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_13/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_13/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_13/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_13/self_attn.q_proj/compressed.log
best_loss 190.98788452148438 running bpv: 2.008439
COMMANDS_FINISHED 414 n_commands 1849
 22%|‚ñà‚ñà‚ñè       | 414/1848 [9:37:44<59:01:37, 148.19s/it] 22%|‚ñà‚ñà‚ñè       | 415/1848 [9:38:09<44:58:56, 113.01s/it] 23%|‚ñà‚ñà‚ñé       | 416/1848 [9:38:24<33:39:19, 84.61s/it]  23%|‚ñà‚ñà‚ñé       | 417/1848 [9:38:59<27:51:35, 70.09s/it] 23%|‚ñà‚ñà‚ñé       | 418/1848 [9:41:34<37:47:19, 95.13s/it] 23%|‚ñà‚ñà‚ñé       | 419/1848 [9:42:19<31:51:53, 80.28s/it] 23%|‚ñà‚ñà‚ñé       | 420/1848 [9:46:14<50:06:08, 126.31s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_13/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_13/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_13/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_13/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_13/mlp.gate_proj/compressed.log
best_loss 144.14047241210938 running bpv: 2.008425
COMMANDS_FINISHED 415 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_13/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_13/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_13/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_13/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_13/self_attn.k_proj/compressed.log
best_loss 189.35595703125 running bpv: 2.008427
COMMANDS_FINISHED 416 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_13/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_13/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_13/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_16/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_16/mlp.down_proj/compressed.log
best_loss 22.60711669921875 running bpv: 2.008413
COMMANDS_FINISHED 417 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_13/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_13/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_13/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_13/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_13/mlp.up_proj/compressed.log
best_loss 137.75259399414062 running bpv: 2.0084
COMMANDS_FINISHED 418 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_10/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_10/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_10/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_13/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_13/self_attn.o_proj/compressed.log
best_loss 7.390631675720215 running bpv: 2.008401
COMMANDS_FINISHED 419 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_10/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_10/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_10/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_13/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_13/self_attn.v_proj/compressed.log
best_loss 89.7502670288086 running bpv: 2.008403
COMMANDS_FINISHED 420 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_10/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_10/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_10/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_10/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_10/self_attn.q_proj/compressed.log
best_loss 152.63497924804688 running bpv: 2.008405
COMMANDS_FINISHED 421 n_commands 1849
 23%|‚ñà‚ñà‚ñé       | 421/1848 [9:48:59<54:38:34, 137.85s/it] 23%|‚ñà‚ñà‚ñé       | 422/1848 [9:50:04<45:59:03, 116.09s/it] 23%|‚ñà‚ñà‚ñé       | 423/1848 [9:50:19<33:58:59, 85.85s/it]  23%|‚ñà‚ñà‚ñé       | 424/1848 [9:51:34<32:40:31, 82.61s/it] 23%|‚ñà‚ñà‚ñé       | 425/1848 [9:52:49<31:45:10, 80.33s/it] 23%|‚ñà‚ñà‚ñé       | 426/1848 [9:54:14<32:17:04, 81.73s/it] 23%|‚ñà‚ñà‚ñé       | 427/1848 [9:58:09<50:24:03, 127.69s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_10/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_10/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_10/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_10/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_10/mlp.gate_proj/compressed.log
best_loss 124.26899719238281 running bpv: 2.008391
COMMANDS_FINISHED 422 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_10/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_10/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_10/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_13/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_13/mlp.down_proj/compressed.log
best_loss 13.301299095153809 running bpv: 2.008378
COMMANDS_FINISHED 423 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_10/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_10/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_10/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_10/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_10/self_attn.k_proj/compressed.log
best_loss 157.86831665039062 running bpv: 2.00838
COMMANDS_FINISHED 424 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_10/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_10/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_10/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_10/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_10/mlp.up_proj/compressed.log
best_loss 110.27168273925781 running bpv: 2.008366
COMMANDS_FINISHED 425 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_19/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_19/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_19/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_10/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_10/self_attn.o_proj/compressed.log
best_loss 4.594633102416992 running bpv: 2.008368
COMMANDS_FINISHED 426 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_19/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_19/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_19/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_10/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_10/self_attn.v_proj/compressed.log
best_loss 60.69303512573242 running bpv: 2.00837
COMMANDS_FINISHED 427 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_19/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_19/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_19/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_19/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_19/self_attn.q_proj/compressed.log
best_loss 210.57232666015625 running bpv: 2.008372
COMMANDS_FINISHED 428 n_commands 1849
 23%|‚ñà‚ñà‚ñé       | 428/1848 [10:01:34<59:30:42, 150.87s/it] 23%|‚ñà‚ñà‚ñé       | 429/1848 [10:01:59<44:35:27, 113.13s/it] 23%|‚ñà‚ñà‚ñé       | 430/1848 [10:02:14<32:58:03, 83.70s/it]  23%|‚ñà‚ñà‚ñé       | 431/1848 [10:02:50<27:11:45, 69.09s/it] 23%|‚ñà‚ñà‚ñé       | 432/1848 [10:05:25<37:18:51, 94.87s/it] 23%|‚ñà‚ñà‚ñé       | 433/1848 [10:06:10<31:24:32, 79.91s/it] 23%|‚ñà‚ñà‚ñé       | 434/1848 [10:10:05<49:39:46, 126.44s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_19/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_19/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_19/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_19/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_19/mlp.gate_proj/compressed.log
best_loss 244.08517456054688 running bpv: 2.008358
COMMANDS_FINISHED 429 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_19/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_19/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_19/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_10/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_10/mlp.down_proj/compressed.log
best_loss 9.016712188720703 running bpv: 2.008346
COMMANDS_FINISHED 430 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_19/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_19/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_19/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_19/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_19/self_attn.k_proj/compressed.log
best_loss 217.25363159179688 running bpv: 2.008348
COMMANDS_FINISHED 431 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_19/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_19/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_19/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_19/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_19/mlp.up_proj/compressed.log
best_loss 223.47119140625 running bpv: 2.008335
COMMANDS_FINISHED 432 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_6/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_6/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_6/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_19/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_19/self_attn.o_proj/compressed.log
best_loss 10.314027786254883 running bpv: 2.008336
COMMANDS_FINISHED 433 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_6/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_6/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_6/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_19/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_19/self_attn.v_proj/compressed.log
best_loss 121.87542724609375 running bpv: 2.008338
COMMANDS_FINISHED 434 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_6/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_6/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_6/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_6/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_6/self_attn.q_proj/compressed.log
best_loss 102.3364028930664 running bpv: 2.00834
COMMANDS_FINISHED 435 n_commands 1849
 24%|‚ñà‚ñà‚ñé       | 435/1848 [10:12:50<54:10:13, 138.01s/it] 24%|‚ñà‚ñà‚ñé       | 436/1848 [10:13:55<45:32:32, 116.11s/it] 24%|‚ñà‚ñà‚ñé       | 437/1848 [10:14:10<33:37:18, 85.78s/it]  24%|‚ñà‚ñà‚ñé       | 438/1848 [10:15:25<32:19:56, 82.55s/it] 24%|‚ñà‚ñà‚ñç       | 439/1848 [10:16:40<31:25:26, 80.29s/it] 24%|‚ñà‚ñà‚ñç       | 440/1848 [10:18:05<31:57:20, 81.70s/it] 24%|‚ñà‚ñà‚ñç       | 441/1848 [10:22:00<49:54:33, 127.70s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_6/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_6/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_6/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_6/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_6/mlp.gate_proj/compressed.log
best_loss 83.32698822021484 running bpv: 2.008327
COMMANDS_FINISHED 436 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_6/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_6/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_6/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_19/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_19/mlp.down_proj/compressed.log
best_loss 30.613258361816406 running bpv: 2.008315
COMMANDS_FINISHED 437 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_6/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_6/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_6/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_6/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_6/self_attn.k_proj/compressed.log
best_loss 105.54981994628906 running bpv: 2.008317
COMMANDS_FINISHED 438 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_6/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_6/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_6/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_6/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_6/mlp.up_proj/compressed.log
best_loss 67.15493774414062 running bpv: 2.008304
COMMANDS_FINISHED 439 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_11/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_11/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_11/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_6/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_6/self_attn.o_proj/compressed.log
best_loss 1.0722832679748535 running bpv: 2.008306
COMMANDS_FINISHED 440 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_11/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_11/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_11/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_6/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_6/self_attn.v_proj/compressed.log
best_loss 37.92683792114258 running bpv: 2.008307
COMMANDS_FINISHED 441 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_11/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_11/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_11/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_11/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_11/self_attn.q_proj/compressed.log
best_loss 150.81756591796875 running bpv: 2.008309
COMMANDS_FINISHED 442 n_commands 1849
 24%|‚ñà‚ñà‚ñç       | 442/1848 [10:25:25<58:55:58, 150.90s/it] 24%|‚ñà‚ñà‚ñç       | 443/1848 [10:25:50<44:09:06, 113.13s/it] 24%|‚ñà‚ñà‚ñç       | 444/1848 [10:26:05<32:38:23, 83.69s/it]  24%|‚ñà‚ñà‚ñç       | 445/1848 [10:26:40<26:55:28, 69.09s/it] 24%|‚ñà‚ñà‚ñç       | 446/1848 [10:29:15<36:56:41, 94.87s/it] 24%|‚ñà‚ñà‚ñç       | 447/1848 [10:30:00<31:05:51, 79.91s/it] 24%|‚ñà‚ñà‚ñç       | 448/1848 [10:33:55<49:10:17, 126.44s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_11/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_11/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_11/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_11/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_11/mlp.gate_proj/compressed.log
best_loss 127.50465393066406 running bpv: 2.008297
COMMANDS_FINISHED 443 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_11/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_11/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_11/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_6/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_6/mlp.down_proj/compressed.log
best_loss 3.7943613529205322 running bpv: 2.008285
COMMANDS_FINISHED 444 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_11/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_11/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_11/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_11/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_11/self_attn.k_proj/compressed.log
best_loss 157.40939331054688 running bpv: 2.008287
COMMANDS_FINISHED 445 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_11/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_11/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_11/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_11/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_11/mlp.up_proj/compressed.log
best_loss 117.09965515136719 running bpv: 2.008275
COMMANDS_FINISHED 446 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_20/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_20/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_20/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_11/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_11/self_attn.o_proj/compressed.log
best_loss 5.308044910430908 running bpv: 2.008276
COMMANDS_FINISHED 447 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_20/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_20/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_20/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_11/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_11/self_attn.v_proj/compressed.log
best_loss 58.30778503417969 running bpv: 2.008278
COMMANDS_FINISHED 448 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_20/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_20/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_20/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_20/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_20/self_attn.q_proj/compressed.log
best_loss 196.92684936523438 running bpv: 2.00828
COMMANDS_FINISHED 449 n_commands 1849
 24%|‚ñà‚ñà‚ñç       | 449/1848 [10:36:40<53:38:00, 138.01s/it] 24%|‚ñà‚ñà‚ñç       | 450/1848 [10:37:45<45:05:26, 116.11s/it] 24%|‚ñà‚ñà‚ñç       | 451/1848 [10:38:00<33:17:15, 85.78s/it]  24%|‚ñà‚ñà‚ñç       | 452/1848 [10:39:25<33:10:27, 85.55s/it] 25%|‚ñà‚ñà‚ñç       | 453/1848 [10:40:30<30:45:45, 79.39s/it] 25%|‚ñà‚ñà‚ñç       | 454/1848 [10:41:55<31:23:37, 81.07s/it] 25%|‚ñà‚ñà‚ñç       | 455/1848 [10:46:00<50:24:08, 130.26s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_20/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_20/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_20/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_20/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_20/mlp.gate_proj/compressed.log
best_loss 257.006103515625 running bpv: 2.008268
COMMANDS_FINISHED 450 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_20/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_20/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_20/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_11/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_11/mlp.down_proj/compressed.log
best_loss 10.130364418029785 running bpv: 2.008256
COMMANDS_FINISHED 451 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_20/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_20/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_20/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_20/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_20/self_attn.k_proj/compressed.log
best_loss 205.71865844726562 running bpv: 2.008258
COMMANDS_FINISHED 452 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_20/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_20/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_20/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_20/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_20/mlp.up_proj/compressed.log
best_loss 233.66976928710938 running bpv: 2.008246
COMMANDS_FINISHED 453 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_23/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_23/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_23/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_20/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_20/self_attn.o_proj/compressed.log
best_loss 12.019889831542969 running bpv: 2.008248
COMMANDS_FINISHED 454 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_23/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_23/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_23/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_20/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_20/self_attn.v_proj/compressed.log
best_loss 111.63287353515625 running bpv: 2.00825
COMMANDS_FINISHED 455 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_23/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_23/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_23/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_23/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_23/self_attn.q_proj/compressed.log
best_loss 223.86810302734375 running bpv: 2.008252
COMMANDS_FINISHED 456 n_commands 1849
 25%|‚ñà‚ñà‚ñç       | 456/1848 [10:49:35<60:11:55, 155.69s/it] 25%|‚ñà‚ñà‚ñç       | 458/1848 [10:49:55<34:09:04, 88.45s/it]  25%|‚ñà‚ñà‚ñç       | 459/1848 [10:50:40<29:58:16, 77.68s/it] 25%|‚ñà‚ñà‚ñç       | 460/1848 [10:53:25<38:45:25, 100.52s/it] 25%|‚ñà‚ñà‚ñç       | 461/1848 [10:53:50<30:48:54, 79.98s/it]  25%|‚ñà‚ñà‚ñå       | 462/1848 [10:57:45<47:29:49, 123.37s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_23/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_23/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_23/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_23/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_23/mlp.gate_proj/compressed.log
best_loss 318.2692565917969 running bpv: 2.00824
COMMANDS_FINISHED 457 n_commands 1849
meta-llama/Llama-2-13b-hf/layer_20/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_20/mlp.down_proj/compressed.log
best_loss 34.35955810546875 running bpv: 2.008229
COMMANDS_FINISHED 458 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_23/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_23/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_23/self_attn.o_proj/compressed.log 2>&1 &
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_23/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_23/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_23/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_23/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_23/self_attn.k_proj/compressed.log
best_loss 229.68048095703125 running bpv: 2.008231
COMMANDS_FINISHED 459 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_23/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_23/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_23/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_23/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_23/mlp.up_proj/compressed.log
best_loss 273.21051025390625 running bpv: 2.008219
COMMANDS_FINISHED 460 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_35/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_35/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_35/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_23/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_23/self_attn.o_proj/compressed.log
best_loss 10.826383590698242 running bpv: 2.008221
COMMANDS_FINISHED 461 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_35/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_35/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_35/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_23/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_23/self_attn.v_proj/compressed.log
best_loss 159.23464965820312 running bpv: 2.008223
COMMANDS_FINISHED 462 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_35/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_35/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_35/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_35/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_35/self_attn.q_proj/compressed.log
best_loss 278.4288330078125 running bpv: 2.008224
COMMANDS_FINISHED 463 n_commands 1849
 25%|‚ñà‚ñà‚ñå       | 463/1848 [11:00:40<53:08:16, 138.12s/it] 25%|‚ñà‚ñà‚ñå       | 464/1848 [11:01:35<43:50:26, 114.04s/it] 25%|‚ñà‚ñà‚ñå       | 465/1848 [11:01:50<32:40:15, 85.04s/it]  25%|‚ñà‚ñà‚ñå       | 466/1848 [11:03:35<34:54:29, 90.93s/it] 25%|‚ñà‚ñà‚ñå       | 467/1848 [11:04:30<30:47:53, 80.28s/it] 25%|‚ñà‚ñà‚ñå       | 468/1848 [11:05:45<30:10:27, 78.72s/it] 25%|‚ñà‚ñà‚ñå       | 469/1848 [11:09:40<48:00:31, 125.33s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_35/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_35/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_35/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_35/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_35/mlp.gate_proj/compressed.log
best_loss 530.8596801757812 running bpv: 2.008213
COMMANDS_FINISHED 464 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_35/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_35/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_35/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_23/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_23/mlp.down_proj/compressed.log
best_loss 46.056129455566406 running bpv: 2.008202
COMMANDS_FINISHED 465 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_35/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_35/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_35/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_35/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_35/self_attn.k_proj/compressed.log
best_loss 286.0562438964844 running bpv: 2.008204
COMMANDS_FINISHED 466 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_35/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_35/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_35/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_35/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_35/mlp.up_proj/compressed.log
best_loss 482.39691162109375 running bpv: 2.008193
COMMANDS_FINISHED 467 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_38/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_38/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_38/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_35/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_35/self_attn.o_proj/compressed.log
best_loss 16.153709411621094 running bpv: 2.008195
COMMANDS_FINISHED 468 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_38/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_38/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_38/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_35/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_35/self_attn.v_proj/compressed.log
best_loss 257.6361389160156 running bpv: 2.008196
COMMANDS_FINISHED 469 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_38/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_38/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_38/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_38/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_38/self_attn.q_proj/compressed.log
best_loss 221.39700317382812 running bpv: 2.008198
COMMANDS_FINISHED 470 n_commands 1849
 25%|‚ñà‚ñà‚ñå       | 470/1848 [11:13:35<60:31:03, 158.10s/it] 26%|‚ñà‚ñà‚ñå       | 473/1848 [11:14:30<30:24:48, 79.63s/it]  26%|‚ñà‚ñà‚ñå       | 474/1848 [11:17:35<38:52:37, 101.86s/it] 26%|‚ñà‚ñà‚ñå       | 475/1848 [11:17:50<31:10:38, 81.75s/it]  26%|‚ñà‚ñà‚ñå       | 476/1848 [11:21:45<45:40:33, 119.85s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_38/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_38/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_38/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_38/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_38/self_attn.k_proj/compressed.log
best_loss 224.821044921875 running bpv: 2.0082
COMMANDS_FINISHED 471 n_commands 1849
meta-llama/Llama-2-13b-hf/layer_38/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_38/mlp.gate_proj/compressed.log
best_loss 535.2064208984375 running bpv: 2.008189
COMMANDS_FINISHED 472 n_commands 1849
meta-llama/Llama-2-13b-hf/layer_35/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_35/mlp.down_proj/compressed.log
best_loss 91.35984802246094 running bpv: 2.008178
COMMANDS_FINISHED 473 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_38/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_38/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_38/self_attn.o_proj/compressed.log 2>&1 &
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_38/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_38/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_38/mlp.down_proj/compressed.log 2>&1 &
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_38/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_38/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_38/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_38/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_38/mlp.up_proj/compressed.log
best_loss 473.6633605957031 running bpv: 2.008167
COMMANDS_FINISHED 474 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_17/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_17/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_17/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_38/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_38/self_attn.o_proj/compressed.log
best_loss 37.67290496826172 running bpv: 2.008169
COMMANDS_FINISHED 475 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_17/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_17/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_17/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_38/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_38/self_attn.v_proj/compressed.log
best_loss 268.7527770996094 running bpv: 2.008171
COMMANDS_FINISHED 476 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_17/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_17/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_17/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_17/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_17/self_attn.q_proj/compressed.log
best_loss 201.1707763671875 running bpv: 2.008173
COMMANDS_FINISHED 477 n_commands 1849
 26%|‚ñà‚ñà‚ñå       | 477/1848 [11:24:30<50:09:01, 131.69s/it] 26%|‚ñà‚ñà‚ñå       | 478/1848 [11:25:25<42:09:54, 110.80s/it] 26%|‚ñà‚ñà‚ñå       | 479/1848 [11:25:40<31:55:47, 83.96s/it]  26%|‚ñà‚ñà‚ñå       | 480/1848 [11:27:55<37:27:02, 98.55s/it] 26%|‚ñà‚ñà‚ñå       | 481/1848 [11:28:20<29:19:35, 77.23s/it] 26%|‚ñà‚ñà‚ñå       | 482/1848 [11:29:35<29:03:29, 76.58s/it] 26%|‚ñà‚ñà‚ñå       | 483/1848 [11:33:30<46:45:25, 123.32s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_17/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_17/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_17/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_17/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_17/mlp.gate_proj/compressed.log
best_loss 205.4396209716797 running bpv: 2.008162
COMMANDS_FINISHED 478 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_17/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_17/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_17/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_38/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_38/mlp.down_proj/compressed.log
best_loss 137.08172607421875 running bpv: 2.008152
COMMANDS_FINISHED 479 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_17/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_17/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_17/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_17/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_17/self_attn.k_proj/compressed.log
best_loss 210.4908447265625 running bpv: 2.008154
COMMANDS_FINISHED 480 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_17/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_17/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_17/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_17/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_17/mlp.up_proj/compressed.log
best_loss 194.77801513671875 running bpv: 2.008143
COMMANDS_FINISHED 481 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_9/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_9/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_9/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_17/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_17/self_attn.o_proj/compressed.log
best_loss 10.332812309265137 running bpv: 2.008145
COMMANDS_FINISHED 482 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_9/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_9/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_9/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_17/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_17/self_attn.v_proj/compressed.log
best_loss 105.48371887207031 running bpv: 2.008147
COMMANDS_FINISHED 483 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_9/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_9/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_9/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_9/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_9/self_attn.q_proj/compressed.log
best_loss 152.96878051757812 running bpv: 2.008148
COMMANDS_FINISHED 484 n_commands 1849
 26%|‚ñà‚ñà‚ñå       | 484/1848 [11:37:15<58:08:49, 153.47s/it] 26%|‚ñà‚ñà‚ñå       | 485/1848 [11:37:30<42:30:27, 112.27s/it] 26%|‚ñà‚ñà‚ñã       | 486/1848 [11:38:15<34:53:10, 92.21s/it]  26%|‚ñà‚ñà‚ñã       | 487/1848 [11:38:30<26:08:24, 69.14s/it] 26%|‚ñà‚ñà‚ñã       | 488/1848 [11:41:05<35:49:31, 94.83s/it] 26%|‚ñà‚ñà‚ñã       | 489/1848 [11:42:10<32:25:42, 85.90s/it] 27%|‚ñà‚ñà‚ñã       | 490/1848 [11:46:05<49:15:21, 130.58s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_9/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_9/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_9/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_17/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_17/mlp.down_proj/compressed.log
best_loss 25.188432693481445 running bpv: 2.008138
COMMANDS_FINISHED 485 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_9/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_9/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_9/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_9/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_9/self_attn.k_proj/compressed.log
best_loss 153.0574951171875 running bpv: 2.00814
COMMANDS_FINISHED 486 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_9/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_9/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_9/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_9/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_9/mlp.gate_proj/compressed.log
best_loss 120.91607666015625 running bpv: 2.00813
COMMANDS_FINISHED 487 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_9/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_9/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_9/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_9/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_9/mlp.up_proj/compressed.log
best_loss 103.48898315429688 running bpv: 2.008119
COMMANDS_FINISHED 488 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_22/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_22/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_22/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_9/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_9/self_attn.o_proj/compressed.log
best_loss 3.138986349105835 running bpv: 2.008121
COMMANDS_FINISHED 489 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_22/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_22/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_22/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_9/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_9/self_attn.v_proj/compressed.log
best_loss 60.94514083862305 running bpv: 2.008123
COMMANDS_FINISHED 490 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_22/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_22/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_22/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_22/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_22/self_attn.q_proj/compressed.log
best_loss 229.6220245361328 running bpv: 2.008125
COMMANDS_FINISHED 491 n_commands 1849
 27%|‚ñà‚ñà‚ñã       | 491/1848 [11:48:30<50:51:02, 134.90s/it] 27%|‚ñà‚ñà‚ñã       | 492/1848 [11:49:25<41:47:29, 110.95s/it] 27%|‚ñà‚ñà‚ñã       | 493/1848 [11:50:10<34:19:04, 91.18s/it]  27%|‚ñà‚ñà‚ñã       | 494/1848 [11:51:05<30:12:46, 80.33s/it] 27%|‚ñà‚ñà‚ñã       | 495/1848 [11:52:20<29:35:27, 78.73s/it] 27%|‚ñà‚ñà‚ñã       | 496/1848 [11:54:15<33:39:20, 89.62s/it] 27%|‚ñà‚ñà‚ñã       | 497/1848 [11:58:20<51:07:28, 136.23s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_22/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_22/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_22/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_22/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_22/mlp.gate_proj/compressed.log
best_loss 299.15460205078125 running bpv: 2.008114
COMMANDS_FINISHED 492 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_22/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_22/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_22/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_9/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_9/mlp.down_proj/compressed.log
best_loss 7.683678150177002 running bpv: 2.008105
COMMANDS_FINISHED 493 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_22/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_22/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_22/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_22/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_22/self_attn.k_proj/compressed.log
best_loss 238.81582641601562 running bpv: 2.008107
COMMANDS_FINISHED 494 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_22/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_22/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_22/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_22/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_22/mlp.up_proj/compressed.log
best_loss 261.2829284667969 running bpv: 2.008097
COMMANDS_FINISHED 495 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_24/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_24/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_24/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_22/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_22/self_attn.o_proj/compressed.log
best_loss 10.549736976623535 running bpv: 2.008098
COMMANDS_FINISHED 496 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_24/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_24/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_24/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_22/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_22/self_attn.v_proj/compressed.log
best_loss 159.86624145507812 running bpv: 2.0081
COMMANDS_FINISHED 497 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_24/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_24/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_24/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_24/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_24/self_attn.q_proj/compressed.log
best_loss 232.24542236328125 running bpv: 2.008102
COMMANDS_FINISHED 498 n_commands 1849
 27%|‚ñà‚ñà‚ñã       | 498/1848 [12:01:15<55:26:59, 147.87s/it] 27%|‚ñà‚ñà‚ñã       | 499/1848 [12:01:30<40:28:25, 108.01s/it] 27%|‚ñà‚ñà‚ñã       | 500/1848 [12:02:25<34:29:25, 92.11s/it]  27%|‚ñà‚ñà‚ñã       | 502/1848 [12:05:05<32:21:04, 86.53s/it] 27%|‚ñà‚ñà‚ñã       | 503/1848 [12:06:20<31:15:36, 83.67s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_24/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_24/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_24/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_24/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_24/mlp.gate_proj/compressed.log
best_loss 333.33953857421875 running bpv: 2.008092
COMMANDS_FINISHED 499 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_24/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_24/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_24/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_22/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_22/mlp.down_proj/compressed.log
best_loss 44.11707305908203 running bpv: 2.008083
COMMANDS_FINISHED 500 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_24/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_24/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_24/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_24/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_24/mlp.up_proj/compressed.log
best_loss 283.8486328125 running bpv: 2.008073
COMMANDS_FINISHED 501 n_commands 1849
meta-llama/Llama-2-13b-hf/layer_24/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_24/self_attn.k_proj/compressed.log
best_loss 238.45675659179688 running bpv: 2.008074
COMMANDS_FINISHED 502 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/hessians_new/pajama/128/layer_24/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_24/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_24/self_attn.v_proj/compressed.log 2>&1 &
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/hessians_new/pajama/128/layer_3/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_3/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_3/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_24/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_24/self_attn.o_proj/compressed.log
best_loss 9.7167387008667 running bpv: 2.008076
COMMANDS_FINISHED 503 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/hessians_new/pajama/128/layer_3/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_3/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_3/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_24/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_24/self_attn.v_proj/compressed.log
best_loss 165.02432250976562 running bpv: 2.008078
COMMANDS_FINISHED 504 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/hessians_new/pajama/128/layer_3/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_3/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_3/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf
 27%|‚ñà‚ñà‚ñã       | 504/1848 [12:13:25<64:34:11, 172.95s/it]meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-hf/layer_24/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_24/mlp.down_proj/compressed.log
best_loss 47.08081817626953 running bpv: 2.008069
COMMANDS_FINISHED 505 n_commands 1849
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/hessians_new/pajama/128/layer_3/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_3/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_3/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-13b-hf
done with meta-llama/Llama-2-13b-hf
done with {'meta-llama/Llama-2-13b-hf/layer_3/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_3/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_3/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_3/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_3/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_3/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_3/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_3/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_3/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_3/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_3/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_3/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_5/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_5/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_3/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_3/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_5/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_5/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_5/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_5/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_5/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_5/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_5/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_5/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_5/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_5/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_8/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_8/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_8/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_8/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_5/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_5/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_8/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_8/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_8/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_8/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_8/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_8/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_8/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_8/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_39/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_39/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_8/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_8/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_39/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_39/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_39/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_39/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_39/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_39/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_39/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_39/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_39/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_39/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_12/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_12/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_12/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_12/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_12/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_12/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_39/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_39/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_12/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_12/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_12/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_12/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_12/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_12/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_1/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_1/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_12/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_12/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_1/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_1/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_1/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_1/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_1/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_1/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_1/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_1/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_1/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_1/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_7/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_7/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_7/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_7/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_1/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_1/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_7/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_7/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_7/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_7/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_7/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_7/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_7/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_7/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_33/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_33/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_33/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_33/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_7/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_7/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_33/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_33/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_33/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_33/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_33/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_33/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_33/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_33/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_34/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_34/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_34/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_34/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_34/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_34/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_33/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_33/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_34/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_34/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_34/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_34/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_34/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_34/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_0/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_0/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_34/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_34/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_0/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_0/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_0/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_0/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_0/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_0/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_0/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_0/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_0/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_0/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_32/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_32/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_32/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_32/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_0/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_0/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_32/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_32/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_32/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_32/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_32/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_32/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_32/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_32/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_31/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_31/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_32/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_32/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_31/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_31/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_31/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_31/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_31/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_31/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_31/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_31/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_31/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_31/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_21/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_21/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_21/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_21/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_31/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_31/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_21/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_21/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_21/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_21/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_21/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_21/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_21/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_21/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_26/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_26/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_26/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_26/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_21/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_21/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_26/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_26/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_26/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_26/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_26/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_26/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_26/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_26/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_14/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_14/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_14/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_14/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_14/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_14/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_26/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_26/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_14/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_14/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_14/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_14/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_14/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_14/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_15/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_15/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_15/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_15/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_14/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_14/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_15/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_15/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_15/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_15/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_15/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_15/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_15/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_15/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_4/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_4/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_4/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_4/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_15/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_15/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_4/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_4/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_4/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_4/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_4/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_4/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_4/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_4/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_27/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_27/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_27/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_27/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_4/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_4/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_27/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_27/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_27/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_27/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_27/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_27/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_27/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_27/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_2/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_2/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_2/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_2/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_27/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_27/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_2/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_2/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_2/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_2/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_2/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_2/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_2/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_2/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_36/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_36/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_36/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_36/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_2/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_2/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_36/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_36/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_36/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_36/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_36/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_36/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_36/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_36/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_28/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_28/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_28/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_28/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_36/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_36/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_28/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_28/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_28/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_28/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_28/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_28/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_28/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_28/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_18/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_18/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_28/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_28/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_18/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_18/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_18/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_18/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_18/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_18/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_18/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_18/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_18/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_18/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_25/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_25/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_25/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_25/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_25/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_25/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_18/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_18/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_25/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_25/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_25/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_25/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_25/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_25/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_37/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_37/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_25/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_25/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_37/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_37/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_37/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_37/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_37/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_37/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_37/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_37/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_37/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_37/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_30/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_30/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_30/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_30/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_37/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_37/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_30/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_30/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_30/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_30/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_30/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_30/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_30/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_30/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_29/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_29/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_29/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_29/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_30/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_30/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_29/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_29/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_29/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_29/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_29/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_29/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_29/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_29/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_16/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_16/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_16/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_16/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_29/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_29/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_16/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_16/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_16/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_16/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_16/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_16/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_16/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_16/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_13/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_13/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_13/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_13/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_13/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_13/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_16/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_16/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_13/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_13/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_13/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_13/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_13/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_13/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_10/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_10/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_10/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_10/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_13/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_13/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_10/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_10/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_10/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_10/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_10/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_10/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_10/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_10/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_19/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_19/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_19/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_19/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_10/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_10/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_19/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_19/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_19/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_19/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_19/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_19/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_19/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_19/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_6/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_6/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_6/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_6/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_19/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_19/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_6/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_6/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_6/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_6/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_6/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_6/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_6/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_6/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_11/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_11/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_11/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_11/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_6/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_6/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_11/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_11/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_11/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_11/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_11/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_11/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_11/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_11/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_20/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_20/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_20/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_20/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_11/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_11/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_20/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_20/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_20/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_20/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_20/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_20/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_20/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_20/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_23/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_23/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_23/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_23/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_20/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_20/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_23/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_23/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_23/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_23/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_23/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_23/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_23/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_23/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_35/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_35/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_35/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_35/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_23/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_23/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_35/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_35/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_35/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_35/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_35/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_35/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_35/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_35/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_38/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_38/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_38/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_38/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_38/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_38/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_35/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_35/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_38/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_38/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_38/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_38/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_38/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_38/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_17/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_17/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_17/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_17/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_38/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_38/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_17/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_17/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_17/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_17/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_17/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_17/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_17/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_17/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_9/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_9/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_17/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_17/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_9/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_9/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_9/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_9/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_9/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_9/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_9/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_9/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_9/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_9/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_22/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_22/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_22/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_22/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_9/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_9/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_22/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_22/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_22/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_22/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_22/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_22/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_22/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_22/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_24/self_attn.q_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_24/self_attn.q_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_24/mlp.gate_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_24/mlp.gate_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_22/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_22/mlp.down_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_24/mlp.up_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_24/mlp.up_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_24/self_attn.k_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_24/self_attn.k_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_24/self_attn.o_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_24/self_attn.o_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_24/self_attn.v_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_24/self_attn.v_proj/compressed.pt', 'meta-llama/Llama-2-13b-hf/layer_24/mlp.down_proj': '/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-13b-hf/layer_24/mlp.down_proj/compressed.pt'} 27%|‚ñà‚ñà‚ñã       | 505/1848 [12:15:10<57:37:58, 154.49s/it] 27%|‚ñà‚ñà‚ñã       | 506/1848 [12:16:15<48:15:20, 129.45s/it] 27%|‚ñà‚ñà‚ñã       | 507/1848 [12:26:11<97:45:12, 262.43s/it] 27%|‚ñà‚ñà‚ñã       | 508/1848 [12:30:41<98:30:02, 264.63s/it]
/data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/checkpoints.yaml
perplexity_inference_command:
 python -u perplexity_eval.py --base_model meta-llama/Llama-2-13b-hf --seqlen 4096 --checkpoint_list_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/checkpoints.yaml --log_wandb --wandb_project compression_no_finetune --wandb_id 29e1fiyr
meta-llama/Llama-2-70b-hf/layer_3/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_3/self_attn.k_proj/compressed.log
best_loss 2.4491939544677734 running bpv: 2.008078
COMMANDS_FINISHED 506 n_commands 1850
running: nohup python -u perplexity_eval.py --base_model meta-llama/Llama-2-13b-hf --seqlen 4096 --checkpoint_list_path /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/checkpoints.yaml --log_wandb --wandb_project compression_no_finetune --wandb_id 29e1fiyr --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-13b-hf/compressed/amber-serenity-59/ppl_eval.log 2>&1 &
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf/layer_3/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_3/self_attn.q_proj/compressed.log
best_loss 7.222670078277588 running bpv: 2.00807
COMMANDS_FINISHED 507 n_commands 1850
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/hessians_new/pajama/128/layer_3/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_3/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_3/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf/layer_3/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_3/self_attn.o_proj/compressed.log
best_loss 0.19789989292621613 running bpv: 2.008062
COMMANDS_FINISHED 508 n_commands 1850
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/hessians_new/pajama/128/layer_3/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_3/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_3/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
eval is done
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/hessians_new/pajama/128/layer_3/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_3/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_3/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf/layer_3/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_3/self_attn.v_proj/compressed.log
best_loss 0.3690339922904968 running bpv: 2.008071
COMMANDS_FINISHED 510 n_commands 1850
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/hessians_new/pajama/128/layer_5/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_5/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_5/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf/layer_3/mlp.gate_proj is done
reading log  28%|‚ñà‚ñà‚ñä       | 509/1848 [12:38:46<122:25:41, 329.16s/it] 28%|‚ñà‚ñà‚ñä       | 510/1848 [12:41:21<103:15:01, 277.80s/it] 28%|‚ñà‚ñà‚ñä       | 511/1848 [12:51:36<140:17:54, 377.77s/it] 28%|‚ñà‚ñà‚ñä       | 512/1848 [12:53:21<110:04:55, 296.63s/it] 28%|‚ñà‚ñà‚ñä       | 513/1848 [13:03:46<146:19:13, 394.57s/it]/data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_3/mlp.gate_proj/compressed.log
best_loss 14.845229148864746 running bpv: 2.008017
COMMANDS_FINISHED 511 n_commands 1850
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/hessians_new/pajama/128/layer_5/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_5/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_5/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf/layer_3/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_3/mlp.up_proj/compressed.log
best_loss 13.563179016113281 running bpv: 2.007965
COMMANDS_FINISHED 512 n_commands 1850
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/hessians_new/pajama/128/layer_5/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_5/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_5/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf/layer_5/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_5/self_attn.q_proj/compressed.log
best_loss 22.937379837036133 running bpv: 2.007957
COMMANDS_FINISHED 513 n_commands 1850
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/hessians_new/pajama/128/layer_5/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_5/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_5/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf/layer_5/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_5/self_attn.k_proj/compressed.log
best_loss 6.593013763427734 running bpv: 2.007966
COMMANDS_FINISHED 514 n_commands 1850
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/hessians_new/pajama/128/layer_5/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_5/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_5/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf/layer_5/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_5/self_attn.o_proj/compressed.log
best_loss 0.2645606994628906 running bpv: 2.007959
COMMANDS_FINISHED 515 n_commands 1850
 28%|‚ñà‚ñà‚ñä       | 514/1848 [13:07:41<128:32:52, 346.91s/it] 28%|‚ñà‚ñà‚ñä       | 515/1848 [13:09:26<101:39:31, 274.55s/it] 28%|‚ñà‚ñà‚ñä       | 516/1848 [13:16:11<116:02:21, 313.62s/it] 28%|‚ñà‚ñà‚ñä       | 517/1848 [13:17:46<91:44:23, 248.13s/it]  28%|‚ñà‚ñà‚ñä       | 518/1848 [13:28:01<132:17:50, 358.10s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/hessians_new/pajama/128/layer_5/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_5/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_5/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf/layer_5/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_5/mlp.gate_proj/compressed.log
best_loss 30.583209991455078 running bpv: 2.007908
COMMANDS_FINISHED 516 n_commands 1850
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/hessians_new/pajama/128/layer_5/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_5/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_5/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf/layer_5/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_5/self_attn.v_proj/compressed.log
best_loss 1.1766016483306885 running bpv: 2.007916
COMMANDS_FINISHED 517 n_commands 1850
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/hessians_new/pajama/128/layer_8/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_8/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_8/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf/layer_5/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_5/mlp.up_proj/compressed.log
best_loss 27.80329132080078 running bpv: 2.007867
COMMANDS_FINISHED 518 n_commands 1850
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/hessians_new/pajama/128/layer_8/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_8/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_8/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf/layer_3/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_3/mlp.down_proj/compressed.log
best_loss 0.3590157926082611 running bpv: 2.007812
COMMANDS_FINISHED 519 n_commands 1850
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/hessians_new/pajama/128/layer_8/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_8/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_8/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf/layer_8/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_8/self_attn.q_proj/compressed.log
best_loss 40.03887176513672 running bpv: 2.007806
COMMANDS_FINISHED 520 n_commands 1850
 28%|‚ñà‚ñà‚ñä       | 519/1848 [13:29:56<105:17:40, 285.22s/it] 28%|‚ñà‚ñà‚ñä       | 520/1848 [13:40:41<145:01:04, 393.12s/it] 28%|‚ñà‚ñà‚ñä       | 521/1848 [13:47:06<144:00:54, 390.70s/it] 28%|‚ñà‚ñà‚ñä       | 522/1848 [13:48:51<112:20:46, 305.01s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/hessians_new/pajama/128/layer_8/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_8/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_8/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf/layer_8/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_8/self_attn.k_proj/compressed.log
best_loss 12.10102367401123 running bpv: 2.007814
COMMANDS_FINISHED 521 n_commands 1850
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/hessians_new/pajama/128/layer_8/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_8/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_8/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf/layer_8/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_8/self_attn.o_proj/compressed.log
best_loss 1.238572597503662 running bpv: 2.007808
COMMANDS_FINISHED 522 n_commands 1850
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/hessians_new/pajama/128/layer_8/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_8/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_8/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf/layer_8/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_8/mlp.gate_proj/compressed.log
best_loss 69.99726867675781 running bpv: 2.00776
COMMANDS_FINISHED 523 n_commands 1850
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/hessians_new/pajama/128/layer_8/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_8/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_8/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf/layer_8/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_8/self_attn.v_proj/compressed.log
best_loss 1.9780762195587158 running bpv: 2.007769
COMMANDS_FINISHED 524 n_commands 1850
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/hessians_new/pajama/128/layer_76/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_76/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_76/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
 28%|‚ñà‚ñà‚ñä       | 523/1848 [13:54:16<114:28:22, 311.02s/it] 28%|‚ñà‚ñà‚ñä       | 524/1848 [13:57:31<101:35:23, 276.23s/it] 28%|‚ñà‚ñà‚ñä       | 525/1848 [14:08:46<145:29:02, 395.87s/it] 28%|‚ñà‚ñà‚ñä       | 526/1848 [14:10:31<113:19:56, 308.62s/it] 29%|‚ñà‚ñà‚ñä       | 527/1848 [14:21:16<150:16:58, 409.55s/it]meta-llama/Llama-2-70b-hf/layer_8/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_8/mlp.up_proj/compressed.log
best_loss 61.48870086669922 running bpv: 2.007722
COMMANDS_FINISHED 525 n_commands 1850
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/hessians_new/pajama/128/layer_76/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_76/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_76/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf/layer_5/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_5/mlp.down_proj/compressed.log
best_loss 0.7981138229370117 running bpv: 2.007672
COMMANDS_FINISHED 526 n_commands 1850
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/hessians_new/pajama/128/layer_76/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_76/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_76/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf/layer_76/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_76/self_attn.q_proj/compressed.log
best_loss 178.71197509765625 running bpv: 2.007666
COMMANDS_FINISHED 527 n_commands 1850
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/hessians_new/pajama/128/layer_76/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_76/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_76/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf/layer_76/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_76/self_attn.k_proj/compressed.log
best_loss 43.47418212890625 running bpv: 2.007674
COMMANDS_FINISHED 528 n_commands 1850
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/hessians_new/pajama/128/layer_76/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_76/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_76/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf/layer_76/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_76/self_attn.o_proj/compressed.log
best_loss 88.3006591796875 running bpv: 2.007668
COMMANDS_FINISHED 529 n_commands 1850
 29%|‚ñà‚ñà‚ñä       | 528/1848 [14:28:11<150:46:20, 411.20s/it] 29%|‚ñà‚ñà‚ñä       | 529/1848 [14:29:56<117:00:13, 319.34s/it] 29%|‚ñà‚ñà‚ñä       | 530/1848 [14:33:31<105:27:28, 288.05s/it] 29%|‚ñà‚ñà‚ñä       | 531/1848 [14:34:56<83:05:43, 227.14s/it]  29%|‚ñà‚ñà‚ñâ       | 532/1848 [14:45:21<126:40:14, 346.52s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/hessians_new/pajama/128/layer_76/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_76/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_76/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf/layer_76/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_76/mlp.gate_proj/compressed.log
best_loss 1185.1412353515625 running bpv: 2.007625
COMMANDS_FINISHED 530 n_commands 1850
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/hessians_new/pajama/128/layer_76/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_76/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_76/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf/layer_76/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_76/self_attn.v_proj/compressed.log
best_loss 45.56121063232422 running bpv: 2.007632
COMMANDS_FINISHED 531 n_commands 1850
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/hessians_new/pajama/128/layer_66/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_66/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_66/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf/layer_76/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_76/mlp.up_proj/compressed.log
best_loss 1077.14501953125 running bpv: 2.007589
COMMANDS_FINISHED 532 n_commands 1850
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/hessians_new/pajama/128/layer_66/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_66/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_66/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf/layer_8/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_8/mlp.down_proj/compressed.log
best_loss 15.019314765930176 running bpv: 2.007542
COMMANDS_FINISHED 533 n_commands 1850
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/hessians_new/pajama/128/layer_66/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_66/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_66/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf/layer_66/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_66/self_attn.q_proj/compressed.log
best_loss 49.47679901123047 running bpv: 2.007537
COMMANDS_FINISHED 534 n_commands 1850
 29%|‚ñà‚ñà‚ñâ       | 533/1848 [14:47:16<101:12:20, 277.06s/it] 29%|‚ñà‚ñà‚ñâ       | 534/1848 [14:57:51<140:19:45, 384.46s/it] 29%|‚ñà‚ñà‚ñâ       | 535/1848 [15:08:07<165:27:15, 453.64s/it] 29%|‚ñà‚ñà‚ñâ       | 536/1848 [15:09:52<127:12:40, 349.06s/it]running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/hessians_new/pajama/128/layer_66/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_66/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_66/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf/layer_66/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_66/self_attn.k_proj/compressed.log
best_loss 15.220499992370605 running bpv: 2.007545
COMMANDS_FINISHED 535 n_commands 1850
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/hessians_new/pajama/128/layer_66/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_66/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_66/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf/layer_66/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_66/self_attn.o_proj/compressed.log
best_loss 10.08659839630127 running bpv: 2.00754
COMMANDS_FINISHED 536 n_commands 1850
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/hessians_new/pajama/128/layer_66/mlp.down_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_66/mlp.down_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:5 > /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_66/mlp.down_proj/compressed.log 2>&1 &
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf/layer_66/mlp.gate_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_66/mlp.gate_proj/compressed.log
best_loss 990.5849609375 running bpv: 2.007499
COMMANDS_FINISHED 537 n_commands 1850
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/hessians_new/pajama/128/layer_66/self_attn.v_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_66/self_attn.v_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_66/self_attn.v_proj/compressed.log 2>&1 &
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf/layer_66/self_attn.v_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_66/self_attn.v_proj/compressed.log
best_loss 15.133639335632324 running bpv: 2.007506
COMMANDS_FINISHED 538 n_commands 1850
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/hessians_new/pajama/128/layer_68/mlp.gate_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_68/mlp.gate_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:7 > /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_68/mlp.gate_proj/compressed.log 2>&1 &
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
 29%|‚ñà‚ñà‚ñâ       | 537/1848 [15:11:37<100:27:13, 275.85s/it] 29%|‚ñà‚ñà‚ñâ       | 538/1848 [15:14:52<91:33:14, 251.60s/it]  29%|‚ñà‚ñà‚ñâ       | 539/1848 [15:25:07<131:07:55, 360.64s/it] 29%|‚ñà‚ñà‚ñâ       | 540/1848 [15:26:52<103:10:07, 283.95s/it] 29%|‚ñà‚ñà‚ñâ       | 541/1848 [15:36:57<138:03:46, 380.28s/it]meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf/layer_66/mlp.up_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_66/mlp.up_proj/compressed.log
best_loss 853.093017578125 running bpv: 2.007466
COMMANDS_FINISHED 539 n_commands 1850
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/hessians_new/pajama/128/layer_68/mlp.up_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_68/mlp.up_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:6 > /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_68/mlp.up_proj/compressed.log 2>&1 &
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf/layer_76/mlp.down_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_76/mlp.down_proj/compressed.log
best_loss 276.35693359375 running bpv: 2.007422
COMMANDS_FINISHED 540 n_commands 1850
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/hessians_new/pajama/128/layer_68/self_attn.q_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_68/self_attn.q_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_68/self_attn.q_proj/compressed.log 2>&1 &
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf/layer_68/self_attn.q_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_68/self_attn.q_proj/compressed.log
best_loss 98.47784423828125 running bpv: 2.007417
COMMANDS_FINISHED 541 n_commands 1850
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/hessians_new/pajama/128/layer_68/self_attn.k_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_68/self_attn.k_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_68/self_attn.k_proj/compressed.log 2>&1 &
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf/layer_68/self_attn.k_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_68/self_attn.k_proj/compressed.log
best_loss 30.12891960144043 running bpv: 2.007425
COMMANDS_FINISHED 542 n_commands 1850
running: nohup python -u scripts/1layer_compress/quantize_compress.py --load_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/hessians_new/pajama/128/layer_68/self_attn.o_proj.pt --save_path /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_68/self_attn.o_proj/compressed.pt --yaml_path /data/lliu/huffman/scripts/1layer_compress/quantizer_args.yaml --device cuda:4 > /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_68/self_attn.o_proj/compressed.log 2>&1 &
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-hf/layer_68/self_attn.o_proj is done
reading log /data/lliu/huffman/models/meta-llama/Llama-2-70b-hf/compressed/amber-serenity-59/meta-llama/Llama-2-70b-hf/layer_68/self_attn.o_proj/compressed.log
best_loss 10.009456634521484 running bpv: 2.00742
COMMANDS_FINISHED 543 n_commands 1850
 29%|‚ñà‚ñà‚ñâ       | 542/1848 [15:46:02<155:53:19, 429.71s/it] 29%|‚ñà‚ñà‚ñâ       | 543/1848 [15:48:17<123:43:17, 341.30s/it] 29%|‚ñà‚ñà‚ñâ       | 544/1848 [15:48:52<90:20:34, 249.41s/it]  29%|‚ñà‚ñà‚ñâ       | 545/1848 [15:49:27<66:59:34, 185.09s/it]Terminated
