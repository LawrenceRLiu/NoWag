seed: 0
output_dir: /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/ft_results/run_38_2
lr: 1.0e-5
soft_labels: True

dataset:
  name: pajama
  ft_n_train: 2048
  ft_n_val: 32
  cache_dir: ./temp/${seed}/${dataset.name}/${soft_labels}
  overwrite: True
  logits_batch_size: 8

model:
  base_model: meta-llama/Llama-2-7b-hf
  quantized_model_path: /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/ft_layer_by_layer/run_38_1
  seqlen: 4096
  freeze_embeddings: False


ft_args:
  _target_: transformers.TrainingArguments
  fp16: True
  gradient_checkpointing: True
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 4
  max_grad_norm: 1.0
  num_train_epochs: 3
  # weight_decay: 0.01
  evaluation_strategy: epoch
  save_strategy: epoch
  save_total_limit: 1
  load_best_model_at_end: True
  dataloader_pin_memory: True
  gradient_accumulation_steps: 4
  lr_scheduler_type: "constant"
  warmup_steps: 1000
  logging_steps: 1
