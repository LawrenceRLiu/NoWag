{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lliu/miniconda3/envs/NoWAC-VQ/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# set to auto reload modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "!CUDA_VISIBLE_DEVICES=0,1\n",
    "\n",
    "import torch\n",
    "import yaml \n",
    "import os \n",
    "import glob\n",
    "import argparse\n",
    "from src.model.llama import LlamaForCausalLM\n",
    "from transformers import LlamaForCausalLM as OrigLlama\n",
    "from transformers import AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = \"meta-llama/Llama-2-7b-hf\"\n",
    "checkpoints_path = \"/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/run_38/checkpoints.yaml\"\n",
    "hf_model_save_path = \"/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed_hf/run_38/\"\n",
    "add_bias = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 13.26it/s]\n"
     ]
    }
   ],
   "source": [
    "orig_config = AutoConfig.from_pretrained(base_model,dtype = \"auto\",\n",
    "                                         device_map=\"cpu\",\n",
    "                                        attn_implementation='sdpa')\n",
    "orig_model = OrigLlama.from_pretrained(base_model, config=orig_config, torch_dtype=\"auto\",\n",
    "                                        device_map=\"cpu\",\n",
    "                                        low_cpu_mem_usage=True, attn_implementation='sdpa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in orig_model.named_parameters():\n",
    "    assert param.dtype == torch.float16, f\"{name} is not fp16, it is {param.dtype}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints_dict = yaml.load(open(checkpoints_path, \"r\"), Loader=yaml.FullLoader)\n",
    "\n",
    "\n",
    "compression_kwargs = yaml.load(open((checkpoints_dict[list(checkpoints_dict.keys())[0]]).replace(\"compressed.pt\", \"compressed_args.yaml\")),\n",
    "                                Loader=yaml.FullLoader)\n",
    "#check that all the other checkpoints have the same compression args\n",
    "for checkpoint in checkpoints_dict.values():\n",
    "    assert compression_kwargs == yaml.load(open(checkpoint.replace(\"compressed.pt\", \"compressed_args.yaml\"), \"r\"), Loader=yaml.FullLoader)\n",
    "\n",
    "#remove dtype from the compression kwargs\n",
    "compression_kwargs.pop(\"dtype\", None)\n",
    "\n",
    "compression_type = compression_kwargs[\"compression_type\"]\n",
    "\n",
    "\n",
    "compression_config = {\"compression_kwargs\": compression_kwargs, \"compression_type\": compression_type,\n",
    "                        \"add_bias\": add_bias, \"skip_list\":None}\n",
    "\n",
    "orig_config.compress_config = compression_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  cpu dtype:  torch.float16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['model.layers.0.self_attn.q_proj.bias', 'model.layers.0.self_attn.q_proj.codebook', 'model.layers.0.self_attn.q_proj.assignments', 'model.layers.0.self_attn.q_proj.normalizer.norms.0', 'model.layers.0.self_attn.q_proj.normalizer.norms.1', 'model.layers.0.self_attn.q_proj.normalizer.zeros.0', 'model.layers.0.self_attn.q_proj.normalizer.zeros.1', 'model.layers.0.self_attn.k_proj.bias', 'model.layers.0.self_attn.k_proj.codebook', 'model.layers.0.self_attn.k_proj.assignments', 'model.layers.0.self_attn.k_proj.normalizer.norms.0', 'model.layers.0.self_attn.k_proj.normalizer.norms.1', 'model.layers.0.self_attn.k_proj.normalizer.zeros.0', 'model.layers.0.self_attn.k_proj.normalizer.zeros.1', 'model.layers.0.self_attn.v_proj.bias', 'model.layers.0.self_attn.v_proj.codebook', 'model.layers.0.self_attn.v_proj.assignments', 'model.layers.0.self_attn.v_proj.normalizer.norms.0', 'model.layers.0.self_attn.v_proj.normalizer.norms.1', 'model.layers.0.self_attn.v_proj.normalizer.zeros.0', 'model.layers.0.self_attn.v_proj.normalizer.zeros.1', 'model.layers.0.self_attn.o_proj.bias', 'model.layers.0.self_attn.o_proj.codebook', 'model.layers.0.self_attn.o_proj.assignments', 'model.layers.0.self_attn.o_proj.normalizer.norms.0', 'model.layers.0.self_attn.o_proj.normalizer.norms.1', 'model.layers.0.self_attn.o_proj.normalizer.zeros.0', 'model.layers.0.self_attn.o_proj.normalizer.zeros.1', 'model.layers.0.mlp.gate_proj.bias', 'model.layers.0.mlp.gate_proj.codebook', 'model.layers.0.mlp.gate_proj.assignments', 'model.layers.0.mlp.gate_proj.normalizer.norms.0', 'model.layers.0.mlp.gate_proj.normalizer.norms.1', 'model.layers.0.mlp.gate_proj.normalizer.zeros.0', 'model.layers.0.mlp.gate_proj.normalizer.zeros.1', 'model.layers.0.mlp.up_proj.bias', 'model.layers.0.mlp.up_proj.codebook', 'model.layers.0.mlp.up_proj.assignments', 'model.layers.0.mlp.up_proj.normalizer.norms.0', 'model.layers.0.mlp.up_proj.normalizer.norms.1', 'model.layers.0.mlp.up_proj.normalizer.zeros.0', 'model.layers.0.mlp.up_proj.normalizer.zeros.1', 'model.layers.0.mlp.down_proj.bias', 'model.layers.0.mlp.down_proj.codebook', 'model.layers.0.mlp.down_proj.assignments', 'model.layers.0.mlp.down_proj.normalizer.norms.0', 'model.layers.0.mlp.down_proj.normalizer.norms.1', 'model.layers.0.mlp.down_proj.normalizer.zeros.0', 'model.layers.0.mlp.down_proj.normalizer.zeros.1', 'model.layers.1.self_attn.q_proj.bias', 'model.layers.1.self_attn.q_proj.codebook', 'model.layers.1.self_attn.q_proj.assignments', 'model.layers.1.self_attn.q_proj.normalizer.norms.0', 'model.layers.1.self_attn.q_proj.normalizer.norms.1', 'model.layers.1.self_attn.q_proj.normalizer.zeros.0', 'model.layers.1.self_attn.q_proj.normalizer.zeros.1', 'model.layers.1.self_attn.k_proj.bias', 'model.layers.1.self_attn.k_proj.codebook', 'model.layers.1.self_attn.k_proj.assignments', 'model.layers.1.self_attn.k_proj.normalizer.norms.0', 'model.layers.1.self_attn.k_proj.normalizer.norms.1', 'model.layers.1.self_attn.k_proj.normalizer.zeros.0', 'model.layers.1.self_attn.k_proj.normalizer.zeros.1', 'model.layers.1.self_attn.v_proj.bias', 'model.layers.1.self_attn.v_proj.codebook', 'model.layers.1.self_attn.v_proj.assignments', 'model.layers.1.self_attn.v_proj.normalizer.norms.0', 'model.layers.1.self_attn.v_proj.normalizer.norms.1', 'model.layers.1.self_attn.v_proj.normalizer.zeros.0', 'model.layers.1.self_attn.v_proj.normalizer.zeros.1', 'model.layers.1.self_attn.o_proj.bias', 'model.layers.1.self_attn.o_proj.codebook', 'model.layers.1.self_attn.o_proj.assignments', 'model.layers.1.self_attn.o_proj.normalizer.norms.0', 'model.layers.1.self_attn.o_proj.normalizer.norms.1', 'model.layers.1.self_attn.o_proj.normalizer.zeros.0', 'model.layers.1.self_attn.o_proj.normalizer.zeros.1', 'model.layers.1.mlp.gate_proj.bias', 'model.layers.1.mlp.gate_proj.codebook', 'model.layers.1.mlp.gate_proj.assignments', 'model.layers.1.mlp.gate_proj.normalizer.norms.0', 'model.layers.1.mlp.gate_proj.normalizer.norms.1', 'model.layers.1.mlp.gate_proj.normalizer.zeros.0', 'model.layers.1.mlp.gate_proj.normalizer.zeros.1', 'model.layers.1.mlp.up_proj.bias', 'model.layers.1.mlp.up_proj.codebook', 'model.layers.1.mlp.up_proj.assignments', 'model.layers.1.mlp.up_proj.normalizer.norms.0', 'model.layers.1.mlp.up_proj.normalizer.norms.1', 'model.layers.1.mlp.up_proj.normalizer.zeros.0', 'model.layers.1.mlp.up_proj.normalizer.zeros.1', 'model.layers.1.mlp.down_proj.bias', 'model.layers.1.mlp.down_proj.codebook', 'model.layers.1.mlp.down_proj.assignments', 'model.layers.1.mlp.down_proj.normalizer.norms.0', 'model.layers.1.mlp.down_proj.normalizer.norms.1', 'model.layers.1.mlp.down_proj.normalizer.zeros.0', 'model.layers.1.mlp.down_proj.normalizer.zeros.1', 'model.layers.2.self_attn.q_proj.bias', 'model.layers.2.self_attn.q_proj.codebook', 'model.layers.2.self_attn.q_proj.assignments', 'model.layers.2.self_attn.q_proj.normalizer.norms.0', 'model.layers.2.self_attn.q_proj.normalizer.norms.1', 'model.layers.2.self_attn.q_proj.normalizer.zeros.0', 'model.layers.2.self_attn.q_proj.normalizer.zeros.1', 'model.layers.2.self_attn.k_proj.bias', 'model.layers.2.self_attn.k_proj.codebook', 'model.layers.2.self_attn.k_proj.assignments', 'model.layers.2.self_attn.k_proj.normalizer.norms.0', 'model.layers.2.self_attn.k_proj.normalizer.norms.1', 'model.layers.2.self_attn.k_proj.normalizer.zeros.0', 'model.layers.2.self_attn.k_proj.normalizer.zeros.1', 'model.layers.2.self_attn.v_proj.bias', 'model.layers.2.self_attn.v_proj.codebook', 'model.layers.2.self_attn.v_proj.assignments', 'model.layers.2.self_attn.v_proj.normalizer.norms.0', 'model.layers.2.self_attn.v_proj.normalizer.norms.1', 'model.layers.2.self_attn.v_proj.normalizer.zeros.0', 'model.layers.2.self_attn.v_proj.normalizer.zeros.1', 'model.layers.2.self_attn.o_proj.bias', 'model.layers.2.self_attn.o_proj.codebook', 'model.layers.2.self_attn.o_proj.assignments', 'model.layers.2.self_attn.o_proj.normalizer.norms.0', 'model.layers.2.self_attn.o_proj.normalizer.norms.1', 'model.layers.2.self_attn.o_proj.normalizer.zeros.0', 'model.layers.2.self_attn.o_proj.normalizer.zeros.1', 'model.layers.2.mlp.gate_proj.bias', 'model.layers.2.mlp.gate_proj.codebook', 'model.layers.2.mlp.gate_proj.assignments', 'model.layers.2.mlp.gate_proj.normalizer.norms.0', 'model.layers.2.mlp.gate_proj.normalizer.norms.1', 'model.layers.2.mlp.gate_proj.normalizer.zeros.0', 'model.layers.2.mlp.gate_proj.normalizer.zeros.1', 'model.layers.2.mlp.up_proj.bias', 'model.layers.2.mlp.up_proj.codebook', 'model.layers.2.mlp.up_proj.assignments', 'model.layers.2.mlp.up_proj.normalizer.norms.0', 'model.layers.2.mlp.up_proj.normalizer.norms.1', 'model.layers.2.mlp.up_proj.normalizer.zeros.0', 'model.layers.2.mlp.up_proj.normalizer.zeros.1', 'model.layers.2.mlp.down_proj.bias', 'model.layers.2.mlp.down_proj.codebook', 'model.layers.2.mlp.down_proj.assignments', 'model.layers.2.mlp.down_proj.normalizer.norms.0', 'model.layers.2.mlp.down_proj.normalizer.norms.1', 'model.layers.2.mlp.down_proj.normalizer.zeros.0', 'model.layers.2.mlp.down_proj.normalizer.zeros.1', 'model.layers.3.self_attn.q_proj.bias', 'model.layers.3.self_attn.q_proj.codebook', 'model.layers.3.self_attn.q_proj.assignments', 'model.layers.3.self_attn.q_proj.normalizer.norms.0', 'model.layers.3.self_attn.q_proj.normalizer.norms.1', 'model.layers.3.self_attn.q_proj.normalizer.zeros.0', 'model.layers.3.self_attn.q_proj.normalizer.zeros.1', 'model.layers.3.self_attn.k_proj.bias', 'model.layers.3.self_attn.k_proj.codebook', 'model.layers.3.self_attn.k_proj.assignments', 'model.layers.3.self_attn.k_proj.normalizer.norms.0', 'model.layers.3.self_attn.k_proj.normalizer.norms.1', 'model.layers.3.self_attn.k_proj.normalizer.zeros.0', 'model.layers.3.self_attn.k_proj.normalizer.zeros.1', 'model.layers.3.self_attn.v_proj.bias', 'model.layers.3.self_attn.v_proj.codebook', 'model.layers.3.self_attn.v_proj.assignments', 'model.layers.3.self_attn.v_proj.normalizer.norms.0', 'model.layers.3.self_attn.v_proj.normalizer.norms.1', 'model.layers.3.self_attn.v_proj.normalizer.zeros.0', 'model.layers.3.self_attn.v_proj.normalizer.zeros.1', 'model.layers.3.self_attn.o_proj.bias', 'model.layers.3.self_attn.o_proj.codebook', 'model.layers.3.self_attn.o_proj.assignments', 'model.layers.3.self_attn.o_proj.normalizer.norms.0', 'model.layers.3.self_attn.o_proj.normalizer.norms.1', 'model.layers.3.self_attn.o_proj.normalizer.zeros.0', 'model.layers.3.self_attn.o_proj.normalizer.zeros.1', 'model.layers.3.mlp.gate_proj.bias', 'model.layers.3.mlp.gate_proj.codebook', 'model.layers.3.mlp.gate_proj.assignments', 'model.layers.3.mlp.gate_proj.normalizer.norms.0', 'model.layers.3.mlp.gate_proj.normalizer.norms.1', 'model.layers.3.mlp.gate_proj.normalizer.zeros.0', 'model.layers.3.mlp.gate_proj.normalizer.zeros.1', 'model.layers.3.mlp.up_proj.bias', 'model.layers.3.mlp.up_proj.codebook', 'model.layers.3.mlp.up_proj.assignments', 'model.layers.3.mlp.up_proj.normalizer.norms.0', 'model.layers.3.mlp.up_proj.normalizer.norms.1', 'model.layers.3.mlp.up_proj.normalizer.zeros.0', 'model.layers.3.mlp.up_proj.normalizer.zeros.1', 'model.layers.3.mlp.down_proj.bias', 'model.layers.3.mlp.down_proj.codebook', 'model.layers.3.mlp.down_proj.assignments', 'model.layers.3.mlp.down_proj.normalizer.norms.0', 'model.layers.3.mlp.down_proj.normalizer.norms.1', 'model.layers.3.mlp.down_proj.normalizer.zeros.0', 'model.layers.3.mlp.down_proj.normalizer.zeros.1', 'model.layers.4.self_attn.q_proj.bias', 'model.layers.4.self_attn.q_proj.codebook', 'model.layers.4.self_attn.q_proj.assignments', 'model.layers.4.self_attn.q_proj.normalizer.norms.0', 'model.layers.4.self_attn.q_proj.normalizer.norms.1', 'model.layers.4.self_attn.q_proj.normalizer.zeros.0', 'model.layers.4.self_attn.q_proj.normalizer.zeros.1', 'model.layers.4.self_attn.k_proj.bias', 'model.layers.4.self_attn.k_proj.codebook', 'model.layers.4.self_attn.k_proj.assignments', 'model.layers.4.self_attn.k_proj.normalizer.norms.0', 'model.layers.4.self_attn.k_proj.normalizer.norms.1', 'model.layers.4.self_attn.k_proj.normalizer.zeros.0', 'model.layers.4.self_attn.k_proj.normalizer.zeros.1', 'model.layers.4.self_attn.v_proj.bias', 'model.layers.4.self_attn.v_proj.codebook', 'model.layers.4.self_attn.v_proj.assignments', 'model.layers.4.self_attn.v_proj.normalizer.norms.0', 'model.layers.4.self_attn.v_proj.normalizer.norms.1', 'model.layers.4.self_attn.v_proj.normalizer.zeros.0', 'model.layers.4.self_attn.v_proj.normalizer.zeros.1', 'model.layers.4.self_attn.o_proj.bias', 'model.layers.4.self_attn.o_proj.codebook', 'model.layers.4.self_attn.o_proj.assignments', 'model.layers.4.self_attn.o_proj.normalizer.norms.0', 'model.layers.4.self_attn.o_proj.normalizer.norms.1', 'model.layers.4.self_attn.o_proj.normalizer.zeros.0', 'model.layers.4.self_attn.o_proj.normalizer.zeros.1', 'model.layers.4.mlp.gate_proj.bias', 'model.layers.4.mlp.gate_proj.codebook', 'model.layers.4.mlp.gate_proj.assignments', 'model.layers.4.mlp.gate_proj.normalizer.norms.0', 'model.layers.4.mlp.gate_proj.normalizer.norms.1', 'model.layers.4.mlp.gate_proj.normalizer.zeros.0', 'model.layers.4.mlp.gate_proj.normalizer.zeros.1', 'model.layers.4.mlp.up_proj.bias', 'model.layers.4.mlp.up_proj.codebook', 'model.layers.4.mlp.up_proj.assignments', 'model.layers.4.mlp.up_proj.normalizer.norms.0', 'model.layers.4.mlp.up_proj.normalizer.norms.1', 'model.layers.4.mlp.up_proj.normalizer.zeros.0', 'model.layers.4.mlp.up_proj.normalizer.zeros.1', 'model.layers.4.mlp.down_proj.bias', 'model.layers.4.mlp.down_proj.codebook', 'model.layers.4.mlp.down_proj.assignments', 'model.layers.4.mlp.down_proj.normalizer.norms.0', 'model.layers.4.mlp.down_proj.normalizer.norms.1', 'model.layers.4.mlp.down_proj.normalizer.zeros.0', 'model.layers.4.mlp.down_proj.normalizer.zeros.1', 'model.layers.5.self_attn.q_proj.bias', 'model.layers.5.self_attn.q_proj.codebook', 'model.layers.5.self_attn.q_proj.assignments', 'model.layers.5.self_attn.q_proj.normalizer.norms.0', 'model.layers.5.self_attn.q_proj.normalizer.norms.1', 'model.layers.5.self_attn.q_proj.normalizer.zeros.0', 'model.layers.5.self_attn.q_proj.normalizer.zeros.1', 'model.layers.5.self_attn.k_proj.bias', 'model.layers.5.self_attn.k_proj.codebook', 'model.layers.5.self_attn.k_proj.assignments', 'model.layers.5.self_attn.k_proj.normalizer.norms.0', 'model.layers.5.self_attn.k_proj.normalizer.norms.1', 'model.layers.5.self_attn.k_proj.normalizer.zeros.0', 'model.layers.5.self_attn.k_proj.normalizer.zeros.1', 'model.layers.5.self_attn.v_proj.bias', 'model.layers.5.self_attn.v_proj.codebook', 'model.layers.5.self_attn.v_proj.assignments', 'model.layers.5.self_attn.v_proj.normalizer.norms.0', 'model.layers.5.self_attn.v_proj.normalizer.norms.1', 'model.layers.5.self_attn.v_proj.normalizer.zeros.0', 'model.layers.5.self_attn.v_proj.normalizer.zeros.1', 'model.layers.5.self_attn.o_proj.bias', 'model.layers.5.self_attn.o_proj.codebook', 'model.layers.5.self_attn.o_proj.assignments', 'model.layers.5.self_attn.o_proj.normalizer.norms.0', 'model.layers.5.self_attn.o_proj.normalizer.norms.1', 'model.layers.5.self_attn.o_proj.normalizer.zeros.0', 'model.layers.5.self_attn.o_proj.normalizer.zeros.1', 'model.layers.5.mlp.gate_proj.bias', 'model.layers.5.mlp.gate_proj.codebook', 'model.layers.5.mlp.gate_proj.assignments', 'model.layers.5.mlp.gate_proj.normalizer.norms.0', 'model.layers.5.mlp.gate_proj.normalizer.norms.1', 'model.layers.5.mlp.gate_proj.normalizer.zeros.0', 'model.layers.5.mlp.gate_proj.normalizer.zeros.1', 'model.layers.5.mlp.up_proj.bias', 'model.layers.5.mlp.up_proj.codebook', 'model.layers.5.mlp.up_proj.assignments', 'model.layers.5.mlp.up_proj.normalizer.norms.0', 'model.layers.5.mlp.up_proj.normalizer.norms.1', 'model.layers.5.mlp.up_proj.normalizer.zeros.0', 'model.layers.5.mlp.up_proj.normalizer.zeros.1', 'model.layers.5.mlp.down_proj.bias', 'model.layers.5.mlp.down_proj.codebook', 'model.layers.5.mlp.down_proj.assignments', 'model.layers.5.mlp.down_proj.normalizer.norms.0', 'model.layers.5.mlp.down_proj.normalizer.norms.1', 'model.layers.5.mlp.down_proj.normalizer.zeros.0', 'model.layers.5.mlp.down_proj.normalizer.zeros.1', 'model.layers.6.self_attn.q_proj.bias', 'model.layers.6.self_attn.q_proj.codebook', 'model.layers.6.self_attn.q_proj.assignments', 'model.layers.6.self_attn.q_proj.normalizer.norms.0', 'model.layers.6.self_attn.q_proj.normalizer.norms.1', 'model.layers.6.self_attn.q_proj.normalizer.zeros.0', 'model.layers.6.self_attn.q_proj.normalizer.zeros.1', 'model.layers.6.self_attn.k_proj.bias', 'model.layers.6.self_attn.k_proj.codebook', 'model.layers.6.self_attn.k_proj.assignments', 'model.layers.6.self_attn.k_proj.normalizer.norms.0', 'model.layers.6.self_attn.k_proj.normalizer.norms.1', 'model.layers.6.self_attn.k_proj.normalizer.zeros.0', 'model.layers.6.self_attn.k_proj.normalizer.zeros.1', 'model.layers.6.self_attn.v_proj.bias', 'model.layers.6.self_attn.v_proj.codebook', 'model.layers.6.self_attn.v_proj.assignments', 'model.layers.6.self_attn.v_proj.normalizer.norms.0', 'model.layers.6.self_attn.v_proj.normalizer.norms.1', 'model.layers.6.self_attn.v_proj.normalizer.zeros.0', 'model.layers.6.self_attn.v_proj.normalizer.zeros.1', 'model.layers.6.self_attn.o_proj.bias', 'model.layers.6.self_attn.o_proj.codebook', 'model.layers.6.self_attn.o_proj.assignments', 'model.layers.6.self_attn.o_proj.normalizer.norms.0', 'model.layers.6.self_attn.o_proj.normalizer.norms.1', 'model.layers.6.self_attn.o_proj.normalizer.zeros.0', 'model.layers.6.self_attn.o_proj.normalizer.zeros.1', 'model.layers.6.mlp.gate_proj.bias', 'model.layers.6.mlp.gate_proj.codebook', 'model.layers.6.mlp.gate_proj.assignments', 'model.layers.6.mlp.gate_proj.normalizer.norms.0', 'model.layers.6.mlp.gate_proj.normalizer.norms.1', 'model.layers.6.mlp.gate_proj.normalizer.zeros.0', 'model.layers.6.mlp.gate_proj.normalizer.zeros.1', 'model.layers.6.mlp.up_proj.bias', 'model.layers.6.mlp.up_proj.codebook', 'model.layers.6.mlp.up_proj.assignments', 'model.layers.6.mlp.up_proj.normalizer.norms.0', 'model.layers.6.mlp.up_proj.normalizer.norms.1', 'model.layers.6.mlp.up_proj.normalizer.zeros.0', 'model.layers.6.mlp.up_proj.normalizer.zeros.1', 'model.layers.6.mlp.down_proj.bias', 'model.layers.6.mlp.down_proj.codebook', 'model.layers.6.mlp.down_proj.assignments', 'model.layers.6.mlp.down_proj.normalizer.norms.0', 'model.layers.6.mlp.down_proj.normalizer.norms.1', 'model.layers.6.mlp.down_proj.normalizer.zeros.0', 'model.layers.6.mlp.down_proj.normalizer.zeros.1', 'model.layers.7.self_attn.q_proj.bias', 'model.layers.7.self_attn.q_proj.codebook', 'model.layers.7.self_attn.q_proj.assignments', 'model.layers.7.self_attn.q_proj.normalizer.norms.0', 'model.layers.7.self_attn.q_proj.normalizer.norms.1', 'model.layers.7.self_attn.q_proj.normalizer.zeros.0', 'model.layers.7.self_attn.q_proj.normalizer.zeros.1', 'model.layers.7.self_attn.k_proj.bias', 'model.layers.7.self_attn.k_proj.codebook', 'model.layers.7.self_attn.k_proj.assignments', 'model.layers.7.self_attn.k_proj.normalizer.norms.0', 'model.layers.7.self_attn.k_proj.normalizer.norms.1', 'model.layers.7.self_attn.k_proj.normalizer.zeros.0', 'model.layers.7.self_attn.k_proj.normalizer.zeros.1', 'model.layers.7.self_attn.v_proj.bias', 'model.layers.7.self_attn.v_proj.codebook', 'model.layers.7.self_attn.v_proj.assignments', 'model.layers.7.self_attn.v_proj.normalizer.norms.0', 'model.layers.7.self_attn.v_proj.normalizer.norms.1', 'model.layers.7.self_attn.v_proj.normalizer.zeros.0', 'model.layers.7.self_attn.v_proj.normalizer.zeros.1', 'model.layers.7.self_attn.o_proj.bias', 'model.layers.7.self_attn.o_proj.codebook', 'model.layers.7.self_attn.o_proj.assignments', 'model.layers.7.self_attn.o_proj.normalizer.norms.0', 'model.layers.7.self_attn.o_proj.normalizer.norms.1', 'model.layers.7.self_attn.o_proj.normalizer.zeros.0', 'model.layers.7.self_attn.o_proj.normalizer.zeros.1', 'model.layers.7.mlp.gate_proj.bias', 'model.layers.7.mlp.gate_proj.codebook', 'model.layers.7.mlp.gate_proj.assignments', 'model.layers.7.mlp.gate_proj.normalizer.norms.0', 'model.layers.7.mlp.gate_proj.normalizer.norms.1', 'model.layers.7.mlp.gate_proj.normalizer.zeros.0', 'model.layers.7.mlp.gate_proj.normalizer.zeros.1', 'model.layers.7.mlp.up_proj.bias', 'model.layers.7.mlp.up_proj.codebook', 'model.layers.7.mlp.up_proj.assignments', 'model.layers.7.mlp.up_proj.normalizer.norms.0', 'model.layers.7.mlp.up_proj.normalizer.norms.1', 'model.layers.7.mlp.up_proj.normalizer.zeros.0', 'model.layers.7.mlp.up_proj.normalizer.zeros.1', 'model.layers.7.mlp.down_proj.bias', 'model.layers.7.mlp.down_proj.codebook', 'model.layers.7.mlp.down_proj.assignments', 'model.layers.7.mlp.down_proj.normalizer.norms.0', 'model.layers.7.mlp.down_proj.normalizer.norms.1', 'model.layers.7.mlp.down_proj.normalizer.zeros.0', 'model.layers.7.mlp.down_proj.normalizer.zeros.1', 'model.layers.8.self_attn.q_proj.bias', 'model.layers.8.self_attn.q_proj.codebook', 'model.layers.8.self_attn.q_proj.assignments', 'model.layers.8.self_attn.q_proj.normalizer.norms.0', 'model.layers.8.self_attn.q_proj.normalizer.norms.1', 'model.layers.8.self_attn.q_proj.normalizer.zeros.0', 'model.layers.8.self_attn.q_proj.normalizer.zeros.1', 'model.layers.8.self_attn.k_proj.bias', 'model.layers.8.self_attn.k_proj.codebook', 'model.layers.8.self_attn.k_proj.assignments', 'model.layers.8.self_attn.k_proj.normalizer.norms.0', 'model.layers.8.self_attn.k_proj.normalizer.norms.1', 'model.layers.8.self_attn.k_proj.normalizer.zeros.0', 'model.layers.8.self_attn.k_proj.normalizer.zeros.1', 'model.layers.8.self_attn.v_proj.bias', 'model.layers.8.self_attn.v_proj.codebook', 'model.layers.8.self_attn.v_proj.assignments', 'model.layers.8.self_attn.v_proj.normalizer.norms.0', 'model.layers.8.self_attn.v_proj.normalizer.norms.1', 'model.layers.8.self_attn.v_proj.normalizer.zeros.0', 'model.layers.8.self_attn.v_proj.normalizer.zeros.1', 'model.layers.8.self_attn.o_proj.bias', 'model.layers.8.self_attn.o_proj.codebook', 'model.layers.8.self_attn.o_proj.assignments', 'model.layers.8.self_attn.o_proj.normalizer.norms.0', 'model.layers.8.self_attn.o_proj.normalizer.norms.1', 'model.layers.8.self_attn.o_proj.normalizer.zeros.0', 'model.layers.8.self_attn.o_proj.normalizer.zeros.1', 'model.layers.8.mlp.gate_proj.bias', 'model.layers.8.mlp.gate_proj.codebook', 'model.layers.8.mlp.gate_proj.assignments', 'model.layers.8.mlp.gate_proj.normalizer.norms.0', 'model.layers.8.mlp.gate_proj.normalizer.norms.1', 'model.layers.8.mlp.gate_proj.normalizer.zeros.0', 'model.layers.8.mlp.gate_proj.normalizer.zeros.1', 'model.layers.8.mlp.up_proj.bias', 'model.layers.8.mlp.up_proj.codebook', 'model.layers.8.mlp.up_proj.assignments', 'model.layers.8.mlp.up_proj.normalizer.norms.0', 'model.layers.8.mlp.up_proj.normalizer.norms.1', 'model.layers.8.mlp.up_proj.normalizer.zeros.0', 'model.layers.8.mlp.up_proj.normalizer.zeros.1', 'model.layers.8.mlp.down_proj.bias', 'model.layers.8.mlp.down_proj.codebook', 'model.layers.8.mlp.down_proj.assignments', 'model.layers.8.mlp.down_proj.normalizer.norms.0', 'model.layers.8.mlp.down_proj.normalizer.norms.1', 'model.layers.8.mlp.down_proj.normalizer.zeros.0', 'model.layers.8.mlp.down_proj.normalizer.zeros.1', 'model.layers.9.self_attn.q_proj.bias', 'model.layers.9.self_attn.q_proj.codebook', 'model.layers.9.self_attn.q_proj.assignments', 'model.layers.9.self_attn.q_proj.normalizer.norms.0', 'model.layers.9.self_attn.q_proj.normalizer.norms.1', 'model.layers.9.self_attn.q_proj.normalizer.zeros.0', 'model.layers.9.self_attn.q_proj.normalizer.zeros.1', 'model.layers.9.self_attn.k_proj.bias', 'model.layers.9.self_attn.k_proj.codebook', 'model.layers.9.self_attn.k_proj.assignments', 'model.layers.9.self_attn.k_proj.normalizer.norms.0', 'model.layers.9.self_attn.k_proj.normalizer.norms.1', 'model.layers.9.self_attn.k_proj.normalizer.zeros.0', 'model.layers.9.self_attn.k_proj.normalizer.zeros.1', 'model.layers.9.self_attn.v_proj.bias', 'model.layers.9.self_attn.v_proj.codebook', 'model.layers.9.self_attn.v_proj.assignments', 'model.layers.9.self_attn.v_proj.normalizer.norms.0', 'model.layers.9.self_attn.v_proj.normalizer.norms.1', 'model.layers.9.self_attn.v_proj.normalizer.zeros.0', 'model.layers.9.self_attn.v_proj.normalizer.zeros.1', 'model.layers.9.self_attn.o_proj.bias', 'model.layers.9.self_attn.o_proj.codebook', 'model.layers.9.self_attn.o_proj.assignments', 'model.layers.9.self_attn.o_proj.normalizer.norms.0', 'model.layers.9.self_attn.o_proj.normalizer.norms.1', 'model.layers.9.self_attn.o_proj.normalizer.zeros.0', 'model.layers.9.self_attn.o_proj.normalizer.zeros.1', 'model.layers.9.mlp.gate_proj.bias', 'model.layers.9.mlp.gate_proj.codebook', 'model.layers.9.mlp.gate_proj.assignments', 'model.layers.9.mlp.gate_proj.normalizer.norms.0', 'model.layers.9.mlp.gate_proj.normalizer.norms.1', 'model.layers.9.mlp.gate_proj.normalizer.zeros.0', 'model.layers.9.mlp.gate_proj.normalizer.zeros.1', 'model.layers.9.mlp.up_proj.bias', 'model.layers.9.mlp.up_proj.codebook', 'model.layers.9.mlp.up_proj.assignments', 'model.layers.9.mlp.up_proj.normalizer.norms.0', 'model.layers.9.mlp.up_proj.normalizer.norms.1', 'model.layers.9.mlp.up_proj.normalizer.zeros.0', 'model.layers.9.mlp.up_proj.normalizer.zeros.1', 'model.layers.9.mlp.down_proj.bias', 'model.layers.9.mlp.down_proj.codebook', 'model.layers.9.mlp.down_proj.assignments', 'model.layers.9.mlp.down_proj.normalizer.norms.0', 'model.layers.9.mlp.down_proj.normalizer.norms.1', 'model.layers.9.mlp.down_proj.normalizer.zeros.0', 'model.layers.9.mlp.down_proj.normalizer.zeros.1', 'model.layers.10.self_attn.q_proj.bias', 'model.layers.10.self_attn.q_proj.codebook', 'model.layers.10.self_attn.q_proj.assignments', 'model.layers.10.self_attn.q_proj.normalizer.norms.0', 'model.layers.10.self_attn.q_proj.normalizer.norms.1', 'model.layers.10.self_attn.q_proj.normalizer.zeros.0', 'model.layers.10.self_attn.q_proj.normalizer.zeros.1', 'model.layers.10.self_attn.k_proj.bias', 'model.layers.10.self_attn.k_proj.codebook', 'model.layers.10.self_attn.k_proj.assignments', 'model.layers.10.self_attn.k_proj.normalizer.norms.0', 'model.layers.10.self_attn.k_proj.normalizer.norms.1', 'model.layers.10.self_attn.k_proj.normalizer.zeros.0', 'model.layers.10.self_attn.k_proj.normalizer.zeros.1', 'model.layers.10.self_attn.v_proj.bias', 'model.layers.10.self_attn.v_proj.codebook', 'model.layers.10.self_attn.v_proj.assignments', 'model.layers.10.self_attn.v_proj.normalizer.norms.0', 'model.layers.10.self_attn.v_proj.normalizer.norms.1', 'model.layers.10.self_attn.v_proj.normalizer.zeros.0', 'model.layers.10.self_attn.v_proj.normalizer.zeros.1', 'model.layers.10.self_attn.o_proj.bias', 'model.layers.10.self_attn.o_proj.codebook', 'model.layers.10.self_attn.o_proj.assignments', 'model.layers.10.self_attn.o_proj.normalizer.norms.0', 'model.layers.10.self_attn.o_proj.normalizer.norms.1', 'model.layers.10.self_attn.o_proj.normalizer.zeros.0', 'model.layers.10.self_attn.o_proj.normalizer.zeros.1', 'model.layers.10.mlp.gate_proj.bias', 'model.layers.10.mlp.gate_proj.codebook', 'model.layers.10.mlp.gate_proj.assignments', 'model.layers.10.mlp.gate_proj.normalizer.norms.0', 'model.layers.10.mlp.gate_proj.normalizer.norms.1', 'model.layers.10.mlp.gate_proj.normalizer.zeros.0', 'model.layers.10.mlp.gate_proj.normalizer.zeros.1', 'model.layers.10.mlp.up_proj.bias', 'model.layers.10.mlp.up_proj.codebook', 'model.layers.10.mlp.up_proj.assignments', 'model.layers.10.mlp.up_proj.normalizer.norms.0', 'model.layers.10.mlp.up_proj.normalizer.norms.1', 'model.layers.10.mlp.up_proj.normalizer.zeros.0', 'model.layers.10.mlp.up_proj.normalizer.zeros.1', 'model.layers.10.mlp.down_proj.bias', 'model.layers.10.mlp.down_proj.codebook', 'model.layers.10.mlp.down_proj.assignments', 'model.layers.10.mlp.down_proj.normalizer.norms.0', 'model.layers.10.mlp.down_proj.normalizer.norms.1', 'model.layers.10.mlp.down_proj.normalizer.zeros.0', 'model.layers.10.mlp.down_proj.normalizer.zeros.1', 'model.layers.11.self_attn.q_proj.bias', 'model.layers.11.self_attn.q_proj.codebook', 'model.layers.11.self_attn.q_proj.assignments', 'model.layers.11.self_attn.q_proj.normalizer.norms.0', 'model.layers.11.self_attn.q_proj.normalizer.norms.1', 'model.layers.11.self_attn.q_proj.normalizer.zeros.0', 'model.layers.11.self_attn.q_proj.normalizer.zeros.1', 'model.layers.11.self_attn.k_proj.bias', 'model.layers.11.self_attn.k_proj.codebook', 'model.layers.11.self_attn.k_proj.assignments', 'model.layers.11.self_attn.k_proj.normalizer.norms.0', 'model.layers.11.self_attn.k_proj.normalizer.norms.1', 'model.layers.11.self_attn.k_proj.normalizer.zeros.0', 'model.layers.11.self_attn.k_proj.normalizer.zeros.1', 'model.layers.11.self_attn.v_proj.bias', 'model.layers.11.self_attn.v_proj.codebook', 'model.layers.11.self_attn.v_proj.assignments', 'model.layers.11.self_attn.v_proj.normalizer.norms.0', 'model.layers.11.self_attn.v_proj.normalizer.norms.1', 'model.layers.11.self_attn.v_proj.normalizer.zeros.0', 'model.layers.11.self_attn.v_proj.normalizer.zeros.1', 'model.layers.11.self_attn.o_proj.bias', 'model.layers.11.self_attn.o_proj.codebook', 'model.layers.11.self_attn.o_proj.assignments', 'model.layers.11.self_attn.o_proj.normalizer.norms.0', 'model.layers.11.self_attn.o_proj.normalizer.norms.1', 'model.layers.11.self_attn.o_proj.normalizer.zeros.0', 'model.layers.11.self_attn.o_proj.normalizer.zeros.1', 'model.layers.11.mlp.gate_proj.bias', 'model.layers.11.mlp.gate_proj.codebook', 'model.layers.11.mlp.gate_proj.assignments', 'model.layers.11.mlp.gate_proj.normalizer.norms.0', 'model.layers.11.mlp.gate_proj.normalizer.norms.1', 'model.layers.11.mlp.gate_proj.normalizer.zeros.0', 'model.layers.11.mlp.gate_proj.normalizer.zeros.1', 'model.layers.11.mlp.up_proj.bias', 'model.layers.11.mlp.up_proj.codebook', 'model.layers.11.mlp.up_proj.assignments', 'model.layers.11.mlp.up_proj.normalizer.norms.0', 'model.layers.11.mlp.up_proj.normalizer.norms.1', 'model.layers.11.mlp.up_proj.normalizer.zeros.0', 'model.layers.11.mlp.up_proj.normalizer.zeros.1', 'model.layers.11.mlp.down_proj.bias', 'model.layers.11.mlp.down_proj.codebook', 'model.layers.11.mlp.down_proj.assignments', 'model.layers.11.mlp.down_proj.normalizer.norms.0', 'model.layers.11.mlp.down_proj.normalizer.norms.1', 'model.layers.11.mlp.down_proj.normalizer.zeros.0', 'model.layers.11.mlp.down_proj.normalizer.zeros.1', 'model.layers.12.self_attn.q_proj.bias', 'model.layers.12.self_attn.q_proj.codebook', 'model.layers.12.self_attn.q_proj.assignments', 'model.layers.12.self_attn.q_proj.normalizer.norms.0', 'model.layers.12.self_attn.q_proj.normalizer.norms.1', 'model.layers.12.self_attn.q_proj.normalizer.zeros.0', 'model.layers.12.self_attn.q_proj.normalizer.zeros.1', 'model.layers.12.self_attn.k_proj.bias', 'model.layers.12.self_attn.k_proj.codebook', 'model.layers.12.self_attn.k_proj.assignments', 'model.layers.12.self_attn.k_proj.normalizer.norms.0', 'model.layers.12.self_attn.k_proj.normalizer.norms.1', 'model.layers.12.self_attn.k_proj.normalizer.zeros.0', 'model.layers.12.self_attn.k_proj.normalizer.zeros.1', 'model.layers.12.self_attn.v_proj.bias', 'model.layers.12.self_attn.v_proj.codebook', 'model.layers.12.self_attn.v_proj.assignments', 'model.layers.12.self_attn.v_proj.normalizer.norms.0', 'model.layers.12.self_attn.v_proj.normalizer.norms.1', 'model.layers.12.self_attn.v_proj.normalizer.zeros.0', 'model.layers.12.self_attn.v_proj.normalizer.zeros.1', 'model.layers.12.self_attn.o_proj.bias', 'model.layers.12.self_attn.o_proj.codebook', 'model.layers.12.self_attn.o_proj.assignments', 'model.layers.12.self_attn.o_proj.normalizer.norms.0', 'model.layers.12.self_attn.o_proj.normalizer.norms.1', 'model.layers.12.self_attn.o_proj.normalizer.zeros.0', 'model.layers.12.self_attn.o_proj.normalizer.zeros.1', 'model.layers.12.mlp.gate_proj.bias', 'model.layers.12.mlp.gate_proj.codebook', 'model.layers.12.mlp.gate_proj.assignments', 'model.layers.12.mlp.gate_proj.normalizer.norms.0', 'model.layers.12.mlp.gate_proj.normalizer.norms.1', 'model.layers.12.mlp.gate_proj.normalizer.zeros.0', 'model.layers.12.mlp.gate_proj.normalizer.zeros.1', 'model.layers.12.mlp.up_proj.bias', 'model.layers.12.mlp.up_proj.codebook', 'model.layers.12.mlp.up_proj.assignments', 'model.layers.12.mlp.up_proj.normalizer.norms.0', 'model.layers.12.mlp.up_proj.normalizer.norms.1', 'model.layers.12.mlp.up_proj.normalizer.zeros.0', 'model.layers.12.mlp.up_proj.normalizer.zeros.1', 'model.layers.12.mlp.down_proj.bias', 'model.layers.12.mlp.down_proj.codebook', 'model.layers.12.mlp.down_proj.assignments', 'model.layers.12.mlp.down_proj.normalizer.norms.0', 'model.layers.12.mlp.down_proj.normalizer.norms.1', 'model.layers.12.mlp.down_proj.normalizer.zeros.0', 'model.layers.12.mlp.down_proj.normalizer.zeros.1', 'model.layers.13.self_attn.q_proj.bias', 'model.layers.13.self_attn.q_proj.codebook', 'model.layers.13.self_attn.q_proj.assignments', 'model.layers.13.self_attn.q_proj.normalizer.norms.0', 'model.layers.13.self_attn.q_proj.normalizer.norms.1', 'model.layers.13.self_attn.q_proj.normalizer.zeros.0', 'model.layers.13.self_attn.q_proj.normalizer.zeros.1', 'model.layers.13.self_attn.k_proj.bias', 'model.layers.13.self_attn.k_proj.codebook', 'model.layers.13.self_attn.k_proj.assignments', 'model.layers.13.self_attn.k_proj.normalizer.norms.0', 'model.layers.13.self_attn.k_proj.normalizer.norms.1', 'model.layers.13.self_attn.k_proj.normalizer.zeros.0', 'model.layers.13.self_attn.k_proj.normalizer.zeros.1', 'model.layers.13.self_attn.v_proj.bias', 'model.layers.13.self_attn.v_proj.codebook', 'model.layers.13.self_attn.v_proj.assignments', 'model.layers.13.self_attn.v_proj.normalizer.norms.0', 'model.layers.13.self_attn.v_proj.normalizer.norms.1', 'model.layers.13.self_attn.v_proj.normalizer.zeros.0', 'model.layers.13.self_attn.v_proj.normalizer.zeros.1', 'model.layers.13.self_attn.o_proj.bias', 'model.layers.13.self_attn.o_proj.codebook', 'model.layers.13.self_attn.o_proj.assignments', 'model.layers.13.self_attn.o_proj.normalizer.norms.0', 'model.layers.13.self_attn.o_proj.normalizer.norms.1', 'model.layers.13.self_attn.o_proj.normalizer.zeros.0', 'model.layers.13.self_attn.o_proj.normalizer.zeros.1', 'model.layers.13.mlp.gate_proj.bias', 'model.layers.13.mlp.gate_proj.codebook', 'model.layers.13.mlp.gate_proj.assignments', 'model.layers.13.mlp.gate_proj.normalizer.norms.0', 'model.layers.13.mlp.gate_proj.normalizer.norms.1', 'model.layers.13.mlp.gate_proj.normalizer.zeros.0', 'model.layers.13.mlp.gate_proj.normalizer.zeros.1', 'model.layers.13.mlp.up_proj.bias', 'model.layers.13.mlp.up_proj.codebook', 'model.layers.13.mlp.up_proj.assignments', 'model.layers.13.mlp.up_proj.normalizer.norms.0', 'model.layers.13.mlp.up_proj.normalizer.norms.1', 'model.layers.13.mlp.up_proj.normalizer.zeros.0', 'model.layers.13.mlp.up_proj.normalizer.zeros.1', 'model.layers.13.mlp.down_proj.bias', 'model.layers.13.mlp.down_proj.codebook', 'model.layers.13.mlp.down_proj.assignments', 'model.layers.13.mlp.down_proj.normalizer.norms.0', 'model.layers.13.mlp.down_proj.normalizer.norms.1', 'model.layers.13.mlp.down_proj.normalizer.zeros.0', 'model.layers.13.mlp.down_proj.normalizer.zeros.1', 'model.layers.14.self_attn.q_proj.bias', 'model.layers.14.self_attn.q_proj.codebook', 'model.layers.14.self_attn.q_proj.assignments', 'model.layers.14.self_attn.q_proj.normalizer.norms.0', 'model.layers.14.self_attn.q_proj.normalizer.norms.1', 'model.layers.14.self_attn.q_proj.normalizer.zeros.0', 'model.layers.14.self_attn.q_proj.normalizer.zeros.1', 'model.layers.14.self_attn.k_proj.bias', 'model.layers.14.self_attn.k_proj.codebook', 'model.layers.14.self_attn.k_proj.assignments', 'model.layers.14.self_attn.k_proj.normalizer.norms.0', 'model.layers.14.self_attn.k_proj.normalizer.norms.1', 'model.layers.14.self_attn.k_proj.normalizer.zeros.0', 'model.layers.14.self_attn.k_proj.normalizer.zeros.1', 'model.layers.14.self_attn.v_proj.bias', 'model.layers.14.self_attn.v_proj.codebook', 'model.layers.14.self_attn.v_proj.assignments', 'model.layers.14.self_attn.v_proj.normalizer.norms.0', 'model.layers.14.self_attn.v_proj.normalizer.norms.1', 'model.layers.14.self_attn.v_proj.normalizer.zeros.0', 'model.layers.14.self_attn.v_proj.normalizer.zeros.1', 'model.layers.14.self_attn.o_proj.bias', 'model.layers.14.self_attn.o_proj.codebook', 'model.layers.14.self_attn.o_proj.assignments', 'model.layers.14.self_attn.o_proj.normalizer.norms.0', 'model.layers.14.self_attn.o_proj.normalizer.norms.1', 'model.layers.14.self_attn.o_proj.normalizer.zeros.0', 'model.layers.14.self_attn.o_proj.normalizer.zeros.1', 'model.layers.14.mlp.gate_proj.bias', 'model.layers.14.mlp.gate_proj.codebook', 'model.layers.14.mlp.gate_proj.assignments', 'model.layers.14.mlp.gate_proj.normalizer.norms.0', 'model.layers.14.mlp.gate_proj.normalizer.norms.1', 'model.layers.14.mlp.gate_proj.normalizer.zeros.0', 'model.layers.14.mlp.gate_proj.normalizer.zeros.1', 'model.layers.14.mlp.up_proj.bias', 'model.layers.14.mlp.up_proj.codebook', 'model.layers.14.mlp.up_proj.assignments', 'model.layers.14.mlp.up_proj.normalizer.norms.0', 'model.layers.14.mlp.up_proj.normalizer.norms.1', 'model.layers.14.mlp.up_proj.normalizer.zeros.0', 'model.layers.14.mlp.up_proj.normalizer.zeros.1', 'model.layers.14.mlp.down_proj.bias', 'model.layers.14.mlp.down_proj.codebook', 'model.layers.14.mlp.down_proj.assignments', 'model.layers.14.mlp.down_proj.normalizer.norms.0', 'model.layers.14.mlp.down_proj.normalizer.norms.1', 'model.layers.14.mlp.down_proj.normalizer.zeros.0', 'model.layers.14.mlp.down_proj.normalizer.zeros.1', 'model.layers.15.self_attn.q_proj.bias', 'model.layers.15.self_attn.q_proj.codebook', 'model.layers.15.self_attn.q_proj.assignments', 'model.layers.15.self_attn.q_proj.normalizer.norms.0', 'model.layers.15.self_attn.q_proj.normalizer.norms.1', 'model.layers.15.self_attn.q_proj.normalizer.zeros.0', 'model.layers.15.self_attn.q_proj.normalizer.zeros.1', 'model.layers.15.self_attn.k_proj.bias', 'model.layers.15.self_attn.k_proj.codebook', 'model.layers.15.self_attn.k_proj.assignments', 'model.layers.15.self_attn.k_proj.normalizer.norms.0', 'model.layers.15.self_attn.k_proj.normalizer.norms.1', 'model.layers.15.self_attn.k_proj.normalizer.zeros.0', 'model.layers.15.self_attn.k_proj.normalizer.zeros.1', 'model.layers.15.self_attn.v_proj.bias', 'model.layers.15.self_attn.v_proj.codebook', 'model.layers.15.self_attn.v_proj.assignments', 'model.layers.15.self_attn.v_proj.normalizer.norms.0', 'model.layers.15.self_attn.v_proj.normalizer.norms.1', 'model.layers.15.self_attn.v_proj.normalizer.zeros.0', 'model.layers.15.self_attn.v_proj.normalizer.zeros.1', 'model.layers.15.self_attn.o_proj.bias', 'model.layers.15.self_attn.o_proj.codebook', 'model.layers.15.self_attn.o_proj.assignments', 'model.layers.15.self_attn.o_proj.normalizer.norms.0', 'model.layers.15.self_attn.o_proj.normalizer.norms.1', 'model.layers.15.self_attn.o_proj.normalizer.zeros.0', 'model.layers.15.self_attn.o_proj.normalizer.zeros.1', 'model.layers.15.mlp.gate_proj.bias', 'model.layers.15.mlp.gate_proj.codebook', 'model.layers.15.mlp.gate_proj.assignments', 'model.layers.15.mlp.gate_proj.normalizer.norms.0', 'model.layers.15.mlp.gate_proj.normalizer.norms.1', 'model.layers.15.mlp.gate_proj.normalizer.zeros.0', 'model.layers.15.mlp.gate_proj.normalizer.zeros.1', 'model.layers.15.mlp.up_proj.bias', 'model.layers.15.mlp.up_proj.codebook', 'model.layers.15.mlp.up_proj.assignments', 'model.layers.15.mlp.up_proj.normalizer.norms.0', 'model.layers.15.mlp.up_proj.normalizer.norms.1', 'model.layers.15.mlp.up_proj.normalizer.zeros.0', 'model.layers.15.mlp.up_proj.normalizer.zeros.1', 'model.layers.15.mlp.down_proj.bias', 'model.layers.15.mlp.down_proj.codebook', 'model.layers.15.mlp.down_proj.assignments', 'model.layers.15.mlp.down_proj.normalizer.norms.0', 'model.layers.15.mlp.down_proj.normalizer.norms.1', 'model.layers.15.mlp.down_proj.normalizer.zeros.0', 'model.layers.15.mlp.down_proj.normalizer.zeros.1', 'model.layers.16.self_attn.q_proj.bias', 'model.layers.16.self_attn.q_proj.codebook', 'model.layers.16.self_attn.q_proj.assignments', 'model.layers.16.self_attn.q_proj.normalizer.norms.0', 'model.layers.16.self_attn.q_proj.normalizer.norms.1', 'model.layers.16.self_attn.q_proj.normalizer.zeros.0', 'model.layers.16.self_attn.q_proj.normalizer.zeros.1', 'model.layers.16.self_attn.k_proj.bias', 'model.layers.16.self_attn.k_proj.codebook', 'model.layers.16.self_attn.k_proj.assignments', 'model.layers.16.self_attn.k_proj.normalizer.norms.0', 'model.layers.16.self_attn.k_proj.normalizer.norms.1', 'model.layers.16.self_attn.k_proj.normalizer.zeros.0', 'model.layers.16.self_attn.k_proj.normalizer.zeros.1', 'model.layers.16.self_attn.v_proj.bias', 'model.layers.16.self_attn.v_proj.codebook', 'model.layers.16.self_attn.v_proj.assignments', 'model.layers.16.self_attn.v_proj.normalizer.norms.0', 'model.layers.16.self_attn.v_proj.normalizer.norms.1', 'model.layers.16.self_attn.v_proj.normalizer.zeros.0', 'model.layers.16.self_attn.v_proj.normalizer.zeros.1', 'model.layers.16.self_attn.o_proj.bias', 'model.layers.16.self_attn.o_proj.codebook', 'model.layers.16.self_attn.o_proj.assignments', 'model.layers.16.self_attn.o_proj.normalizer.norms.0', 'model.layers.16.self_attn.o_proj.normalizer.norms.1', 'model.layers.16.self_attn.o_proj.normalizer.zeros.0', 'model.layers.16.self_attn.o_proj.normalizer.zeros.1', 'model.layers.16.mlp.gate_proj.bias', 'model.layers.16.mlp.gate_proj.codebook', 'model.layers.16.mlp.gate_proj.assignments', 'model.layers.16.mlp.gate_proj.normalizer.norms.0', 'model.layers.16.mlp.gate_proj.normalizer.norms.1', 'model.layers.16.mlp.gate_proj.normalizer.zeros.0', 'model.layers.16.mlp.gate_proj.normalizer.zeros.1', 'model.layers.16.mlp.up_proj.bias', 'model.layers.16.mlp.up_proj.codebook', 'model.layers.16.mlp.up_proj.assignments', 'model.layers.16.mlp.up_proj.normalizer.norms.0', 'model.layers.16.mlp.up_proj.normalizer.norms.1', 'model.layers.16.mlp.up_proj.normalizer.zeros.0', 'model.layers.16.mlp.up_proj.normalizer.zeros.1', 'model.layers.16.mlp.down_proj.bias', 'model.layers.16.mlp.down_proj.codebook', 'model.layers.16.mlp.down_proj.assignments', 'model.layers.16.mlp.down_proj.normalizer.norms.0', 'model.layers.16.mlp.down_proj.normalizer.norms.1', 'model.layers.16.mlp.down_proj.normalizer.zeros.0', 'model.layers.16.mlp.down_proj.normalizer.zeros.1', 'model.layers.17.self_attn.q_proj.bias', 'model.layers.17.self_attn.q_proj.codebook', 'model.layers.17.self_attn.q_proj.assignments', 'model.layers.17.self_attn.q_proj.normalizer.norms.0', 'model.layers.17.self_attn.q_proj.normalizer.norms.1', 'model.layers.17.self_attn.q_proj.normalizer.zeros.0', 'model.layers.17.self_attn.q_proj.normalizer.zeros.1', 'model.layers.17.self_attn.k_proj.bias', 'model.layers.17.self_attn.k_proj.codebook', 'model.layers.17.self_attn.k_proj.assignments', 'model.layers.17.self_attn.k_proj.normalizer.norms.0', 'model.layers.17.self_attn.k_proj.normalizer.norms.1', 'model.layers.17.self_attn.k_proj.normalizer.zeros.0', 'model.layers.17.self_attn.k_proj.normalizer.zeros.1', 'model.layers.17.self_attn.v_proj.bias', 'model.layers.17.self_attn.v_proj.codebook', 'model.layers.17.self_attn.v_proj.assignments', 'model.layers.17.self_attn.v_proj.normalizer.norms.0', 'model.layers.17.self_attn.v_proj.normalizer.norms.1', 'model.layers.17.self_attn.v_proj.normalizer.zeros.0', 'model.layers.17.self_attn.v_proj.normalizer.zeros.1', 'model.layers.17.self_attn.o_proj.bias', 'model.layers.17.self_attn.o_proj.codebook', 'model.layers.17.self_attn.o_proj.assignments', 'model.layers.17.self_attn.o_proj.normalizer.norms.0', 'model.layers.17.self_attn.o_proj.normalizer.norms.1', 'model.layers.17.self_attn.o_proj.normalizer.zeros.0', 'model.layers.17.self_attn.o_proj.normalizer.zeros.1', 'model.layers.17.mlp.gate_proj.bias', 'model.layers.17.mlp.gate_proj.codebook', 'model.layers.17.mlp.gate_proj.assignments', 'model.layers.17.mlp.gate_proj.normalizer.norms.0', 'model.layers.17.mlp.gate_proj.normalizer.norms.1', 'model.layers.17.mlp.gate_proj.normalizer.zeros.0', 'model.layers.17.mlp.gate_proj.normalizer.zeros.1', 'model.layers.17.mlp.up_proj.bias', 'model.layers.17.mlp.up_proj.codebook', 'model.layers.17.mlp.up_proj.assignments', 'model.layers.17.mlp.up_proj.normalizer.norms.0', 'model.layers.17.mlp.up_proj.normalizer.norms.1', 'model.layers.17.mlp.up_proj.normalizer.zeros.0', 'model.layers.17.mlp.up_proj.normalizer.zeros.1', 'model.layers.17.mlp.down_proj.bias', 'model.layers.17.mlp.down_proj.codebook', 'model.layers.17.mlp.down_proj.assignments', 'model.layers.17.mlp.down_proj.normalizer.norms.0', 'model.layers.17.mlp.down_proj.normalizer.norms.1', 'model.layers.17.mlp.down_proj.normalizer.zeros.0', 'model.layers.17.mlp.down_proj.normalizer.zeros.1', 'model.layers.18.self_attn.q_proj.bias', 'model.layers.18.self_attn.q_proj.codebook', 'model.layers.18.self_attn.q_proj.assignments', 'model.layers.18.self_attn.q_proj.normalizer.norms.0', 'model.layers.18.self_attn.q_proj.normalizer.norms.1', 'model.layers.18.self_attn.q_proj.normalizer.zeros.0', 'model.layers.18.self_attn.q_proj.normalizer.zeros.1', 'model.layers.18.self_attn.k_proj.bias', 'model.layers.18.self_attn.k_proj.codebook', 'model.layers.18.self_attn.k_proj.assignments', 'model.layers.18.self_attn.k_proj.normalizer.norms.0', 'model.layers.18.self_attn.k_proj.normalizer.norms.1', 'model.layers.18.self_attn.k_proj.normalizer.zeros.0', 'model.layers.18.self_attn.k_proj.normalizer.zeros.1', 'model.layers.18.self_attn.v_proj.bias', 'model.layers.18.self_attn.v_proj.codebook', 'model.layers.18.self_attn.v_proj.assignments', 'model.layers.18.self_attn.v_proj.normalizer.norms.0', 'model.layers.18.self_attn.v_proj.normalizer.norms.1', 'model.layers.18.self_attn.v_proj.normalizer.zeros.0', 'model.layers.18.self_attn.v_proj.normalizer.zeros.1', 'model.layers.18.self_attn.o_proj.bias', 'model.layers.18.self_attn.o_proj.codebook', 'model.layers.18.self_attn.o_proj.assignments', 'model.layers.18.self_attn.o_proj.normalizer.norms.0', 'model.layers.18.self_attn.o_proj.normalizer.norms.1', 'model.layers.18.self_attn.o_proj.normalizer.zeros.0', 'model.layers.18.self_attn.o_proj.normalizer.zeros.1', 'model.layers.18.mlp.gate_proj.bias', 'model.layers.18.mlp.gate_proj.codebook', 'model.layers.18.mlp.gate_proj.assignments', 'model.layers.18.mlp.gate_proj.normalizer.norms.0', 'model.layers.18.mlp.gate_proj.normalizer.norms.1', 'model.layers.18.mlp.gate_proj.normalizer.zeros.0', 'model.layers.18.mlp.gate_proj.normalizer.zeros.1', 'model.layers.18.mlp.up_proj.bias', 'model.layers.18.mlp.up_proj.codebook', 'model.layers.18.mlp.up_proj.assignments', 'model.layers.18.mlp.up_proj.normalizer.norms.0', 'model.layers.18.mlp.up_proj.normalizer.norms.1', 'model.layers.18.mlp.up_proj.normalizer.zeros.0', 'model.layers.18.mlp.up_proj.normalizer.zeros.1', 'model.layers.18.mlp.down_proj.bias', 'model.layers.18.mlp.down_proj.codebook', 'model.layers.18.mlp.down_proj.assignments', 'model.layers.18.mlp.down_proj.normalizer.norms.0', 'model.layers.18.mlp.down_proj.normalizer.norms.1', 'model.layers.18.mlp.down_proj.normalizer.zeros.0', 'model.layers.18.mlp.down_proj.normalizer.zeros.1', 'model.layers.19.self_attn.q_proj.bias', 'model.layers.19.self_attn.q_proj.codebook', 'model.layers.19.self_attn.q_proj.assignments', 'model.layers.19.self_attn.q_proj.normalizer.norms.0', 'model.layers.19.self_attn.q_proj.normalizer.norms.1', 'model.layers.19.self_attn.q_proj.normalizer.zeros.0', 'model.layers.19.self_attn.q_proj.normalizer.zeros.1', 'model.layers.19.self_attn.k_proj.bias', 'model.layers.19.self_attn.k_proj.codebook', 'model.layers.19.self_attn.k_proj.assignments', 'model.layers.19.self_attn.k_proj.normalizer.norms.0', 'model.layers.19.self_attn.k_proj.normalizer.norms.1', 'model.layers.19.self_attn.k_proj.normalizer.zeros.0', 'model.layers.19.self_attn.k_proj.normalizer.zeros.1', 'model.layers.19.self_attn.v_proj.bias', 'model.layers.19.self_attn.v_proj.codebook', 'model.layers.19.self_attn.v_proj.assignments', 'model.layers.19.self_attn.v_proj.normalizer.norms.0', 'model.layers.19.self_attn.v_proj.normalizer.norms.1', 'model.layers.19.self_attn.v_proj.normalizer.zeros.0', 'model.layers.19.self_attn.v_proj.normalizer.zeros.1', 'model.layers.19.self_attn.o_proj.bias', 'model.layers.19.self_attn.o_proj.codebook', 'model.layers.19.self_attn.o_proj.assignments', 'model.layers.19.self_attn.o_proj.normalizer.norms.0', 'model.layers.19.self_attn.o_proj.normalizer.norms.1', 'model.layers.19.self_attn.o_proj.normalizer.zeros.0', 'model.layers.19.self_attn.o_proj.normalizer.zeros.1', 'model.layers.19.mlp.gate_proj.bias', 'model.layers.19.mlp.gate_proj.codebook', 'model.layers.19.mlp.gate_proj.assignments', 'model.layers.19.mlp.gate_proj.normalizer.norms.0', 'model.layers.19.mlp.gate_proj.normalizer.norms.1', 'model.layers.19.mlp.gate_proj.normalizer.zeros.0', 'model.layers.19.mlp.gate_proj.normalizer.zeros.1', 'model.layers.19.mlp.up_proj.bias', 'model.layers.19.mlp.up_proj.codebook', 'model.layers.19.mlp.up_proj.assignments', 'model.layers.19.mlp.up_proj.normalizer.norms.0', 'model.layers.19.mlp.up_proj.normalizer.norms.1', 'model.layers.19.mlp.up_proj.normalizer.zeros.0', 'model.layers.19.mlp.up_proj.normalizer.zeros.1', 'model.layers.19.mlp.down_proj.bias', 'model.layers.19.mlp.down_proj.codebook', 'model.layers.19.mlp.down_proj.assignments', 'model.layers.19.mlp.down_proj.normalizer.norms.0', 'model.layers.19.mlp.down_proj.normalizer.norms.1', 'model.layers.19.mlp.down_proj.normalizer.zeros.0', 'model.layers.19.mlp.down_proj.normalizer.zeros.1', 'model.layers.20.self_attn.q_proj.bias', 'model.layers.20.self_attn.q_proj.codebook', 'model.layers.20.self_attn.q_proj.assignments', 'model.layers.20.self_attn.q_proj.normalizer.norms.0', 'model.layers.20.self_attn.q_proj.normalizer.norms.1', 'model.layers.20.self_attn.q_proj.normalizer.zeros.0', 'model.layers.20.self_attn.q_proj.normalizer.zeros.1', 'model.layers.20.self_attn.k_proj.bias', 'model.layers.20.self_attn.k_proj.codebook', 'model.layers.20.self_attn.k_proj.assignments', 'model.layers.20.self_attn.k_proj.normalizer.norms.0', 'model.layers.20.self_attn.k_proj.normalizer.norms.1', 'model.layers.20.self_attn.k_proj.normalizer.zeros.0', 'model.layers.20.self_attn.k_proj.normalizer.zeros.1', 'model.layers.20.self_attn.v_proj.bias', 'model.layers.20.self_attn.v_proj.codebook', 'model.layers.20.self_attn.v_proj.assignments', 'model.layers.20.self_attn.v_proj.normalizer.norms.0', 'model.layers.20.self_attn.v_proj.normalizer.norms.1', 'model.layers.20.self_attn.v_proj.normalizer.zeros.0', 'model.layers.20.self_attn.v_proj.normalizer.zeros.1', 'model.layers.20.self_attn.o_proj.bias', 'model.layers.20.self_attn.o_proj.codebook', 'model.layers.20.self_attn.o_proj.assignments', 'model.layers.20.self_attn.o_proj.normalizer.norms.0', 'model.layers.20.self_attn.o_proj.normalizer.norms.1', 'model.layers.20.self_attn.o_proj.normalizer.zeros.0', 'model.layers.20.self_attn.o_proj.normalizer.zeros.1', 'model.layers.20.mlp.gate_proj.bias', 'model.layers.20.mlp.gate_proj.codebook', 'model.layers.20.mlp.gate_proj.assignments', 'model.layers.20.mlp.gate_proj.normalizer.norms.0', 'model.layers.20.mlp.gate_proj.normalizer.norms.1', 'model.layers.20.mlp.gate_proj.normalizer.zeros.0', 'model.layers.20.mlp.gate_proj.normalizer.zeros.1', 'model.layers.20.mlp.up_proj.bias', 'model.layers.20.mlp.up_proj.codebook', 'model.layers.20.mlp.up_proj.assignments', 'model.layers.20.mlp.up_proj.normalizer.norms.0', 'model.layers.20.mlp.up_proj.normalizer.norms.1', 'model.layers.20.mlp.up_proj.normalizer.zeros.0', 'model.layers.20.mlp.up_proj.normalizer.zeros.1', 'model.layers.20.mlp.down_proj.bias', 'model.layers.20.mlp.down_proj.codebook', 'model.layers.20.mlp.down_proj.assignments', 'model.layers.20.mlp.down_proj.normalizer.norms.0', 'model.layers.20.mlp.down_proj.normalizer.norms.1', 'model.layers.20.mlp.down_proj.normalizer.zeros.0', 'model.layers.20.mlp.down_proj.normalizer.zeros.1', 'model.layers.21.self_attn.q_proj.bias', 'model.layers.21.self_attn.q_proj.codebook', 'model.layers.21.self_attn.q_proj.assignments', 'model.layers.21.self_attn.q_proj.normalizer.norms.0', 'model.layers.21.self_attn.q_proj.normalizer.norms.1', 'model.layers.21.self_attn.q_proj.normalizer.zeros.0', 'model.layers.21.self_attn.q_proj.normalizer.zeros.1', 'model.layers.21.self_attn.k_proj.bias', 'model.layers.21.self_attn.k_proj.codebook', 'model.layers.21.self_attn.k_proj.assignments', 'model.layers.21.self_attn.k_proj.normalizer.norms.0', 'model.layers.21.self_attn.k_proj.normalizer.norms.1', 'model.layers.21.self_attn.k_proj.normalizer.zeros.0', 'model.layers.21.self_attn.k_proj.normalizer.zeros.1', 'model.layers.21.self_attn.v_proj.bias', 'model.layers.21.self_attn.v_proj.codebook', 'model.layers.21.self_attn.v_proj.assignments', 'model.layers.21.self_attn.v_proj.normalizer.norms.0', 'model.layers.21.self_attn.v_proj.normalizer.norms.1', 'model.layers.21.self_attn.v_proj.normalizer.zeros.0', 'model.layers.21.self_attn.v_proj.normalizer.zeros.1', 'model.layers.21.self_attn.o_proj.bias', 'model.layers.21.self_attn.o_proj.codebook', 'model.layers.21.self_attn.o_proj.assignments', 'model.layers.21.self_attn.o_proj.normalizer.norms.0', 'model.layers.21.self_attn.o_proj.normalizer.norms.1', 'model.layers.21.self_attn.o_proj.normalizer.zeros.0', 'model.layers.21.self_attn.o_proj.normalizer.zeros.1', 'model.layers.21.mlp.gate_proj.bias', 'model.layers.21.mlp.gate_proj.codebook', 'model.layers.21.mlp.gate_proj.assignments', 'model.layers.21.mlp.gate_proj.normalizer.norms.0', 'model.layers.21.mlp.gate_proj.normalizer.norms.1', 'model.layers.21.mlp.gate_proj.normalizer.zeros.0', 'model.layers.21.mlp.gate_proj.normalizer.zeros.1', 'model.layers.21.mlp.up_proj.bias', 'model.layers.21.mlp.up_proj.codebook', 'model.layers.21.mlp.up_proj.assignments', 'model.layers.21.mlp.up_proj.normalizer.norms.0', 'model.layers.21.mlp.up_proj.normalizer.norms.1', 'model.layers.21.mlp.up_proj.normalizer.zeros.0', 'model.layers.21.mlp.up_proj.normalizer.zeros.1', 'model.layers.21.mlp.down_proj.bias', 'model.layers.21.mlp.down_proj.codebook', 'model.layers.21.mlp.down_proj.assignments', 'model.layers.21.mlp.down_proj.normalizer.norms.0', 'model.layers.21.mlp.down_proj.normalizer.norms.1', 'model.layers.21.mlp.down_proj.normalizer.zeros.0', 'model.layers.21.mlp.down_proj.normalizer.zeros.1', 'model.layers.22.self_attn.q_proj.bias', 'model.layers.22.self_attn.q_proj.codebook', 'model.layers.22.self_attn.q_proj.assignments', 'model.layers.22.self_attn.q_proj.normalizer.norms.0', 'model.layers.22.self_attn.q_proj.normalizer.norms.1', 'model.layers.22.self_attn.q_proj.normalizer.zeros.0', 'model.layers.22.self_attn.q_proj.normalizer.zeros.1', 'model.layers.22.self_attn.k_proj.bias', 'model.layers.22.self_attn.k_proj.codebook', 'model.layers.22.self_attn.k_proj.assignments', 'model.layers.22.self_attn.k_proj.normalizer.norms.0', 'model.layers.22.self_attn.k_proj.normalizer.norms.1', 'model.layers.22.self_attn.k_proj.normalizer.zeros.0', 'model.layers.22.self_attn.k_proj.normalizer.zeros.1', 'model.layers.22.self_attn.v_proj.bias', 'model.layers.22.self_attn.v_proj.codebook', 'model.layers.22.self_attn.v_proj.assignments', 'model.layers.22.self_attn.v_proj.normalizer.norms.0', 'model.layers.22.self_attn.v_proj.normalizer.norms.1', 'model.layers.22.self_attn.v_proj.normalizer.zeros.0', 'model.layers.22.self_attn.v_proj.normalizer.zeros.1', 'model.layers.22.self_attn.o_proj.bias', 'model.layers.22.self_attn.o_proj.codebook', 'model.layers.22.self_attn.o_proj.assignments', 'model.layers.22.self_attn.o_proj.normalizer.norms.0', 'model.layers.22.self_attn.o_proj.normalizer.norms.1', 'model.layers.22.self_attn.o_proj.normalizer.zeros.0', 'model.layers.22.self_attn.o_proj.normalizer.zeros.1', 'model.layers.22.mlp.gate_proj.bias', 'model.layers.22.mlp.gate_proj.codebook', 'model.layers.22.mlp.gate_proj.assignments', 'model.layers.22.mlp.gate_proj.normalizer.norms.0', 'model.layers.22.mlp.gate_proj.normalizer.norms.1', 'model.layers.22.mlp.gate_proj.normalizer.zeros.0', 'model.layers.22.mlp.gate_proj.normalizer.zeros.1', 'model.layers.22.mlp.up_proj.bias', 'model.layers.22.mlp.up_proj.codebook', 'model.layers.22.mlp.up_proj.assignments', 'model.layers.22.mlp.up_proj.normalizer.norms.0', 'model.layers.22.mlp.up_proj.normalizer.norms.1', 'model.layers.22.mlp.up_proj.normalizer.zeros.0', 'model.layers.22.mlp.up_proj.normalizer.zeros.1', 'model.layers.22.mlp.down_proj.bias', 'model.layers.22.mlp.down_proj.codebook', 'model.layers.22.mlp.down_proj.assignments', 'model.layers.22.mlp.down_proj.normalizer.norms.0', 'model.layers.22.mlp.down_proj.normalizer.norms.1', 'model.layers.22.mlp.down_proj.normalizer.zeros.0', 'model.layers.22.mlp.down_proj.normalizer.zeros.1', 'model.layers.23.self_attn.q_proj.bias', 'model.layers.23.self_attn.q_proj.codebook', 'model.layers.23.self_attn.q_proj.assignments', 'model.layers.23.self_attn.q_proj.normalizer.norms.0', 'model.layers.23.self_attn.q_proj.normalizer.norms.1', 'model.layers.23.self_attn.q_proj.normalizer.zeros.0', 'model.layers.23.self_attn.q_proj.normalizer.zeros.1', 'model.layers.23.self_attn.k_proj.bias', 'model.layers.23.self_attn.k_proj.codebook', 'model.layers.23.self_attn.k_proj.assignments', 'model.layers.23.self_attn.k_proj.normalizer.norms.0', 'model.layers.23.self_attn.k_proj.normalizer.norms.1', 'model.layers.23.self_attn.k_proj.normalizer.zeros.0', 'model.layers.23.self_attn.k_proj.normalizer.zeros.1', 'model.layers.23.self_attn.v_proj.bias', 'model.layers.23.self_attn.v_proj.codebook', 'model.layers.23.self_attn.v_proj.assignments', 'model.layers.23.self_attn.v_proj.normalizer.norms.0', 'model.layers.23.self_attn.v_proj.normalizer.norms.1', 'model.layers.23.self_attn.v_proj.normalizer.zeros.0', 'model.layers.23.self_attn.v_proj.normalizer.zeros.1', 'model.layers.23.self_attn.o_proj.bias', 'model.layers.23.self_attn.o_proj.codebook', 'model.layers.23.self_attn.o_proj.assignments', 'model.layers.23.self_attn.o_proj.normalizer.norms.0', 'model.layers.23.self_attn.o_proj.normalizer.norms.1', 'model.layers.23.self_attn.o_proj.normalizer.zeros.0', 'model.layers.23.self_attn.o_proj.normalizer.zeros.1', 'model.layers.23.mlp.gate_proj.bias', 'model.layers.23.mlp.gate_proj.codebook', 'model.layers.23.mlp.gate_proj.assignments', 'model.layers.23.mlp.gate_proj.normalizer.norms.0', 'model.layers.23.mlp.gate_proj.normalizer.norms.1', 'model.layers.23.mlp.gate_proj.normalizer.zeros.0', 'model.layers.23.mlp.gate_proj.normalizer.zeros.1', 'model.layers.23.mlp.up_proj.bias', 'model.layers.23.mlp.up_proj.codebook', 'model.layers.23.mlp.up_proj.assignments', 'model.layers.23.mlp.up_proj.normalizer.norms.0', 'model.layers.23.mlp.up_proj.normalizer.norms.1', 'model.layers.23.mlp.up_proj.normalizer.zeros.0', 'model.layers.23.mlp.up_proj.normalizer.zeros.1', 'model.layers.23.mlp.down_proj.bias', 'model.layers.23.mlp.down_proj.codebook', 'model.layers.23.mlp.down_proj.assignments', 'model.layers.23.mlp.down_proj.normalizer.norms.0', 'model.layers.23.mlp.down_proj.normalizer.norms.1', 'model.layers.23.mlp.down_proj.normalizer.zeros.0', 'model.layers.23.mlp.down_proj.normalizer.zeros.1', 'model.layers.24.self_attn.q_proj.bias', 'model.layers.24.self_attn.q_proj.codebook', 'model.layers.24.self_attn.q_proj.assignments', 'model.layers.24.self_attn.q_proj.normalizer.norms.0', 'model.layers.24.self_attn.q_proj.normalizer.norms.1', 'model.layers.24.self_attn.q_proj.normalizer.zeros.0', 'model.layers.24.self_attn.q_proj.normalizer.zeros.1', 'model.layers.24.self_attn.k_proj.bias', 'model.layers.24.self_attn.k_proj.codebook', 'model.layers.24.self_attn.k_proj.assignments', 'model.layers.24.self_attn.k_proj.normalizer.norms.0', 'model.layers.24.self_attn.k_proj.normalizer.norms.1', 'model.layers.24.self_attn.k_proj.normalizer.zeros.0', 'model.layers.24.self_attn.k_proj.normalizer.zeros.1', 'model.layers.24.self_attn.v_proj.bias', 'model.layers.24.self_attn.v_proj.codebook', 'model.layers.24.self_attn.v_proj.assignments', 'model.layers.24.self_attn.v_proj.normalizer.norms.0', 'model.layers.24.self_attn.v_proj.normalizer.norms.1', 'model.layers.24.self_attn.v_proj.normalizer.zeros.0', 'model.layers.24.self_attn.v_proj.normalizer.zeros.1', 'model.layers.24.self_attn.o_proj.bias', 'model.layers.24.self_attn.o_proj.codebook', 'model.layers.24.self_attn.o_proj.assignments', 'model.layers.24.self_attn.o_proj.normalizer.norms.0', 'model.layers.24.self_attn.o_proj.normalizer.norms.1', 'model.layers.24.self_attn.o_proj.normalizer.zeros.0', 'model.layers.24.self_attn.o_proj.normalizer.zeros.1', 'model.layers.24.mlp.gate_proj.bias', 'model.layers.24.mlp.gate_proj.codebook', 'model.layers.24.mlp.gate_proj.assignments', 'model.layers.24.mlp.gate_proj.normalizer.norms.0', 'model.layers.24.mlp.gate_proj.normalizer.norms.1', 'model.layers.24.mlp.gate_proj.normalizer.zeros.0', 'model.layers.24.mlp.gate_proj.normalizer.zeros.1', 'model.layers.24.mlp.up_proj.bias', 'model.layers.24.mlp.up_proj.codebook', 'model.layers.24.mlp.up_proj.assignments', 'model.layers.24.mlp.up_proj.normalizer.norms.0', 'model.layers.24.mlp.up_proj.normalizer.norms.1', 'model.layers.24.mlp.up_proj.normalizer.zeros.0', 'model.layers.24.mlp.up_proj.normalizer.zeros.1', 'model.layers.24.mlp.down_proj.bias', 'model.layers.24.mlp.down_proj.codebook', 'model.layers.24.mlp.down_proj.assignments', 'model.layers.24.mlp.down_proj.normalizer.norms.0', 'model.layers.24.mlp.down_proj.normalizer.norms.1', 'model.layers.24.mlp.down_proj.normalizer.zeros.0', 'model.layers.24.mlp.down_proj.normalizer.zeros.1', 'model.layers.25.self_attn.q_proj.bias', 'model.layers.25.self_attn.q_proj.codebook', 'model.layers.25.self_attn.q_proj.assignments', 'model.layers.25.self_attn.q_proj.normalizer.norms.0', 'model.layers.25.self_attn.q_proj.normalizer.norms.1', 'model.layers.25.self_attn.q_proj.normalizer.zeros.0', 'model.layers.25.self_attn.q_proj.normalizer.zeros.1', 'model.layers.25.self_attn.k_proj.bias', 'model.layers.25.self_attn.k_proj.codebook', 'model.layers.25.self_attn.k_proj.assignments', 'model.layers.25.self_attn.k_proj.normalizer.norms.0', 'model.layers.25.self_attn.k_proj.normalizer.norms.1', 'model.layers.25.self_attn.k_proj.normalizer.zeros.0', 'model.layers.25.self_attn.k_proj.normalizer.zeros.1', 'model.layers.25.self_attn.v_proj.bias', 'model.layers.25.self_attn.v_proj.codebook', 'model.layers.25.self_attn.v_proj.assignments', 'model.layers.25.self_attn.v_proj.normalizer.norms.0', 'model.layers.25.self_attn.v_proj.normalizer.norms.1', 'model.layers.25.self_attn.v_proj.normalizer.zeros.0', 'model.layers.25.self_attn.v_proj.normalizer.zeros.1', 'model.layers.25.self_attn.o_proj.bias', 'model.layers.25.self_attn.o_proj.codebook', 'model.layers.25.self_attn.o_proj.assignments', 'model.layers.25.self_attn.o_proj.normalizer.norms.0', 'model.layers.25.self_attn.o_proj.normalizer.norms.1', 'model.layers.25.self_attn.o_proj.normalizer.zeros.0', 'model.layers.25.self_attn.o_proj.normalizer.zeros.1', 'model.layers.25.mlp.gate_proj.bias', 'model.layers.25.mlp.gate_proj.codebook', 'model.layers.25.mlp.gate_proj.assignments', 'model.layers.25.mlp.gate_proj.normalizer.norms.0', 'model.layers.25.mlp.gate_proj.normalizer.norms.1', 'model.layers.25.mlp.gate_proj.normalizer.zeros.0', 'model.layers.25.mlp.gate_proj.normalizer.zeros.1', 'model.layers.25.mlp.up_proj.bias', 'model.layers.25.mlp.up_proj.codebook', 'model.layers.25.mlp.up_proj.assignments', 'model.layers.25.mlp.up_proj.normalizer.norms.0', 'model.layers.25.mlp.up_proj.normalizer.norms.1', 'model.layers.25.mlp.up_proj.normalizer.zeros.0', 'model.layers.25.mlp.up_proj.normalizer.zeros.1', 'model.layers.25.mlp.down_proj.bias', 'model.layers.25.mlp.down_proj.codebook', 'model.layers.25.mlp.down_proj.assignments', 'model.layers.25.mlp.down_proj.normalizer.norms.0', 'model.layers.25.mlp.down_proj.normalizer.norms.1', 'model.layers.25.mlp.down_proj.normalizer.zeros.0', 'model.layers.25.mlp.down_proj.normalizer.zeros.1', 'model.layers.26.self_attn.q_proj.bias', 'model.layers.26.self_attn.q_proj.codebook', 'model.layers.26.self_attn.q_proj.assignments', 'model.layers.26.self_attn.q_proj.normalizer.norms.0', 'model.layers.26.self_attn.q_proj.normalizer.norms.1', 'model.layers.26.self_attn.q_proj.normalizer.zeros.0', 'model.layers.26.self_attn.q_proj.normalizer.zeros.1', 'model.layers.26.self_attn.k_proj.bias', 'model.layers.26.self_attn.k_proj.codebook', 'model.layers.26.self_attn.k_proj.assignments', 'model.layers.26.self_attn.k_proj.normalizer.norms.0', 'model.layers.26.self_attn.k_proj.normalizer.norms.1', 'model.layers.26.self_attn.k_proj.normalizer.zeros.0', 'model.layers.26.self_attn.k_proj.normalizer.zeros.1', 'model.layers.26.self_attn.v_proj.bias', 'model.layers.26.self_attn.v_proj.codebook', 'model.layers.26.self_attn.v_proj.assignments', 'model.layers.26.self_attn.v_proj.normalizer.norms.0', 'model.layers.26.self_attn.v_proj.normalizer.norms.1', 'model.layers.26.self_attn.v_proj.normalizer.zeros.0', 'model.layers.26.self_attn.v_proj.normalizer.zeros.1', 'model.layers.26.self_attn.o_proj.bias', 'model.layers.26.self_attn.o_proj.codebook', 'model.layers.26.self_attn.o_proj.assignments', 'model.layers.26.self_attn.o_proj.normalizer.norms.0', 'model.layers.26.self_attn.o_proj.normalizer.norms.1', 'model.layers.26.self_attn.o_proj.normalizer.zeros.0', 'model.layers.26.self_attn.o_proj.normalizer.zeros.1', 'model.layers.26.mlp.gate_proj.bias', 'model.layers.26.mlp.gate_proj.codebook', 'model.layers.26.mlp.gate_proj.assignments', 'model.layers.26.mlp.gate_proj.normalizer.norms.0', 'model.layers.26.mlp.gate_proj.normalizer.norms.1', 'model.layers.26.mlp.gate_proj.normalizer.zeros.0', 'model.layers.26.mlp.gate_proj.normalizer.zeros.1', 'model.layers.26.mlp.up_proj.bias', 'model.layers.26.mlp.up_proj.codebook', 'model.layers.26.mlp.up_proj.assignments', 'model.layers.26.mlp.up_proj.normalizer.norms.0', 'model.layers.26.mlp.up_proj.normalizer.norms.1', 'model.layers.26.mlp.up_proj.normalizer.zeros.0', 'model.layers.26.mlp.up_proj.normalizer.zeros.1', 'model.layers.26.mlp.down_proj.bias', 'model.layers.26.mlp.down_proj.codebook', 'model.layers.26.mlp.down_proj.assignments', 'model.layers.26.mlp.down_proj.normalizer.norms.0', 'model.layers.26.mlp.down_proj.normalizer.norms.1', 'model.layers.26.mlp.down_proj.normalizer.zeros.0', 'model.layers.26.mlp.down_proj.normalizer.zeros.1', 'model.layers.27.self_attn.q_proj.bias', 'model.layers.27.self_attn.q_proj.codebook', 'model.layers.27.self_attn.q_proj.assignments', 'model.layers.27.self_attn.q_proj.normalizer.norms.0', 'model.layers.27.self_attn.q_proj.normalizer.norms.1', 'model.layers.27.self_attn.q_proj.normalizer.zeros.0', 'model.layers.27.self_attn.q_proj.normalizer.zeros.1', 'model.layers.27.self_attn.k_proj.bias', 'model.layers.27.self_attn.k_proj.codebook', 'model.layers.27.self_attn.k_proj.assignments', 'model.layers.27.self_attn.k_proj.normalizer.norms.0', 'model.layers.27.self_attn.k_proj.normalizer.norms.1', 'model.layers.27.self_attn.k_proj.normalizer.zeros.0', 'model.layers.27.self_attn.k_proj.normalizer.zeros.1', 'model.layers.27.self_attn.v_proj.bias', 'model.layers.27.self_attn.v_proj.codebook', 'model.layers.27.self_attn.v_proj.assignments', 'model.layers.27.self_attn.v_proj.normalizer.norms.0', 'model.layers.27.self_attn.v_proj.normalizer.norms.1', 'model.layers.27.self_attn.v_proj.normalizer.zeros.0', 'model.layers.27.self_attn.v_proj.normalizer.zeros.1', 'model.layers.27.self_attn.o_proj.bias', 'model.layers.27.self_attn.o_proj.codebook', 'model.layers.27.self_attn.o_proj.assignments', 'model.layers.27.self_attn.o_proj.normalizer.norms.0', 'model.layers.27.self_attn.o_proj.normalizer.norms.1', 'model.layers.27.self_attn.o_proj.normalizer.zeros.0', 'model.layers.27.self_attn.o_proj.normalizer.zeros.1', 'model.layers.27.mlp.gate_proj.bias', 'model.layers.27.mlp.gate_proj.codebook', 'model.layers.27.mlp.gate_proj.assignments', 'model.layers.27.mlp.gate_proj.normalizer.norms.0', 'model.layers.27.mlp.gate_proj.normalizer.norms.1', 'model.layers.27.mlp.gate_proj.normalizer.zeros.0', 'model.layers.27.mlp.gate_proj.normalizer.zeros.1', 'model.layers.27.mlp.up_proj.bias', 'model.layers.27.mlp.up_proj.codebook', 'model.layers.27.mlp.up_proj.assignments', 'model.layers.27.mlp.up_proj.normalizer.norms.0', 'model.layers.27.mlp.up_proj.normalizer.norms.1', 'model.layers.27.mlp.up_proj.normalizer.zeros.0', 'model.layers.27.mlp.up_proj.normalizer.zeros.1', 'model.layers.27.mlp.down_proj.bias', 'model.layers.27.mlp.down_proj.codebook', 'model.layers.27.mlp.down_proj.assignments', 'model.layers.27.mlp.down_proj.normalizer.norms.0', 'model.layers.27.mlp.down_proj.normalizer.norms.1', 'model.layers.27.mlp.down_proj.normalizer.zeros.0', 'model.layers.27.mlp.down_proj.normalizer.zeros.1', 'model.layers.28.self_attn.q_proj.bias', 'model.layers.28.self_attn.q_proj.codebook', 'model.layers.28.self_attn.q_proj.assignments', 'model.layers.28.self_attn.q_proj.normalizer.norms.0', 'model.layers.28.self_attn.q_proj.normalizer.norms.1', 'model.layers.28.self_attn.q_proj.normalizer.zeros.0', 'model.layers.28.self_attn.q_proj.normalizer.zeros.1', 'model.layers.28.self_attn.k_proj.bias', 'model.layers.28.self_attn.k_proj.codebook', 'model.layers.28.self_attn.k_proj.assignments', 'model.layers.28.self_attn.k_proj.normalizer.norms.0', 'model.layers.28.self_attn.k_proj.normalizer.norms.1', 'model.layers.28.self_attn.k_proj.normalizer.zeros.0', 'model.layers.28.self_attn.k_proj.normalizer.zeros.1', 'model.layers.28.self_attn.v_proj.bias', 'model.layers.28.self_attn.v_proj.codebook', 'model.layers.28.self_attn.v_proj.assignments', 'model.layers.28.self_attn.v_proj.normalizer.norms.0', 'model.layers.28.self_attn.v_proj.normalizer.norms.1', 'model.layers.28.self_attn.v_proj.normalizer.zeros.0', 'model.layers.28.self_attn.v_proj.normalizer.zeros.1', 'model.layers.28.self_attn.o_proj.bias', 'model.layers.28.self_attn.o_proj.codebook', 'model.layers.28.self_attn.o_proj.assignments', 'model.layers.28.self_attn.o_proj.normalizer.norms.0', 'model.layers.28.self_attn.o_proj.normalizer.norms.1', 'model.layers.28.self_attn.o_proj.normalizer.zeros.0', 'model.layers.28.self_attn.o_proj.normalizer.zeros.1', 'model.layers.28.mlp.gate_proj.bias', 'model.layers.28.mlp.gate_proj.codebook', 'model.layers.28.mlp.gate_proj.assignments', 'model.layers.28.mlp.gate_proj.normalizer.norms.0', 'model.layers.28.mlp.gate_proj.normalizer.norms.1', 'model.layers.28.mlp.gate_proj.normalizer.zeros.0', 'model.layers.28.mlp.gate_proj.normalizer.zeros.1', 'model.layers.28.mlp.up_proj.bias', 'model.layers.28.mlp.up_proj.codebook', 'model.layers.28.mlp.up_proj.assignments', 'model.layers.28.mlp.up_proj.normalizer.norms.0', 'model.layers.28.mlp.up_proj.normalizer.norms.1', 'model.layers.28.mlp.up_proj.normalizer.zeros.0', 'model.layers.28.mlp.up_proj.normalizer.zeros.1', 'model.layers.28.mlp.down_proj.bias', 'model.layers.28.mlp.down_proj.codebook', 'model.layers.28.mlp.down_proj.assignments', 'model.layers.28.mlp.down_proj.normalizer.norms.0', 'model.layers.28.mlp.down_proj.normalizer.norms.1', 'model.layers.28.mlp.down_proj.normalizer.zeros.0', 'model.layers.28.mlp.down_proj.normalizer.zeros.1', 'model.layers.29.self_attn.q_proj.bias', 'model.layers.29.self_attn.q_proj.codebook', 'model.layers.29.self_attn.q_proj.assignments', 'model.layers.29.self_attn.q_proj.normalizer.norms.0', 'model.layers.29.self_attn.q_proj.normalizer.norms.1', 'model.layers.29.self_attn.q_proj.normalizer.zeros.0', 'model.layers.29.self_attn.q_proj.normalizer.zeros.1', 'model.layers.29.self_attn.k_proj.bias', 'model.layers.29.self_attn.k_proj.codebook', 'model.layers.29.self_attn.k_proj.assignments', 'model.layers.29.self_attn.k_proj.normalizer.norms.0', 'model.layers.29.self_attn.k_proj.normalizer.norms.1', 'model.layers.29.self_attn.k_proj.normalizer.zeros.0', 'model.layers.29.self_attn.k_proj.normalizer.zeros.1', 'model.layers.29.self_attn.v_proj.bias', 'model.layers.29.self_attn.v_proj.codebook', 'model.layers.29.self_attn.v_proj.assignments', 'model.layers.29.self_attn.v_proj.normalizer.norms.0', 'model.layers.29.self_attn.v_proj.normalizer.norms.1', 'model.layers.29.self_attn.v_proj.normalizer.zeros.0', 'model.layers.29.self_attn.v_proj.normalizer.zeros.1', 'model.layers.29.self_attn.o_proj.bias', 'model.layers.29.self_attn.o_proj.codebook', 'model.layers.29.self_attn.o_proj.assignments', 'model.layers.29.self_attn.o_proj.normalizer.norms.0', 'model.layers.29.self_attn.o_proj.normalizer.norms.1', 'model.layers.29.self_attn.o_proj.normalizer.zeros.0', 'model.layers.29.self_attn.o_proj.normalizer.zeros.1', 'model.layers.29.mlp.gate_proj.bias', 'model.layers.29.mlp.gate_proj.codebook', 'model.layers.29.mlp.gate_proj.assignments', 'model.layers.29.mlp.gate_proj.normalizer.norms.0', 'model.layers.29.mlp.gate_proj.normalizer.norms.1', 'model.layers.29.mlp.gate_proj.normalizer.zeros.0', 'model.layers.29.mlp.gate_proj.normalizer.zeros.1', 'model.layers.29.mlp.up_proj.bias', 'model.layers.29.mlp.up_proj.codebook', 'model.layers.29.mlp.up_proj.assignments', 'model.layers.29.mlp.up_proj.normalizer.norms.0', 'model.layers.29.mlp.up_proj.normalizer.norms.1', 'model.layers.29.mlp.up_proj.normalizer.zeros.0', 'model.layers.29.mlp.up_proj.normalizer.zeros.1', 'model.layers.29.mlp.down_proj.bias', 'model.layers.29.mlp.down_proj.codebook', 'model.layers.29.mlp.down_proj.assignments', 'model.layers.29.mlp.down_proj.normalizer.norms.0', 'model.layers.29.mlp.down_proj.normalizer.norms.1', 'model.layers.29.mlp.down_proj.normalizer.zeros.0', 'model.layers.29.mlp.down_proj.normalizer.zeros.1', 'model.layers.30.self_attn.q_proj.bias', 'model.layers.30.self_attn.q_proj.codebook', 'model.layers.30.self_attn.q_proj.assignments', 'model.layers.30.self_attn.q_proj.normalizer.norms.0', 'model.layers.30.self_attn.q_proj.normalizer.norms.1', 'model.layers.30.self_attn.q_proj.normalizer.zeros.0', 'model.layers.30.self_attn.q_proj.normalizer.zeros.1', 'model.layers.30.self_attn.k_proj.bias', 'model.layers.30.self_attn.k_proj.codebook', 'model.layers.30.self_attn.k_proj.assignments', 'model.layers.30.self_attn.k_proj.normalizer.norms.0', 'model.layers.30.self_attn.k_proj.normalizer.norms.1', 'model.layers.30.self_attn.k_proj.normalizer.zeros.0', 'model.layers.30.self_attn.k_proj.normalizer.zeros.1', 'model.layers.30.self_attn.v_proj.bias', 'model.layers.30.self_attn.v_proj.codebook', 'model.layers.30.self_attn.v_proj.assignments', 'model.layers.30.self_attn.v_proj.normalizer.norms.0', 'model.layers.30.self_attn.v_proj.normalizer.norms.1', 'model.layers.30.self_attn.v_proj.normalizer.zeros.0', 'model.layers.30.self_attn.v_proj.normalizer.zeros.1', 'model.layers.30.self_attn.o_proj.bias', 'model.layers.30.self_attn.o_proj.codebook', 'model.layers.30.self_attn.o_proj.assignments', 'model.layers.30.self_attn.o_proj.normalizer.norms.0', 'model.layers.30.self_attn.o_proj.normalizer.norms.1', 'model.layers.30.self_attn.o_proj.normalizer.zeros.0', 'model.layers.30.self_attn.o_proj.normalizer.zeros.1', 'model.layers.30.mlp.gate_proj.bias', 'model.layers.30.mlp.gate_proj.codebook', 'model.layers.30.mlp.gate_proj.assignments', 'model.layers.30.mlp.gate_proj.normalizer.norms.0', 'model.layers.30.mlp.gate_proj.normalizer.norms.1', 'model.layers.30.mlp.gate_proj.normalizer.zeros.0', 'model.layers.30.mlp.gate_proj.normalizer.zeros.1', 'model.layers.30.mlp.up_proj.bias', 'model.layers.30.mlp.up_proj.codebook', 'model.layers.30.mlp.up_proj.assignments', 'model.layers.30.mlp.up_proj.normalizer.norms.0', 'model.layers.30.mlp.up_proj.normalizer.norms.1', 'model.layers.30.mlp.up_proj.normalizer.zeros.0', 'model.layers.30.mlp.up_proj.normalizer.zeros.1', 'model.layers.30.mlp.down_proj.bias', 'model.layers.30.mlp.down_proj.codebook', 'model.layers.30.mlp.down_proj.assignments', 'model.layers.30.mlp.down_proj.normalizer.norms.0', 'model.layers.30.mlp.down_proj.normalizer.norms.1', 'model.layers.30.mlp.down_proj.normalizer.zeros.0', 'model.layers.30.mlp.down_proj.normalizer.zeros.1', 'model.layers.31.self_attn.q_proj.bias', 'model.layers.31.self_attn.q_proj.codebook', 'model.layers.31.self_attn.q_proj.assignments', 'model.layers.31.self_attn.q_proj.normalizer.norms.0', 'model.layers.31.self_attn.q_proj.normalizer.norms.1', 'model.layers.31.self_attn.q_proj.normalizer.zeros.0', 'model.layers.31.self_attn.q_proj.normalizer.zeros.1', 'model.layers.31.self_attn.k_proj.bias', 'model.layers.31.self_attn.k_proj.codebook', 'model.layers.31.self_attn.k_proj.assignments', 'model.layers.31.self_attn.k_proj.normalizer.norms.0', 'model.layers.31.self_attn.k_proj.normalizer.norms.1', 'model.layers.31.self_attn.k_proj.normalizer.zeros.0', 'model.layers.31.self_attn.k_proj.normalizer.zeros.1', 'model.layers.31.self_attn.v_proj.bias', 'model.layers.31.self_attn.v_proj.codebook', 'model.layers.31.self_attn.v_proj.assignments', 'model.layers.31.self_attn.v_proj.normalizer.norms.0', 'model.layers.31.self_attn.v_proj.normalizer.norms.1', 'model.layers.31.self_attn.v_proj.normalizer.zeros.0', 'model.layers.31.self_attn.v_proj.normalizer.zeros.1', 'model.layers.31.self_attn.o_proj.bias', 'model.layers.31.self_attn.o_proj.codebook', 'model.layers.31.self_attn.o_proj.assignments', 'model.layers.31.self_attn.o_proj.normalizer.norms.0', 'model.layers.31.self_attn.o_proj.normalizer.norms.1', 'model.layers.31.self_attn.o_proj.normalizer.zeros.0', 'model.layers.31.self_attn.o_proj.normalizer.zeros.1', 'model.layers.31.mlp.gate_proj.bias', 'model.layers.31.mlp.gate_proj.codebook', 'model.layers.31.mlp.gate_proj.assignments', 'model.layers.31.mlp.gate_proj.normalizer.norms.0', 'model.layers.31.mlp.gate_proj.normalizer.norms.1', 'model.layers.31.mlp.gate_proj.normalizer.zeros.0', 'model.layers.31.mlp.gate_proj.normalizer.zeros.1', 'model.layers.31.mlp.up_proj.bias', 'model.layers.31.mlp.up_proj.codebook', 'model.layers.31.mlp.up_proj.assignments', 'model.layers.31.mlp.up_proj.normalizer.norms.0', 'model.layers.31.mlp.up_proj.normalizer.norms.1', 'model.layers.31.mlp.up_proj.normalizer.zeros.0', 'model.layers.31.mlp.up_proj.normalizer.zeros.1', 'model.layers.31.mlp.down_proj.bias', 'model.layers.31.mlp.down_proj.codebook', 'model.layers.31.mlp.down_proj.assignments', 'model.layers.31.mlp.down_proj.normalizer.norms.0', 'model.layers.31.mlp.down_proj.normalizer.norms.1', 'model.layers.31.mlp.down_proj.normalizer.zeros.0', 'model.layers.31.mlp.down_proj.normalizer.zeros.1'], unexpected_keys=['model.layers.0.self_attn.q_proj.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.0.mlp.up_proj.weight', 'model.layers.0.mlp.down_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.2.mlp.up_proj.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.3.mlp.up_proj.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.7.mlp.up_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.9.self_attn.v_proj.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.11.mlp.up_proj.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.13.mlp.up_proj.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.15.mlp.up_proj.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.16.mlp.up_proj.weight', 'model.layers.16.mlp.down_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.17.mlp.up_proj.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.18.mlp.gate_proj.weight', 'model.layers.18.mlp.up_proj.weight', 'model.layers.18.mlp.down_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.19.mlp.gate_proj.weight', 'model.layers.19.mlp.up_proj.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.20.mlp.gate_proj.weight', 'model.layers.20.mlp.up_proj.weight', 'model.layers.20.mlp.down_proj.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.21.mlp.gate_proj.weight', 'model.layers.21.mlp.up_proj.weight', 'model.layers.21.mlp.down_proj.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.22.mlp.gate_proj.weight', 'model.layers.22.mlp.up_proj.weight', 'model.layers.22.mlp.down_proj.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.23.mlp.gate_proj.weight', 'model.layers.23.mlp.up_proj.weight', 'model.layers.23.mlp.down_proj.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.24.mlp.gate_proj.weight', 'model.layers.24.mlp.up_proj.weight', 'model.layers.24.mlp.down_proj.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.25.self_attn.o_proj.weight', 'model.layers.25.mlp.gate_proj.weight', 'model.layers.25.mlp.up_proj.weight', 'model.layers.25.mlp.down_proj.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.26.self_attn.o_proj.weight', 'model.layers.26.mlp.gate_proj.weight', 'model.layers.26.mlp.up_proj.weight', 'model.layers.26.mlp.down_proj.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.27.self_attn.o_proj.weight', 'model.layers.27.mlp.gate_proj.weight', 'model.layers.27.mlp.up_proj.weight', 'model.layers.27.mlp.down_proj.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.28.self_attn.o_proj.weight', 'model.layers.28.mlp.gate_proj.weight', 'model.layers.28.mlp.up_proj.weight', 'model.layers.28.mlp.down_proj.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.29.self_attn.o_proj.weight', 'model.layers.29.mlp.gate_proj.weight', 'model.layers.29.mlp.up_proj.weight', 'model.layers.29.mlp.down_proj.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.30.self_attn.o_proj.weight', 'model.layers.30.mlp.gate_proj.weight', 'model.layers.30.mlp.up_proj.weight', 'model.layers.30.mlp.down_proj.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.31.self_attn.o_proj.weight', 'model.layers.31.mlp.gate_proj.weight', 'model.layers.31.mlp.up_proj.weight', 'model.layers.31.mlp.down_proj.weight'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LlamaForCausalLM(orig_config)\n",
    "model.to(orig_config.torch_dtype)\n",
    "#iterate through all parameters and assert that they are fp16\n",
    "for name, param in model.named_parameters():\n",
    "    assert param.dtype == torch.float16, f\"{name} is not fp16, it is {param.dtype}\"\n",
    "model.load_state_dict(orig_model.state_dict(), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta-llama/Llama-2-7b-hf/layer_0/mlp.down_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_0/mlp.gate_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_0/mlp.up_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_0/self_attn.k_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_0/self_attn.o_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_0/self_attn.q_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_0/self_attn.v_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_1/mlp.down_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_1/mlp.gate_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_1/mlp.up_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_1/self_attn.k_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_1/self_attn.o_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_1/self_attn.q_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_1/self_attn.v_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_10/mlp.down_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_10/mlp.gate_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_10/mlp.up_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_10/self_attn.k_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_10/self_attn.o_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_10/self_attn.q_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_10/self_attn.v_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_11/mlp.down_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_11/mlp.gate_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_11/mlp.up_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_11/self_attn.k_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_11/self_attn.o_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_11/self_attn.q_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_11/self_attn.v_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_12/mlp.down_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_12/mlp.gate_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_12/mlp.up_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_12/self_attn.k_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_12/self_attn.o_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_12/self_attn.q_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_12/self_attn.v_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_13/mlp.down_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_13/mlp.gate_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_13/mlp.up_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_13/self_attn.k_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_13/self_attn.o_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_13/self_attn.q_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_13/self_attn.v_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_14/mlp.down_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_14/mlp.gate_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_14/mlp.up_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_14/self_attn.k_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_14/self_attn.o_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_14/self_attn.q_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_14/self_attn.v_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_15/mlp.down_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_15/mlp.gate_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_15/mlp.up_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_15/self_attn.k_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_15/self_attn.o_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_15/self_attn.q_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_15/self_attn.v_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_16/mlp.down_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_16/mlp.gate_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_16/mlp.up_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_16/self_attn.k_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_16/self_attn.o_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_16/self_attn.q_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_16/self_attn.v_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_17/mlp.down_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_17/mlp.gate_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_17/mlp.up_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_17/self_attn.k_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_17/self_attn.o_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_17/self_attn.q_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_17/self_attn.v_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_18/mlp.down_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_18/mlp.gate_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_18/mlp.up_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_18/self_attn.k_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_18/self_attn.o_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_18/self_attn.q_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_18/self_attn.v_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_19/mlp.down_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_19/mlp.gate_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_19/mlp.up_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_19/self_attn.k_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_19/self_attn.o_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_19/self_attn.q_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_19/self_attn.v_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_2/mlp.down_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_2/mlp.gate_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_2/mlp.up_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_2/self_attn.k_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_2/self_attn.o_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_2/self_attn.q_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_2/self_attn.v_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_20/mlp.down_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_20/mlp.gate_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_20/mlp.up_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_20/self_attn.k_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_20/self_attn.o_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_20/self_attn.q_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_20/self_attn.v_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_21/mlp.down_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_21/mlp.gate_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_21/mlp.up_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_21/self_attn.k_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_21/self_attn.o_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_21/self_attn.q_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_21/self_attn.v_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_22/mlp.down_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_22/mlp.gate_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_22/mlp.up_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_22/self_attn.k_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_22/self_attn.o_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_22/self_attn.q_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_22/self_attn.v_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_23/mlp.down_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_23/mlp.gate_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_23/mlp.up_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_23/self_attn.k_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_23/self_attn.o_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_23/self_attn.q_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_23/self_attn.v_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_24/mlp.down_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_24/mlp.gate_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_24/mlp.up_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_24/self_attn.k_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_24/self_attn.o_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_24/self_attn.q_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_24/self_attn.v_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_25/mlp.down_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_25/mlp.gate_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_25/mlp.up_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_25/self_attn.k_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_25/self_attn.o_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_25/self_attn.q_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_25/self_attn.v_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_26/mlp.down_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_26/mlp.gate_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_26/mlp.up_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_26/self_attn.k_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_26/self_attn.o_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_26/self_attn.q_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_26/self_attn.v_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_27/mlp.down_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_27/mlp.gate_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_27/mlp.up_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_27/self_attn.k_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_27/self_attn.o_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_27/self_attn.q_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_27/self_attn.v_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_28/mlp.down_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_28/mlp.gate_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_28/mlp.up_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_28/self_attn.k_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_28/self_attn.o_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_28/self_attn.q_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_28/self_attn.v_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_29/mlp.down_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_29/mlp.gate_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_29/mlp.up_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_29/self_attn.k_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_29/self_attn.o_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_29/self_attn.q_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_29/self_attn.v_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_3/mlp.down_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_3/mlp.gate_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_3/mlp.up_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_3/self_attn.k_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_3/self_attn.o_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_3/self_attn.q_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_3/self_attn.v_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_30/mlp.down_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_30/mlp.gate_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_30/mlp.up_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_30/self_attn.k_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_30/self_attn.o_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_30/self_attn.q_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_30/self_attn.v_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_31/mlp.down_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_31/mlp.gate_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_31/mlp.up_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_31/self_attn.k_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_31/self_attn.o_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_31/self_attn.q_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_31/self_attn.v_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_4/mlp.down_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_4/mlp.gate_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_4/mlp.up_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_4/self_attn.k_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_4/self_attn.o_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_4/self_attn.q_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_4/self_attn.v_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_5/mlp.down_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_5/mlp.gate_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_5/mlp.up_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_5/self_attn.k_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_5/self_attn.o_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_5/self_attn.q_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_5/self_attn.v_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_6/mlp.down_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_6/mlp.gate_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_6/mlp.up_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_6/self_attn.k_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_6/self_attn.o_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_6/self_attn.q_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_6/self_attn.v_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_7/mlp.down_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_7/mlp.gate_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_7/mlp.up_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_7/self_attn.k_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_7/self_attn.o_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_7/self_attn.q_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_7/self_attn.v_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_8/mlp.down_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_8/mlp.gate_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_8/mlp.up_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_8/self_attn.k_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_8/self_attn.o_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_8/self_attn.q_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_8/self_attn.v_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_9/mlp.down_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_9/mlp.gate_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_9/mlp.up_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_9/self_attn.k_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_9/self_attn.o_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_9/self_attn.q_proj\n",
      "torch.float16 cpu\n",
      "meta-llama/Llama-2-7b-hf/layer_9/self_attn.v_proj\n",
      "torch.float16 cpu\n"
     ]
    }
   ],
   "source": [
    "#for each checkpoint, load the right weight\n",
    "for checkpoint_name,checkpoint_path in checkpoints_dict.items():\n",
    "    print(checkpoint_name)\n",
    "    #first remove the base_model name from it\n",
    "    checkpoint_name = checkpoint_name.replace(base_model, \"\")\n",
    "    #now split by /\n",
    "    checkpoint_name = checkpoint_name.split(\"/\")[-2:]\n",
    "    #from the first part, we can get which layer it is\n",
    "    i_layer = int(checkpoint_name[0].replace(\"layer_\", \"\"))\n",
    "    #from the second part we can get which module (self_attn, mlp, etc) and which layer it is\n",
    "    submodule_name, linear_name = checkpoint_name[1].split(\".\")\n",
    "    \n",
    "    #now we get the right module\n",
    "    layer = getattr(getattr(model.model.layers[i_layer], submodule_name), linear_name)\n",
    "    #record the original dtype\n",
    "    orig_dtype = layer.codebook.dtype\n",
    "    orig_device = layer.codebook.device\n",
    "    print(orig_dtype, orig_device)\n",
    "    #load the state dict\n",
    "    layer.load_state_dict(torch.load(checkpoint_path, map_location=orig_device), strict=False)\n",
    "    #convert to the right dtype\n",
    "    layer.to(orig_dtype)\n",
    "    # raise ValueError(\"stop here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterate through all parameters and assert that they are fp16\n",
    "for name, param in model.named_parameters():\n",
    "    assert param.dtype == torch.float16, f\"{name} is not fp16, it is {param.dtype}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.33it/s]\n"
     ]
    }
   ],
   "source": [
    "original_model = OrigLlama.from_pretrained(base_model, device_map=\"cpu\", torch_dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'float16'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.torch_dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model\n",
    "model.save_pretrained(hf_model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float16"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers[0].self_attn.q_proj.reconstruct().dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n",
      "device None dtype torch.float16\n",
      "kwargs {'d': 6, 'ignore_norms': True, 'initialize_kwargs': {'deterministic': False, 'multiple_each_time': 1.0}, 'initialize_method': 'kmeans++', 'n_bits': 2, 'n_inits': 1, 'n_iters': 100, 'normalizer_kwargs': {'norm_order': [0, 1], 'p': 2, 'zero': [False, False]}}\n",
      "codebook shape:  torch.Size([4096, 6]) device:  meta dtype:  torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.66it/s]\n"
     ]
    }
   ],
   "source": [
    "#try to load the model\n",
    "loaded_model = LlamaForCausalLM.from_pretrained(hf_model_save_path,\n",
    "                                                torch_dtype = 'auto',\n",
    "                                                low_cpu_mem_usage=True,\n",
    "                                                attn_implementation='sdpa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float16"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model.model.layers[0].self_attn.q_proj.reconstruct().dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2513,  0.5720,  0.5787,  ...,  0.3616, -0.9614,  0.5276]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(1, 1, 4096).cpu()\n",
    "loaded_model.model.layers[0].self_attn.q_proj(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2513,  0.5720,  0.5787,  ...,  0.3616, -0.9614,  0.5276]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model.model.layers[0].self_attn.q_proj.cache_reconstruct()\n",
    "loaded_model.model.layers[0].self_attn.q_proj(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  5.02it/s]\n"
     ]
    }
   ],
   "source": [
    "original_model = OrigLlama.from_pretrained(base_model, device_map=\"cpu\", torch_dtype=\"auto\",\n",
    "                                             attn_implementation='sdpa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try to save the model to a temp dir\n",
    "temp_dir = \"/data/lliu/huffman/temp/Llama-2-7b-hf/\"\n",
    "original_model.save_pretrained(temp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  1.88it/s]\n"
     ]
    }
   ],
   "source": [
    "loaded_orig_llama = OrigLlama.from_pretrained(temp_dir, device_map=\"cpu\", torch_dtype=\"auto\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float16"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_orig_llama.model.layers[0].self_attn.q_proj.weight.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterate through all parameters and assert that they are fp16\n",
    "for name, param in loaded_orig_llama.named_parameters():\n",
    "    assert param.dtype == torch.float16, f\"{name} is not fp16, it is {param.dtype}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NoWAC-VQ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
