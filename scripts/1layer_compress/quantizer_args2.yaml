quantizer_kwargs:
  d: 4
  n_bits: 3
  cluster_ignore_norms: True
  n_iters: 100
  norm_order: 
    - 0 
    - 1
  zero:
    - False
    - False
  # time_clustering: True
  initialize_method: 'kmeans'
  minibatch_size: 1
  random_minibatch: True
# alignment_kwargs:
#   clip_grad: 0.1
#   discrete_update_every: 100
#   low_bound: 1.0e-05
#   lr: 0.001
#   lr_multiplier: 0.33333333
#   n_iters: 1000
#   patience: 1000
#   patience_scheduler: 10
#   verbose: 10
#   reinitialize_optimizer: True
#   n_discrete_updates: 1
seed: 1234
dtype: 'float32'
# allocation_config: /data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/allocation/test_allocation2/allocation.yaml
# max_d_prod: 15
# max_d: 6
# quantizer_type: '1st_order'
# initial_discrete_update: True
# n_discrete_updates: 1
# final_discrete_update: True

# hessian_regularization: 0.01
