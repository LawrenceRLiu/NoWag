python -u post_quantization_fine_tune.py \
    --checkpoint_list_path="/data/lliu/huffman/models/meta-llama/Llama-2-7b-hf/compressed/127/checkpoints.yaml" \
    --device=cuda:0 \
    --log_wandb \
    --finetune_epochs=10 \
    --train_seqlen=1024 \
    --finetune_lr=1e-5 \
    --eval_every_samples=128 \
    --update_every_n_tokens=4096 \
    --finetune_nsamples_train=16 \
    --finetune_nsamples_val=0 \
    --finetune_adam_beta2=0.95 \
    --add_bias \
    --soft_labels --amp_finetuning
