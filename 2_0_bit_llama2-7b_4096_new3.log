/home/lliu/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.19s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.29it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.19it/s]
Model loaded. LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
4096
Loading tokenizer for meta-llama/Llama-2-7b-hf
Starting...
position_ids torch.Size([1, 4096])
attention_mask torch.Size([1, 1, 4096, 4096])
Ready.
layer original dtype torch.float16
0 self_attn.q_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
finished in: 19.886
not here
to: args ('cuda:7',) kwargs {}
torch.float32
Parameter containing:
tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:7', requires_grad=True)
39185 MiB free out of 48676 MiB total
0 self_attn.k_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
finished in: 17.758
not here
to: args ('cuda:7',) kwargs {}
torch.float32
Parameter containing:
tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:7', requires_grad=True)
39153 MiB free out of 48676 MiB total
0 self_attn.v_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
finished in: 17.857
not here
to: args ('cuda:7',) kwargs {}
torch.float32
Parameter containing:
tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:7', requires_grad=True)
39101 MiB free out of 48676 MiB total
0 self_attn.o_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
finished in: 17.636
not here
to: args ('cuda:7',) kwargs {}
torch.float32
Parameter containing:
tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:7', requires_grad=True)
39037 MiB free out of 48676 MiB total
0 mlp.gate_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
finished in: 44.336
not here
to: args ('cuda:7',) kwargs {}
torch.float32
Parameter containing:
tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:7', requires_grad=True)
38639 MiB free out of 48676 MiB total
0 mlp.up_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
finished in: 44.484
not here
to: args ('cuda:7',) kwargs {}
torch.float32
Parameter containing:
tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:7', requires_grad=True)
38327 MiB free out of 48676 MiB total
0 mlp.down_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
finished in: 44.792
not here
to: args ('cuda:7',) kwargs {}
torch.float32
Parameter containing:
tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:7', requires_grad=True)
38559 MiB free out of 48676 MiB total
Fine tuning ...
LlamaDecoderLayer(
  (self_attn): LlamaAttention(
    (q_proj): QuantizedLinear()
    (k_proj): QuantizedLinear()
    (v_proj): QuantizedLinear()
    (o_proj): QuantizedLinear()
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (mlp): LlamaMLP(
    (gate_proj): QuantizedLinear()
    (up_proj): QuantizedLinear()
    (down_proj): QuantizedLinear()
    (act_fn): SiLUActivation()
  )
  (input_layernorm): LlamaRMSNorm()
  (post_attention_layernorm): LlamaRMSNorm()
)
attempting to cast to float32
38559 MiB free out of 48676 MiB total
attention_mask torch.Size([1, 1, 4096, 4096]) cuda:7 torch.float32
position_ids torch.Size([1, 4096]) cuda:7 torch.int64
Found 30 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.codebooks', 'self_attn.q_proj.rowise_norms', 'self_attn.q_proj.colwise_norms', 'self_attn.q_proj.bias', 'self_attn.k_proj.codebooks', 'self_attn.k_proj.rowise_norms', 'self_attn.k_proj.colwise_norms', 'self_attn.k_proj.bias', 'self_attn.v_proj.codebooks', 'self_attn.v_proj.rowise_norms', 'self_attn.v_proj.colwise_norms', 'self_attn.v_proj.bias', 'self_attn.o_proj.codebooks', 'self_attn.o_proj.rowise_norms', 'self_attn.o_proj.colwise_norms', 'self_attn.o_proj.bias', 'mlp.gate_proj.codebooks', 'mlp.gate_proj.rowise_norms', 'mlp.gate_proj.colwise_norms', 'mlp.gate_proj.bias', 'mlp.up_proj.codebooks', 'mlp.up_proj.rowise_norms', 'mlp.up_proj.colwise_norms', 'mlp.up_proj.bias', 'mlp.down_proj.codebooks', 'mlp.down_proj.rowise_norms', 'mlp.down_proj.colwise_norms', 'mlp.down_proj.bias', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 135936 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:7 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:7 torch.float32
----------
epoch=0
train loss=7.32e-06	
9613 MiB free out of 48676 MiB total
----------
epoch=1
train loss=5.28e-06	
9613 MiB free out of 48676 MiB total
----------
epoch=2
train loss=5.08e-06	
9613 MiB free out of 48676 MiB total
----------
epoch=3
train loss=4.99e-06	
9613 MiB free out of 48676 MiB total
----------
epoch=4
train loss=4.94e-06	
9613 MiB free out of 48676 MiB total
9613 MiB free out of 48676 MiB total
trying to convert back to original dtype
Fine tuned
9613 MiB free out of 48676 MiB total
after cast to cpu
38517 MiB free out of 48676 MiB total
layer original dtype torch.float16
1 self_attn.q_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
finished in: 19.081
not here
to: args ('cuda:7',) kwargs {}
torch.float32
Parameter containing:
tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:7', requires_grad=True)
38687 MiB free out of 48676 MiB total
1 self_attn.k_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
finished in: 17.791
not here
to: args ('cuda:7',) kwargs {}
torch.float32
Parameter containing:
tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:7', requires_grad=True)
38687 MiB free out of 48676 MiB total
1 self_attn.v_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
finished in: 17.794
not here
to: args ('cuda:7',) kwargs {}
torch.float32
Parameter containing:
tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:7', requires_grad=True)
38687 MiB free out of 48676 MiB total
1 self_attn.o_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
finished in: 17.785
not here
to: args ('cuda:7',) kwargs {}
torch.float32
Parameter containing:
tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:7', requires_grad=True)
38687 MiB free out of 48676 MiB total
1 mlp.gate_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
finished in: 44.477
not here
to: args ('cuda:7',) kwargs {}
torch.float32
Parameter containing:
tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:7', requires_grad=True)
38311 MiB free out of 48676 MiB total
1 mlp.up_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
finished in: 44.565
not here
to: args ('cuda:7',) kwargs {}
torch.float32
Parameter containing:
tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:7', requires_grad=True)
37903 MiB free out of 48676 MiB total
1 mlp.down_proj
Pruning ...
Normalized clustering
using all the rows
using all the columns
Traceback (most recent call last):
  File "/home/lliu/huffman/llama.py", line 661, in <module>
    llama_sequential(model, dataloader, args.device)
  File "/home/lliu/anaconda3/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/lliu/huffman/llama.py", line 248, in llama_sequential
    weights_new, n_bits, n_params = gpts[name].normalized_clustering(
                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lliu/huffman/vector_quantizer.py", line 516, in normalized_clustering
    mappings, codebooks = cluster(weights_reshaped.reshape(-1,subvector_dim), k_codebook, 
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lliu/huffman/vector_quantizer.py", line 113, in cluster
    assignments = cluster_e_step(X, centriods, weights)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
