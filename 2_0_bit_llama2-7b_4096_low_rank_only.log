/home/lliu/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.16s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.35it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.24it/s]
Model loaded. LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
4096
Loading tokenizer for meta-llama/Llama-2-7b-hf
Starting...
38748 MiB free out of 48676 MiB total
attention_mask torch.Size([1, 1, 4096, 4096]) cuda:7 torch.float16
position_ids torch.Size([1, 4096]) cuda:7 torch.int64
position_ids torch.Size([1, 4096])
attention_mask torch.Size([1, 1, 4096, 4096])
Ready.
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
0 self_attn.k_proj
low_ranking ...
using low rank =  512
early stopping at epoch 100
last loss 0.09359163045883179 best loss 0.07638201117515564
0 self_attn.v_proj
low_ranking ...
using low rank =  512
last loss 0.723954975605011 best loss 0.723954975605011
0 self_attn.q_proj
low_ranking ...
using low rank =  512
last loss 0.05139857158064842 best loss 0.05139857158064842
0 self_attn.o_proj
low_ranking ...
using low rank =  512
early stopping at epoch 356
last loss 0.25418251752853394 best loss 0.2541825771331787
0 mlp.up_proj
low_ranking ...
0 mlp.down_proj
low_ranking ...
0 mlp.gate_proj
low_ranking ...
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
31330 MiB free out of 48676 MiB total
after cast to cpu
36838 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
1 self_attn.k_proj
low_ranking ...
using low rank =  512
last loss 4.082372665405273 best loss 4.082372665405273
1 self_attn.v_proj
low_ranking ...
using low rank =  512
last loss 1.7297354936599731 best loss 1.7297354936599731
1 self_attn.q_proj
low_ranking ...
using low rank =  512
last loss 3.3155503273010254 best loss 3.3155503273010254
1 self_attn.o_proj
low_ranking ...
using low rank =  512
last loss 0.5823419690132141 best loss 0.5823419690132141
1 mlp.up_proj
low_ranking ...
1 mlp.down_proj
low_ranking ...
1 mlp.gate_proj
low_ranking ...
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
30370 MiB free out of 48676 MiB total
after cast to cpu
35750 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
2 self_attn.k_proj
low_ranking ...
using low rank =  512
last loss 49.364505767822266 best loss 49.364505767822266
2 self_attn.v_proj
low_ranking ...
using low rank =  512
last loss 20.92196273803711 best loss 20.92196273803711
2 self_attn.q_proj
low_ranking ...
using low rank =  512
last loss 45.21408462524414 best loss 45.21408462524414
2 self_attn.o_proj
low_ranking ...
using low rank =  512
last loss 2.265394926071167 best loss 2.265394926071167
2 mlp.up_proj
low_ranking ...
2 mlp.down_proj
low_ranking ...
2 mlp.gate_proj
low_ranking ...
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
29474 MiB free out of 48676 MiB total
after cast to cpu
34662 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
3 self_attn.k_proj
low_ranking ...
using low rank =  512
last loss 193.16961669921875 best loss 193.16961669921875
3 self_attn.v_proj
low_ranking ...
using low rank =  512
last loss 65.66407775878906 best loss 65.66407775878906
3 self_attn.q_proj
low_ranking ...
using low rank =  512
last loss 169.85804748535156 best loss 169.85804748535156
3 self_attn.o_proj
low_ranking ...
using low rank =  512
last loss 2.1304075717926025 best loss 2.1304075717926025
3 mlp.up_proj
low_ranking ...
3 mlp.down_proj
low_ranking ...
3 mlp.gate_proj
low_ranking ...
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
28514 MiB free out of 48676 MiB total
after cast to cpu
33638 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
4 self_attn.k_proj
low_ranking ...
using low rank =  512
last loss 187.5740509033203 best loss 187.5740509033203
4 self_attn.v_proj
low_ranking ...
using low rank =  512
last loss 71.39517211914062 best loss 71.39517211914062
4 self_attn.q_proj
low_ranking ...
using low rank =  512
last loss 171.61099243164062 best loss 171.61099243164062
4 self_attn.o_proj
low_ranking ...
using low rank =  512
last loss 2.8558077812194824 best loss 2.8558077812194824
4 mlp.up_proj
low_ranking ...
4 mlp.down_proj
low_ranking ...
4 mlp.gate_proj
low_ranking ...
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
27490 MiB free out of 48676 MiB total
after cast to cpu
32614 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
5 self_attn.k_proj
low_ranking ...
using low rank =  512
last loss 214.8740234375 best loss 214.8740234375
5 self_attn.v_proj
low_ranking ...
using low rank =  512
last loss 83.50138854980469 best loss 83.50138854980469
5 self_attn.q_proj
low_ranking ...
using low rank =  512
last loss 193.48651123046875 best loss 193.48651123046875
5 self_attn.o_proj
low_ranking ...
using low rank =  512
last loss 2.8852076530456543 best loss 2.8852076530456543
5 mlp.up_proj
low_ranking ...
5 mlp.down_proj
low_ranking ...
5 mlp.gate_proj
low_ranking ...
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
26466 MiB free out of 48676 MiB total
after cast to cpu
31590 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
6 self_attn.k_proj
low_ranking ...
using low rank =  512
last loss 261.00048828125 best loss 261.00048828125
6 self_attn.v_proj
low_ranking ...
using low rank =  512
last loss 104.02674865722656 best loss 104.02674865722656
6 self_attn.q_proj
low_ranking ...
using low rank =  512
last loss 241.55001831054688 best loss 241.55001831054688
6 self_attn.o_proj
low_ranking ...
using low rank =  512
last loss 3.033780574798584 best loss 3.033780574798584
6 mlp.up_proj
low_ranking ...
6 mlp.down_proj
low_ranking ...
6 mlp.gate_proj
low_ranking ...
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
25442 MiB free out of 48676 MiB total
after cast to cpu
30566 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
7 self_attn.k_proj
low_ranking ...
using low rank =  512
last loss 275.65179443359375 best loss 275.65179443359375
7 self_attn.v_proj
low_ranking ...
using low rank =  512
last loss 112.2703857421875 best loss 112.2703857421875
7 self_attn.q_proj
low_ranking ...
using low rank =  512
last loss 266.52325439453125 best loss 266.52325439453125
7 self_attn.o_proj
low_ranking ...
using low rank =  512
last loss 3.865236759185791 best loss 3.865236759185791
7 mlp.up_proj
low_ranking ...
7 mlp.down_proj
low_ranking ...
7 mlp.gate_proj
low_ranking ...
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
24418 MiB free out of 48676 MiB total
after cast to cpu
29542 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
8 self_attn.k_proj
low_ranking ...
using low rank =  512
last loss 300.3657531738281 best loss 300.3657531738281
8 self_attn.v_proj
low_ranking ...
using low rank =  512
last loss 111.42613220214844 best loss 111.42613220214844
8 self_attn.q_proj
low_ranking ...
using low rank =  512
last loss 277.3271484375 best loss 277.3271484375
8 self_attn.o_proj
low_ranking ...
using low rank =  512
last loss 4.9948649406433105 best loss 4.9948649406433105
8 mlp.up_proj
low_ranking ...
8 mlp.down_proj
low_ranking ...
8 mlp.gate_proj
low_ranking ...
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
23394 MiB free out of 48676 MiB total
after cast to cpu
29542 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
9 self_attn.k_proj
low_ranking ...
using low rank =  512
last loss 344.082275390625 best loss 344.082275390625
9 self_attn.v_proj
low_ranking ...
using low rank =  512
last loss 127.31729125976562 best loss 127.31729125976562
9 self_attn.q_proj
low_ranking ...
using low rank =  512
last loss 323.59600830078125 best loss 323.59600830078125
9 self_attn.o_proj
low_ranking ...
using low rank =  512
last loss 6.322350025177002 best loss 6.322350025177002
9 mlp.up_proj
low_ranking ...
9 mlp.down_proj
low_ranking ...
9 mlp.gate_proj
low_ranking ...
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
23266 MiB free out of 48676 MiB total
after cast to cpu
28518 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
10 self_attn.k_proj
low_ranking ...
using low rank =  512
last loss 339.5364074707031 best loss 339.5364074707031
10 self_attn.v_proj
low_ranking ...
using low rank =  512
last loss 129.34512329101562 best loss 129.34512329101562
10 self_attn.q_proj
low_ranking ...
using low rank =  512
last loss 304.3883972167969 best loss 304.3883972167969
10 self_attn.o_proj
low_ranking ...
using low rank =  512
last loss 6.8997015953063965 best loss 6.8997015953063965
10 mlp.up_proj
low_ranking ...
10 mlp.down_proj
low_ranking ...
10 mlp.gate_proj
low_ranking ...
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
22370 MiB free out of 48676 MiB total
after cast to cpu
26470 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
11 self_attn.k_proj
low_ranking ...
using low rank =  512
last loss 325.6869201660156 best loss 325.6869201660156
11 self_attn.v_proj
low_ranking ...
using low rank =  512
last loss 166.10525512695312 best loss 166.10525512695312
11 self_attn.q_proj
low_ranking ...
using low rank =  512
last loss 310.6235046386719 best loss 310.6235046386719
11 self_attn.o_proj
low_ranking ...
using low rank =  512
last loss 6.522082328796387 best loss 6.522082328796387
11 mlp.up_proj
low_ranking ...
11 mlp.down_proj
low_ranking ...
11 mlp.gate_proj
low_ranking ...
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
20322 MiB free out of 48676 MiB total
after cast to cpu
25446 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
12 self_attn.k_proj
low_ranking ...
using low rank =  512
last loss 396.3678283691406 best loss 396.3678283691406
12 self_attn.v_proj
low_ranking ...
using low rank =  512
last loss 175.8560028076172 best loss 175.8560028076172
12 self_attn.q_proj
low_ranking ...
using low rank =  512
last loss 378.24700927734375 best loss 378.24700927734375
12 self_attn.o_proj
low_ranking ...
using low rank =  512
last loss 8.003228187561035 best loss 8.003228187561035
12 mlp.up_proj
low_ranking ...
12 mlp.down_proj
low_ranking ...
12 mlp.gate_proj
low_ranking ...
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
19298 MiB free out of 48676 MiB total
after cast to cpu
25446 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
13 self_attn.k_proj
low_ranking ...
using low rank =  512
last loss 427.15155029296875 best loss 427.15155029296875
13 self_attn.v_proj
low_ranking ...
using low rank =  512
last loss 205.75144958496094 best loss 205.75144958496094
13 self_attn.q_proj
low_ranking ...
using low rank =  512
last loss 392.2934875488281 best loss 392.2934875488281
13 self_attn.o_proj
low_ranking ...
using low rank =  512
last loss 8.202251434326172 best loss 8.202251434326172
13 mlp.up_proj
low_ranking ...
13 mlp.down_proj
low_ranking ...
13 mlp.gate_proj
low_ranking ...
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
19298 MiB free out of 48676 MiB total
after cast to cpu
25446 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
14 self_attn.k_proj
low_ranking ...
using low rank =  512
last loss 376.08587646484375 best loss 376.08587646484375
14 self_attn.v_proj
low_ranking ...
using low rank =  512
last loss 198.29270935058594 best loss 198.29270935058594
14 self_attn.q_proj
low_ranking ...
using low rank =  512
last loss 358.85687255859375 best loss 358.85687255859375
14 self_attn.o_proj
low_ranking ...
using low rank =  512
last loss 10.00003719329834 best loss 10.00003719329834
14 mlp.up_proj
low_ranking ...
14 mlp.down_proj
low_ranking ...
14 mlp.gate_proj
low_ranking ...
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
18870 MiB free out of 48676 MiB total
after cast to cpu
24122 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
15 self_attn.k_proj
low_ranking ...
using low rank =  512
last loss 392.468505859375 best loss 392.468505859375
15 self_attn.v_proj
low_ranking ...
using low rank =  512
last loss 208.7941436767578 best loss 208.7941436767578
15 self_attn.q_proj
low_ranking ...
using low rank =  512
last loss 371.363525390625 best loss 371.363525390625
15 self_attn.o_proj
low_ranking ...
using low rank =  512
last loss 12.99350643157959 best loss 12.99350643157959
15 mlp.up_proj
low_ranking ...
15 mlp.down_proj
low_ranking ...
15 mlp.gate_proj
low_ranking ...
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
17974 MiB free out of 48676 MiB total
after cast to cpu
23098 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
16 self_attn.k_proj
low_ranking ...
using low rank =  512
last loss 384.91790771484375 best loss 384.91790771484375
16 self_attn.v_proj
low_ranking ...
using low rank =  512
last loss 237.22921752929688 best loss 237.22921752929688
16 self_attn.q_proj
low_ranking ...
using low rank =  512
last loss 366.43865966796875 best loss 366.43865966796875
16 self_attn.o_proj
low_ranking ...
using low rank =  512
last loss 16.440555572509766 best loss 16.440555572509766
16 mlp.up_proj
low_ranking ...
16 mlp.down_proj
low_ranking ...
16 mlp.gate_proj
low_ranking ...
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
16950 MiB free out of 48676 MiB total
after cast to cpu
22074 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
17 self_attn.k_proj
low_ranking ...
using low rank =  512
last loss 387.79559326171875 best loss 387.79559326171875
17 self_attn.v_proj
low_ranking ...
using low rank =  512
last loss 233.27691650390625 best loss 233.27691650390625
17 self_attn.q_proj
low_ranking ...
using low rank =  512
last loss 377.2043151855469 best loss 377.2043151855469
17 self_attn.o_proj
low_ranking ...
using low rank =  512
last loss 11.932563781738281 best loss 11.932563781738281
17 mlp.up_proj
low_ranking ...
17 mlp.down_proj
low_ranking ...
17 mlp.gate_proj
low_ranking ...
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
15926 MiB free out of 48676 MiB total
after cast to cpu
21050 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
18 self_attn.k_proj
low_ranking ...
using low rank =  512
last loss 442.31976318359375 best loss 442.31976318359375
18 self_attn.v_proj
low_ranking ...
using low rank =  512
last loss 279.66162109375 best loss 279.66162109375
18 self_attn.q_proj
low_ranking ...
using low rank =  512
last loss 424.30487060546875 best loss 424.30487060546875
18 self_attn.o_proj
low_ranking ...
using low rank =  512
last loss 12.524012565612793 best loss 12.524012565612793
18 mlp.up_proj
low_ranking ...
18 mlp.down_proj
low_ranking ...
18 mlp.gate_proj
low_ranking ...
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
14902 MiB free out of 48676 MiB total
after cast to cpu
20026 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
19 self_attn.k_proj
low_ranking ...
using low rank =  512
last loss 399.6041259765625 best loss 399.6041259765625
19 self_attn.v_proj
low_ranking ...
using low rank =  512
last loss 281.89794921875 best loss 281.89794921875
19 self_attn.q_proj
low_ranking ...
using low rank =  512
last loss 391.85540771484375 best loss 391.85540771484375
19 self_attn.o_proj
low_ranking ...
using low rank =  512
last loss 12.525148391723633 best loss 12.525148391723633
19 mlp.up_proj
low_ranking ...
19 mlp.down_proj
low_ranking ...
19 mlp.gate_proj
low_ranking ...
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
13878 MiB free out of 48676 MiB total
after cast to cpu
19002 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
20 self_attn.k_proj
low_ranking ...
using low rank =  512
last loss 405.7458801269531 best loss 405.7458801269531
20 self_attn.v_proj
low_ranking ...
using low rank =  512
last loss 288.98223876953125 best loss 288.98223876953125
20 self_attn.q_proj
low_ranking ...
using low rank =  512
last loss 396.43963623046875 best loss 396.43963623046875
20 self_attn.o_proj
low_ranking ...
using low rank =  512
last loss 14.959181785583496 best loss 14.959181785583496
20 mlp.up_proj
low_ranking ...
20 mlp.down_proj
low_ranking ...
20 mlp.gate_proj
low_ranking ...
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
12854 MiB free out of 48676 MiB total
after cast to cpu
19002 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
21 self_attn.k_proj
low_ranking ...
using low rank =  512
last loss 416.8188781738281 best loss 416.8188781738281
21 self_attn.v_proj
low_ranking ...
using low rank =  512
last loss 320.4892883300781 best loss 320.4892883300781
21 self_attn.q_proj
low_ranking ...
using low rank =  512
last loss 405.47509765625 best loss 405.47509765625
21 self_attn.o_proj
low_ranking ...
using low rank =  512
last loss 11.616931915283203 best loss 11.616931915283203
21 mlp.up_proj
low_ranking ...
21 mlp.down_proj
low_ranking ...
21 mlp.gate_proj
low_ranking ...
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
12598 MiB free out of 48676 MiB total
after cast to cpu
17850 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
22 self_attn.k_proj
low_ranking ...
using low rank =  512
last loss 463.21722412109375 best loss 463.21722412109375
22 self_attn.v_proj
low_ranking ...
using low rank =  512
last loss 344.640625 best loss 344.640625
22 self_attn.q_proj
low_ranking ...
using low rank =  512
last loss 451.3304443359375 best loss 451.3304443359375
22 self_attn.o_proj
low_ranking ...
using low rank =  512
last loss 17.418376922607422 best loss 17.418376922607422
22 mlp.up_proj
low_ranking ...
22 mlp.down_proj
low_ranking ...
22 mlp.gate_proj
low_ranking ...
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
11702 MiB free out of 48676 MiB total
after cast to cpu
16826 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
23 self_attn.k_proj
low_ranking ...
using low rank =  512
last loss 549.6234130859375 best loss 549.6234130859375
23 self_attn.v_proj
low_ranking ...
using low rank =  512
last loss 430.6018981933594 best loss 430.6018981933594
23 self_attn.q_proj
low_ranking ...
using low rank =  512
last loss 545.5985107421875 best loss 545.5985107421875
23 self_attn.o_proj
low_ranking ...
using low rank =  512
last loss 16.981365203857422 best loss 16.981365203857422
23 mlp.up_proj
low_ranking ...
23 mlp.down_proj
low_ranking ...
23 mlp.gate_proj
low_ranking ...
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
10678 MiB free out of 48676 MiB total
after cast to cpu
15802 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
24 self_attn.k_proj
low_ranking ...
using low rank =  512
last loss 457.2400207519531 best loss 457.2400207519531
24 self_attn.v_proj
low_ranking ...
using low rank =  512
last loss 406.63568115234375 best loss 406.63568115234375
24 self_attn.q_proj
low_ranking ...
using low rank =  512
last loss 448.8950500488281 best loss 448.8950500488281
24 self_attn.o_proj
low_ranking ...
using low rank =  512
last loss 16.900379180908203 best loss 16.900379180908203
24 mlp.up_proj
low_ranking ...
24 mlp.down_proj
low_ranking ...
24 mlp.gate_proj
low_ranking ...
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
9654 MiB free out of 48676 MiB total
after cast to cpu
14778 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
25 self_attn.k_proj
low_ranking ...
using low rank =  512
last loss 615.1508178710938 best loss 615.1508178710938
25 self_attn.v_proj
low_ranking ...
using low rank =  512
last loss 534.0828247070312 best loss 534.0828247070312
25 self_attn.q_proj
low_ranking ...
using low rank =  512
last loss 597.815673828125 best loss 597.815673828125
25 self_attn.o_proj
low_ranking ...
using low rank =  512
last loss 16.077560424804688 best loss 16.077560424804688
25 mlp.up_proj
low_ranking ...
25 mlp.down_proj
low_ranking ...
25 mlp.gate_proj
low_ranking ...
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
8630 MiB free out of 48676 MiB total
after cast to cpu
13754 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
26 self_attn.k_proj
low_ranking ...
using low rank =  512
last loss 548.6689453125 best loss 548.6689453125
26 self_attn.v_proj
low_ranking ...
using low rank =  512
last loss 528.8197631835938 best loss 528.8197631835938
26 self_attn.q_proj
low_ranking ...
using low rank =  512
last loss 529.7461547851562 best loss 529.7461547851562
26 self_attn.o_proj
low_ranking ...
using low rank =  512
last loss 23.05294418334961 best loss 23.05294418334961
26 mlp.up_proj
low_ranking ...
26 mlp.down_proj
low_ranking ...
26 mlp.gate_proj
low_ranking ...
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
7606 MiB free out of 48676 MiB total
after cast to cpu
12730 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
27 self_attn.k_proj
low_ranking ...
using low rank =  512
last loss 691.6891479492188 best loss 691.6891479492188
27 self_attn.v_proj
low_ranking ...
using low rank =  512
last loss 580.5625610351562 best loss 580.5625610351562
27 self_attn.q_proj
low_ranking ...
using low rank =  512
last loss 664.7130126953125 best loss 664.7130126953125
27 self_attn.o_proj
low_ranking ...
using low rank =  512
last loss 22.904354095458984 best loss 22.904354095458984
27 mlp.up_proj
low_ranking ...
27 mlp.down_proj
low_ranking ...
27 mlp.gate_proj
low_ranking ...
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
6582 MiB free out of 48676 MiB total
after cast to cpu
11706 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
28 self_attn.k_proj
low_ranking ...
using low rank =  512
last loss 679.9119873046875 best loss 679.9119873046875
28 self_attn.v_proj
low_ranking ...
using low rank =  512
last loss 649.8799438476562 best loss 649.8799438476562
28 self_attn.q_proj
low_ranking ...
using low rank =  512
last loss 643.0855712890625 best loss 643.0855712890625
28 self_attn.o_proj
low_ranking ...
using low rank =  512
last loss 27.777973175048828 best loss 27.777973175048828
28 mlp.up_proj
low_ranking ...
28 mlp.down_proj
low_ranking ...
28 mlp.gate_proj
low_ranking ...
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
5558 MiB free out of 48676 MiB total
after cast to cpu
11706 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
29 self_attn.k_proj
low_ranking ...
using low rank =  512
last loss 551.2447509765625 best loss 551.2447509765625
29 self_attn.v_proj
low_ranking ...
using low rank =  512
last loss 616.5543212890625 best loss 616.5543212890625
29 self_attn.q_proj
low_ranking ...
using low rank =  512
last loss 526.2863159179688 best loss 526.2863159179688
29 self_attn.o_proj
low_ranking ...
using low rank =  512
last loss 25.929452896118164 best loss 25.929452896118164
29 mlp.up_proj
low_ranking ...
29 mlp.down_proj
low_ranking ...
29 mlp.gate_proj
low_ranking ...
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
5494 MiB free out of 48676 MiB total
after cast to cpu
10682 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
30 self_attn.k_proj
low_ranking ...
using low rank =  512
last loss 653.6451416015625 best loss 653.6451416015625
30 self_attn.v_proj
low_ranking ...
using low rank =  512
last loss 704.5257568359375 best loss 704.5257568359375
30 self_attn.q_proj
low_ranking ...
using low rank =  512
last loss 619.7572631835938 best loss 619.7572631835938
30 self_attn.o_proj
low_ranking ...
using low rank =  512
last loss 27.86043930053711 best loss 27.86043930053711
30 mlp.up_proj
low_ranking ...
30 mlp.down_proj
low_ranking ...
30 mlp.gate_proj
low_ranking ...
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
4534 MiB free out of 48676 MiB total
after cast to cpu
8634 MiB free out of 48676 MiB total
layer original dtype torch.float16
sequential [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']]
finished adding batches
subset ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
31 self_attn.k_proj
low_ranking ...
using low rank =  512
last loss 398.93621826171875 best loss 398.93621826171875
31 self_attn.v_proj
low_ranking ...
using low rank =  512
last loss 387.69818115234375 best loss 387.69818115234375
31 self_attn.q_proj
low_ranking ...
using low rank =  512
last loss 359.1895446777344 best loss 359.1895446777344
31 self_attn.o_proj
low_ranking ...
using low rank =  512
last loss 24.60346221923828 best loss 24.60346221923828
31 mlp.up_proj
low_ranking ...
31 mlp.down_proj
low_ranking ...
31 mlp.gate_proj
low_ranking ...
finished: ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj', 'self_attn.o_proj', 'mlp.up_proj', 'mlp.down_proj', 'mlp.gate_proj']
2486 MiB free out of 48676 MiB total
after cast to cpu
8634 MiB free out of 48676 MiB total
Total bits: 77846282240 Total params: 6476005376
average bits per value: 12.020725388601036
total time taken: 3361.4658370018005
Loading tokenizer for meta-llama/Llama-2-7b-hf
/home/lliu/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Dataset: wikitext2
Evaluating ...
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
Perplexity: 30.262136
