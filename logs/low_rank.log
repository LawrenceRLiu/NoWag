/home/lliu/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.42s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.02s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.08s/it]
Model loaded. LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
4096
Loading tokenizer for meta-llama/Llama-2-7b-hf
Starting...
position_ids torch.Size([1, 4096])
attention_mask torch.Size([1, 1, 4096, 4096])
Ready.
Layer 0
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
Pruning ...
using importances
keeping 8264 top channels and 0 bottom channels
Pre fine tuning: 24203 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 109072160 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
----------
epoch=0
train loss=1.71e-05	
9839 MiB free out of 48676 MiB total
----------
epoch=1
train loss=1.27e-05	
9839 MiB free out of 48676 MiB total
----------
epoch=2
train loss=1.16e-05	
9839 MiB free out of 48676 MiB total
----------
epoch=3
train loss=1.09e-05	
9839 MiB free out of 48676 MiB total
----------
epoch=4
train loss=1.04e-05	
9839 MiB free out of 48676 MiB total
post_fine_tuning 9839 MiB free out of 48676 MiB total
total size:  202375168 total Megabytes:  tensor(208.0540, device='cuda:6') bits per value:  tensor(8.6240, device='cuda:6')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([8264, 4096])
quantized
weights.shape =  torch.Size([4096, 8264])
quantized
weights.shape =  torch.Size([8264, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(15.8695, device='cuda:6') bits per value:  tensor(0.6578, device='cuda:6')
Pre fine tuning: 9839 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1169248 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
----------
epoch=0
train loss=6.13e-05	
9839 MiB free out of 48676 MiB total
----------
epoch=1
train loss=4.25e-05	
9839 MiB free out of 48676 MiB total
----------
epoch=2
train loss=3.48e-05	
9839 MiB free out of 48676 MiB total
----------
epoch=3
train loss=3.18e-05	
9839 MiB free out of 48676 MiB total
----------
epoch=4
train loss=3.04e-05	
9839 MiB free out of 48676 MiB total
post_fine_tuning 9839 MiB free out of 48676 MiB total
9839 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(15.8695, device='cuda:6') bits per value:  tensor(0.6578, device='cuda:6')
9833 MiB free out of 48676 MiB total
layer: 0
done in 0:17:33.528714 overall time: 0:17:33.528746 estimated time left: 9:04:19.391137
after cast to cpu
39287 MiB free out of 48676 MiB total
Layer 1
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
Pruning ...
using importances
keeping 8264 top channels and 0 bottom channels
Pre fine tuning: 23725 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 109072160 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
----------
epoch=0
train loss=1.03e-02	
9369 MiB free out of 48676 MiB total
----------
epoch=1
train loss=9.49e-04	
9369 MiB free out of 48676 MiB total
----------
epoch=2
train loss=5.58e-04	
9369 MiB free out of 48676 MiB total
----------
epoch=3
train loss=3.99e-04	
9369 MiB free out of 48676 MiB total
----------
epoch=4
train loss=3.32e-04	
9369 MiB free out of 48676 MiB total
post_fine_tuning 9369 MiB free out of 48676 MiB total
total size:  202375168 total Megabytes:  tensor(208.0540, device='cuda:6') bits per value:  tensor(8.6240, device='cuda:6')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([8264, 4096])
quantized
weights.shape =  torch.Size([4096, 8264])
quantized
weights.shape =  torch.Size([8264, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(15.8695, device='cuda:6') bits per value:  tensor(0.6578, device='cuda:6')
Pre fine tuning: 9369 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1169248 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
----------
epoch=0
train loss=3.23e-01	
9369 MiB free out of 48676 MiB total
----------
epoch=1
train loss=2.82e-01	
9369 MiB free out of 48676 MiB total
----------
epoch=2
train loss=2.35e-01	
9369 MiB free out of 48676 MiB total
----------
epoch=3
train loss=1.87e-01	
9369 MiB free out of 48676 MiB total
----------
epoch=4
train loss=1.48e-01	
9369 MiB free out of 48676 MiB total
post_fine_tuning 9369 MiB free out of 48676 MiB total
9369 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(15.8695, device='cuda:6') bits per value:  tensor(0.6578, device='cuda:6')
9369 MiB free out of 48676 MiB total
layer: 1
done in 0:17:21.106703 overall time: 0:34:54.850486 estimated time left: 8:43:42.757287
after cast to cpu
39287 MiB free out of 48676 MiB total
Layer 2
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
Pruning ...
using importances
keeping 8264 top channels and 0 bottom channels
Pre fine tuning: 23725 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 109072160 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
----------
epoch=0
train loss=1.01e-04	
9369 MiB free out of 48676 MiB total
----------
epoch=1
train loss=4.92e-05	
9369 MiB free out of 48676 MiB total
----------
epoch=2
train loss=4.02e-05	
9369 MiB free out of 48676 MiB total
----------
epoch=3
train loss=3.66e-05	
9369 MiB free out of 48676 MiB total
----------
epoch=4
train loss=3.45e-05	
9369 MiB free out of 48676 MiB total
post_fine_tuning 9369 MiB free out of 48676 MiB total
total size:  202375168 total Megabytes:  tensor(208.0540, device='cuda:6') bits per value:  tensor(8.6240, device='cuda:6')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([8264, 4096])
quantized
weights.shape =  torch.Size([4096, 8264])
quantized
weights.shape =  torch.Size([8264, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(15.8695, device='cuda:6') bits per value:  tensor(0.6578, device='cuda:6')
Pre fine tuning: 9369 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1169248 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
----------
epoch=0
train loss=2.33e-04	
9369 MiB free out of 48676 MiB total
----------
epoch=1
train loss=1.99e-04	
9369 MiB free out of 48676 MiB total
----------
epoch=2
train loss=1.88e-04	
9369 MiB free out of 48676 MiB total
----------
epoch=3
train loss=1.83e-04	
9369 MiB free out of 48676 MiB total
----------
epoch=4
train loss=1.80e-04	
9369 MiB free out of 48676 MiB total
post_fine_tuning 9369 MiB free out of 48676 MiB total
9369 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(15.8695, device='cuda:6') bits per value:  tensor(0.6578, device='cuda:6')
9369 MiB free out of 48676 MiB total
layer: 2
done in 0:16:55.177488 overall time: 0:51:50.267726 estimated time left: 8:21:05.921355
after cast to cpu
39285 MiB free out of 48676 MiB total
Layer 3
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
Pruning ...
using importances
keeping 8264 top channels and 0 bottom channels
Pre fine tuning: 23725 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 109072160 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
----------
epoch=0
train loss=1.97e-04	
9369 MiB free out of 48676 MiB total
----------
epoch=1
train loss=1.15e-04	
9369 MiB free out of 48676 MiB total
----------
epoch=2
train loss=9.99e-05	
9369 MiB free out of 48676 MiB total
----------
epoch=3
train loss=9.13e-05	
9369 MiB free out of 48676 MiB total
----------
epoch=4
train loss=8.52e-05	
9369 MiB free out of 48676 MiB total
post_fine_tuning 9369 MiB free out of 48676 MiB total
total size:  202375168 total Megabytes:  tensor(208.0540, device='cuda:6') bits per value:  tensor(8.6240, device='cuda:6')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([8264, 4096])
quantized
weights.shape =  torch.Size([4096, 8264])
quantized
weights.shape =  torch.Size([8264, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(15.8695, device='cuda:6') bits per value:  tensor(0.6578, device='cuda:6')
Pre fine tuning: 9369 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1169248 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
----------
epoch=0
train loss=6.25e-04	
9369 MiB free out of 48676 MiB total
----------
epoch=1
train loss=5.25e-04	
9369 MiB free out of 48676 MiB total
----------
epoch=2
train loss=4.88e-04	
9369 MiB free out of 48676 MiB total
----------
epoch=3
train loss=4.71e-04	
9369 MiB free out of 48676 MiB total
----------
epoch=4
train loss=4.61e-04	
9369 MiB free out of 48676 MiB total
post_fine_tuning 9369 MiB free out of 48676 MiB total
9369 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(15.8695, device='cuda:6') bits per value:  tensor(0.6578, device='cuda:6')
9369 MiB free out of 48676 MiB total
layer: 3
done in 0:11:38.770346 overall time: 1:03:29.182767 estimated time left: 7:24:24.279372
after cast to cpu
39287 MiB free out of 48676 MiB total
Layer 4
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
Pruning ...
using importances
keeping 8264 top channels and 0 bottom channels
Pre fine tuning: 23725 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 109072160 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
----------
epoch=0
train loss=8.73e-04	
9369 MiB free out of 48676 MiB total
----------
epoch=1
train loss=3.31e-04	
9369 MiB free out of 48676 MiB total
----------
epoch=2
train loss=2.70e-04	
9369 MiB free out of 48676 MiB total
----------
epoch=3
train loss=2.39e-04	
9369 MiB free out of 48676 MiB total
----------
epoch=4
train loss=2.19e-04	
9369 MiB free out of 48676 MiB total
post_fine_tuning 9369 MiB free out of 48676 MiB total
total size:  202375168 total Megabytes:  tensor(208.0540, device='cuda:6') bits per value:  tensor(8.6240, device='cuda:6')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([8264, 4096])
quantized
weights.shape =  torch.Size([4096, 8264])
quantized
weights.shape =  torch.Size([8264, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(15.8695, device='cuda:6') bits per value:  tensor(0.6578, device='cuda:6')
Pre fine tuning: 9369 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1169248 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
----------
epoch=0
train loss=1.97e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=1
train loss=1.44e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=2
train loss=1.24e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=3
train loss=1.14e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=4
train loss=1.09e-03	
9369 MiB free out of 48676 MiB total
post_fine_tuning 9369 MiB free out of 48676 MiB total
9369 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(15.8695, device='cuda:6') bits per value:  tensor(0.6578, device='cuda:6')
9369 MiB free out of 48676 MiB total
layer: 4
done in 0:11:39.984015 overall time: 1:15:09.310779 estimated time left: 6:45:50.278207
after cast to cpu
39287 MiB free out of 48676 MiB total
Layer 5
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
Pruning ...
using importances
keeping 8264 top channels and 0 bottom channels
Pre fine tuning: 23725 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 109072160 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
----------
epoch=0
train loss=7.55e-04	
9369 MiB free out of 48676 MiB total
----------
epoch=1
train loss=3.75e-04	
9369 MiB free out of 48676 MiB total
----------
epoch=2
train loss=3.13e-04	
9369 MiB free out of 48676 MiB total
----------
epoch=3
train loss=2.81e-04	
9369 MiB free out of 48676 MiB total
----------
epoch=4
train loss=2.59e-04	
9369 MiB free out of 48676 MiB total
post_fine_tuning 9369 MiB free out of 48676 MiB total
total size:  202375168 total Megabytes:  tensor(208.0540, device='cuda:6') bits per value:  tensor(8.6240, device='cuda:6')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([8264, 4096])
quantized
weights.shape =  torch.Size([4096, 8264])
quantized
weights.shape =  torch.Size([8264, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(15.8695, device='cuda:6') bits per value:  tensor(0.6578, device='cuda:6')
Pre fine tuning: 9369 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1169248 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
----------
epoch=0
train loss=2.11e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=1
train loss=1.40e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=2
train loss=1.14e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=3
train loss=1.03e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=4
train loss=9.84e-04	
9369 MiB free out of 48676 MiB total
post_fine_tuning 9369 MiB free out of 48676 MiB total
9369 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(15.8695, device='cuda:6') bits per value:  tensor(0.6578, device='cuda:6')
9369 MiB free out of 48676 MiB total
layer: 5
done in 0:11:40.348684 overall time: 1:26:49.818089 estimated time left: 6:16:15.878384
after cast to cpu
39285 MiB free out of 48676 MiB total
Layer 6
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
Pruning ...
using importances
keeping 8264 top channels and 0 bottom channels
Pre fine tuning: 23725 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 109072160 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
----------
epoch=0
train loss=1.33e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=1
train loss=4.27e-04	
9369 MiB free out of 48676 MiB total
----------
epoch=2
train loss=3.34e-04	
9369 MiB free out of 48676 MiB total
----------
epoch=3
train loss=2.88e-04	
9369 MiB free out of 48676 MiB total
----------
epoch=4
train loss=2.60e-04	
9369 MiB free out of 48676 MiB total
post_fine_tuning 9369 MiB free out of 48676 MiB total
total size:  202375168 total Megabytes:  tensor(208.0540, device='cuda:6') bits per value:  tensor(8.6240, device='cuda:6')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([8264, 4096])
quantized
weights.shape =  torch.Size([4096, 8264])
quantized
weights.shape =  torch.Size([8264, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(15.8695, device='cuda:6') bits per value:  tensor(0.6578, device='cuda:6')
Pre fine tuning: 9369 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1169248 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
----------
epoch=0
train loss=2.42e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=1
train loss=1.35e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=2
train loss=1.07e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=3
train loss=9.88e-04	
9369 MiB free out of 48676 MiB total
----------
epoch=4
train loss=9.46e-04	
9369 MiB free out of 48676 MiB total
post_fine_tuning 9369 MiB free out of 48676 MiB total
9369 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(15.8695, device='cuda:6') bits per value:  tensor(0.6578, device='cuda:6')
9369 MiB free out of 48676 MiB total
layer: 6
done in 0:11:33.768328 overall time: 1:38:23.737925 estimated time left: 5:51:24.778304
after cast to cpu
39285 MiB free out of 48676 MiB total
Layer 7
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
Pruning ...
using importances
keeping 8264 top channels and 0 bottom channels
Pre fine tuning: 23725 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 109072160 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
----------
epoch=0
train loss=1.76e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=1
train loss=6.44e-04	
9369 MiB free out of 48676 MiB total
----------
epoch=2
train loss=4.98e-04	
9369 MiB free out of 48676 MiB total
----------
epoch=3
train loss=4.28e-04	
9369 MiB free out of 48676 MiB total
----------
epoch=4
train loss=3.86e-04	
9369 MiB free out of 48676 MiB total
post_fine_tuning 9369 MiB free out of 48676 MiB total
total size:  202375168 total Megabytes:  tensor(208.0540, device='cuda:6') bits per value:  tensor(8.6240, device='cuda:6')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([8264, 4096])
quantized
weights.shape =  torch.Size([4096, 8264])
quantized
weights.shape =  torch.Size([8264, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(15.8695, device='cuda:6') bits per value:  tensor(0.6578, device='cuda:6')
Pre fine tuning: 9369 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1169248 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
----------
epoch=0
train loss=4.03e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=1
train loss=2.44e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=2
train loss=1.75e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=3
train loss=1.45e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=4
train loss=1.32e-03	
9369 MiB free out of 48676 MiB total
post_fine_tuning 9369 MiB free out of 48676 MiB total
9369 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(15.8695, device='cuda:6') bits per value:  tensor(0.6578, device='cuda:6')
9369 MiB free out of 48676 MiB total
layer: 7
done in 0:11:33.052878 overall time: 1:49:56.942029 estimated time left: 5:29:50.826086
after cast to cpu
39285 MiB free out of 48676 MiB total
Layer 8
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
Pruning ...
using importances
keeping 8264 top channels and 0 bottom channels
Pre fine tuning: 23725 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 109072160 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
----------
epoch=0
train loss=2.16e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=1
train loss=6.63e-04	
9369 MiB free out of 48676 MiB total
----------
epoch=2
train loss=5.07e-04	
9369 MiB free out of 48676 MiB total
----------
epoch=3
train loss=4.36e-04	
9369 MiB free out of 48676 MiB total
----------
epoch=4
train loss=3.92e-04	
9369 MiB free out of 48676 MiB total
post_fine_tuning 9369 MiB free out of 48676 MiB total
total size:  202375168 total Megabytes:  tensor(208.0540, device='cuda:6') bits per value:  tensor(8.6240, device='cuda:6')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([8264, 4096])
quantized
weights.shape =  torch.Size([4096, 8264])
quantized
weights.shape =  torch.Size([8264, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(15.8695, device='cuda:6') bits per value:  tensor(0.6578, device='cuda:6')
Pre fine tuning: 9369 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1169248 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
----------
epoch=0
train loss=5.04e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=1
train loss=2.81e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=2
train loss=2.00e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=3
train loss=1.73e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=4
train loss=1.64e-03	
9369 MiB free out of 48676 MiB total
post_fine_tuning 9369 MiB free out of 48676 MiB total
9369 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(15.8695, device='cuda:6') bits per value:  tensor(0.6578, device='cuda:6')
9369 MiB free out of 48676 MiB total
layer: 8
done in 0:11:33.479544 overall time: 2:01:30.574207 estimated time left: 5:10:31.467418
after cast to cpu
39285 MiB free out of 48676 MiB total
Layer 9
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
Pruning ...
using importances
keeping 8264 top channels and 0 bottom channels
Pre fine tuning: 23725 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 109072160 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
----------
epoch=0
train loss=1.82e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=1
train loss=7.79e-04	
9369 MiB free out of 48676 MiB total
----------
epoch=2
train loss=6.08e-04	
9369 MiB free out of 48676 MiB total
----------
epoch=3
train loss=5.19e-04	
9369 MiB free out of 48676 MiB total
----------
epoch=4
train loss=4.60e-04	
9369 MiB free out of 48676 MiB total
post_fine_tuning 9369 MiB free out of 48676 MiB total
total size:  202375168 total Megabytes:  tensor(208.0540, device='cuda:6') bits per value:  tensor(8.6240, device='cuda:6')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([8264, 4096])
quantized
weights.shape =  torch.Size([4096, 8264])
quantized
weights.shape =  torch.Size([8264, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(15.8695, device='cuda:6') bits per value:  tensor(0.6578, device='cuda:6')
Pre fine tuning: 9369 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1169248 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
----------
epoch=0
train loss=6.62e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=1
train loss=4.28e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=2
train loss=3.08e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=3
train loss=2.50e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=4
train loss=2.25e-03	
9369 MiB free out of 48676 MiB total
post_fine_tuning 9369 MiB free out of 48676 MiB total
9369 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(15.8695, device='cuda:6') bits per value:  tensor(0.6578, device='cuda:6')
9369 MiB free out of 48676 MiB total
layer: 9
done in 0:11:33.215435 overall time: 2:13:03.940831 estimated time left: 4:52:44.669829
after cast to cpu
39285 MiB free out of 48676 MiB total
Layer 10
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
Pruning ...
using importances
keeping 8264 top channels and 0 bottom channels
Pre fine tuning: 23725 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 109072160 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
----------
epoch=0
train loss=3.10e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=1
train loss=1.21e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=2
train loss=9.40e-04	
9369 MiB free out of 48676 MiB total
----------
epoch=3
train loss=7.96e-04	
9369 MiB free out of 48676 MiB total
----------
epoch=4
train loss=7.04e-04	
9369 MiB free out of 48676 MiB total
post_fine_tuning 9369 MiB free out of 48676 MiB total
total size:  202375168 total Megabytes:  tensor(208.0540, device='cuda:6') bits per value:  tensor(8.6240, device='cuda:6')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([8264, 4096])
quantized
weights.shape =  torch.Size([4096, 8264])
quantized
weights.shape =  torch.Size([8264, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(15.8695, device='cuda:6') bits per value:  tensor(0.6578, device='cuda:6')
Pre fine tuning: 9369 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1169248 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
----------
epoch=0
train loss=7.76e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=1
train loss=5.02e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=2
train loss=3.74e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=3
train loss=3.18e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=4
train loss=2.94e-03	
9369 MiB free out of 48676 MiB total
post_fine_tuning 9369 MiB free out of 48676 MiB total
9369 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(15.8695, device='cuda:6') bits per value:  tensor(0.6578, device='cuda:6')
9369 MiB free out of 48676 MiB total
layer: 10
done in 0:11:32.785011 overall time: 2:24:36.875031 estimated time left: 4:36:04.943241
after cast to cpu
39285 MiB free out of 48676 MiB total
Layer 11
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
Pruning ...
using importances
keeping 8264 top channels and 0 bottom channels
Pre fine tuning: 23725 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 109072160 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
----------
epoch=0
train loss=4.15e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=1
train loss=1.27e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=2
train loss=9.49e-04	
9369 MiB free out of 48676 MiB total
----------
epoch=3
train loss=8.00e-04	
9369 MiB free out of 48676 MiB total
----------
epoch=4
train loss=7.07e-04	
9369 MiB free out of 48676 MiB total
post_fine_tuning 9369 MiB free out of 48676 MiB total
total size:  202375168 total Megabytes:  tensor(208.0540, device='cuda:6') bits per value:  tensor(8.6240, device='cuda:6')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([8264, 4096])
quantized
weights.shape =  torch.Size([4096, 8264])
quantized
weights.shape =  torch.Size([8264, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(15.8695, device='cuda:6') bits per value:  tensor(0.6578, device='cuda:6')
Pre fine tuning: 9369 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1169248 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
----------
epoch=0
train loss=1.14e-02	
9369 MiB free out of 48676 MiB total
----------
epoch=1
train loss=7.31e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=2
train loss=5.11e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=3
train loss=4.06e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=4
train loss=3.58e-03	
9369 MiB free out of 48676 MiB total
post_fine_tuning 9369 MiB free out of 48676 MiB total
9369 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(15.8695, device='cuda:6') bits per value:  tensor(0.6578, device='cuda:6')
9369 MiB free out of 48676 MiB total
layer: 11
done in 0:11:32.996755 overall time: 2:36:10.022561 estimated time left: 4:20:16.704268
after cast to cpu
39285 MiB free out of 48676 MiB total
Layer 12
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
Pruning ...
using importances
keeping 8264 top channels and 0 bottom channels
Pre fine tuning: 23725 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 109072160 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
----------
epoch=0
train loss=3.94e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=1
train loss=1.24e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=2
train loss=9.20e-04	
9369 MiB free out of 48676 MiB total
----------
epoch=3
train loss=7.70e-04	
9369 MiB free out of 48676 MiB total
----------
epoch=4
train loss=6.78e-04	
9369 MiB free out of 48676 MiB total
post_fine_tuning 9369 MiB free out of 48676 MiB total
total size:  202375168 total Megabytes:  tensor(208.0540, device='cuda:6') bits per value:  tensor(8.6240, device='cuda:6')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([8264, 4096])
quantized
weights.shape =  torch.Size([4096, 8264])
quantized
weights.shape =  torch.Size([8264, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(15.8695, device='cuda:6') bits per value:  tensor(0.6578, device='cuda:6')
Pre fine tuning: 9369 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1169248 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
----------
epoch=0
train loss=8.57e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=1
train loss=5.29e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=2
train loss=3.83e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=3
train loss=3.22e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=4
train loss=2.98e-03	
9369 MiB free out of 48676 MiB total
post_fine_tuning 9369 MiB free out of 48676 MiB total
9369 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(15.8695, device='cuda:6') bits per value:  tensor(0.6578, device='cuda:6')
9369 MiB free out of 48676 MiB total
layer: 12
done in 0:11:32.080590 overall time: 2:47:42.247620 estimated time left: 4:05:06.361906
after cast to cpu
39287 MiB free out of 48676 MiB total
Layer 13
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
Pruning ...
using importances
keeping 8264 top channels and 0 bottom channels
Pre fine tuning: 23725 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 109072160 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
----------
epoch=0
train loss=3.45e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=1
train loss=1.29e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=2
train loss=9.71e-04	
9369 MiB free out of 48676 MiB total
----------
epoch=3
train loss=8.21e-04	
9369 MiB free out of 48676 MiB total
----------
epoch=4
train loss=7.27e-04	
9369 MiB free out of 48676 MiB total
post_fine_tuning 9369 MiB free out of 48676 MiB total
total size:  202375168 total Megabytes:  tensor(208.0540, device='cuda:6') bits per value:  tensor(8.6240, device='cuda:6')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([8264, 4096])
quantized
weights.shape =  torch.Size([4096, 8264])
quantized
weights.shape =  torch.Size([8264, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(15.8695, device='cuda:6') bits per value:  tensor(0.6578, device='cuda:6')
Pre fine tuning: 9369 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1169248 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
----------
epoch=0
train loss=9.25e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=1
train loss=5.92e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=2
train loss=4.43e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=3
train loss=3.76e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=4
train loss=3.47e-03	
9369 MiB free out of 48676 MiB total
post_fine_tuning 9369 MiB free out of 48676 MiB total
9369 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(15.8695, device='cuda:6') bits per value:  tensor(0.6578, device='cuda:6')
9369 MiB free out of 48676 MiB total
layer: 13
done in 0:11:32.519549 overall time: 2:59:14.916258 estimated time left: 3:50:27.749474
after cast to cpu
39287 MiB free out of 48676 MiB total
Layer 14
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
Pruning ...
using importances
keeping 8264 top channels and 0 bottom channels
Pre fine tuning: 23725 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 109072160 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
----------
epoch=0
train loss=3.84e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=1
train loss=1.35e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=2
train loss=1.02e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=3
train loss=8.55e-04	
9369 MiB free out of 48676 MiB total
----------
epoch=4
train loss=7.52e-04	
9369 MiB free out of 48676 MiB total
post_fine_tuning 9369 MiB free out of 48676 MiB total
total size:  202375168 total Megabytes:  tensor(208.0540, device='cuda:6') bits per value:  tensor(8.6240, device='cuda:6')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([8264, 4096])
quantized
weights.shape =  torch.Size([4096, 8264])
quantized
weights.shape =  torch.Size([8264, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(15.8695, device='cuda:6') bits per value:  tensor(0.6578, device='cuda:6')
Pre fine tuning: 9369 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1169248 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
----------
epoch=0
train loss=9.88e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=1
train loss=5.97e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=2
train loss=4.40e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=3
train loss=3.84e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=4
train loss=3.63e-03	
9369 MiB free out of 48676 MiB total
post_fine_tuning 9369 MiB free out of 48676 MiB total
9369 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(15.8695, device='cuda:6') bits per value:  tensor(0.6578, device='cuda:6')
9369 MiB free out of 48676 MiB total
layer: 14
done in 0:11:33.008008 overall time: 3:10:48.070987 estimated time left: 3:36:14.480452
after cast to cpu
39285 MiB free out of 48676 MiB total
Layer 15
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
Pruning ...
using importances
keeping 8264 top channels and 0 bottom channels
Pre fine tuning: 23725 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 109072160 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
----------
epoch=0
train loss=4.87e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=1
train loss=1.68e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=2
train loss=1.26e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=3
train loss=1.07e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=4
train loss=9.52e-04	
9369 MiB free out of 48676 MiB total
post_fine_tuning 9369 MiB free out of 48676 MiB total
total size:  202375168 total Megabytes:  tensor(208.0540, device='cuda:6') bits per value:  tensor(8.6240, device='cuda:6')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([8264, 4096])
quantized
weights.shape =  torch.Size([4096, 8264])
quantized
weights.shape =  torch.Size([8264, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(15.8695, device='cuda:6') bits per value:  tensor(0.6578, device='cuda:6')
Pre fine tuning: 9369 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1169248 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
----------
epoch=0
train loss=1.26e-02	
9369 MiB free out of 48676 MiB total
----------
epoch=1
train loss=8.15e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=2
train loss=6.07e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=3
train loss=5.12e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=4
train loss=4.73e-03	
9369 MiB free out of 48676 MiB total
post_fine_tuning 9369 MiB free out of 48676 MiB total
9369 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(15.8695, device='cuda:6') bits per value:  tensor(0.6578, device='cuda:6')
9369 MiB free out of 48676 MiB total
layer: 15
done in 0:11:32.609556 overall time: 3:22:20.825426 estimated time left: 3:22:20.825426
after cast to cpu
39285 MiB free out of 48676 MiB total
Layer 16
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
Pruning ...
using importances
keeping 8264 top channels and 0 bottom channels
Pre fine tuning: 23725 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 109072160 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
----------
epoch=0
train loss=5.21e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=1
train loss=1.56e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=2
train loss=1.15e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=3
train loss=9.65e-04	
9369 MiB free out of 48676 MiB total
----------
epoch=4
train loss=8.52e-04	
9369 MiB free out of 48676 MiB total
post_fine_tuning 9369 MiB free out of 48676 MiB total
total size:  202375168 total Megabytes:  tensor(208.0540, device='cuda:6') bits per value:  tensor(8.6240, device='cuda:6')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([8264, 4096])
quantized
weights.shape =  torch.Size([4096, 8264])
quantized
weights.shape =  torch.Size([8264, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(15.8695, device='cuda:6') bits per value:  tensor(0.6578, device='cuda:6')
Pre fine tuning: 9369 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1169248 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
----------
epoch=0
train loss=1.34e-02	
9369 MiB free out of 48676 MiB total
----------
epoch=1
train loss=8.32e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=2
train loss=6.13e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=3
train loss=5.23e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=4
train loss=4.87e-03	
9369 MiB free out of 48676 MiB total
post_fine_tuning 9369 MiB free out of 48676 MiB total
9369 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(15.8695, device='cuda:6') bits per value:  tensor(0.6578, device='cuda:6')
9369 MiB free out of 48676 MiB total
layer: 16
done in 0:11:31.985185 overall time: 3:33:52.956425 estimated time left: 3:08:43.196846
after cast to cpu
39285 MiB free out of 48676 MiB total
Layer 17
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
Pruning ...
using importances
keeping 8264 top channels and 0 bottom channels
Pre fine tuning: 23725 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 109072160 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
----------
epoch=0
train loss=4.94e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=1
train loss=1.50e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=2
train loss=1.13e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=3
train loss=9.58e-04	
9369 MiB free out of 48676 MiB total
----------
epoch=4
train loss=8.49e-04	
9369 MiB free out of 48676 MiB total
post_fine_tuning 9369 MiB free out of 48676 MiB total
total size:  202375168 total Megabytes:  tensor(208.0540, device='cuda:6') bits per value:  tensor(8.6240, device='cuda:6')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([8264, 4096])
quantized
weights.shape =  torch.Size([4096, 8264])
quantized
weights.shape =  torch.Size([8264, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(15.8695, device='cuda:6') bits per value:  tensor(0.6578, device='cuda:6')
Pre fine tuning: 9369 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1169248 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
----------
epoch=0
train loss=1.51e-02	
9369 MiB free out of 48676 MiB total
----------
epoch=1
train loss=8.41e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=2
train loss=5.97e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=3
train loss=4.97e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=4
train loss=4.61e-03	
9369 MiB free out of 48676 MiB total
post_fine_tuning 9369 MiB free out of 48676 MiB total
9369 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(15.8695, device='cuda:6') bits per value:  tensor(0.6578, device='cuda:6')
9369 MiB free out of 48676 MiB total
layer: 17
done in 0:11:32.372317 overall time: 3:45:25.474581 estimated time left: 2:55:19.813563
after cast to cpu
39285 MiB free out of 48676 MiB total
Layer 18
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
Pruning ...
using importances
keeping 8264 top channels and 0 bottom channels
Pre fine tuning: 23725 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 109072160 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
----------
epoch=0
train loss=5.64e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=1
train loss=1.78e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=2
train loss=1.33e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=3
train loss=1.10e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=4
train loss=9.62e-04	
9369 MiB free out of 48676 MiB total
post_fine_tuning 9369 MiB free out of 48676 MiB total
total size:  202375168 total Megabytes:  tensor(208.0540, device='cuda:6') bits per value:  tensor(8.6240, device='cuda:6')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([8264, 4096])
quantized
weights.shape =  torch.Size([4096, 8264])
quantized
weights.shape =  torch.Size([8264, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(15.8695, device='cuda:6') bits per value:  tensor(0.6578, device='cuda:6')
Pre fine tuning: 9369 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1169248 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
----------
epoch=0
train loss=1.36e-02	
9369 MiB free out of 48676 MiB total
----------
epoch=1
train loss=7.63e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=2
train loss=5.46e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=3
train loss=4.72e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=4
train loss=4.48e-03	
9369 MiB free out of 48676 MiB total
post_fine_tuning 9369 MiB free out of 48676 MiB total
9369 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(15.8695, device='cuda:6') bits per value:  tensor(0.6578, device='cuda:6')
9369 MiB free out of 48676 MiB total
layer: 18
done in 0:11:32.417958 overall time: 3:56:58.041259 estimated time left: 2:42:08.133493
after cast to cpu
39285 MiB free out of 48676 MiB total
Layer 19
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
Pruning ...
using importances
keeping 8264 top channels and 0 bottom channels
Pre fine tuning: 23725 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 109072160 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
----------
epoch=0
train loss=3.94e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=1
train loss=1.30e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=2
train loss=9.68e-04	
9369 MiB free out of 48676 MiB total
----------
epoch=3
train loss=8.16e-04	
9369 MiB free out of 48676 MiB total
----------
epoch=4
train loss=7.22e-04	
9369 MiB free out of 48676 MiB total
post_fine_tuning 9369 MiB free out of 48676 MiB total
total size:  202375168 total Megabytes:  tensor(208.0540, device='cuda:6') bits per value:  tensor(8.6240, device='cuda:6')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([8264, 4096])
quantized
weights.shape =  torch.Size([4096, 8264])
quantized
weights.shape =  torch.Size([8264, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(15.8695, device='cuda:6') bits per value:  tensor(0.6578, device='cuda:6')
Pre fine tuning: 9369 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1169248 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
----------
epoch=0
train loss=1.25e-02	
9369 MiB free out of 48676 MiB total
----------
epoch=1
train loss=7.64e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=2
train loss=5.46e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=3
train loss=4.53e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=4
train loss=4.18e-03	
9369 MiB free out of 48676 MiB total
post_fine_tuning 9369 MiB free out of 48676 MiB total
9369 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(15.8695, device='cuda:6') bits per value:  tensor(0.6578, device='cuda:6')
9369 MiB free out of 48676 MiB total
layer: 19
done in 0:11:32.715031 overall time: 4:08:30.901897 estimated time left: 2:29:06.541138
after cast to cpu
39285 MiB free out of 48676 MiB total
Layer 20
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
Pruning ...
using importances
keeping 8264 top channels and 0 bottom channels
Pre fine tuning: 23725 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 109072160 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
----------
epoch=0
train loss=2.25e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=1
train loss=8.46e-04	
9369 MiB free out of 48676 MiB total
----------
epoch=2
train loss=6.44e-04	
9369 MiB free out of 48676 MiB total
----------
epoch=3
train loss=5.48e-04	
9369 MiB free out of 48676 MiB total
----------
epoch=4
train loss=4.88e-04	
9369 MiB free out of 48676 MiB total
post_fine_tuning 9369 MiB free out of 48676 MiB total
total size:  202375168 total Megabytes:  tensor(208.0540, device='cuda:6') bits per value:  tensor(8.6240, device='cuda:6')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([8264, 4096])
quantized
weights.shape =  torch.Size([4096, 8264])
quantized
weights.shape =  torch.Size([8264, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(15.8695, device='cuda:6') bits per value:  tensor(0.6578, device='cuda:6')
Pre fine tuning: 9369 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1169248 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
----------
epoch=0
train loss=1.08e-02	
9369 MiB free out of 48676 MiB total
----------
epoch=1
train loss=6.65e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=2
train loss=4.76e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=3
train loss=3.90e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=4
train loss=3.56e-03	
9369 MiB free out of 48676 MiB total
post_fine_tuning 9369 MiB free out of 48676 MiB total
9369 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(15.8695, device='cuda:6') bits per value:  tensor(0.6578, device='cuda:6')
9369 MiB free out of 48676 MiB total
layer: 20
done in 0:11:32.458276 overall time: 4:20:03.504769 estimated time left: 2:16:13.264403
after cast to cpu
39285 MiB free out of 48676 MiB total
Layer 21
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
Pruning ...
using importances
keeping 8264 top channels and 0 bottom channels
Pre fine tuning: 23725 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 109072160 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
----------
epoch=0
train loss=4.32e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=1
train loss=1.25e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=2
train loss=9.47e-04	
9369 MiB free out of 48676 MiB total
----------
epoch=3
train loss=8.05e-04	
9369 MiB free out of 48676 MiB total
----------
epoch=4
train loss=7.16e-04	
9369 MiB free out of 48676 MiB total
post_fine_tuning 9369 MiB free out of 48676 MiB total
total size:  202375168 total Megabytes:  tensor(208.0540, device='cuda:6') bits per value:  tensor(8.6240, device='cuda:6')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([8264, 4096])
quantized
weights.shape =  torch.Size([4096, 8264])
quantized
weights.shape =  torch.Size([8264, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(15.8695, device='cuda:6') bits per value:  tensor(0.6578, device='cuda:6')
Pre fine tuning: 9369 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1169248 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
----------
epoch=0
train loss=1.44e-02	
9369 MiB free out of 48676 MiB total
----------
epoch=1
train loss=7.11e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=2
train loss=4.63e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=3
train loss=3.84e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=4
train loss=3.63e-03	
9369 MiB free out of 48676 MiB total
post_fine_tuning 9369 MiB free out of 48676 MiB total
9369 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(15.8695, device='cuda:6') bits per value:  tensor(0.6578, device='cuda:6')
9369 MiB free out of 48676 MiB total
layer: 21
done in 0:11:31.970336 overall time: 4:31:35.621629 estimated time left: 2:03:27.100741
after cast to cpu
39287 MiB free out of 48676 MiB total
Layer 22
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
Pruning ...
using importances
keeping 8264 top channels and 0 bottom channels
Pre fine tuning: 23725 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 109072160 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
----------
epoch=0
train loss=5.08e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=1
train loss=1.35e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=2
train loss=9.91e-04	
9369 MiB free out of 48676 MiB total
----------
epoch=3
train loss=8.31e-04	
9369 MiB free out of 48676 MiB total
----------
epoch=4
train loss=7.34e-04	
9369 MiB free out of 48676 MiB total
post_fine_tuning 9369 MiB free out of 48676 MiB total
total size:  202375168 total Megabytes:  tensor(208.0540, device='cuda:6') bits per value:  tensor(8.6240, device='cuda:6')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([8264, 4096])
quantized
weights.shape =  torch.Size([4096, 8264])
quantized
weights.shape =  torch.Size([8264, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(15.8695, device='cuda:6') bits per value:  tensor(0.6578, device='cuda:6')
Pre fine tuning: 9369 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1169248 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
----------
epoch=0
train loss=1.17e-02	
9369 MiB free out of 48676 MiB total
----------
epoch=1
train loss=6.73e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=2
train loss=4.53e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=3
train loss=3.71e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=4
train loss=3.46e-03	
9369 MiB free out of 48676 MiB total
post_fine_tuning 9369 MiB free out of 48676 MiB total
9369 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(15.8695, device='cuda:6') bits per value:  tensor(0.6578, device='cuda:6')
9369 MiB free out of 48676 MiB total
layer: 22
done in 0:11:32.383048 overall time: 4:43:08.153536 estimated time left: 1:50:47.538340
after cast to cpu
39287 MiB free out of 48676 MiB total
Layer 23
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
Pruning ...
using importances
keeping 8264 top channels and 0 bottom channels
Pre fine tuning: 23725 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 109072160 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
----------
epoch=0
train loss=4.51e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=1
train loss=1.35e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=2
train loss=9.98e-04	
9369 MiB free out of 48676 MiB total
----------
epoch=3
train loss=8.37e-04	
9369 MiB free out of 48676 MiB total
----------
epoch=4
train loss=7.37e-04	
9369 MiB free out of 48676 MiB total
post_fine_tuning 9369 MiB free out of 48676 MiB total
total size:  202375168 total Megabytes:  tensor(208.0540, device='cuda:6') bits per value:  tensor(8.6240, device='cuda:6')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([8264, 4096])
quantized
weights.shape =  torch.Size([4096, 8264])
quantized
weights.shape =  torch.Size([8264, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(15.8695, device='cuda:6') bits per value:  tensor(0.6578, device='cuda:6')
Pre fine tuning: 9369 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1169248 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
----------
epoch=0
train loss=1.18e-02	
9369 MiB free out of 48676 MiB total
----------
epoch=1
train loss=6.86e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=2
train loss=4.58e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=3
train loss=3.71e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=4
train loss=3.44e-03	
9369 MiB free out of 48676 MiB total
post_fine_tuning 9369 MiB free out of 48676 MiB total
9369 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(15.8695, device='cuda:6') bits per value:  tensor(0.6578, device='cuda:6')
9369 MiB free out of 48676 MiB total
layer: 23
done in 0:11:31.759289 overall time: 4:54:40.061019 estimated time left: 1:38:13.353673
after cast to cpu
39285 MiB free out of 48676 MiB total
Layer 24
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
Pruning ...
using importances
keeping 8264 top channels and 0 bottom channels
Pre fine tuning: 23725 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 109072160 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
----------
epoch=0
train loss=5.53e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=1
train loss=1.27e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=2
train loss=9.33e-04	
9369 MiB free out of 48676 MiB total
----------
epoch=3
train loss=7.79e-04	
9369 MiB free out of 48676 MiB total
----------
epoch=4
train loss=6.85e-04	
9369 MiB free out of 48676 MiB total
post_fine_tuning 9369 MiB free out of 48676 MiB total
total size:  202375168 total Megabytes:  tensor(208.0540, device='cuda:6') bits per value:  tensor(8.6240, device='cuda:6')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([8264, 4096])
quantized
weights.shape =  torch.Size([4096, 8264])
quantized
weights.shape =  torch.Size([8264, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(15.8695, device='cuda:6') bits per value:  tensor(0.6578, device='cuda:6')
Pre fine tuning: 9369 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1169248 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
----------
epoch=0
train loss=1.09e-02	
9369 MiB free out of 48676 MiB total
----------
epoch=1
train loss=5.65e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=2
train loss=3.74e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=3
train loss=3.21e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=4
train loss=3.07e-03	
9369 MiB free out of 48676 MiB total
post_fine_tuning 9369 MiB free out of 48676 MiB total
9369 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(15.8695, device='cuda:6') bits per value:  tensor(0.6578, device='cuda:6')
9369 MiB free out of 48676 MiB total
layer: 24
done in 0:11:33.287843 overall time: 5:06:13.495337 estimated time left: 1:25:44.578694
after cast to cpu
39285 MiB free out of 48676 MiB total
Layer 25
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
Pruning ...
using importances
keeping 8264 top channels and 0 bottom channels
Pre fine tuning: 23725 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 109072160 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
----------
epoch=0
train loss=7.49e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=1
train loss=1.86e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=2
train loss=1.34e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=3
train loss=1.12e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=4
train loss=9.83e-04	
9369 MiB free out of 48676 MiB total
post_fine_tuning 9369 MiB free out of 48676 MiB total
total size:  202375168 total Megabytes:  tensor(208.0540, device='cuda:6') bits per value:  tensor(8.6240, device='cuda:6')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([8264, 4096])
quantized
weights.shape =  torch.Size([4096, 8264])
quantized
weights.shape =  torch.Size([8264, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(15.8695, device='cuda:6') bits per value:  tensor(0.6578, device='cuda:6')
Pre fine tuning: 9369 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1169248 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
----------
epoch=0
train loss=1.78e-02	
9369 MiB free out of 48676 MiB total
----------
epoch=1
train loss=9.52e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=2
train loss=6.02e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=3
train loss=4.63e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=4
train loss=4.16e-03	
9369 MiB free out of 48676 MiB total
post_fine_tuning 9369 MiB free out of 48676 MiB total
9369 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(15.8695, device='cuda:6') bits per value:  tensor(0.6578, device='cuda:6')
9369 MiB free out of 48676 MiB total
layer: 25
done in 0:11:32.684784 overall time: 5:17:46.321976 estimated time left: 1:13:19.920456
after cast to cpu
39285 MiB free out of 48676 MiB total
Layer 26
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
Pruning ...
using importances
keeping 8264 top channels and 0 bottom channels
Pre fine tuning: 23725 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 109072160 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
----------
epoch=0
train loss=1.12e-02	
9369 MiB free out of 48676 MiB total
----------
epoch=1
train loss=2.12e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=2
train loss=1.47e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=3
train loss=1.19e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=4
train loss=1.03e-03	
9369 MiB free out of 48676 MiB total
post_fine_tuning 9369 MiB free out of 48676 MiB total
total size:  202375168 total Megabytes:  tensor(208.0540, device='cuda:6') bits per value:  tensor(8.6240, device='cuda:6')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([8264, 4096])
quantized
weights.shape =  torch.Size([4096, 8264])
quantized
weights.shape =  torch.Size([8264, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(15.8695, device='cuda:6') bits per value:  tensor(0.6578, device='cuda:6')
Pre fine tuning: 9369 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1169248 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
----------
epoch=0
train loss=2.04e-02	
9369 MiB free out of 48676 MiB total
----------
epoch=1
train loss=1.10e-02	
9369 MiB free out of 48676 MiB total
----------
epoch=2
train loss=6.73e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=3
train loss=4.98e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=4
train loss=4.41e-03	
9369 MiB free out of 48676 MiB total
post_fine_tuning 9369 MiB free out of 48676 MiB total
9369 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(15.8695, device='cuda:6') bits per value:  tensor(0.6578, device='cuda:6')
9369 MiB free out of 48676 MiB total
layer: 26
done in 0:11:31.499788 overall time: 5:29:17.973953 estimated time left: 1:00:58.884065
after cast to cpu
39285 MiB free out of 48676 MiB total
Layer 27
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
Pruning ...
using importances
keeping 8264 top channels and 0 bottom channels
Pre fine tuning: 23725 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 109072160 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
----------
epoch=0
train loss=3.06e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=1
train loss=1.14e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=2
train loss=8.49e-04	
9369 MiB free out of 48676 MiB total
----------
epoch=3
train loss=7.10e-04	
9369 MiB free out of 48676 MiB total
----------
epoch=4
train loss=6.24e-04	
9369 MiB free out of 48676 MiB total
post_fine_tuning 9369 MiB free out of 48676 MiB total
total size:  202375168 total Megabytes:  tensor(208.0540, device='cuda:6') bits per value:  tensor(8.6240, device='cuda:6')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([8264, 4096])
quantized
weights.shape =  torch.Size([4096, 8264])
quantized
weights.shape =  torch.Size([8264, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(15.8695, device='cuda:6') bits per value:  tensor(0.6578, device='cuda:6')
Pre fine tuning: 9369 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1169248 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
----------
epoch=0
train loss=1.53e-02	
9369 MiB free out of 48676 MiB total
----------
epoch=1
train loss=8.48e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=2
train loss=5.34e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=3
train loss=3.95e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=4
train loss=3.45e-03	
9369 MiB free out of 48676 MiB total
post_fine_tuning 9369 MiB free out of 48676 MiB total
9369 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(15.8695, device='cuda:6') bits per value:  tensor(0.6578, device='cuda:6')
9369 MiB free out of 48676 MiB total
layer: 27
done in 0:11:32.234852 overall time: 5:40:50.361443 estimated time left: 0:48:41.480206
after cast to cpu
39285 MiB free out of 48676 MiB total
Layer 28
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
Pruning ...
using importances
keeping 8264 top channels and 0 bottom channels
Pre fine tuning: 23725 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 109072160 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
----------
epoch=0
train loss=6.15e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=1
train loss=1.48e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=2
train loss=1.05e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=3
train loss=8.66e-04	
9369 MiB free out of 48676 MiB total
----------
epoch=4
train loss=7.57e-04	
9369 MiB free out of 48676 MiB total
post_fine_tuning 9369 MiB free out of 48676 MiB total
total size:  202375168 total Megabytes:  tensor(208.0540, device='cuda:6') bits per value:  tensor(8.6240, device='cuda:6')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([8264, 4096])
quantized
weights.shape =  torch.Size([4096, 8264])
quantized
weights.shape =  torch.Size([8264, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(15.8695, device='cuda:6') bits per value:  tensor(0.6578, device='cuda:6')
Pre fine tuning: 9369 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1169248 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
----------
epoch=0
train loss=2.01e-02	
9369 MiB free out of 48676 MiB total
----------
epoch=1
train loss=1.06e-02	
9369 MiB free out of 48676 MiB total
----------
epoch=2
train loss=6.44e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=3
train loss=4.79e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=4
train loss=4.27e-03	
9369 MiB free out of 48676 MiB total
post_fine_tuning 9369 MiB free out of 48676 MiB total
9369 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(15.8695, device='cuda:6') bits per value:  tensor(0.6578, device='cuda:6')
9369 MiB free out of 48676 MiB total
layer: 28
done in 0:11:32.739122 overall time: 5:52:23.248302 estimated time left: 0:36:27.232583
after cast to cpu
39285 MiB free out of 48676 MiB total
Layer 29
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
Pruning ...
using importances
keeping 8264 top channels and 0 bottom channels
Pre fine tuning: 23725 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 109072160 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
----------
epoch=0
train loss=8.69e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=1
train loss=1.86e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=2
train loss=1.32e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=3
train loss=1.08e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=4
train loss=9.46e-04	
9369 MiB free out of 48676 MiB total
post_fine_tuning 9369 MiB free out of 48676 MiB total
total size:  202375168 total Megabytes:  tensor(208.0540, device='cuda:6') bits per value:  tensor(8.6240, device='cuda:6')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([8264, 4096])
quantized
weights.shape =  torch.Size([4096, 8264])
quantized
weights.shape =  torch.Size([8264, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(15.8695, device='cuda:6') bits per value:  tensor(0.6578, device='cuda:6')
Pre fine tuning: 9369 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1169248 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
----------
epoch=0
train loss=3.49e-02	
9369 MiB free out of 48676 MiB total
----------
epoch=1
train loss=2.16e-02	
9369 MiB free out of 48676 MiB total
----------
epoch=2
train loss=1.50e-02	
9369 MiB free out of 48676 MiB total
----------
epoch=3
train loss=1.15e-02	
9369 MiB free out of 48676 MiB total
----------
epoch=4
train loss=9.83e-03	
9369 MiB free out of 48676 MiB total
post_fine_tuning 9369 MiB free out of 48676 MiB total
9369 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(15.8695, device='cuda:6') bits per value:  tensor(0.6578, device='cuda:6')
9369 MiB free out of 48676 MiB total
layer: 29
done in 0:11:31.840397 overall time: 6:03:55.237979 estimated time left: 0:24:15.682532
after cast to cpu
39285 MiB free out of 48676 MiB total
Layer 30
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
Pruning ...
using importances
keeping 8264 top channels and 0 bottom channels
Pre fine tuning: 23725 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 109072160 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
----------
epoch=0
train loss=7.40e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=1
train loss=2.53e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=2
train loss=2.01e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=3
train loss=1.46e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=4
train loss=1.86e-03	
9369 MiB free out of 48676 MiB total
post_fine_tuning 9369 MiB free out of 48676 MiB total
total size:  202375168 total Megabytes:  tensor(208.0540, device='cuda:6') bits per value:  tensor(8.6240, device='cuda:6')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([8264, 4096])
quantized
weights.shape =  torch.Size([4096, 8264])
quantized
weights.shape =  torch.Size([8264, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(15.8695, device='cuda:6') bits per value:  tensor(0.6578, device='cuda:6')
Pre fine tuning: 9369 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1169248 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
----------
epoch=0
train loss=4.89e-01	
9369 MiB free out of 48676 MiB total
----------
epoch=1
train loss=4.33e-01	
9369 MiB free out of 48676 MiB total
----------
epoch=2
train loss=3.97e-01	
9369 MiB free out of 48676 MiB total
----------
epoch=3
train loss=3.68e-01	
9369 MiB free out of 48676 MiB total
----------
epoch=4
train loss=3.36e-01	
9369 MiB free out of 48676 MiB total
post_fine_tuning 9369 MiB free out of 48676 MiB total
9369 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(15.8695, device='cuda:6') bits per value:  tensor(0.6578, device='cuda:6')
9369 MiB free out of 48676 MiB total
layer: 30
done in 0:11:31.733026 overall time: 6:15:27.121116 estimated time left: 0:12:06.681326
after cast to cpu
39287 MiB free out of 48676 MiB total
Layer 31
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
Pruning ...
using importances
keeping 8264 top channels and 0 bottom channels
Pre fine tuning: 23725 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.A', 'self_attn.q_proj.B', 'self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.k_proj.A', 'self_attn.k_proj.B', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.v_proj.A', 'self_attn.v_proj.B', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.o_proj.A', 'self_attn.o_proj.B', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'mlp.w1', 'mlp.w3', 'mlp.w2', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 109072160 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
----------
epoch=0
train loss=3.23e-02	
9369 MiB free out of 48676 MiB total
----------
epoch=1
train loss=1.20e-02	
9369 MiB free out of 48676 MiB total
----------
epoch=2
train loss=9.25e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=3
train loss=7.32e-03	
9369 MiB free out of 48676 MiB total
----------
epoch=4
train loss=5.54e-03	
9369 MiB free out of 48676 MiB total
post_fine_tuning 9369 MiB free out of 48676 MiB total
total size:  202375168 total Megabytes:  tensor(208.0540, device='cuda:6') bits per value:  tensor(8.6240, device='cuda:6')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([8264, 4096])
quantized
weights.shape =  torch.Size([4096, 8264])
quantized
weights.shape =  torch.Size([8264, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(15.8695, device='cuda:6') bits per value:  tensor(0.6578, device='cuda:6')
Pre fine tuning: 9369 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1169248 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:6 torch.float32
----------
epoch=0
train loss=1.02e+00	
9369 MiB free out of 48676 MiB total
----------
epoch=1
train loss=2.62e-01	
9369 MiB free out of 48676 MiB total
----------
epoch=2
train loss=1.59e-01	
9369 MiB free out of 48676 MiB total
----------
epoch=3
train loss=1.19e-01	
9369 MiB free out of 48676 MiB total
----------
epoch=4
train loss=9.68e-02	
9369 MiB free out of 48676 MiB total
post_fine_tuning 9369 MiB free out of 48676 MiB total
9369 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(15.8695, device='cuda:6') bits per value:  tensor(0.6578, device='cuda:6')
9369 MiB free out of 48676 MiB total
layer: 31
done in 0:11:33.119747 overall time: 6:27:00.395381 estimated time left: 0:00:00
after cast to cpu
39287 MiB free out of 48676 MiB total
Total bits: tensor(8519892992, device='cuda:6') Total params: 12952010752
average bits per value: tensor(0.6578, device='cuda:6')
23223.12946343422
Loading tokenizer for meta-llama/Llama-2-7b-hf
/home/lliu/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Dataset: wikitext2
Evaluating ...
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
Perplexity: 7749.497559
