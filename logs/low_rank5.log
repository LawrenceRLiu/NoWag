Arguments: Namespace(model='meta-llama/Llama-2-7b-hf', dataset='wikitext2', seed=0, device='cuda:5', nsamples=128, percdamp=1, sparsity=0, prunen=0, prunem=0, blocksize=128, gmp=False, wbits=16, minlayer=-1, maxlayer=1000, prune_only='', invert=False, save='/data/lliu/model_compression_weights/llama2_7b/low_rank/try1', true_sequential=False, log_wandb=False, quantize=True, low_rank=196, keep_top_rowise=0.5, keep_top_colwise=1.0, keep_top_frac=0.75, keep_bottom_frac=0, add_bias=True, subvector_dim_mha=4, bits_per_value_mha=2, normalize_rowise_mha=False, normalize_columnwise_mha=False, diagonal_only_mha=True, subvector_dim_mlp=4, bits_per_value_mlp=2, normalize_rowise_mlp=False, normalize_columnwise_mlp=False, diagonal_only_mlp=True, n_iters_quantize=100, finetune_max_epochs=5, finetune_early_stop=3, finetune_lr=1e-05, finetune_batch_size=1, offload_activations=False, finetune_adam_beta1=0.9, finetune_adam_beta2=0.95, finetune_keep_best=False, local_batch_size=None)
/home/lliu/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.51s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.04it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.04s/it]
Model loaded. LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
4096
Loading tokenizer for meta-llama/Llama-2-7b-hf
Starting...
position_ids torch.Size([1, 4096])
attention_mask torch.Size([1, 1, 4096, 4096])
Ready.
Layer 0
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using importances
keeping 8260 top channels and 0 bottom channels
total size:  202375168 total Megabytes:  tensor(207.9602, device='cuda:5') bits per value:  tensor(8.6201, device='cuda:5')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([8260, 4096])
quantized
weights.shape =  torch.Size([4096, 8260])
quantized
weights.shape =  torch.Size([8260, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(27.9574, device='cuda:5') bits per value:  tensor(1.1589, device='cuda:5')
Pre fine tuning: 24199 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1166168 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
----------
epoch=0
train loss=3.66e-05	
11887 MiB free out of 48676 MiB total
----------
epoch=1
train loss=2.60e-05	
11887 MiB free out of 48676 MiB total
----------
epoch=2
train loss=2.28e-05	
11887 MiB free out of 48676 MiB total
----------
epoch=3
train loss=2.15e-05	
11887 MiB free out of 48676 MiB total
----------
epoch=4
train loss=2.07e-05	
11887 MiB free out of 48676 MiB total
post_fine_tuning 11887 MiB free out of 48676 MiB total
11887 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(27.9574, device='cuda:5') bits per value:  tensor(1.1589, device='cuda:5')
11881 MiB free out of 48676 MiB total
layer: 0
done in 0:10:06.749059 overall time: 0:10:06.749091 estimated time left: 5:13:29.221826
after cast to cpu
39287 MiB free out of 48676 MiB total
Layer 1
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using importances
keeping 8260 top channels and 0 bottom channels
total size:  202375168 total Megabytes:  tensor(207.9602, device='cuda:5') bits per value:  tensor(8.6201, device='cuda:5')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([8260, 4096])
quantized
weights.shape =  torch.Size([4096, 8260])
quantized
weights.shape =  torch.Size([8260, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(27.9574, device='cuda:5') bits per value:  tensor(1.1589, device='cuda:5')
Pre fine tuning: 23721 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1166168 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
----------
epoch=0
train loss=2.90e-01	
11417 MiB free out of 48676 MiB total
----------
epoch=1
train loss=2.21e-01	
11417 MiB free out of 48676 MiB total
----------
epoch=2
train loss=1.50e-01	
11417 MiB free out of 48676 MiB total
----------
epoch=3
train loss=8.51e-02	
11417 MiB free out of 48676 MiB total
----------
epoch=4
train loss=4.18e-02	
11417 MiB free out of 48676 MiB total
post_fine_tuning 11417 MiB free out of 48676 MiB total
11417 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(27.9574, device='cuda:5') bits per value:  tensor(1.1589, device='cuda:5')
11417 MiB free out of 48676 MiB total
layer: 1
done in 0:10:21.484071 overall time: 0:20:28.402019 estimated time left: 5:07:06.030285
after cast to cpu
39287 MiB free out of 48676 MiB total
Layer 2
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using importances
keeping 8260 top channels and 0 bottom channels
total size:  202375168 total Megabytes:  tensor(207.9602, device='cuda:5') bits per value:  tensor(8.6201, device='cuda:5')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([8260, 4096])
quantized
weights.shape =  torch.Size([4096, 8260])
quantized
weights.shape =  torch.Size([8260, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(27.9574, device='cuda:5') bits per value:  tensor(1.1589, device='cuda:5')
Pre fine tuning: 23721 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1166168 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
----------
epoch=0
train loss=2.32e-04	
11417 MiB free out of 48676 MiB total
----------
epoch=1
train loss=1.91e-04	
11417 MiB free out of 48676 MiB total
----------
epoch=2
train loss=1.75e-04	
11417 MiB free out of 48676 MiB total
----------
epoch=3
train loss=1.68e-04	
11417 MiB free out of 48676 MiB total
----------
epoch=4
train loss=1.63e-04	
11417 MiB free out of 48676 MiB total
post_fine_tuning 11417 MiB free out of 48676 MiB total
11417 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(27.9574, device='cuda:5') bits per value:  tensor(1.1589, device='cuda:5')
11417 MiB free out of 48676 MiB total
layer: 2
done in 0:10:31.085190 overall time: 0:30:59.700108 estimated time left: 4:59:37.101042
after cast to cpu
39285 MiB free out of 48676 MiB total
Layer 3
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using importances
keeping 8260 top channels and 0 bottom channels
total size:  202375168 total Megabytes:  tensor(207.9602, device='cuda:5') bits per value:  tensor(8.6201, device='cuda:5')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([8260, 4096])
quantized
weights.shape =  torch.Size([4096, 8260])
quantized
weights.shape =  torch.Size([8260, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(27.9574, device='cuda:5') bits per value:  tensor(1.1589, device='cuda:5')
Pre fine tuning: 23721 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1166168 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
----------
epoch=0
train loss=7.61e-04	
11417 MiB free out of 48676 MiB total
----------
epoch=1
train loss=6.25e-04	
11417 MiB free out of 48676 MiB total
----------
epoch=2
train loss=5.67e-04	
11417 MiB free out of 48676 MiB total
----------
epoch=3
train loss=5.34e-04	
11417 MiB free out of 48676 MiB total
----------
epoch=4
train loss=5.14e-04	
11417 MiB free out of 48676 MiB total
post_fine_tuning 11417 MiB free out of 48676 MiB total
11417 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(27.9574, device='cuda:5') bits per value:  tensor(1.1589, device='cuda:5')
11417 MiB free out of 48676 MiB total
layer: 3
done in 0:10:17.788786 overall time: 0:41:17.749432 estimated time left: 4:49:04.246021
after cast to cpu
39287 MiB free out of 48676 MiB total
Layer 4
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using importances
keeping 8260 top channels and 0 bottom channels
total size:  202375168 total Megabytes:  tensor(207.9602, device='cuda:5') bits per value:  tensor(8.6201, device='cuda:5')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([8260, 4096])
quantized
weights.shape =  torch.Size([4096, 8260])
quantized
weights.shape =  torch.Size([8260, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(27.9574, device='cuda:5') bits per value:  tensor(1.1589, device='cuda:5')
Pre fine tuning: 23721 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1166168 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
----------
epoch=0
train loss=1.77e-03	
11417 MiB free out of 48676 MiB total
----------
epoch=1
train loss=1.48e-03	
11417 MiB free out of 48676 MiB total
----------
epoch=2
train loss=1.33e-03	
11417 MiB free out of 48676 MiB total
----------
epoch=3
train loss=1.23e-03	
11417 MiB free out of 48676 MiB total
----------
epoch=4
train loss=1.16e-03	
11417 MiB free out of 48676 MiB total
post_fine_tuning 11417 MiB free out of 48676 MiB total
11417 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(27.9574, device='cuda:5') bits per value:  tensor(1.1589, device='cuda:5')
11417 MiB free out of 48676 MiB total
layer: 4
done in 0:10:17.617141 overall time: 0:51:35.573564 estimated time left: 4:38:36.097246
after cast to cpu
39287 MiB free out of 48676 MiB total
Layer 5
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using importances
keeping 8260 top channels and 0 bottom channels
total size:  202375168 total Megabytes:  tensor(207.9602, device='cuda:5') bits per value:  tensor(8.6201, device='cuda:5')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([8260, 4096])
quantized
weights.shape =  torch.Size([4096, 8260])
quantized
weights.shape =  torch.Size([8260, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(27.9574, device='cuda:5') bits per value:  tensor(1.1589, device='cuda:5')
Pre fine tuning: 23721 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1166168 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
----------
epoch=0
train loss=2.71e-03	
11417 MiB free out of 48676 MiB total
----------
epoch=1
train loss=2.09e-03	
11417 MiB free out of 48676 MiB total
----------
epoch=2
train loss=1.80e-03	
11417 MiB free out of 48676 MiB total
----------
epoch=3
train loss=1.67e-03	
11417 MiB free out of 48676 MiB total
----------
epoch=4
train loss=1.61e-03	
11417 MiB free out of 48676 MiB total
post_fine_tuning 11417 MiB free out of 48676 MiB total
11417 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(27.9574, device='cuda:5') bits per value:  tensor(1.1589, device='cuda:5')
11417 MiB free out of 48676 MiB total
layer: 5
done in 0:10:18.432283 overall time: 1:01:54.219903 estimated time left: 4:28:14.952912
after cast to cpu
39287 MiB free out of 48676 MiB total
Layer 6
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using importances
keeping 8260 top channels and 0 bottom channels
total size:  202375168 total Megabytes:  tensor(207.9602, device='cuda:5') bits per value:  tensor(8.6201, device='cuda:5')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([8260, 4096])
quantized
weights.shape =  torch.Size([4096, 8260])
quantized
weights.shape =  torch.Size([8260, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(27.9574, device='cuda:5') bits per value:  tensor(1.1589, device='cuda:5')
Pre fine tuning: 23721 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1166168 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
----------
epoch=0
train loss=2.52e-03	
11417 MiB free out of 48676 MiB total
----------
epoch=1
train loss=1.83e-03	
11417 MiB free out of 48676 MiB total
----------
epoch=2
train loss=1.53e-03	
11417 MiB free out of 48676 MiB total
----------
epoch=3
train loss=1.37e-03	
11417 MiB free out of 48676 MiB total
----------
epoch=4
train loss=1.28e-03	
11417 MiB free out of 48676 MiB total
post_fine_tuning 11417 MiB free out of 48676 MiB total
11417 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(27.9574, device='cuda:5') bits per value:  tensor(1.1589, device='cuda:5')
11417 MiB free out of 48676 MiB total
layer: 6
done in 0:10:37.756457 overall time: 1:12:32.197899 estimated time left: 4:19:03.563925
after cast to cpu
39287 MiB free out of 48676 MiB total
Layer 7
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using importances
keeping 8260 top channels and 0 bottom channels
total size:  202375168 total Megabytes:  tensor(207.9602, device='cuda:5') bits per value:  tensor(8.6201, device='cuda:5')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([8260, 4096])
quantized
weights.shape =  torch.Size([4096, 8260])
quantized
weights.shape =  torch.Size([8260, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(27.9574, device='cuda:5') bits per value:  tensor(1.1589, device='cuda:5')
Pre fine tuning: 23721 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1166168 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
----------
epoch=0
train loss=4.76e-03	
11417 MiB free out of 48676 MiB total
----------
epoch=1
train loss=3.21e-03	
11417 MiB free out of 48676 MiB total
----------
epoch=2
train loss=2.47e-03	
11417 MiB free out of 48676 MiB total
----------
epoch=3
train loss=2.17e-03	
11417 MiB free out of 48676 MiB total
----------
epoch=4
train loss=2.04e-03	
11417 MiB free out of 48676 MiB total
post_fine_tuning 11417 MiB free out of 48676 MiB total
11417 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(27.9574, device='cuda:5') bits per value:  tensor(1.1589, device='cuda:5')
11417 MiB free out of 48676 MiB total
layer: 7
done in 0:10:06.529431 overall time: 1:22:38.945325 estimated time left: 4:07:56.835975
after cast to cpu
39287 MiB free out of 48676 MiB total
Layer 8
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using importances
keeping 8260 top channels and 0 bottom channels
total size:  202375168 total Megabytes:  tensor(207.9602, device='cuda:5') bits per value:  tensor(8.6201, device='cuda:5')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([8260, 4096])
quantized
weights.shape =  torch.Size([4096, 8260])
quantized
weights.shape =  torch.Size([8260, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(27.9574, device='cuda:5') bits per value:  tensor(1.1589, device='cuda:5')
Pre fine tuning: 23721 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1166168 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
----------
epoch=0
train loss=8.35e-03	
11417 MiB free out of 48676 MiB total
----------
epoch=1
train loss=5.86e-03	
11417 MiB free out of 48676 MiB total
----------
epoch=2
train loss=4.50e-03	
11417 MiB free out of 48676 MiB total
----------
epoch=3
train loss=3.88e-03	
11417 MiB free out of 48676 MiB total
----------
epoch=4
train loss=3.61e-03	
11417 MiB free out of 48676 MiB total
post_fine_tuning 11417 MiB free out of 48676 MiB total
11417 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(27.9574, device='cuda:5') bits per value:  tensor(1.1589, device='cuda:5')
11417 MiB free out of 48676 MiB total
layer: 8
done in 0:10:17.817172 overall time: 1:32:56.973307 estimated time left: 3:57:32.265117
after cast to cpu
39287 MiB free out of 48676 MiB total
Layer 9
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using importances
keeping 8260 top channels and 0 bottom channels
total size:  202375168 total Megabytes:  tensor(207.9602, device='cuda:5') bits per value:  tensor(8.6201, device='cuda:5')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([8260, 4096])
quantized
weights.shape =  torch.Size([4096, 8260])
quantized
weights.shape =  torch.Size([8260, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(27.9574, device='cuda:5') bits per value:  tensor(1.1589, device='cuda:5')
Pre fine tuning: 23721 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1166168 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
----------
epoch=0
train loss=8.29e-03	
11417 MiB free out of 48676 MiB total
----------
epoch=1
train loss=6.33e-03	
11417 MiB free out of 48676 MiB total
----------
epoch=2
train loss=5.15e-03	
11417 MiB free out of 48676 MiB total
----------
epoch=3
train loss=4.52e-03	
11417 MiB free out of 48676 MiB total
----------
epoch=4
train loss=4.22e-03	
11417 MiB free out of 48676 MiB total
post_fine_tuning 11417 MiB free out of 48676 MiB total
11417 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(27.9574, device='cuda:5') bits per value:  tensor(1.1589, device='cuda:5')
11417 MiB free out of 48676 MiB total
layer: 9
done in 0:10:35.546642 overall time: 1:43:32.814991 estimated time left: 3:47:48.192980
after cast to cpu
39287 MiB free out of 48676 MiB total
Layer 10
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using importances
keeping 8260 top channels and 0 bottom channels
total size:  202375168 total Megabytes:  tensor(207.9602, device='cuda:5') bits per value:  tensor(8.6201, device='cuda:5')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([8260, 4096])
quantized
weights.shape =  torch.Size([4096, 8260])
quantized
weights.shape =  torch.Size([8260, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(27.9574, device='cuda:5') bits per value:  tensor(1.1589, device='cuda:5')
Pre fine tuning: 23721 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1166168 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
----------
epoch=0
train loss=1.18e-02	
11417 MiB free out of 48676 MiB total
----------
epoch=1
train loss=8.77e-03	
11417 MiB free out of 48676 MiB total
----------
epoch=2
train loss=7.03e-03	
11417 MiB free out of 48676 MiB total
----------
epoch=3
train loss=6.19e-03	
11417 MiB free out of 48676 MiB total
----------
epoch=4
train loss=5.81e-03	
11417 MiB free out of 48676 MiB total
post_fine_tuning 11417 MiB free out of 48676 MiB total
11417 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(27.9574, device='cuda:5') bits per value:  tensor(1.1589, device='cuda:5')
11417 MiB free out of 48676 MiB total
layer: 10
done in 0:08:53.174859 overall time: 1:52:26.209162 estimated time left: 3:34:39.126581
after cast to cpu
39287 MiB free out of 48676 MiB total
Layer 11
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using importances
keeping 8260 top channels and 0 bottom channels
total size:  202375168 total Megabytes:  tensor(207.9602, device='cuda:5') bits per value:  tensor(8.6201, device='cuda:5')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([8260, 4096])
quantized
weights.shape =  torch.Size([4096, 8260])
quantized
weights.shape =  torch.Size([8260, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(27.9574, device='cuda:5') bits per value:  tensor(1.1589, device='cuda:5')
Pre fine tuning: 23721 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1166168 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
----------
epoch=0
train loss=1.27e-02	
11417 MiB free out of 48676 MiB total
----------
epoch=1
train loss=9.39e-03	
11417 MiB free out of 48676 MiB total
----------
epoch=2
train loss=7.14e-03	
11417 MiB free out of 48676 MiB total
----------
epoch=3
train loss=5.88e-03	
11417 MiB free out of 48676 MiB total
----------
epoch=4
train loss=5.28e-03	
11417 MiB free out of 48676 MiB total
post_fine_tuning 11417 MiB free out of 48676 MiB total
11417 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(27.9574, device='cuda:5') bits per value:  tensor(1.1589, device='cuda:5')
11417 MiB free out of 48676 MiB total
layer: 11
done in 0:08:46.216154 overall time: 2:01:12.643401 estimated time left: 3:22:01.072335
after cast to cpu
39287 MiB free out of 48676 MiB total
Layer 12
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using importances
keeping 8260 top channels and 0 bottom channels
total size:  202375168 total Megabytes:  tensor(207.9602, device='cuda:5') bits per value:  tensor(8.6201, device='cuda:5')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([8260, 4096])
quantized
weights.shape =  torch.Size([4096, 8260])
quantized
weights.shape =  torch.Size([8260, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(27.9574, device='cuda:5') bits per value:  tensor(1.1589, device='cuda:5')
Pre fine tuning: 23721 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1166168 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
----------
epoch=0
train loss=1.28e-02	
11417 MiB free out of 48676 MiB total
----------
epoch=1
train loss=9.59e-03	
11417 MiB free out of 48676 MiB total
----------
epoch=2
train loss=7.41e-03	
11417 MiB free out of 48676 MiB total
----------
epoch=3
train loss=6.20e-03	
11417 MiB free out of 48676 MiB total
----------
epoch=4
train loss=5.63e-03	
11417 MiB free out of 48676 MiB total
post_fine_tuning 11417 MiB free out of 48676 MiB total
11417 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(27.9574, device='cuda:5') bits per value:  tensor(1.1589, device='cuda:5')
11417 MiB free out of 48676 MiB total
layer: 12
done in 0:08:43.097901 overall time: 2:09:55.960445 estimated time left: 3:09:54.096035
after cast to cpu
39287 MiB free out of 48676 MiB total
Layer 13
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using importances
keeping 8260 top channels and 0 bottom channels
total size:  202375168 total Megabytes:  tensor(207.9602, device='cuda:5') bits per value:  tensor(8.6201, device='cuda:5')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([8260, 4096])
quantized
weights.shape =  torch.Size([4096, 8260])
quantized
weights.shape =  torch.Size([8260, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(27.9574, device='cuda:5') bits per value:  tensor(1.1589, device='cuda:5')
Pre fine tuning: 23721 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1166168 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
----------
epoch=0
train loss=1.40e-02	
11417 MiB free out of 48676 MiB total
----------
epoch=1
train loss=1.07e-02	
11417 MiB free out of 48676 MiB total
----------
epoch=2
train loss=8.75e-03	
11417 MiB free out of 48676 MiB total
----------
epoch=3
train loss=7.45e-03	
11417 MiB free out of 48676 MiB total
----------
epoch=4
train loss=6.75e-03	
11417 MiB free out of 48676 MiB total
post_fine_tuning 11417 MiB free out of 48676 MiB total
11417 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(27.9574, device='cuda:5') bits per value:  tensor(1.1589, device='cuda:5')
11417 MiB free out of 48676 MiB total
layer: 13
done in 0:08:45.098631 overall time: 2:18:41.275664 estimated time left: 2:58:18.782996
after cast to cpu
39287 MiB free out of 48676 MiB total
Layer 14
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using importances
keeping 8260 top channels and 0 bottom channels
total size:  202375168 total Megabytes:  tensor(207.9602, device='cuda:5') bits per value:  tensor(8.6201, device='cuda:5')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([8260, 4096])
quantized
weights.shape =  torch.Size([4096, 8260])
quantized
weights.shape =  torch.Size([8260, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(27.9574, device='cuda:5') bits per value:  tensor(1.1589, device='cuda:5')
Pre fine tuning: 23721 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1166168 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
----------
epoch=0
train loss=1.49e-02	
11417 MiB free out of 48676 MiB total
----------
epoch=1
train loss=1.18e-02	
11417 MiB free out of 48676 MiB total
----------
epoch=2
train loss=9.71e-03	
11417 MiB free out of 48676 MiB total
----------
epoch=3
train loss=8.53e-03	
11417 MiB free out of 48676 MiB total
----------
epoch=4
train loss=7.95e-03	
11417 MiB free out of 48676 MiB total
post_fine_tuning 11417 MiB free out of 48676 MiB total
11417 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(27.9574, device='cuda:5') bits per value:  tensor(1.1589, device='cuda:5')
11417 MiB free out of 48676 MiB total
layer: 14
done in 0:08:41.541987 overall time: 2:27:23.036279 estimated time left: 2:47:02.107783
after cast to cpu
39287 MiB free out of 48676 MiB total
Layer 15
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using importances
keeping 8260 top channels and 0 bottom channels
total size:  202375168 total Megabytes:  tensor(207.9602, device='cuda:5') bits per value:  tensor(8.6201, device='cuda:5')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([8260, 4096])
quantized
weights.shape =  torch.Size([4096, 8260])
quantized
weights.shape =  torch.Size([8260, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(27.9574, device='cuda:5') bits per value:  tensor(1.1589, device='cuda:5')
Pre fine tuning: 23721 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1166168 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
----------
epoch=0
train loss=2.05e-02	
11417 MiB free out of 48676 MiB total
----------
epoch=1
train loss=1.70e-02	
11417 MiB free out of 48676 MiB total
----------
epoch=2
train loss=1.47e-02	
11417 MiB free out of 48676 MiB total
----------
epoch=3
train loss=1.33e-02	
11417 MiB free out of 48676 MiB total
----------
epoch=4
train loss=1.26e-02	
11417 MiB free out of 48676 MiB total
post_fine_tuning 11417 MiB free out of 48676 MiB total
11417 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(27.9574, device='cuda:5') bits per value:  tensor(1.1589, device='cuda:5')
11417 MiB free out of 48676 MiB total
layer: 15
done in 0:08:45.929908 overall time: 2:36:09.184113 estimated time left: 2:36:09.184113
after cast to cpu
39287 MiB free out of 48676 MiB total
Layer 16
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using importances
keeping 8260 top channels and 0 bottom channels
total size:  202375168 total Megabytes:  tensor(207.9602, device='cuda:5') bits per value:  tensor(8.6201, device='cuda:5')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([8260, 4096])
quantized
weights.shape =  torch.Size([4096, 8260])
quantized
weights.shape =  torch.Size([8260, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(27.9574, device='cuda:5') bits per value:  tensor(1.1589, device='cuda:5')
Pre fine tuning: 23721 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1166168 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
----------
epoch=0
train loss=2.14e-02	
11417 MiB free out of 48676 MiB total
----------
epoch=1
train loss=1.71e-02	
11417 MiB free out of 48676 MiB total
----------
epoch=2
train loss=1.42e-02	
11417 MiB free out of 48676 MiB total
----------
epoch=3
train loss=1.23e-02	
11417 MiB free out of 48676 MiB total
----------
epoch=4
train loss=1.13e-02	
11417 MiB free out of 48676 MiB total
post_fine_tuning 11417 MiB free out of 48676 MiB total
11417 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(27.9574, device='cuda:5') bits per value:  tensor(1.1589, device='cuda:5')
11417 MiB free out of 48676 MiB total
layer: 16
done in 0:08:42.802952 overall time: 2:44:52.206073 estimated time left: 2:25:28.417123
after cast to cpu
39287 MiB free out of 48676 MiB total
Layer 17
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using importances
keeping 8260 top channels and 0 bottom channels
total size:  202375168 total Megabytes:  tensor(207.9602, device='cuda:5') bits per value:  tensor(8.6201, device='cuda:5')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([8260, 4096])
quantized
weights.shape =  torch.Size([4096, 8260])
quantized
weights.shape =  torch.Size([8260, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(27.9574, device='cuda:5') bits per value:  tensor(1.1589, device='cuda:5')
Pre fine tuning: 23721 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1166168 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
----------
epoch=0
train loss=2.23e-02	
11417 MiB free out of 48676 MiB total
----------
epoch=1
train loss=1.80e-02	
11417 MiB free out of 48676 MiB total
----------
epoch=2
train loss=1.54e-02	
11417 MiB free out of 48676 MiB total
----------
epoch=3
train loss=1.35e-02	
11417 MiB free out of 48676 MiB total
----------
epoch=4
train loss=1.23e-02	
11417 MiB free out of 48676 MiB total
post_fine_tuning 11417 MiB free out of 48676 MiB total
11417 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(27.9574, device='cuda:5') bits per value:  tensor(1.1589, device='cuda:5')
11417 MiB free out of 48676 MiB total
layer: 17
done in 0:08:43.561126 overall time: 2:53:35.981333 estimated time left: 2:15:01.318814
after cast to cpu
39287 MiB free out of 48676 MiB total
Layer 18
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using importances
keeping 8260 top channels and 0 bottom channels
total size:  202375168 total Megabytes:  tensor(207.9602, device='cuda:5') bits per value:  tensor(8.6201, device='cuda:5')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([8260, 4096])
quantized
weights.shape =  torch.Size([4096, 8260])
quantized
weights.shape =  torch.Size([8260, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(27.9574, device='cuda:5') bits per value:  tensor(1.1589, device='cuda:5')
Pre fine tuning: 23721 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1166168 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
----------
epoch=0
train loss=2.32e-02	
11417 MiB free out of 48676 MiB total
----------
epoch=1
train loss=1.79e-02	
11417 MiB free out of 48676 MiB total
----------
epoch=2
train loss=1.56e-02	
11417 MiB free out of 48676 MiB total
----------
epoch=3
train loss=1.39e-02	
11417 MiB free out of 48676 MiB total
----------
epoch=4
train loss=1.29e-02	
11417 MiB free out of 48676 MiB total
post_fine_tuning 11417 MiB free out of 48676 MiB total
11417 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(27.9574, device='cuda:5') bits per value:  tensor(1.1589, device='cuda:5')
11417 MiB free out of 48676 MiB total
layer: 18
done in 0:08:41.674248 overall time: 3:02:17.867890 estimated time left: 2:04:43.804346
after cast to cpu
39287 MiB free out of 48676 MiB total
Layer 19
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using importances
keeping 8260 top channels and 0 bottom channels
total size:  202375168 total Megabytes:  tensor(207.9602, device='cuda:5') bits per value:  tensor(8.6201, device='cuda:5')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([8260, 4096])
quantized
weights.shape =  torch.Size([4096, 8260])
quantized
weights.shape =  torch.Size([8260, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(27.9574, device='cuda:5') bits per value:  tensor(1.1589, device='cuda:5')
Pre fine tuning: 23721 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1166168 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
----------
epoch=0
train loss=2.69e-02	
11417 MiB free out of 48676 MiB total
----------
epoch=1
train loss=2.10e-02	
11417 MiB free out of 48676 MiB total
----------
epoch=2
train loss=1.74e-02	
11417 MiB free out of 48676 MiB total
----------
epoch=3
train loss=1.47e-02	
11417 MiB free out of 48676 MiB total
----------
epoch=4
train loss=1.32e-02	
11417 MiB free out of 48676 MiB total
post_fine_tuning 11417 MiB free out of 48676 MiB total
11417 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(27.9574, device='cuda:5') bits per value:  tensor(1.1589, device='cuda:5')
11417 MiB free out of 48676 MiB total
layer: 19
done in 0:08:42.098451 overall time: 3:11:00.180861 estimated time left: 1:54:36.108517
after cast to cpu
39287 MiB free out of 48676 MiB total
Layer 20
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using importances
keeping 8260 top channels and 0 bottom channels
total size:  202375168 total Megabytes:  tensor(207.9602, device='cuda:5') bits per value:  tensor(8.6201, device='cuda:5')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([8260, 4096])
quantized
weights.shape =  torch.Size([4096, 8260])
quantized
weights.shape =  torch.Size([8260, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(27.9574, device='cuda:5') bits per value:  tensor(1.1589, device='cuda:5')
Pre fine tuning: 23721 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1166168 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
----------
epoch=0
train loss=2.64e-02	
11417 MiB free out of 48676 MiB total
----------
epoch=1
train loss=1.93e-02	
11417 MiB free out of 48676 MiB total
----------
epoch=2
train loss=1.57e-02	
11417 MiB free out of 48676 MiB total
----------
epoch=3
train loss=1.36e-02	
11417 MiB free out of 48676 MiB total
----------
epoch=4
train loss=1.24e-02	
11417 MiB free out of 48676 MiB total
post_fine_tuning 11417 MiB free out of 48676 MiB total
11417 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(27.9574, device='cuda:5') bits per value:  tensor(1.1589, device='cuda:5')
11417 MiB free out of 48676 MiB total
layer: 20
done in 0:08:42.511944 overall time: 3:19:42.907645 estimated time left: 1:44:36.761147
after cast to cpu
39287 MiB free out of 48676 MiB total
Layer 21
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using importances
keeping 8260 top channels and 0 bottom channels
total size:  202375168 total Megabytes:  tensor(207.9602, device='cuda:5') bits per value:  tensor(8.6201, device='cuda:5')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([8260, 4096])
quantized
weights.shape =  torch.Size([4096, 8260])
quantized
weights.shape =  torch.Size([8260, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(27.9574, device='cuda:5') bits per value:  tensor(1.1589, device='cuda:5')
Pre fine tuning: 23721 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1166168 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
----------
epoch=0
train loss=2.62e-02	
11417 MiB free out of 48676 MiB total
----------
epoch=1
train loss=2.04e-02	
11417 MiB free out of 48676 MiB total
----------
epoch=2
train loss=1.68e-02	
11417 MiB free out of 48676 MiB total
----------
epoch=3
train loss=1.42e-02	
11417 MiB free out of 48676 MiB total
----------
epoch=4
train loss=1.26e-02	
11417 MiB free out of 48676 MiB total
post_fine_tuning 11417 MiB free out of 48676 MiB total
11417 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(27.9574, device='cuda:5') bits per value:  tensor(1.1589, device='cuda:5')
11417 MiB free out of 48676 MiB total
layer: 21
done in 0:08:43.532499 overall time: 3:28:26.653317 estimated time left: 1:34:44.842417
after cast to cpu
39287 MiB free out of 48676 MiB total
Layer 22
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using importances
keeping 8260 top channels and 0 bottom channels
total size:  202375168 total Megabytes:  tensor(207.9602, device='cuda:5') bits per value:  tensor(8.6201, device='cuda:5')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([8260, 4096])
quantized
weights.shape =  torch.Size([4096, 8260])
quantized
weights.shape =  torch.Size([8260, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(27.9574, device='cuda:5') bits per value:  tensor(1.1589, device='cuda:5')
Pre fine tuning: 23721 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1166168 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
----------
epoch=0
train loss=2.97e-02	
11417 MiB free out of 48676 MiB total
----------
epoch=1
train loss=2.29e-02	
11417 MiB free out of 48676 MiB total
----------
epoch=2
train loss=1.80e-02	
11417 MiB free out of 48676 MiB total
----------
epoch=3
train loss=1.46e-02	
11417 MiB free out of 48676 MiB total
----------
epoch=4
train loss=1.26e-02	
11417 MiB free out of 48676 MiB total
post_fine_tuning 11417 MiB free out of 48676 MiB total
11417 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(27.9574, device='cuda:5') bits per value:  tensor(1.1589, device='cuda:5')
11417 MiB free out of 48676 MiB total
layer: 22
done in 0:08:41.837801 overall time: 3:37:08.700397 estimated time left: 1:24:58.187112
after cast to cpu
39287 MiB free out of 48676 MiB total
Layer 23
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using importances
keeping 8260 top channels and 0 bottom channels
total size:  202375168 total Megabytes:  tensor(207.9602, device='cuda:5') bits per value:  tensor(8.6201, device='cuda:5')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([8260, 4096])
quantized
weights.shape =  torch.Size([4096, 8260])
quantized
weights.shape =  torch.Size([8260, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(27.9574, device='cuda:5') bits per value:  tensor(1.1589, device='cuda:5')
Pre fine tuning: 23721 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1166168 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
----------
epoch=0
train loss=2.80e-02	
11417 MiB free out of 48676 MiB total
----------
epoch=1
train loss=2.22e-02	
11417 MiB free out of 48676 MiB total
----------
epoch=2
train loss=1.83e-02	
11417 MiB free out of 48676 MiB total
----------
epoch=3
train loss=1.53e-02	
11417 MiB free out of 48676 MiB total
----------
epoch=4
train loss=1.34e-02	
11417 MiB free out of 48676 MiB total
post_fine_tuning 11417 MiB free out of 48676 MiB total
11417 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(27.9574, device='cuda:5') bits per value:  tensor(1.1589, device='cuda:5')
11417 MiB free out of 48676 MiB total
layer: 23
done in 0:08:43.446956 overall time: 3:45:52.366059 estimated time left: 1:15:17.455353
after cast to cpu
39287 MiB free out of 48676 MiB total
Layer 24
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using importances
keeping 8260 top channels and 0 bottom channels
total size:  202375168 total Megabytes:  tensor(207.9602, device='cuda:5') bits per value:  tensor(8.6201, device='cuda:5')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([8260, 4096])
quantized
weights.shape =  torch.Size([4096, 8260])
quantized
weights.shape =  torch.Size([8260, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(27.9574, device='cuda:5') bits per value:  tensor(1.1589, device='cuda:5')
Pre fine tuning: 23721 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1166168 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
----------
epoch=0
train loss=3.31e-02	
11417 MiB free out of 48676 MiB total
----------
epoch=1
train loss=2.61e-02	
11417 MiB free out of 48676 MiB total
----------
epoch=2
train loss=2.12e-02	
11417 MiB free out of 48676 MiB total
----------
epoch=3
train loss=1.78e-02	
11417 MiB free out of 48676 MiB total
----------
epoch=4
train loss=1.58e-02	
11417 MiB free out of 48676 MiB total
post_fine_tuning 11417 MiB free out of 48676 MiB total
11417 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(27.9574, device='cuda:5') bits per value:  tensor(1.1589, device='cuda:5')
11417 MiB free out of 48676 MiB total
layer: 24
done in 0:08:43.596285 overall time: 3:54:36.178301 estimated time left: 1:05:41.329924
after cast to cpu
39287 MiB free out of 48676 MiB total
Layer 25
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using importances
keeping 8260 top channels and 0 bottom channels
total size:  202375168 total Megabytes:  tensor(207.9602, device='cuda:5') bits per value:  tensor(8.6201, device='cuda:5')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([8260, 4096])
quantized
weights.shape =  torch.Size([4096, 8260])
quantized
weights.shape =  torch.Size([8260, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(27.9574, device='cuda:5') bits per value:  tensor(1.1589, device='cuda:5')
Pre fine tuning: 23721 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1166168 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
----------
epoch=0
train loss=4.73e-02	
11417 MiB free out of 48676 MiB total
----------
epoch=1
train loss=3.60e-02	
11417 MiB free out of 48676 MiB total
----------
epoch=2
train loss=2.76e-02	
11417 MiB free out of 48676 MiB total
----------
epoch=3
train loss=2.12e-02	
11417 MiB free out of 48676 MiB total
----------
epoch=4
train loss=1.72e-02	
11417 MiB free out of 48676 MiB total
post_fine_tuning 11417 MiB free out of 48676 MiB total
11417 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(27.9574, device='cuda:5') bits per value:  tensor(1.1589, device='cuda:5')
11417 MiB free out of 48676 MiB total
layer: 25
done in 0:08:43.502208 overall time: 4:03:19.894191 estimated time left: 0:56:09.206352
after cast to cpu
39287 MiB free out of 48676 MiB total
Layer 26
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using importances
keeping 8260 top channels and 0 bottom channels
total size:  202375168 total Megabytes:  tensor(207.9602, device='cuda:5') bits per value:  tensor(8.6201, device='cuda:5')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([8260, 4096])
quantized
weights.shape =  torch.Size([4096, 8260])
quantized
weights.shape =  torch.Size([8260, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(27.9574, device='cuda:5') bits per value:  tensor(1.1589, device='cuda:5')
Pre fine tuning: 23721 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1166168 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
----------
epoch=0
train loss=3.79e-02	
11417 MiB free out of 48676 MiB total
----------
epoch=1
train loss=2.81e-02	
11417 MiB free out of 48676 MiB total
----------
epoch=2
train loss=2.15e-02	
11417 MiB free out of 48676 MiB total
----------
epoch=3
train loss=1.69e-02	
11417 MiB free out of 48676 MiB total
----------
epoch=4
train loss=1.43e-02	
11417 MiB free out of 48676 MiB total
post_fine_tuning 11417 MiB free out of 48676 MiB total
11417 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(27.9574, device='cuda:5') bits per value:  tensor(1.1589, device='cuda:5')
11417 MiB free out of 48676 MiB total
layer: 26
done in 0:08:42.512055 overall time: 4:12:02.621475 estimated time left: 0:46:40.485458
after cast to cpu
39287 MiB free out of 48676 MiB total
Layer 27
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using importances
keeping 8260 top channels and 0 bottom channels
total size:  202375168 total Megabytes:  tensor(207.9602, device='cuda:5') bits per value:  tensor(8.6201, device='cuda:5')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([8260, 4096])
quantized
weights.shape =  torch.Size([4096, 8260])
quantized
weights.shape =  torch.Size([8260, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(27.9574, device='cuda:5') bits per value:  tensor(1.1589, device='cuda:5')
Pre fine tuning: 23721 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1166168 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
----------
epoch=0
train loss=3.88e-02	
11417 MiB free out of 48676 MiB total
----------
epoch=1
train loss=2.61e-02	
11417 MiB free out of 48676 MiB total
----------
epoch=2
train loss=1.86e-02	
11417 MiB free out of 48676 MiB total
----------
epoch=3
train loss=1.39e-02	
11417 MiB free out of 48676 MiB total
----------
epoch=4
train loss=1.13e-02	
11417 MiB free out of 48676 MiB total
post_fine_tuning 11417 MiB free out of 48676 MiB total
11417 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(27.9574, device='cuda:5') bits per value:  tensor(1.1589, device='cuda:5')
11417 MiB free out of 48676 MiB total
layer: 27
done in 0:08:41.189812 overall time: 4:20:44.024945 estimated time left: 0:37:14.860706
after cast to cpu
39287 MiB free out of 48676 MiB total
Layer 28
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using importances
keeping 8260 top channels and 0 bottom channels
total size:  202375168 total Megabytes:  tensor(207.9602, device='cuda:5') bits per value:  tensor(8.6201, device='cuda:5')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([8260, 4096])
quantized
weights.shape =  torch.Size([4096, 8260])
quantized
weights.shape =  torch.Size([8260, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(27.9574, device='cuda:5') bits per value:  tensor(1.1589, device='cuda:5')
Pre fine tuning: 23721 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1166168 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
----------
epoch=0
train loss=6.09e-02	
11417 MiB free out of 48676 MiB total
----------
epoch=1
train loss=4.31e-02	
11417 MiB free out of 48676 MiB total
----------
epoch=2
train loss=3.23e-02	
11417 MiB free out of 48676 MiB total
----------
epoch=3
train loss=2.49e-02	
11417 MiB free out of 48676 MiB total
----------
epoch=4
train loss=2.04e-02	
11417 MiB free out of 48676 MiB total
post_fine_tuning 11417 MiB free out of 48676 MiB total
11417 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(27.9574, device='cuda:5') bits per value:  tensor(1.1589, device='cuda:5')
11417 MiB free out of 48676 MiB total
layer: 28
done in 0:08:41.407570 overall time: 4:29:25.649124 estimated time left: 0:27:52.308530
after cast to cpu
39287 MiB free out of 48676 MiB total
Layer 29
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using importances
keeping 8260 top channels and 0 bottom channels
total size:  202375168 total Megabytes:  tensor(207.9602, device='cuda:5') bits per value:  tensor(8.6201, device='cuda:5')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([8260, 4096])
quantized
weights.shape =  torch.Size([4096, 8260])
quantized
weights.shape =  torch.Size([8260, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(27.9574, device='cuda:5') bits per value:  tensor(1.1589, device='cuda:5')
Pre fine tuning: 23721 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1166168 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
----------
epoch=0
train loss=7.42e-02	
11417 MiB free out of 48676 MiB total
----------
epoch=1
train loss=5.41e-02	
11417 MiB free out of 48676 MiB total
----------
epoch=2
train loss=4.10e-02	
11417 MiB free out of 48676 MiB total
----------
epoch=3
train loss=3.07e-02	
11417 MiB free out of 48676 MiB total
----------
epoch=4
train loss=2.42e-02	
11417 MiB free out of 48676 MiB total
post_fine_tuning 11417 MiB free out of 48676 MiB total
11417 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(27.9574, device='cuda:5') bits per value:  tensor(1.1589, device='cuda:5')
11417 MiB free out of 48676 MiB total
layer: 29
done in 0:08:42.377558 overall time: 4:38:08.241951 estimated time left: 0:18:32.549463
after cast to cpu
39287 MiB free out of 48676 MiB total
Layer 30
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using importances
keeping 8260 top channels and 0 bottom channels
total size:  202375168 total Megabytes:  tensor(207.9602, device='cuda:5') bits per value:  tensor(8.6201, device='cuda:5')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([8260, 4096])
quantized
weights.shape =  torch.Size([4096, 8260])
quantized
weights.shape =  torch.Size([8260, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(27.9574, device='cuda:5') bits per value:  tensor(1.1589, device='cuda:5')
Pre fine tuning: 23721 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1166168 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
----------
epoch=0
train loss=5.18e-01	
11417 MiB free out of 48676 MiB total
----------
epoch=1
train loss=3.92e-01	
11417 MiB free out of 48676 MiB total
----------
epoch=2
train loss=3.46e-01	
11417 MiB free out of 48676 MiB total
----------
epoch=3
train loss=3.08e-01	
11417 MiB free out of 48676 MiB total
----------
epoch=4
train loss=2.76e-01	
11417 MiB free out of 48676 MiB total
post_fine_tuning 11417 MiB free out of 48676 MiB total
11417 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(27.9574, device='cuda:5') bits per value:  tensor(1.1589, device='cuda:5')
11417 MiB free out of 48676 MiB total
layer: 30
done in 0:08:40.437114 overall time: 4:46:48.913889 estimated time left: 0:09:15.126254
after cast to cpu
39287 MiB free out of 48676 MiB total
Layer 31
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using importances
keeping 8260 top channels and 0 bottom channels
total size:  202375168 total Megabytes:  tensor(207.9602, device='cuda:5') bits per value:  tensor(8.6201, device='cuda:5')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([8260, 4096])
quantized
weights.shape =  torch.Size([4096, 8260])
quantized
weights.shape =  torch.Size([8260, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(27.9574, device='cuda:5') bits per value:  tensor(1.1589, device='cuda:5')
Pre fine tuning: 23721 MiB free out of 48676 MiB total
Found 28 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.b1', 'mlp.b3', 'mlp.b2', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1166168 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:5 torch.float32
----------
epoch=0
train loss=4.91e-01	
11417 MiB free out of 48676 MiB total
----------
epoch=1
train loss=2.94e-01	
11417 MiB free out of 48676 MiB total
----------
epoch=2
train loss=2.37e-01	
11417 MiB free out of 48676 MiB total
----------
epoch=3
train loss=2.04e-01	
11417 MiB free out of 48676 MiB total
----------
epoch=4
train loss=1.82e-01	
11417 MiB free out of 48676 MiB total
post_fine_tuning 11417 MiB free out of 48676 MiB total
11417 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(27.9574, device='cuda:5') bits per value:  tensor(1.1589, device='cuda:5')
11417 MiB free out of 48676 MiB total
layer: 31
done in 0:08:42.014734 overall time: 4:55:31.147522 estimated time left: 0:00:00
after cast to cpu
39287 MiB free out of 48676 MiB total
Total bits: tensor(15009521664, device='cuda:5') Total params: 12952010752
average bits per value: tensor(1.1589, device='cuda:5')
17732.80461382866
Loading tokenizer for meta-llama/Llama-2-7b-hf
/home/lliu/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Dataset: wikitext2
Evaluating ...
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
Perplexity: 648.938843
