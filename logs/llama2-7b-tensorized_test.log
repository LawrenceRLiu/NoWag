2024-12-05 18:04:42.505264: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-12-05 18:04:42.522046: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-12-05 18:04:42.527406: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-12-05 18:04:42.540460: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-12-05 18:04:43.569249: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  4.47it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  4.95it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  4.86it/s]
Model loaded. LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
Starting...
getting inputs:   0%|          | 0/128 [00:00<?, ?it/s]getting inputs:   1%|          | 1/128 [00:00<01:29,  1.41it/s]getting inputs:   4%|▍         | 5/128 [00:00<00:16,  7.58it/s]getting inputs:   7%|▋         | 9/128 [00:00<00:09, 13.03it/s]getting inputs:  10%|█         | 13/128 [00:01<00:06, 17.57it/s]getting inputs:  13%|█▎        | 17/128 [00:01<00:05, 21.26it/s]getting inputs:  16%|█▋        | 21/128 [00:01<00:04, 23.94it/s]getting inputs:  20%|█▉        | 25/128 [00:01<00:03, 25.81it/s]getting inputs:  22%|██▏       | 28/128 [00:01<00:03, 26.68it/s]getting inputs:  24%|██▍       | 31/128 [00:01<00:03, 27.33it/s]getting inputs:  27%|██▋       | 34/128 [00:01<00:03, 27.54it/s]getting inputs:  30%|██▉       | 38/128 [00:01<00:03, 29.10it/s]getting inputs:  32%|███▏      | 41/128 [00:02<00:02, 29.00it/s]getting inputs:  34%|███▍      | 44/128 [00:02<00:02, 28.69it/s]getting inputs:  37%|███▋      | 47/128 [00:02<00:02, 28.41it/s]getting inputs:  39%|███▉      | 50/128 [00:02<00:02, 28.30it/s]getting inputs:  42%|████▏     | 54/128 [00:02<00:02, 29.89it/s]getting inputs:  45%|████▍     | 57/128 [00:02<00:02, 29.37it/s]getting inputs:  48%|████▊     | 61/128 [00:02<00:02, 30.27it/s]getting inputs:  51%|█████     | 65/128 [00:02<00:02, 31.19it/s]getting inputs:  54%|█████▍    | 69/128 [00:02<00:01, 31.61it/s]getting inputs:  57%|█████▋    | 73/128 [00:03<00:01, 31.91it/s]getting inputs:  60%|██████    | 77/128 [00:03<00:01, 32.10it/s]getting inputs:  63%|██████▎   | 81/128 [00:03<00:01, 32.25it/s]getting inputs:  66%|██████▌   | 84/128 [00:03<00:01, 31.15it/s]getting inputs:  69%|██████▉   | 88/128 [00:03<00:01, 31.32it/s]getting inputs:  72%|███████▏  | 92/128 [00:03<00:01, 31.72it/s]getting inputs:  75%|███████▌  | 96/128 [00:03<00:01, 31.85it/s]getting inputs:  78%|███████▊  | 100/128 [00:03<00:00, 31.95it/s]getting inputs:  81%|████████▏ | 104/128 [00:04<00:00, 31.85it/s]getting inputs:  84%|████████▎ | 107/128 [00:04<00:00, 31.27it/s]getting inputs:  87%|████████▋ | 111/128 [00:04<00:00, 31.00it/s]getting inputs:  90%|████████▉ | 115/128 [00:04<00:00, 30.86it/s]getting inputs:  93%|█████████▎| 119/128 [00:04<00:00, 30.87it/s]getting inputs:  96%|█████████▌| 123/128 [00:04<00:00, 30.92it/s]getting inputs:  98%|█████████▊| 126/128 [00:04<00:00, 29.19it/s]getting inputs: 100%|██████████| 128/128 [00:04<00:00, 26.49it/s]
43112 MiB free out of 48676 MiB total
Ready.
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:29,  1.04it/s]Inference:   6%|▋         | 2/32 [00:01<00:25,  1.16it/s]Inference:   9%|▉         | 3/32 [00:02<00:24,  1.19it/s]Inference:  12%|█▎        | 4/32 [00:03<00:23,  1.21it/s]Inference:  16%|█▌        | 5/32 [00:04<00:22,  1.22it/s]Inference:  19%|█▉        | 6/32 [00:04<00:21,  1.23it/s]Inference:  22%|██▏       | 7/32 [00:05<00:20,  1.24it/s]Inference:  25%|██▌       | 8/32 [00:06<00:19,  1.25it/s]Inference:  28%|██▊       | 9/32 [00:07<00:18,  1.25it/s]Inference:  31%|███▏      | 10/32 [00:08<00:17,  1.27it/s]Inference:  34%|███▍      | 11/32 [00:08<00:16,  1.26it/s]Inference:  38%|███▊      | 12/32 [00:09<00:15,  1.26it/s]Inference:  41%|████      | 13/32 [00:10<00:15,  1.25it/s]Inference:  44%|████▍     | 14/32 [00:11<00:14,  1.25it/s]Inference:  47%|████▋     | 15/32 [00:12<00:13,  1.25it/s]Inference:  50%|█████     | 16/32 [00:12<00:12,  1.24it/s]Inference:  53%|█████▎    | 17/32 [00:13<00:12,  1.18it/s]Inference:  56%|█████▋    | 18/32 [00:14<00:12,  1.16it/s]Inference:  59%|█████▉    | 19/32 [00:15<00:10,  1.18it/s]Inference:  62%|██████▎   | 20/32 [00:16<00:10,  1.19it/s]Inference:  66%|██████▌   | 21/32 [00:17<00:09,  1.20it/s]Inference:  69%|██████▉   | 22/32 [00:18<00:08,  1.21it/s]Inference:  72%|███████▏  | 23/32 [00:18<00:07,  1.20it/s]Inference:  75%|███████▌  | 24/32 [00:19<00:06,  1.21it/s]Inference:  78%|███████▊  | 25/32 [00:20<00:05,  1.22it/s]Inference:  81%|████████▏ | 26/32 [00:21<00:04,  1.22it/s]Inference:  84%|████████▍ | 27/32 [00:22<00:04,  1.18it/s]Inference:  88%|████████▊ | 28/32 [00:23<00:03,  1.19it/s]Inference:  91%|█████████ | 29/32 [00:23<00:02,  1.20it/s]Inference:  94%|█████████▍| 30/32 [00:24<00:01,  1.21it/s]Inference:  97%|█████████▋| 31/32 [00:25<00:00,  1.22it/s]Inference: 100%|██████████| 32/32 [00:26<00:00,  1.22it/s]Inference: 100%|██████████| 32/32 [00:26<00:00,  1.22it/s]
layer0: self_attn.q_proj
iter 0, train loss 21638.42578125, val loss None, lr 0.01
iter 250, train loss 50.91909408569336, val loss None, lr 0.01
iter 500, train loss 44.44975280761719, val loss None, lr 0.01
iter 750, train loss 33.35799789428711, val loss None, lr 0.003333
iter 1000, train loss 25.7224063873291, val loss None, lr 0.001111
iter 1250, train loss 24.117931365966797, val loss None, lr 0.001111
iter 1500, train loss 23.270755767822266, val loss None, lr 0.001111
iter 1750, train loss 22.398818969726562, val loss None, lr 0.001111
iter 2000, train loss 21.809982299804688, val loss None, lr 0.00037
iter 2250, train loss 21.29465675354004, val loss None, lr 0.00037
best loss 20.965177536010742
layer0: self_attn.k_proj
iter 0, train loss 13561.328125, val loss None, lr 0.01
iter 250, train loss 58.00823974609375, val loss None, lr 0.01
iter 500, train loss 46.42461395263672, val loss None, lr 0.003333
iter 750, train loss 42.32351303100586, val loss None, lr 0.003333
iter 1000, train loss 39.725852966308594, val loss None, lr 0.001111
iter 1250, train loss 37.75968933105469, val loss None, lr 0.001111
iter 1500, train loss 36.87096405029297, val loss None, lr 0.001111
iter 1750, train loss 36.16233825683594, val loss None, lr 0.001111
iter 2000, train loss 35.78289031982422, val loss None, lr 0.001111
iter 2250, train loss 34.90768051147461, val loss None, lr 0.00037
best loss 34.518733978271484
layer0: self_attn.v_proj
iter 0, train loss 33.00835418701172, val loss None, lr 0.01
iter 250, train loss 2.549180507659912, val loss None, lr 0.01
iter 500, train loss 2.399240732192993, val loss None, lr 0.01
iter 750, train loss 2.383100748062134, val loss None, lr 0.01
iter 1000, train loss 2.340376138687134, val loss None, lr 0.003333
iter 1250, train loss 2.333765983581543, val loss None, lr 0.003333
iter 1500, train loss 2.3079659938812256, val loss None, lr 0.001111
iter 1750, train loss 2.301856279373169, val loss None, lr 0.001111
iter 2000, train loss 2.297268867492676, val loss None, lr 0.001111
iter 2250, train loss 2.294407844543457, val loss None, lr 0.001111
best loss 2.289778232574463
layer0: self_attn.o_proj
iter 0, train loss 7.724525451660156, val loss None, lr 0.01
iter 250, train loss 0.21658146381378174, val loss None, lr 0.01
iter 500, train loss 0.20091411471366882, val loss None, lr 0.01
iter 750, train loss 0.18394158780574799, val loss None, lr 0.003333
iter 1000, train loss 0.18145063519477844, val loss None, lr 0.001111
iter 1250, train loss 0.17999912798404694, val loss None, lr 0.001111
iter 1500, train loss 0.17863747477531433, val loss None, lr 0.001111
iter 1750, train loss 0.17737403512001038, val loss None, lr 0.001111
iter 2000, train loss 0.17640210688114166, val loss None, lr 0.001111
iter 2250, train loss 0.1755029261112213, val loss None, lr 0.001111
best loss 0.17457014322280884
layer0: mlp.gate_proj
iter 0, train loss 502.828369140625, val loss None, lr 0.01
iter 250, train loss 177.98361206054688, val loss None, lr 0.01
iter 500, train loss 172.11248779296875, val loss None, lr 0.01
iter 750, train loss 169.28787231445312, val loss None, lr 0.01
iter 1000, train loss 166.94615173339844, val loss None, lr 0.01
iter 1250, train loss 166.62347412109375, val loss None, lr 0.01
iter 1500, train loss 164.98828125, val loss None, lr 0.01
iter 1750, train loss 166.1776580810547, val loss None, lr 0.01
iter 2000, train loss 163.6714630126953, val loss None, lr 0.003333
iter 2250, train loss 162.83216857910156, val loss None, lr 0.003333
best loss 162.25245666503906
layer0: mlp.up_proj
iter 0, train loss 341.34027099609375, val loss None, lr 0.01
iter 250, train loss 166.2803955078125, val loss None, lr 0.01
iter 500, train loss 162.88926696777344, val loss None, lr 0.01
iter 750, train loss 160.31802368164062, val loss None, lr 0.01
iter 1000, train loss 159.33897399902344, val loss None, lr 0.01
iter 1250, train loss 158.4512939453125, val loss None, lr 0.01
iter 1500, train loss 157.27011108398438, val loss None, lr 0.01
iter 1750, train loss 156.23477172851562, val loss None, lr 0.003333
iter 2000, train loss 155.45899963378906, val loss None, lr 0.003333
iter 2250, train loss 155.34896850585938, val loss None, lr 0.001111
best loss 154.76016235351562
layer0: mlp.down_proj
iter 0, train loss 1.9190449714660645, val loss None, lr 0.01
iter 250, train loss 0.7181807160377502, val loss None, lr 0.01
iter 500, train loss 0.7108822464942932, val loss None, lr 0.01
iter 750, train loss 0.7067725658416748, val loss None, lr 0.01
iter 1000, train loss 0.7043942213058472, val loss None, lr 0.01
iter 1250, train loss 0.701360285282135, val loss None, lr 0.003333
iter 1500, train loss 0.7003548741340637, val loss None, lr 0.003333
iter 1750, train loss 0.6999793648719788, val loss None, lr 0.003333
iter 2000, train loss 0.7006363868713379, val loss None, lr 0.003333
iter 2250, train loss 0.6988788843154907, val loss None, lr 0.001111
best loss 0.6986587047576904
43112 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:09,  3.13it/s]Inference:   6%|▋         | 2/32 [00:00<00:10,  2.95it/s]Inference:   9%|▉         | 3/32 [00:01<00:10,  2.85it/s]Inference:  12%|█▎        | 4/32 [00:01<00:09,  2.84it/s]Inference:  16%|█▌        | 5/32 [00:01<00:09,  2.83it/s]Inference:  19%|█▉        | 6/32 [00:02<00:09,  2.83it/s]Inference:  22%|██▏       | 7/32 [00:02<00:08,  2.81it/s]Inference:  25%|██▌       | 8/32 [00:02<00:08,  2.80it/s]Inference:  28%|██▊       | 9/32 [00:03<00:08,  2.79it/s]Inference:  31%|███▏      | 10/32 [00:03<00:07,  2.81it/s]Inference:  34%|███▍      | 11/32 [00:03<00:07,  2.80it/s]Inference:  38%|███▊      | 12/32 [00:04<00:07,  2.81it/s]Inference:  41%|████      | 13/32 [00:04<00:06,  2.80it/s]Inference:  44%|████▍     | 14/32 [00:04<00:06,  2.79it/s]Inference:  47%|████▋     | 15/32 [00:05<00:06,  2.77it/s]Inference:  50%|█████     | 16/32 [00:05<00:05,  2.81it/s]Inference:  53%|█████▎    | 17/32 [00:06<00:05,  2.84it/s]Inference:  56%|█████▋    | 18/32 [00:06<00:04,  2.85it/s]Inference:  59%|█████▉    | 19/32 [00:06<00:04,  2.84it/s]Inference:  62%|██████▎   | 20/32 [00:07<00:04,  2.85it/s]Inference:  66%|██████▌   | 21/32 [00:07<00:03,  2.84it/s]Inference:  69%|██████▉   | 22/32 [00:07<00:03,  2.87it/s]Inference:  72%|███████▏  | 23/32 [00:08<00:03,  2.89it/s]Inference:  75%|███████▌  | 24/32 [00:08<00:02,  2.88it/s]Inference:  78%|███████▊  | 25/32 [00:08<00:02,  2.87it/s]Inference:  81%|████████▏ | 26/32 [00:09<00:02,  2.86it/s]Inference:  84%|████████▍ | 27/32 [00:09<00:01,  2.86it/s]Inference:  88%|████████▊ | 28/32 [00:09<00:01,  2.97it/s]Inference:  91%|█████████ | 29/32 [00:10<00:01,  2.95it/s]Inference:  94%|█████████▍| 30/32 [00:10<00:00,  2.94it/s]Inference:  97%|█████████▋| 31/32 [00:10<00:00,  2.90it/s]Inference: 100%|██████████| 32/32 [00:11<00:00,  2.90it/s]Inference: 100%|██████████| 32/32 [00:11<00:00,  2.86it/s]
37236 MiB free out of 48676 MiB total
Saved layer 0 to ./models/meta-llama/Llama-2-7b-hf/tensorized/128/no_finetune2/layer_0.pt
after cast to cpu
41230 MiB free out of 48676 MiB total
Done with layer 0 total_time elapsed: 1445 estimated time left: 44782
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:26,  1.17it/s]Inference:   6%|▋         | 2/32 [00:01<00:25,  1.19it/s]Inference:   9%|▉         | 3/32 [00:02<00:24,  1.20it/s]Inference:  12%|█▎        | 4/32 [00:03<00:23,  1.19it/s]Inference:  16%|█▌        | 5/32 [00:04<00:22,  1.19it/s]Inference:  19%|█▉        | 6/32 [00:05<00:21,  1.19it/s]Inference:  22%|██▏       | 7/32 [00:05<00:20,  1.19it/s]Inference:  25%|██▌       | 8/32 [00:06<00:20,  1.20it/s]Inference:  28%|██▊       | 9/32 [00:07<00:19,  1.19it/s]Inference:  31%|███▏      | 10/32 [00:08<00:18,  1.19it/s]Inference:  34%|███▍      | 11/32 [00:09<00:17,  1.20it/s]Inference:  38%|███▊      | 12/32 [00:10<00:16,  1.20it/s]Inference:  41%|████      | 13/32 [00:10<00:15,  1.20it/s]Inference:  44%|████▍     | 14/32 [00:11<00:15,  1.20it/s]Inference:  47%|████▋     | 15/32 [00:12<00:14,  1.20it/s]Inference:  50%|█████     | 16/32 [00:13<00:13,  1.20it/s]Inference:  53%|█████▎    | 17/32 [00:14<00:12,  1.20it/s]Inference:  56%|█████▋    | 18/32 [00:15<00:11,  1.20it/s]Inference:  59%|█████▉    | 19/32 [00:15<00:10,  1.20it/s]Inference:  62%|██████▎   | 20/32 [00:16<00:10,  1.20it/s]Inference:  66%|██████▌   | 21/32 [00:17<00:09,  1.20it/s]Inference:  69%|██████▉   | 22/32 [00:18<00:08,  1.20it/s]Inference:  72%|███████▏  | 23/32 [00:19<00:07,  1.20it/s]Inference:  75%|███████▌  | 24/32 [00:20<00:06,  1.20it/s]Inference:  78%|███████▊  | 25/32 [00:20<00:05,  1.20it/s]Inference:  81%|████████▏ | 26/32 [00:21<00:05,  1.20it/s]Inference:  84%|████████▍ | 27/32 [00:22<00:04,  1.20it/s]Inference:  88%|████████▊ | 28/32 [00:23<00:03,  1.20it/s]Inference:  91%|█████████ | 29/32 [00:24<00:02,  1.20it/s]Inference:  94%|█████████▍| 30/32 [00:25<00:01,  1.20it/s]Inference:  97%|█████████▋| 31/32 [00:25<00:00,  1.19it/s]Inference: 100%|██████████| 32/32 [00:26<00:00,  1.20it/s]Inference: 100%|██████████| 32/32 [00:26<00:00,  1.20it/s]
layer1: self_attn.q_proj
iter 0, train loss 47152.46875, val loss None, lr 0.01
iter 250, train loss 773.15185546875, val loss None, lr 0.01
iter 500, train loss 640.0050048828125, val loss None, lr 0.01
iter 750, train loss 630.357421875, val loss None, lr 0.01
iter 1000, train loss 543.1507568359375, val loss None, lr 0.003333
iter 1250, train loss 542.83984375, val loss None, lr 0.003333
iter 1500, train loss 516.594970703125, val loss None, lr 0.001111
iter 1750, train loss 509.9415588378906, val loss None, lr 0.00037
iter 2000, train loss 505.9959716796875, val loss None, lr 0.00037
iter 2250, train loss 503.5263671875, val loss None, lr 0.00037
best loss 501.17681884765625
layer1: self_attn.k_proj
iter 0, train loss 49204.15625, val loss None, lr 0.01
iter 250, train loss 732.4271240234375, val loss None, lr 0.01
iter 500, train loss 684.8289184570312, val loss None, lr 0.01
iter 750, train loss 603.2945556640625, val loss None, lr 0.003333
iter 1000, train loss 594.2141723632812, val loss None, lr 0.003333
iter 1250, train loss 580.9993896484375, val loss None, lr 0.003333
iter 1500, train loss 555.5601806640625, val loss None, lr 0.001111
iter 1750, train loss 551.1119995117188, val loss None, lr 0.001111
iter 2000, train loss 546.640869140625, val loss None, lr 0.001111
iter 2250, train loss 540.1027221679688, val loss None, lr 0.00037
best loss 537.54150390625
layer1: self_attn.v_proj
iter 0, train loss 119.34864044189453, val loss None, lr 0.01
iter 250, train loss 25.876760482788086, val loss None, lr 0.01
iter 500, train loss 25.36777114868164, val loss None, lr 0.01
iter 750, train loss 25.239768981933594, val loss None, lr 0.01
iter 1000, train loss 25.047269821166992, val loss None, lr 0.003333
iter 1250, train loss 24.976696014404297, val loss None, lr 0.003333
iter 1500, train loss 24.854564666748047, val loss None, lr 0.001111
iter 1750, train loss 24.899524688720703, val loss None, lr 0.001111
iter 2000, train loss 24.797271728515625, val loss None, lr 0.001111
iter 2250, train loss 24.776527404785156, val loss None, lr 0.001111
best loss 24.759544372558594
layer1: self_attn.o_proj
iter 0, train loss 12.747848510742188, val loss None, lr 0.01
iter 250, train loss 2.7830114364624023, val loss None, lr 0.01
iter 500, train loss 2.735700845718384, val loss None, lr 0.01
iter 750, train loss 2.6913390159606934, val loss None, lr 0.01
iter 1000, train loss 2.6781325340270996, val loss None, lr 0.003333
iter 1250, train loss 2.6706299781799316, val loss None, lr 0.003333
iter 1500, train loss 2.6614022254943848, val loss None, lr 0.001111
iter 1750, train loss 2.6590752601623535, val loss None, lr 0.001111
iter 2000, train loss 2.6570963859558105, val loss None, lr 0.001111
iter 2250, train loss 2.655043125152588, val loss None, lr 0.001111
best loss 2.6536545753479004
layer1: mlp.gate_proj
iter 0, train loss 1859.539306640625, val loss None, lr 0.01
iter 250, train loss 784.565673828125, val loss None, lr 0.01
iter 500, train loss 748.2094116210938, val loss None, lr 0.01
iter 750, train loss 738.7184448242188, val loss None, lr 0.01
iter 1000, train loss 726.5361328125, val loss None, lr 0.01
iter 1250, train loss 722.5480346679688, val loss None, lr 0.01
iter 1500, train loss 727.9984130859375, val loss None, lr 0.01
iter 1750, train loss 716.00439453125, val loss None, lr 0.003333
iter 2000, train loss 713.4901733398438, val loss None, lr 0.001111
iter 2250, train loss 712.5068359375, val loss None, lr 0.001111
best loss 711.9230346679688
layer1: mlp.up_proj
iter 0, train loss 848.5439453125, val loss None, lr 0.01
iter 250, train loss 573.25341796875, val loss None, lr 0.01
iter 500, train loss 564.691162109375, val loss None, lr 0.01
iter 750, train loss 558.376220703125, val loss None, lr 0.01
iter 1000, train loss 554.4348754882812, val loss None, lr 0.003333
iter 1250, train loss 553.2200317382812, val loss None, lr 0.003333
iter 1500, train loss 551.9647827148438, val loss None, lr 0.003333
iter 1750, train loss 551.0964965820312, val loss None, lr 0.003333
iter 2000, train loss 549.6918334960938, val loss None, lr 0.001111
iter 2250, train loss 549.0928955078125, val loss None, lr 0.001111
best loss 548.58642578125
layer1: mlp.down_proj
iter 0, train loss 273.317138671875, val loss None, lr 0.01
iter 250, train loss 3.142930746078491, val loss None, lr 0.01
iter 500, train loss 2.997877597808838, val loss None, lr 0.01
iter 750, train loss 2.948408365249634, val loss None, lr 0.01
iter 1000, train loss 2.8701601028442383, val loss None, lr 0.003333
iter 1250, train loss 2.8451895713806152, val loss None, lr 0.003333
iter 1500, train loss 2.842041015625, val loss None, lr 0.003333
iter 1750, train loss 2.8369193077087402, val loss None, lr 0.003333
iter 2000, train loss 2.8259005546569824, val loss None, lr 0.001111
iter 2250, train loss 2.823418140411377, val loss None, lr 0.00037
best loss 2.821324348449707
41230 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:09,  3.14it/s]Inference:   6%|▋         | 2/32 [00:00<00:10,  2.95it/s]Inference:   9%|▉         | 3/32 [00:01<00:10,  2.87it/s]Inference:  12%|█▎        | 4/32 [00:01<00:09,  2.87it/s]Inference:  16%|█▌        | 5/32 [00:01<00:09,  2.84it/s]Inference:  19%|█▉        | 6/32 [00:02<00:09,  2.82it/s]Inference:  22%|██▏       | 7/32 [00:02<00:08,  2.80it/s]Inference:  25%|██▌       | 8/32 [00:02<00:08,  2.80it/s]Inference:  28%|██▊       | 9/32 [00:03<00:08,  2.80it/s]Inference:  31%|███▏      | 10/32 [00:03<00:07,  2.80it/s]Inference:  34%|███▍      | 11/32 [00:03<00:07,  2.81it/s]Inference:  38%|███▊      | 12/32 [00:04<00:07,  2.80it/s]Inference:  41%|████      | 13/32 [00:04<00:06,  2.80it/s]Inference:  44%|████▍     | 14/32 [00:04<00:06,  2.78it/s]Inference:  47%|████▋     | 15/32 [00:05<00:06,  2.78it/s]Inference:  50%|█████     | 16/32 [00:05<00:05,  2.80it/s]Inference:  53%|█████▎    | 17/32 [00:06<00:05,  2.80it/s]Inference:  56%|█████▋    | 18/32 [00:06<00:05,  2.79it/s]Inference:  59%|█████▉    | 19/32 [00:06<00:04,  2.79it/s]Inference:  62%|██████▎   | 20/32 [00:07<00:04,  2.80it/s]Inference:  66%|██████▌   | 21/32 [00:07<00:03,  2.80it/s]Inference:  69%|██████▉   | 22/32 [00:07<00:03,  2.80it/s]Inference:  72%|███████▏  | 23/32 [00:08<00:03,  2.80it/s]Inference:  75%|███████▌  | 24/32 [00:08<00:02,  2.80it/s]Inference:  78%|███████▊  | 25/32 [00:08<00:02,  2.80it/s]Inference:  81%|████████▏ | 26/32 [00:09<00:02,  2.80it/s]Inference:  84%|████████▍ | 27/32 [00:09<00:01,  2.78it/s]Inference:  88%|████████▊ | 28/32 [00:09<00:01,  2.80it/s]Inference:  91%|█████████ | 29/32 [00:10<00:01,  2.80it/s]Inference:  94%|█████████▍| 30/32 [00:10<00:00,  2.81it/s]Inference:  97%|█████████▋| 31/32 [00:11<00:00,  2.80it/s]Inference: 100%|██████████| 32/32 [00:11<00:00,  2.78it/s]Inference: 100%|██████████| 32/32 [00:11<00:00,  2.81it/s]
35926 MiB free out of 48676 MiB total
Saved layer 1 to ./models/meta-llama/Llama-2-7b-hf/tensorized/128/no_finetune2/layer_1.pt
after cast to cpu
39942 MiB free out of 48676 MiB total
Done with layer 1 total_time elapsed: 2881 estimated time left: 43221
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:23,  1.30it/s]Inference:   6%|▋         | 2/32 [00:01<00:23,  1.30it/s]Inference:   9%|▉         | 3/32 [00:02<00:22,  1.29it/s]Inference:  12%|█▎        | 4/32 [00:03<00:21,  1.29it/s]Inference:  16%|█▌        | 5/32 [00:03<00:20,  1.29it/s]Inference:  19%|█▉        | 6/32 [00:04<00:20,  1.28it/s]Inference:  22%|██▏       | 7/32 [00:05<00:19,  1.28it/s]Inference:  25%|██▌       | 8/32 [00:06<00:18,  1.29it/s]Inference:  28%|██▊       | 9/32 [00:06<00:17,  1.28it/s]Inference:  31%|███▏      | 10/32 [00:07<00:17,  1.28it/s]Inference:  34%|███▍      | 11/32 [00:08<00:16,  1.28it/s]Inference:  38%|███▊      | 12/32 [00:09<00:15,  1.28it/s]Inference:  41%|████      | 13/32 [00:10<00:14,  1.28it/s]Inference:  44%|████▍     | 14/32 [00:10<00:14,  1.28it/s]Inference:  47%|████▋     | 15/32 [00:11<00:13,  1.28it/s]Inference:  50%|█████     | 16/32 [00:12<00:12,  1.28it/s]Inference:  53%|█████▎    | 17/32 [00:13<00:11,  1.29it/s]Inference:  56%|█████▋    | 18/32 [00:13<00:10,  1.30it/s]Inference:  59%|█████▉    | 19/32 [00:14<00:10,  1.30it/s]Inference:  62%|██████▎   | 20/32 [00:15<00:09,  1.29it/s]Inference:  66%|██████▌   | 21/32 [00:16<00:08,  1.29it/s]Inference:  69%|██████▉   | 22/32 [00:17<00:07,  1.28it/s]Inference:  72%|███████▏  | 23/32 [00:17<00:07,  1.28it/s]Inference:  75%|███████▌  | 24/32 [00:18<00:06,  1.28it/s]Inference:  78%|███████▊  | 25/32 [00:19<00:05,  1.28it/s]Inference:  81%|████████▏ | 26/32 [00:20<00:04,  1.28it/s]Inference:  84%|████████▍ | 27/32 [00:21<00:03,  1.28it/s]Inference:  88%|████████▊ | 28/32 [00:21<00:03,  1.28it/s]Inference:  91%|█████████ | 29/32 [00:22<00:02,  1.30it/s]Inference:  94%|█████████▍| 30/32 [00:23<00:01,  1.29it/s]Inference:  97%|█████████▋| 31/32 [00:24<00:00,  1.28it/s]Inference: 100%|██████████| 32/32 [00:24<00:00,  1.29it/s]Inference: 100%|██████████| 32/32 [00:24<00:00,  1.29it/s]
layer2: self_attn.q_proj
iter 0, train loss 110270.671875, val loss None, lr 0.01
iter 250, train loss 1581.9017333984375, val loss None, lr 0.01
iter 500, train loss 1370.390625, val loss None, lr 0.003333
iter 750, train loss 1318.358154296875, val loss None, lr 0.003333
iter 1000, train loss 1376.0050048828125, val loss None, lr 0.003333
iter 1250, train loss 1262.58642578125, val loss None, lr 0.001111
iter 1500, train loss 1248.2596435546875, val loss None, lr 0.00037
iter 1750, train loss 1239.5419921875, val loss None, lr 0.00037
iter 2000, train loss 1232.9931640625, val loss None, lr 0.00037
iter 2250, train loss 1227.8499755859375, val loss None, lr 0.00037
best loss 1223.207763671875
layer2: self_attn.k_proj
iter 0, train loss 146187.59375, val loss None, lr 0.01
iter 250, train loss 1742.125, val loss None, lr 0.01
iter 500, train loss 1551.98681640625, val loss None, lr 0.01
iter 750, train loss 1585.9952392578125, val loss None, lr 0.01
iter 1000, train loss 1437.6075439453125, val loss None, lr 0.003333
iter 1250, train loss 1394.170166015625, val loss None, lr 0.003333
iter 1500, train loss 1358.3955078125, val loss None, lr 0.001111
iter 1750, train loss 1345.274658203125, val loss None, lr 0.001111
iter 2000, train loss 1336.00732421875, val loss None, lr 0.00037
iter 2250, train loss 1329.79248046875, val loss None, lr 0.00037
best loss 1324.5084228515625
layer2: self_attn.v_proj
iter 0, train loss 720.111083984375, val loss None, lr 0.01
iter 250, train loss 239.99607849121094, val loss None, lr 0.01
iter 500, train loss 236.40440368652344, val loss None, lr 0.01
iter 750, train loss 234.77825927734375, val loss None, lr 0.01
iter 1000, train loss 233.59152221679688, val loss None, lr 0.01
iter 1250, train loss 233.2535400390625, val loss None, lr 0.01
iter 1500, train loss 231.7009735107422, val loss None, lr 0.003333
iter 1750, train loss 230.9300537109375, val loss None, lr 0.001111
iter 2000, train loss 230.71923828125, val loss None, lr 0.001111
iter 2250, train loss 230.52435302734375, val loss None, lr 0.00037
best loss 230.40223693847656
layer2: self_attn.o_proj
iter 0, train loss 251.55056762695312, val loss None, lr 0.01
iter 250, train loss 60.08263397216797, val loss None, lr 0.01
iter 500, train loss 58.92577362060547, val loss None, lr 0.01
iter 750, train loss 59.211055755615234, val loss None, lr 0.01
iter 1000, train loss 57.706687927246094, val loss None, lr 0.01
iter 1250, train loss 57.118927001953125, val loss None, lr 0.003333
iter 1500, train loss 56.888702392578125, val loss None, lr 0.001111
iter 1750, train loss 56.80482482910156, val loss None, lr 0.001111
iter 2000, train loss 56.765838623046875, val loss None, lr 0.001111
iter 2250, train loss 56.7133903503418, val loss None, lr 0.001111
best loss 56.673606872558594
layer2: mlp.gate_proj
iter 0, train loss 2633.478515625, val loss None, lr 0.01
iter 250, train loss 1149.83154296875, val loss None, lr 0.01
iter 500, train loss 1126.43017578125, val loss None, lr 0.01
iter 750, train loss 1114.441650390625, val loss None, lr 0.01
iter 1000, train loss 1111.8350830078125, val loss None, lr 0.01
iter 1250, train loss 1104.0555419921875, val loss None, lr 0.01
iter 1500, train loss 1099.10546875, val loss None, lr 0.01
iter 1750, train loss 1095.037353515625, val loss None, lr 0.01
iter 2000, train loss 1094.122314453125, val loss None, lr 0.01
iter 2250, train loss 1088.4034423828125, val loss None, lr 0.003333
best loss 1083.7021484375
layer2: mlp.up_proj
iter 0, train loss 1460.4124755859375, val loss None, lr 0.01
iter 250, train loss 738.7559814453125, val loss None, lr 0.01
iter 500, train loss 728.1185913085938, val loss None, lr 0.01
iter 750, train loss 723.6760864257812, val loss None, lr 0.01
iter 1000, train loss 721.4520263671875, val loss None, lr 0.01
iter 1250, train loss 719.8674926757812, val loss None, lr 0.01
iter 1500, train loss 718.1294555664062, val loss None, lr 0.01
iter 1750, train loss 716.9800415039062, val loss None, lr 0.01
iter 2000, train loss 715.6466064453125, val loss None, lr 0.003333
iter 2250, train loss 714.7176513671875, val loss None, lr 0.003333
best loss 713.7133178710938
layer2: mlp.down_proj
iter 0, train loss 12.00086784362793, val loss None, lr 0.01
iter 250, train loss 3.516124725341797, val loss None, lr 0.01
iter 500, train loss 3.48388671875, val loss None, lr 0.01
iter 750, train loss 3.439760684967041, val loss None, lr 0.01
iter 1000, train loss 3.395901679992676, val loss None, lr 0.003333
iter 1250, train loss 3.3858890533447266, val loss None, lr 0.001111
iter 1500, train loss 3.3806118965148926, val loss None, lr 0.001111
iter 1750, train loss 3.3766565322875977, val loss None, lr 0.00037
iter 2000, train loss 3.3748791217803955, val loss None, lr 0.00037
iter 2250, train loss 3.3731043338775635, val loss None, lr 0.00037
best loss 3.371363401412964
39942 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:11,  2.65it/s]Inference:   6%|▋         | 2/32 [00:00<00:11,  2.61it/s]Inference:   9%|▉         | 3/32 [00:01<00:11,  2.59it/s]Inference:  12%|█▎        | 4/32 [00:01<00:10,  2.58it/s]Inference:  16%|█▌        | 5/32 [00:01<00:10,  2.58it/s]Inference:  19%|█▉        | 6/32 [00:02<00:10,  2.58it/s]Inference:  22%|██▏       | 7/32 [00:02<00:09,  2.58it/s]Inference:  25%|██▌       | 8/32 [00:03<00:09,  2.59it/s]Inference:  28%|██▊       | 9/32 [00:03<00:08,  2.60it/s]Inference:  31%|███▏      | 10/32 [00:03<00:08,  2.59it/s]Inference:  34%|███▍      | 11/32 [00:04<00:08,  2.60it/s]Inference:  38%|███▊      | 12/32 [00:04<00:07,  2.58it/s]Inference:  41%|████      | 13/32 [00:05<00:07,  2.58it/s]Inference:  44%|████▍     | 14/32 [00:05<00:06,  2.59it/s]Inference:  47%|████▋     | 15/32 [00:05<00:06,  2.59it/s]Inference:  50%|█████     | 16/32 [00:06<00:06,  2.59it/s]Inference:  53%|█████▎    | 17/32 [00:06<00:05,  2.60it/s]Inference:  56%|█████▋    | 18/32 [00:06<00:05,  2.59it/s]Inference:  59%|█████▉    | 19/32 [00:07<00:04,  2.60it/s]Inference:  62%|██████▎   | 20/32 [00:07<00:04,  2.59it/s]Inference:  66%|██████▌   | 21/32 [00:08<00:04,  2.59it/s]Inference:  69%|██████▉   | 22/32 [00:08<00:03,  2.59it/s]Inference:  72%|███████▏  | 23/32 [00:08<00:03,  2.59it/s]Inference:  75%|███████▌  | 24/32 [00:09<00:03,  2.59it/s]Inference:  78%|███████▊  | 25/32 [00:09<00:02,  2.59it/s]Inference:  81%|████████▏ | 26/32 [00:10<00:02,  2.57it/s]Inference:  84%|████████▍ | 27/32 [00:10<00:01,  2.59it/s]Inference:  88%|████████▊ | 28/32 [00:10<00:01,  2.59it/s]Inference:  91%|█████████ | 29/32 [00:11<00:01,  2.59it/s]Inference:  94%|█████████▍| 30/32 [00:11<00:00,  2.60it/s]Inference:  97%|█████████▋| 31/32 [00:11<00:00,  2.61it/s]Inference: 100%|██████████| 32/32 [00:12<00:00,  2.61it/s]Inference: 100%|██████████| 32/32 [00:12<00:00,  2.59it/s]
34638 MiB free out of 48676 MiB total
Saved layer 2 to ./models/meta-llama/Llama-2-7b-hf/tensorized/128/no_finetune2/layer_2.pt
after cast to cpu
38654 MiB free out of 48676 MiB total
Done with layer 2 total_time elapsed: 4322 estimated time left: 41778
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:26,  1.17it/s]Inference:   6%|▋         | 2/32 [00:01<00:25,  1.17it/s]Inference:   9%|▉         | 3/32 [00:02<00:24,  1.18it/s]Inference:  12%|█▎        | 4/32 [00:03<00:23,  1.18it/s]Inference:  16%|█▌        | 5/32 [00:04<00:22,  1.18it/s]Inference:  19%|█▉        | 6/32 [00:05<00:21,  1.18it/s]Inference:  22%|██▏       | 7/32 [00:05<00:21,  1.18it/s]Inference:  25%|██▌       | 8/32 [00:06<00:20,  1.19it/s]Inference:  28%|██▊       | 9/32 [00:07<00:19,  1.19it/s]Inference:  31%|███▏      | 10/32 [00:08<00:18,  1.19it/s]Inference:  34%|███▍      | 11/32 [00:09<00:17,  1.19it/s]Inference:  38%|███▊      | 12/32 [00:10<00:16,  1.19it/s]Inference:  41%|████      | 13/32 [00:10<00:16,  1.19it/s]Inference:  44%|████▍     | 14/32 [00:11<00:15,  1.19it/s]Inference:  47%|████▋     | 15/32 [00:12<00:14,  1.19it/s]Inference:  50%|█████     | 16/32 [00:13<00:13,  1.18it/s]Inference:  53%|█████▎    | 17/32 [00:14<00:12,  1.18it/s]Inference:  56%|█████▋    | 18/32 [00:15<00:11,  1.19it/s]Inference:  59%|█████▉    | 19/32 [00:16<00:10,  1.19it/s]Inference:  62%|██████▎   | 20/32 [00:16<00:10,  1.19it/s]Inference:  66%|██████▌   | 21/32 [00:17<00:09,  1.19it/s]Inference:  69%|██████▉   | 22/32 [00:18<00:08,  1.20it/s]Inference:  72%|███████▏  | 23/32 [00:19<00:07,  1.20it/s]Inference:  75%|███████▌  | 24/32 [00:20<00:06,  1.20it/s]Inference:  78%|███████▊  | 25/32 [00:21<00:05,  1.19it/s]Inference:  81%|████████▏ | 26/32 [00:21<00:05,  1.19it/s]Inference:  84%|████████▍ | 27/32 [00:22<00:04,  1.19it/s]Inference:  88%|████████▊ | 28/32 [00:23<00:03,  1.19it/s]Inference:  91%|█████████ | 29/32 [00:24<00:02,  1.19it/s]Inference:  94%|█████████▍| 30/32 [00:25<00:01,  1.19it/s]Inference:  97%|█████████▋| 31/32 [00:26<00:00,  1.19it/s]Inference: 100%|██████████| 32/32 [00:26<00:00,  1.19it/s]Inference: 100%|██████████| 32/32 [00:26<00:00,  1.19it/s]
layer3: self_attn.q_proj
iter 0, train loss 78823.6875, val loss None, lr 0.01
iter 250, train loss 2191.488525390625, val loss None, lr 0.01
iter 500, train loss 2025.719970703125, val loss None, lr 0.01
iter 750, train loss 1939.1527099609375, val loss None, lr 0.01
iter 1000, train loss 1934.99169921875, val loss None, lr 0.01
iter 1250, train loss 1852.681396484375, val loss None, lr 0.003333
iter 1500, train loss 1819.99365234375, val loss None, lr 0.003333
iter 1750, train loss 1796.5638427734375, val loss None, lr 0.001111
iter 2000, train loss 1789.489501953125, val loss None, lr 0.001111
iter 2250, train loss 1783.5445556640625, val loss None, lr 0.001111
best loss 1775.190673828125
layer3: self_attn.k_proj
iter 0, train loss 94920.296875, val loss None, lr 0.01
iter 250, train loss 2219.50146484375, val loss None, lr 0.01
iter 500, train loss 2178.12744140625, val loss None, lr 0.01
iter 750, train loss 1938.8466796875, val loss None, lr 0.01
iter 1000, train loss 1950.522705078125, val loss None, lr 0.01
iter 1250, train loss 1922.0654296875, val loss None, lr 0.01
iter 1500, train loss 1823.5206298828125, val loss None, lr 0.003333
iter 1750, train loss 1787.9814453125, val loss None, lr 0.001111
iter 2000, train loss 1780.4534912109375, val loss None, lr 0.001111
iter 2250, train loss 1768.999267578125, val loss None, lr 0.00037
best loss 1763.716552734375
layer3: self_attn.v_proj
iter 0, train loss 1725.40478515625, val loss None, lr 0.01
iter 250, train loss 366.1335754394531, val loss None, lr 0.01
iter 500, train loss 358.39544677734375, val loss None, lr 0.01
iter 750, train loss 352.9621276855469, val loss None, lr 0.01
iter 1000, train loss 349.40472412109375, val loss None, lr 0.01
iter 1250, train loss 347.34136962890625, val loss None, lr 0.01
iter 1500, train loss 347.6097412109375, val loss None, lr 0.01
iter 1750, train loss 344.0452880859375, val loss None, lr 0.003333
iter 2000, train loss 342.9756164550781, val loss None, lr 0.001111
iter 2250, train loss 342.61492919921875, val loss None, lr 0.001111
best loss 342.33526611328125
layer3: self_attn.o_proj
iter 0, train loss 477.6779479980469, val loss None, lr 0.01
iter 250, train loss 15.761824607849121, val loss None, lr 0.01
iter 500, train loss 13.06209659576416, val loss None, lr 0.01
iter 750, train loss 13.212011337280273, val loss None, lr 0.01
iter 1000, train loss 11.898763656616211, val loss None, lr 0.003333
iter 1250, train loss 11.673822402954102, val loss None, lr 0.003333
iter 1500, train loss 11.534601211547852, val loss None, lr 0.003333
iter 1750, train loss 11.337860107421875, val loss None, lr 0.001111
iter 2000, train loss 11.238380432128906, val loss None, lr 0.001111
iter 2250, train loss 11.161450386047363, val loss None, lr 0.001111
best loss 11.109980583190918
layer3: mlp.gate_proj
iter 0, train loss 3005.998291015625, val loss None, lr 0.01
iter 250, train loss 744.236572265625, val loss None, lr 0.01
iter 500, train loss 714.0528564453125, val loss None, lr 0.01
iter 750, train loss 704.8716430664062, val loss None, lr 0.01
iter 1000, train loss 696.912841796875, val loss None, lr 0.01
iter 1250, train loss 696.6514892578125, val loss None, lr 0.01
iter 1500, train loss 686.1241455078125, val loss None, lr 0.003333
iter 1750, train loss 687.9378051757812, val loss None, lr 0.003333
iter 2000, train loss 682.057861328125, val loss None, lr 0.001111
iter 2250, train loss 681.6460571289062, val loss None, lr 0.001111
best loss 680.5938720703125
layer3: mlp.up_proj
iter 0, train loss 2035.55126953125, val loss None, lr 0.01
iter 250, train loss 553.4443359375, val loss None, lr 0.01
iter 500, train loss 539.820068359375, val loss None, lr 0.01
iter 750, train loss 534.63720703125, val loss None, lr 0.01
iter 1000, train loss 530.277587890625, val loss None, lr 0.01
iter 1250, train loss 527.8546142578125, val loss None, lr 0.01
iter 1500, train loss 524.9066162109375, val loss None, lr 0.003333
iter 1750, train loss 523.4398803710938, val loss None, lr 0.003333
iter 2000, train loss 522.3714599609375, val loss None, lr 0.001111
iter 2250, train loss 521.9546508789062, val loss None, lr 0.001111
best loss 521.4606323242188
layer3: mlp.down_proj
iter 0, train loss 17.361469268798828, val loss None, lr 0.01
iter 250, train loss 3.492222309112549, val loss None, lr 0.01
iter 500, train loss 3.302220582962036, val loss None, lr 0.01
iter 750, train loss 3.2490181922912598, val loss None, lr 0.01
iter 1000, train loss 3.1934115886688232, val loss None, lr 0.003333
iter 1250, train loss 3.169698476791382, val loss None, lr 0.003333
iter 1500, train loss 3.161436080932617, val loss None, lr 0.001111
iter 1750, train loss 3.1552178859710693, val loss None, lr 0.001111
iter 2000, train loss 3.1517789363861084, val loss None, lr 0.001111
iter 2250, train loss 3.147993564605713, val loss None, lr 0.001111
best loss 3.1445722579956055
38654 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:08,  3.78it/s]Inference:   6%|▋         | 2/32 [00:00<00:08,  3.55it/s]Inference:   9%|▉         | 3/32 [00:00<00:08,  3.50it/s]Inference:  12%|█▎        | 4/32 [00:01<00:08,  3.49it/s]Inference:  16%|█▌        | 5/32 [00:01<00:07,  3.49it/s]Inference:  19%|█▉        | 6/32 [00:01<00:07,  3.49it/s]Inference:  22%|██▏       | 7/32 [00:02<00:07,  3.47it/s]Inference:  25%|██▌       | 8/32 [00:02<00:06,  3.61it/s]Inference:  28%|██▊       | 9/32 [00:02<00:06,  3.55it/s]Inference:  31%|███▏      | 10/32 [00:02<00:06,  3.49it/s]Inference:  34%|███▍      | 11/32 [00:03<00:06,  3.46it/s]Inference:  38%|███▊      | 12/32 [00:03<00:05,  3.42it/s]Inference:  41%|████      | 13/32 [00:03<00:05,  3.39it/s]Inference:  44%|████▍     | 14/32 [00:04<00:05,  3.37it/s]Inference:  47%|████▋     | 15/32 [00:04<00:05,  3.36it/s]Inference:  50%|█████     | 16/32 [00:04<00:04,  3.35it/s]Inference:  53%|█████▎    | 17/32 [00:04<00:04,  3.35it/s]Inference:  56%|█████▋    | 18/32 [00:05<00:04,  3.41it/s]Inference:  59%|█████▉    | 19/32 [00:05<00:03,  3.48it/s]Inference:  62%|██████▎   | 20/32 [00:05<00:03,  3.54it/s]Inference:  66%|██████▌   | 21/32 [00:06<00:03,  3.59it/s]Inference:  69%|██████▉   | 22/32 [00:06<00:02,  3.59it/s]Inference:  72%|███████▏  | 23/32 [00:06<00:02,  3.52it/s]Inference:  75%|███████▌  | 24/32 [00:06<00:02,  3.61it/s]Inference:  78%|███████▊  | 25/32 [00:07<00:01,  3.53it/s]Inference:  81%|████████▏ | 26/32 [00:07<00:01,  3.48it/s]Inference:  84%|████████▍ | 27/32 [00:07<00:01,  3.58it/s]Inference:  88%|████████▊ | 28/32 [00:08<00:01,  3.50it/s]Inference:  91%|█████████ | 29/32 [00:08<00:00,  3.61it/s]Inference:  94%|█████████▍| 30/32 [00:08<00:00,  3.49it/s]Inference:  97%|█████████▋| 31/32 [00:08<00:00,  3.58it/s]Inference: 100%|██████████| 32/32 [00:09<00:00,  3.47it/s]Inference: 100%|██████████| 32/32 [00:09<00:00,  3.49it/s]
38649 MiB free out of 48676 MiB total
Saved layer 3 to ./models/meta-llama/Llama-2-7b-hf/tensorized/128/no_finetune2/layer_3.pt
after cast to cpu
42665 MiB free out of 48676 MiB total
Done with layer 3 total_time elapsed: 5586 estimated time left: 39100
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:18,  1.64it/s]Inference:   6%|▋         | 2/32 [00:01<00:17,  1.67it/s]Inference:   9%|▉         | 3/32 [00:01<00:17,  1.66it/s]Inference:  12%|█▎        | 4/32 [00:02<00:16,  1.67it/s]Inference:  16%|█▌        | 5/32 [00:02<00:16,  1.68it/s]Inference:  19%|█▉        | 6/32 [00:03<00:15,  1.71it/s]Inference:  22%|██▏       | 7/32 [00:04<00:14,  1.70it/s]Inference:  25%|██▌       | 8/32 [00:04<00:13,  1.72it/s]Inference:  28%|██▊       | 9/32 [00:05<00:12,  1.80it/s]Inference:  31%|███▏      | 10/32 [00:05<00:12,  1.80it/s]Inference:  34%|███▍      | 11/32 [00:06<00:11,  1.83it/s]Inference:  38%|███▊      | 12/32 [00:06<00:10,  1.84it/s]Inference:  41%|████      | 13/32 [00:07<00:10,  1.86it/s]Inference:  44%|████▍     | 14/32 [00:07<00:09,  1.86it/s]Inference:  47%|████▋     | 15/32 [00:08<00:09,  1.87it/s]Inference:  50%|█████     | 16/32 [00:08<00:08,  1.86it/s]Inference:  53%|█████▎    | 17/32 [00:09<00:08,  1.87it/s]Inference:  56%|█████▋    | 18/32 [00:10<00:07,  1.87it/s]Inference:  59%|█████▉    | 19/32 [00:10<00:06,  1.88it/s]Inference:  62%|██████▎   | 20/32 [00:11<00:06,  1.88it/s]Inference:  66%|██████▌   | 21/32 [00:11<00:05,  1.88it/s]Inference:  69%|██████▉   | 22/32 [00:12<00:05,  1.88it/s]Inference:  72%|███████▏  | 23/32 [00:12<00:04,  1.88it/s]Inference:  75%|███████▌  | 24/32 [00:13<00:04,  1.87it/s]Inference:  78%|███████▊  | 25/32 [00:13<00:03,  1.87it/s]Inference:  81%|████████▏ | 26/32 [00:14<00:03,  1.87it/s]Inference:  84%|████████▍ | 27/32 [00:14<00:02,  1.88it/s]Inference:  88%|████████▊ | 28/32 [00:15<00:02,  1.88it/s]Inference:  91%|█████████ | 29/32 [00:15<00:01,  1.87it/s]Inference:  94%|█████████▍| 30/32 [00:16<00:01,  1.88it/s]Inference:  97%|█████████▋| 31/32 [00:16<00:00,  1.88it/s]Inference: 100%|██████████| 32/32 [00:17<00:00,  1.88it/s]Inference: 100%|██████████| 32/32 [00:17<00:00,  1.83it/s]
layer4: self_attn.q_proj
iter 0, train loss 81705.9765625, val loss None, lr 0.01
iter 250, train loss 1629.359619140625, val loss None, lr 0.01
iter 500, train loss 1507.6756591796875, val loss None, lr 0.01
iter 750, train loss 1397.8040771484375, val loss None, lr 0.003333
iter 1000, train loss 1377.7081298828125, val loss None, lr 0.003333
iter 1250, train loss 1366.472412109375, val loss None, lr 0.003333
iter 1500, train loss 1328.460693359375, val loss None, lr 0.001111
iter 1750, train loss 1318.4100341796875, val loss None, lr 0.001111
iter 2000, train loss 1313.55615234375, val loss None, lr 0.001111
iter 2250, train loss 1303.7357177734375, val loss None, lr 0.00037
best loss 1298.363037109375
layer4: self_attn.k_proj
iter 0, train loss 108561.03125, val loss None, lr 0.01
iter 250, train loss 1678.7540283203125, val loss None, lr 0.01
iter 500, train loss 1691.561279296875, val loss None, lr 0.01
iter 750, train loss 1407.41943359375, val loss None, lr 0.003333
iter 1000, train loss 1366.5177001953125, val loss None, lr 0.003333
iter 1250, train loss 1329.163330078125, val loss None, lr 0.001111
iter 1500, train loss 1313.8438720703125, val loss None, lr 0.001111
iter 1750, train loss 1306.6226806640625, val loss None, lr 0.001111
iter 2000, train loss 1296.5543212890625, val loss None, lr 0.001111
iter 2250, train loss 1286.160888671875, val loss None, lr 0.00037
best loss 1280.5279541015625
layer4: self_attn.v_proj
iter 0, train loss 1759.3525390625, val loss None, lr 0.01
iter 250, train loss 274.8398132324219, val loss None, lr 0.01
iter 500, train loss 267.0682678222656, val loss None, lr 0.01
iter 750, train loss 259.96295166015625, val loss None, lr 0.003333
iter 1000, train loss 257.56829833984375, val loss None, lr 0.003333
iter 1250, train loss 256.358154296875, val loss None, lr 0.001111
iter 1500, train loss 255.2841796875, val loss None, lr 0.001111
iter 1750, train loss 254.88845825195312, val loss None, lr 0.001111
iter 2000, train loss 254.48086547851562, val loss None, lr 0.001111
iter 2250, train loss 253.98773193359375, val loss None, lr 0.00037
best loss 253.69967651367188
layer4: self_attn.o_proj
iter 0, train loss 738.0509643554688, val loss None, lr 0.01
iter 250, train loss 12.112990379333496, val loss None, lr 0.01
iter 500, train loss 8.247063636779785, val loss None, lr 0.003333
iter 750, train loss 7.686200141906738, val loss None, lr 0.003333
iter 1000, train loss 7.30070686340332, val loss None, lr 0.001111
iter 1250, train loss 7.0601959228515625, val loss None, lr 0.001111
iter 1500, train loss 6.978318691253662, val loss None, lr 0.001111
iter 1750, train loss 6.773716449737549, val loss None, lr 0.001111
iter 2000, train loss 6.722836017608643, val loss None, lr 0.001111
iter 2250, train loss 6.665131568908691, val loss None, lr 0.001111
best loss 6.559739112854004
layer4: mlp.gate_proj
iter 0, train loss 5539.10205078125, val loss None, lr 0.01
iter 250, train loss 916.8505859375, val loss None, lr 0.01
iter 500, train loss 889.2452392578125, val loss None, lr 0.01
iter 750, train loss 873.4244995117188, val loss None, lr 0.01
iter 1000, train loss 871.2777099609375, val loss None, lr 0.003333
iter 1250, train loss 849.691162109375, val loss None, lr 0.003333
iter 1500, train loss 847.8295288085938, val loss None, lr 0.003333
iter 1750, train loss 842.7116088867188, val loss None, lr 0.001111
iter 2000, train loss 840.6358642578125, val loss None, lr 0.001111
iter 2250, train loss 839.798828125, val loss None, lr 0.001111
best loss 838.475830078125
layer4: mlp.up_proj
iter 0, train loss 3162.72265625, val loss None, lr 0.01
iter 250, train loss 643.887451171875, val loss None, lr 0.01
iter 500, train loss 630.6580810546875, val loss None, lr 0.01
iter 750, train loss 626.659912109375, val loss None, lr 0.01
iter 1000, train loss 616.4363403320312, val loss None, lr 0.01
iter 1250, train loss 614.1370849609375, val loss None, lr 0.01
iter 1500, train loss 608.9109497070312, val loss None, lr 0.01
iter 1750, train loss 606.278076171875, val loss None, lr 0.003333
iter 2000, train loss 604.3311767578125, val loss None, lr 0.001111
iter 2250, train loss 603.5028686523438, val loss None, lr 0.001111
best loss 602.885009765625
layer4: mlp.down_proj
iter 0, train loss 35.60075378417969, val loss None, lr 0.01
iter 250, train loss 6.350635528564453, val loss None, lr 0.01
iter 500, train loss 6.088484287261963, val loss None, lr 0.01
iter 750, train loss 6.078981399536133, val loss None, lr 0.01
iter 1000, train loss 5.937300682067871, val loss None, lr 0.01
iter 1250, train loss 5.844603538513184, val loss None, lr 0.003333
iter 1500, train loss 5.829565525054932, val loss None, lr 0.003333
iter 1750, train loss 5.809988021850586, val loss None, lr 0.001111
iter 2000, train loss 5.790448188781738, val loss None, lr 0.001111
iter 2250, train loss 5.783875465393066, val loss None, lr 0.001111
best loss 5.776835918426514
42665 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:06,  4.52it/s]Inference:   6%|▋         | 2/32 [00:00<00:07,  4.24it/s]Inference:   9%|▉         | 3/32 [00:00<00:07,  4.12it/s]Inference:  12%|█▎        | 4/32 [00:00<00:06,  4.06it/s]Inference:  16%|█▌        | 5/32 [00:01<00:06,  4.03it/s]Inference:  19%|█▉        | 6/32 [00:01<00:06,  3.94it/s]Inference:  22%|██▏       | 7/32 [00:01<00:06,  3.95it/s]Inference:  25%|██▌       | 8/32 [00:02<00:06,  3.90it/s]Inference:  28%|██▊       | 9/32 [00:02<00:06,  3.58it/s]Inference:  31%|███▏      | 10/32 [00:02<00:06,  3.67it/s]Inference:  34%|███▍      | 11/32 [00:02<00:06,  3.46it/s]Inference:  38%|███▊      | 12/32 [00:03<00:05,  3.34it/s]Inference:  41%|████      | 13/32 [00:03<00:05,  3.27it/s]Inference:  44%|████▍     | 14/32 [00:03<00:05,  3.21it/s]Inference:  47%|████▋     | 15/32 [00:04<00:05,  3.17it/s]Inference:  50%|█████     | 16/32 [00:04<00:05,  3.16it/s]Inference:  53%|█████▎    | 17/32 [00:04<00:04,  3.16it/s]Inference:  56%|█████▋    | 18/32 [00:05<00:04,  3.14it/s]Inference:  59%|█████▉    | 19/32 [00:05<00:04,  3.13it/s]Inference:  62%|██████▎   | 20/32 [00:05<00:03,  3.12it/s]Inference:  66%|██████▌   | 21/32 [00:06<00:03,  3.12it/s]Inference:  69%|██████▉   | 22/32 [00:06<00:03,  3.11it/s]Inference:  72%|███████▏  | 23/32 [00:06<00:02,  3.07it/s]Inference:  75%|███████▌  | 24/32 [00:07<00:02,  3.08it/s]Inference:  78%|███████▊  | 25/32 [00:07<00:02,  3.11it/s]Inference:  81%|████████▏ | 26/32 [00:07<00:01,  3.14it/s]Inference:  84%|████████▍ | 27/32 [00:08<00:01,  3.17it/s]Inference:  88%|████████▊ | 28/32 [00:08<00:01,  3.14it/s]Inference:  91%|█████████ | 29/32 [00:08<00:00,  3.13it/s]Inference:  94%|█████████▍| 30/32 [00:09<00:00,  3.11it/s]Inference:  97%|█████████▋| 31/32 [00:09<00:00,  3.10it/s]Inference: 100%|██████████| 32/32 [00:09<00:00,  3.13it/s]Inference: 100%|██████████| 32/32 [00:09<00:00,  3.31it/s]
37361 MiB free out of 48676 MiB total
Saved layer 4 to ./models/meta-llama/Llama-2-7b-hf/tensorized/128/no_finetune2/layer_4.pt
after cast to cpu
41377 MiB free out of 48676 MiB total
Done with layer 4 total_time elapsed: 6457 estimated time left: 34868
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:18,  1.65it/s]Inference:   6%|▋         | 2/32 [00:01<00:17,  1.68it/s]Inference:   9%|▉         | 3/32 [00:01<00:17,  1.70it/s]Inference:  12%|█▎        | 4/32 [00:02<00:16,  1.71it/s]Inference:  16%|█▌        | 5/32 [00:02<00:15,  1.71it/s]Inference:  19%|█▉        | 6/32 [00:03<00:15,  1.72it/s]Inference:  22%|██▏       | 7/32 [00:04<00:14,  1.72it/s]Inference:  25%|██▌       | 8/32 [00:04<00:13,  1.74it/s]Inference:  28%|██▊       | 9/32 [00:05<00:13,  1.74it/s]Inference:  31%|███▏      | 10/32 [00:05<00:12,  1.74it/s]Inference:  34%|███▍      | 11/32 [00:06<00:12,  1.75it/s]Inference:  38%|███▊      | 12/32 [00:06<00:11,  1.75it/s]Inference:  41%|████      | 13/32 [00:07<00:10,  1.75it/s]Inference:  44%|████▍     | 14/32 [00:08<00:10,  1.75it/s]Inference:  47%|████▋     | 15/32 [00:08<00:09,  1.74it/s]Inference:  50%|█████     | 16/32 [00:09<00:09,  1.74it/s]Inference:  53%|█████▎    | 17/32 [00:09<00:08,  1.74it/s]Inference:  56%|█████▋    | 18/32 [00:10<00:08,  1.73it/s]Inference:  59%|█████▉    | 19/32 [00:10<00:07,  1.72it/s]Inference:  62%|██████▎   | 20/32 [00:11<00:07,  1.71it/s]Inference:  66%|██████▌   | 21/32 [00:12<00:06,  1.71it/s]Inference:  69%|██████▉   | 22/32 [00:12<00:05,  1.71it/s]Inference:  72%|███████▏  | 23/32 [00:13<00:05,  1.71it/s]Inference:  75%|███████▌  | 24/32 [00:13<00:04,  1.70it/s]Inference:  78%|███████▊  | 25/32 [00:14<00:04,  1.70it/s]Inference:  81%|████████▏ | 26/32 [00:15<00:03,  1.68it/s]Inference:  84%|████████▍ | 27/32 [00:15<00:02,  1.68it/s]Inference:  88%|████████▊ | 28/32 [00:16<00:02,  1.67it/s]Inference:  91%|█████████ | 29/32 [00:16<00:01,  1.68it/s]Inference:  94%|█████████▍| 30/32 [00:17<00:01,  1.68it/s]Inference:  97%|█████████▋| 31/32 [00:18<00:00,  1.69it/s]Inference: 100%|██████████| 32/32 [00:18<00:00,  1.69it/s]Inference: 100%|██████████| 32/32 [00:18<00:00,  1.71it/s]
layer5: self_attn.q_proj
iter 0, train loss 82139.3671875, val loss None, lr 0.01
iter 250, train loss 1363.6483154296875, val loss None, lr 0.01
iter 500, train loss 1231.999755859375, val loss None, lr 0.01
iter 750, train loss 1194.4156494140625, val loss None, lr 0.01
iter 1000, train loss 1128.8377685546875, val loss None, lr 0.003333
iter 1250, train loss 1111.94580078125, val loss None, lr 0.003333
iter 1500, train loss 1080.9447021484375, val loss None, lr 0.001111
iter 1750, train loss 1074.3883056640625, val loss None, lr 0.001111
iter 2000, train loss 1083.7071533203125, val loss None, lr 0.001111
iter 2250, train loss 1059.9962158203125, val loss None, lr 0.00037
best loss 1055.7679443359375
layer5: self_attn.k_proj
iter 0, train loss 112802.5078125, val loss None, lr 0.01
iter 250, train loss 1507.282958984375, val loss None, lr 0.01
iter 500, train loss 1323.3924560546875, val loss None, lr 0.01
iter 750, train loss 1293.4078369140625, val loss None, lr 0.003333
iter 1000, train loss 1194.774658203125, val loss None, lr 0.003333
iter 1250, train loss 1168.97900390625, val loss None, lr 0.001111
iter 1500, train loss 1152.651123046875, val loss None, lr 0.001111
iter 1750, train loss 1143.3292236328125, val loss None, lr 0.001111
iter 2000, train loss 1133.595703125, val loss None, lr 0.001111
iter 2250, train loss 1125.19140625, val loss None, lr 0.00037
best loss 1119.9388427734375
layer5: self_attn.v_proj
iter 0, train loss 1991.116943359375, val loss None, lr 0.01
iter 250, train loss 240.65191650390625, val loss None, lr 0.01
iter 500, train loss 227.77301025390625, val loss None, lr 0.01
iter 750, train loss 223.7732391357422, val loss None, lr 0.01
iter 1000, train loss 220.0148468017578, val loss None, lr 0.003333
iter 1250, train loss 218.47689819335938, val loss None, lr 0.001111
iter 1500, train loss 217.70443725585938, val loss None, lr 0.001111
iter 1750, train loss 217.17079162597656, val loss None, lr 0.001111
iter 2000, train loss 216.6378173828125, val loss None, lr 0.001111
iter 2250, train loss 216.28973388671875, val loss None, lr 0.001111
best loss 215.87579345703125
layer5: self_attn.o_proj
iter 0, train loss 725.8147583007812, val loss None, lr 0.01
iter 250, train loss 16.16657066345215, val loss None, lr 0.01
iter 500, train loss 13.8739595413208, val loss None, lr 0.01
iter 750, train loss 12.64276123046875, val loss None, lr 0.01
iter 1000, train loss 11.552857398986816, val loss None, lr 0.01
iter 1250, train loss 10.688278198242188, val loss None, lr 0.003333
iter 1500, train loss 11.266143798828125, val loss None, lr 0.003333
iter 1750, train loss 10.259222030639648, val loss None, lr 0.001111
iter 2000, train loss 10.139788627624512, val loss None, lr 0.001111
iter 2250, train loss 10.081413269042969, val loss None, lr 0.001111
best loss 9.999168395996094
layer5: mlp.gate_proj
iter 0, train loss 8328.0, val loss None, lr 0.01
iter 250, train loss 852.3077392578125, val loss None, lr 0.01
iter 500, train loss 831.2399291992188, val loss None, lr 0.01
iter 750, train loss 807.50048828125, val loss None, lr 0.01
iter 1000, train loss 797.710693359375, val loss None, lr 0.01
iter 1250, train loss 787.3125610351562, val loss None, lr 0.01
iter 1500, train loss 769.970458984375, val loss None, lr 0.003333
iter 1750, train loss 769.1978759765625, val loss None, lr 0.003333
iter 2000, train loss 763.0026245117188, val loss None, lr 0.001111
iter 2250, train loss 761.4575805664062, val loss None, lr 0.001111
best loss 759.9362182617188
layer5: mlp.up_proj
iter 0, train loss 4135.2138671875, val loss None, lr 0.01
iter 250, train loss 591.2565307617188, val loss None, lr 0.01
iter 500, train loss 570.330810546875, val loss None, lr 0.01
iter 750, train loss 561.5166625976562, val loss None, lr 0.01
iter 1000, train loss 555.6555786132812, val loss None, lr 0.01
iter 1250, train loss 554.1380004882812, val loss None, lr 0.01
iter 1500, train loss 545.1597290039062, val loss None, lr 0.003333
iter 1750, train loss 542.8151245117188, val loss None, lr 0.001111
iter 2000, train loss 541.6635131835938, val loss None, lr 0.001111
iter 2250, train loss 540.8033447265625, val loss None, lr 0.001111
best loss 539.9302978515625
layer5: mlp.down_proj
iter 0, train loss 65.39746856689453, val loss None, lr 0.01
iter 250, train loss 7.823132038116455, val loss None, lr 0.01
iter 500, train loss 7.516127586364746, val loss None, lr 0.01
iter 750, train loss 7.335396766662598, val loss None, lr 0.01
iter 1000, train loss 7.1363677978515625, val loss None, lr 0.003333
iter 1250, train loss 7.091039657592773, val loss None, lr 0.003333
iter 1500, train loss 7.047399997711182, val loss None, lr 0.001111
iter 1750, train loss 7.026785850524902, val loss None, lr 0.001111
iter 2000, train loss 7.013238906860352, val loss None, lr 0.001111
iter 2250, train loss 7.0040998458862305, val loss None, lr 0.001111
best loss 6.9859795570373535
41377 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:08,  3.82it/s]Inference:   6%|▋         | 2/32 [00:00<00:08,  3.61it/s]Inference:   9%|▉         | 3/32 [00:00<00:08,  3.54it/s]Inference:  12%|█▎        | 4/32 [00:01<00:07,  3.52it/s]Inference:  16%|█▌        | 5/32 [00:01<00:07,  3.50it/s]Inference:  19%|█▉        | 6/32 [00:01<00:07,  3.51it/s]Inference:  22%|██▏       | 7/32 [00:01<00:07,  3.49it/s]Inference:  25%|██▌       | 8/32 [00:02<00:06,  3.48it/s]Inference:  28%|██▊       | 9/32 [00:02<00:06,  3.48it/s]Inference:  31%|███▏      | 10/32 [00:02<00:06,  3.49it/s]Inference:  34%|███▍      | 11/32 [00:03<00:06,  3.47it/s]Inference:  38%|███▊      | 12/32 [00:03<00:05,  3.45it/s]Inference:  41%|████      | 13/32 [00:03<00:05,  3.45it/s]Inference:  44%|████▍     | 14/32 [00:04<00:05,  3.46it/s]Inference:  47%|████▋     | 15/32 [00:04<00:04,  3.45it/s]Inference:  50%|█████     | 16/32 [00:04<00:04,  3.46it/s]Inference:  53%|█████▎    | 17/32 [00:04<00:04,  3.45it/s]Inference:  56%|█████▋    | 18/32 [00:05<00:04,  3.45it/s]Inference:  59%|█████▉    | 19/32 [00:05<00:03,  3.41it/s]Inference:  62%|██████▎   | 20/32 [00:05<00:03,  3.39it/s]Inference:  66%|██████▌   | 21/32 [00:06<00:03,  3.40it/s]Inference:  69%|██████▉   | 22/32 [00:06<00:02,  3.42it/s]Inference:  72%|███████▏  | 23/32 [00:06<00:02,  3.43it/s]Inference:  75%|███████▌  | 24/32 [00:06<00:02,  3.40it/s]Inference:  78%|███████▊  | 25/32 [00:07<00:02,  3.41it/s]Inference:  81%|████████▏ | 26/32 [00:07<00:01,  3.43it/s]Inference:  84%|████████▍ | 27/32 [00:07<00:01,  3.44it/s]Inference:  88%|████████▊ | 28/32 [00:08<00:01,  3.45it/s]Inference:  91%|█████████ | 29/32 [00:08<00:00,  3.46it/s]Inference:  94%|█████████▍| 30/32 [00:08<00:00,  3.47it/s]Inference:  97%|█████████▋| 31/32 [00:08<00:00,  3.45it/s]Inference: 100%|██████████| 32/32 [00:09<00:00,  3.39it/s]Inference: 100%|██████████| 32/32 [00:09<00:00,  3.45it/s]
36073 MiB free out of 48676 MiB total
Saved layer 5 to ./models/meta-llama/Llama-2-7b-hf/tensorized/128/no_finetune2/layer_5.pt
after cast to cpu
40089 MiB free out of 48676 MiB total
Done with layer 5 total_time elapsed: 7329 estimated time left: 31760
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:18,  1.64it/s]Inference:   6%|▋         | 2/32 [00:01<00:18,  1.66it/s]Inference:   9%|▉         | 3/32 [00:01<00:17,  1.65it/s]Inference:  12%|█▎        | 4/32 [00:02<00:16,  1.65it/s]Inference:  16%|█▌        | 5/32 [00:03<00:16,  1.66it/s]Inference:  19%|█▉        | 6/32 [00:03<00:15,  1.66it/s]Inference:  22%|██▏       | 7/32 [00:04<00:15,  1.66it/s]Inference:  25%|██▌       | 8/32 [00:04<00:14,  1.65it/s]Inference:  28%|██▊       | 9/32 [00:05<00:13,  1.66it/s]Inference:  31%|███▏      | 10/32 [00:06<00:13,  1.65it/s]Inference:  34%|███▍      | 11/32 [00:06<00:12,  1.65it/s]Inference:  38%|███▊      | 12/32 [00:07<00:12,  1.66it/s]Inference:  41%|████      | 13/32 [00:07<00:11,  1.66it/s]Inference:  44%|████▍     | 14/32 [00:08<00:10,  1.67it/s]Inference:  47%|████▋     | 15/32 [00:09<00:10,  1.67it/s]Inference:  50%|█████     | 16/32 [00:09<00:09,  1.66it/s]Inference:  53%|█████▎    | 17/32 [00:10<00:09,  1.66it/s]Inference:  56%|█████▋    | 18/32 [00:10<00:08,  1.67it/s]Inference:  59%|█████▉    | 19/32 [00:11<00:07,  1.67it/s]Inference:  62%|██████▎   | 20/32 [00:12<00:07,  1.68it/s]Inference:  66%|██████▌   | 21/32 [00:12<00:06,  1.68it/s]Inference:  69%|██████▉   | 22/32 [00:13<00:05,  1.68it/s]Inference:  72%|███████▏  | 23/32 [00:13<00:05,  1.68it/s]Inference:  75%|███████▌  | 24/32 [00:14<00:04,  1.69it/s]Inference:  78%|███████▊  | 25/32 [00:15<00:04,  1.69it/s]Inference:  81%|████████▏ | 26/32 [00:15<00:03,  1.69it/s]Inference:  84%|████████▍ | 27/32 [00:16<00:02,  1.69it/s]Inference:  88%|████████▊ | 28/32 [00:16<00:02,  1.69it/s]Inference:  91%|█████████ | 29/32 [00:17<00:01,  1.69it/s]Inference:  94%|█████████▍| 30/32 [00:17<00:01,  1.69it/s]Inference:  97%|█████████▋| 31/32 [00:18<00:00,  1.69it/s]Inference: 100%|██████████| 32/32 [00:19<00:00,  1.70it/s]Inference: 100%|██████████| 32/32 [00:19<00:00,  1.67it/s]
layer6: self_attn.q_proj
iter 0, train loss 70780.09375, val loss None, lr 0.01
iter 250, train loss 1309.1395263671875, val loss None, lr 0.01
iter 500, train loss 1198.9102783203125, val loss None, lr 0.01
iter 750, train loss 1113.728271484375, val loss None, lr 0.003333
iter 1000, train loss 1076.0247802734375, val loss None, lr 0.003333
iter 1250, train loss 1052.1800537109375, val loss None, lr 0.003333
iter 1500, train loss 1031.470703125, val loss None, lr 0.001111
iter 1750, train loss 1023.2698364257812, val loss None, lr 0.001111
iter 2000, train loss 1017.6650390625, val loss None, lr 0.001111
iter 2250, train loss 1010.0494384765625, val loss None, lr 0.001111
best loss 1004.3150634765625
layer6: self_attn.k_proj
iter 0, train loss 89369.2734375, val loss None, lr 0.01
iter 250, train loss 1362.0543212890625, val loss None, lr 0.01
iter 500, train loss 1197.0321044921875, val loss None, lr 0.01
iter 750, train loss 1192.869873046875, val loss None, lr 0.01
iter 1000, train loss 1056.9212646484375, val loss None, lr 0.01
iter 1250, train loss 1024.995849609375, val loss None, lr 0.003333
iter 1500, train loss 995.430419921875, val loss None, lr 0.001111
iter 1750, train loss 983.8150634765625, val loss None, lr 0.001111
iter 2000, train loss 974.7658081054688, val loss None, lr 0.001111
iter 2250, train loss 969.718505859375, val loss None, lr 0.001111
best loss 961.6177978515625
layer6: self_attn.v_proj
iter 0, train loss 3027.587890625, val loss None, lr 0.01
iter 250, train loss 245.91680908203125, val loss None, lr 0.01
iter 500, train loss 230.64651489257812, val loss None, lr 0.01
iter 750, train loss 225.1256103515625, val loss None, lr 0.01
iter 1000, train loss 219.48719787597656, val loss None, lr 0.003333
iter 1250, train loss 217.0014190673828, val loss None, lr 0.001111
iter 1500, train loss 216.10617065429688, val loss None, lr 0.001111
iter 1750, train loss 215.31704711914062, val loss None, lr 0.001111
iter 2000, train loss 214.57467651367188, val loss None, lr 0.001111
iter 2250, train loss 214.17042541503906, val loss None, lr 0.001111
best loss 213.61297607421875
layer6: self_attn.o_proj
iter 0, train loss 923.8880615234375, val loss None, lr 0.01
iter 250, train loss 22.623271942138672, val loss None, lr 0.01
iter 500, train loss 20.768402099609375, val loss None, lr 0.01
iter 750, train loss 18.436059951782227, val loss None, lr 0.003333
iter 1000, train loss 16.6199951171875, val loss None, lr 0.003333
iter 1250, train loss 16.471729278564453, val loss None, lr 0.003333
iter 1500, train loss 17.040843963623047, val loss None, lr 0.003333
iter 1750, train loss 15.73763656616211, val loss None, lr 0.001111
iter 2000, train loss 15.536436080932617, val loss None, lr 0.001111
iter 2250, train loss 15.445762634277344, val loss None, lr 0.001111
best loss 15.347765922546387
layer6: mlp.gate_proj
iter 0, train loss 14638.6669921875, val loss None, lr 0.01
iter 250, train loss 723.0736083984375, val loss None, lr 0.01
iter 500, train loss 674.6511840820312, val loss None, lr 0.01
iter 750, train loss 656.5986328125, val loss None, lr 0.01
iter 1000, train loss 659.3883666992188, val loss None, lr 0.01
iter 1250, train loss 654.0535278320312, val loss None, lr 0.01
iter 1500, train loss 627.6675415039062, val loss None, lr 0.003333
iter 1750, train loss 621.0494995117188, val loss None, lr 0.001111
iter 2000, train loss 618.4827880859375, val loss None, lr 0.001111
iter 2250, train loss 617.039306640625, val loss None, lr 0.001111
best loss 615.1630859375
layer6: mlp.up_proj
iter 0, train loss 5602.9287109375, val loss None, lr 0.01
iter 250, train loss 466.075439453125, val loss None, lr 0.01
iter 500, train loss 447.33984375, val loss None, lr 0.01
iter 750, train loss 431.445556640625, val loss None, lr 0.003333
iter 1000, train loss 428.6451416015625, val loss None, lr 0.003333
iter 1250, train loss 424.383544921875, val loss None, lr 0.001111
iter 1500, train loss 422.9830322265625, val loss None, lr 0.001111
iter 1750, train loss 421.89263916015625, val loss None, lr 0.001111
iter 2000, train loss 420.9781799316406, val loss None, lr 0.001111
iter 2250, train loss 419.92041015625, val loss None, lr 0.001111
best loss 418.9576416015625
layer6: mlp.down_proj
iter 0, train loss 134.88690185546875, val loss None, lr 0.01
iter 250, train loss 8.402414321899414, val loss None, lr 0.01
iter 500, train loss 7.66162633895874, val loss None, lr 0.01
iter 750, train loss 7.472440719604492, val loss None, lr 0.01
iter 1000, train loss 7.267545700073242, val loss None, lr 0.01
iter 1250, train loss 7.066901206970215, val loss None, lr 0.003333
iter 1500, train loss 7.19856595993042, val loss None, lr 0.003333
iter 1750, train loss 6.936875343322754, val loss None, lr 0.001111
iter 2000, train loss 6.90639591217041, val loss None, lr 0.001111
iter 2250, train loss 6.883575916290283, val loss None, lr 0.001111
best loss 6.862916469573975
40089 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:08,  3.78it/s]Inference:   6%|▋         | 2/32 [00:00<00:08,  3.62it/s]Inference:   9%|▉         | 3/32 [00:00<00:08,  3.56it/s]Inference:  12%|█▎        | 4/32 [00:01<00:07,  3.52it/s]Inference:  16%|█▌        | 5/32 [00:01<00:07,  3.51it/s]Inference:  19%|█▉        | 6/32 [00:01<00:07,  3.50it/s]Inference:  22%|██▏       | 7/32 [00:01<00:07,  3.48it/s]Inference:  25%|██▌       | 8/32 [00:02<00:06,  3.44it/s]Inference:  28%|██▊       | 9/32 [00:02<00:06,  3.46it/s]Inference:  31%|███▏      | 10/32 [00:02<00:06,  3.46it/s]Inference:  34%|███▍      | 11/32 [00:03<00:06,  3.47it/s]Inference:  38%|███▊      | 12/32 [00:03<00:05,  3.62it/s]Inference:  41%|████      | 13/32 [00:03<00:05,  3.57it/s]Inference:  44%|████▍     | 14/32 [00:03<00:05,  3.56it/s]Inference:  47%|████▋     | 15/32 [00:04<00:04,  3.51it/s]Inference:  50%|█████     | 16/32 [00:04<00:04,  3.49it/s]Inference:  53%|█████▎    | 17/32 [00:04<00:04,  3.48it/s]Inference:  56%|█████▋    | 18/32 [00:05<00:04,  3.50it/s]Inference:  59%|█████▉    | 19/32 [00:05<00:03,  3.52it/s]Inference:  62%|██████▎   | 20/32 [00:05<00:03,  3.53it/s]Inference:  66%|██████▌   | 21/32 [00:05<00:03,  3.49it/s]Inference:  69%|██████▉   | 22/32 [00:06<00:02,  3.50it/s]Inference:  72%|███████▏  | 23/32 [00:06<00:02,  3.50it/s]Inference:  75%|███████▌  | 24/32 [00:06<00:02,  3.51it/s]Inference:  78%|███████▊  | 25/32 [00:07<00:01,  3.51it/s]Inference:  81%|████████▏ | 26/32 [00:07<00:01,  3.53it/s]Inference:  84%|████████▍ | 27/32 [00:07<00:01,  3.53it/s]Inference:  88%|████████▊ | 28/32 [00:07<00:01,  3.54it/s]Inference:  91%|█████████ | 29/32 [00:08<00:00,  3.54it/s]Inference:  94%|█████████▍| 30/32 [00:08<00:00,  3.52it/s]Inference:  97%|█████████▋| 31/32 [00:08<00:00,  3.52it/s]Inference: 100%|██████████| 32/32 [00:09<00:00,  3.52it/s]Inference: 100%|██████████| 32/32 [00:09<00:00,  3.52it/s]
34849 MiB free out of 48676 MiB total
Saved layer 6 to ./models/meta-llama/Llama-2-7b-hf/tensorized/128/no_finetune2/layer_6.pt
after cast to cpu
38865 MiB free out of 48676 MiB total
Done with layer 6 total_time elapsed: 8201 estimated time left: 29289
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:18,  1.65it/s]Inference:   6%|▋         | 2/32 [00:01<00:18,  1.66it/s]Inference:   9%|▉         | 3/32 [00:01<00:17,  1.65it/s]Inference:  12%|█▎        | 4/32 [00:02<00:16,  1.66it/s]Inference:  16%|█▌        | 5/32 [00:03<00:16,  1.67it/s]Inference:  19%|█▉        | 6/32 [00:03<00:15,  1.67it/s]Inference:  22%|██▏       | 7/32 [00:04<00:14,  1.68it/s]Inference:  25%|██▌       | 8/32 [00:04<00:14,  1.69it/s]Inference:  28%|██▊       | 9/32 [00:05<00:13,  1.68it/s]Inference:  31%|███▏      | 10/32 [00:05<00:13,  1.68it/s]Inference:  34%|███▍      | 11/32 [00:06<00:12,  1.69it/s]Inference:  38%|███▊      | 12/32 [00:07<00:11,  1.71it/s]Inference:  41%|████      | 13/32 [00:07<00:10,  1.73it/s]Inference:  44%|████▍     | 14/32 [00:08<00:10,  1.74it/s]Inference:  47%|████▋     | 15/32 [00:08<00:09,  1.75it/s]Inference:  50%|█████     | 16/32 [00:09<00:09,  1.76it/s]Inference:  53%|█████▎    | 17/32 [00:09<00:08,  1.77it/s]Inference:  56%|█████▋    | 18/32 [00:10<00:07,  1.77it/s]Inference:  59%|█████▉    | 19/32 [00:11<00:07,  1.78it/s]Inference:  62%|██████▎   | 20/32 [00:11<00:06,  1.78it/s]Inference:  66%|██████▌   | 21/32 [00:12<00:06,  1.78it/s]Inference:  69%|██████▉   | 22/32 [00:12<00:05,  1.78it/s]Inference:  72%|███████▏  | 23/32 [00:13<00:05,  1.78it/s]Inference:  75%|███████▌  | 24/32 [00:13<00:04,  1.78it/s]Inference:  78%|███████▊  | 25/32 [00:14<00:03,  1.78it/s]Inference:  81%|████████▏ | 26/32 [00:15<00:03,  1.78it/s]Inference:  84%|████████▍ | 27/32 [00:15<00:02,  1.78it/s]Inference:  88%|████████▊ | 28/32 [00:16<00:02,  1.78it/s]Inference:  91%|█████████ | 29/32 [00:16<00:01,  1.78it/s]Inference:  94%|█████████▍| 30/32 [00:17<00:01,  1.78it/s]Inference:  97%|█████████▋| 31/32 [00:17<00:00,  1.78it/s]Inference: 100%|██████████| 32/32 [00:18<00:00,  1.78it/s]Inference: 100%|██████████| 32/32 [00:18<00:00,  1.74it/s]
layer7: self_attn.q_proj
iter 0, train loss 59259.3125, val loss None, lr 0.01
iter 250, train loss 1104.5921630859375, val loss None, lr 0.01
iter 500, train loss 955.958984375, val loss None, lr 0.01
iter 750, train loss 975.888916015625, val loss None, lr 0.01
iter 1000, train loss 908.8726806640625, val loss None, lr 0.01
iter 1250, train loss 832.8932495117188, val loss None, lr 0.003333
iter 1500, train loss 822.3924560546875, val loss None, lr 0.001111
iter 1750, train loss 803.0142822265625, val loss None, lr 0.001111
iter 2000, train loss 796.7159423828125, val loss None, lr 0.001111
iter 2250, train loss 791.9705200195312, val loss None, lr 0.001111
best loss 787.0955810546875
layer7: self_attn.k_proj
iter 0, train loss 78036.359375, val loss None, lr 0.01
iter 250, train loss 1080.1126708984375, val loss None, lr 0.01
iter 500, train loss 899.7105102539062, val loss None, lr 0.003333
iter 750, train loss 859.8271484375, val loss None, lr 0.003333
iter 1000, train loss 813.4843139648438, val loss None, lr 0.001111
iter 1250, train loss 798.1805419921875, val loss None, lr 0.001111
iter 1500, train loss 788.967041015625, val loss None, lr 0.001111
iter 1750, train loss 779.2257080078125, val loss None, lr 0.001111
iter 2000, train loss 771.7490234375, val loss None, lr 0.001111
iter 2250, train loss 764.7207641601562, val loss None, lr 0.00037
best loss 759.8153686523438
layer7: self_attn.v_proj
iter 0, train loss 3253.2734375, val loss None, lr 0.01
iter 250, train loss 208.43902587890625, val loss None, lr 0.01
iter 500, train loss 195.62509155273438, val loss None, lr 0.01
iter 750, train loss 191.34860229492188, val loss None, lr 0.01
iter 1000, train loss 184.51380920410156, val loss None, lr 0.003333
iter 1250, train loss 183.14987182617188, val loss None, lr 0.001111
iter 1500, train loss 181.57058715820312, val loss None, lr 0.001111
iter 1750, train loss 180.8905487060547, val loss None, lr 0.001111
iter 2000, train loss 180.2220001220703, val loss None, lr 0.001111
iter 2250, train loss 179.5960693359375, val loss None, lr 0.001111
best loss 179.15821838378906
layer7: self_attn.o_proj
iter 0, train loss 943.8046264648438, val loss None, lr 0.01
iter 250, train loss 27.938453674316406, val loss None, lr 0.01
iter 500, train loss 25.840200424194336, val loss None, lr 0.01
iter 750, train loss 22.268310546875, val loss None, lr 0.01
iter 1000, train loss 20.77182388305664, val loss None, lr 0.01
iter 1250, train loss 19.110382080078125, val loss None, lr 0.003333
iter 1500, train loss 18.694522857666016, val loss None, lr 0.001111
iter 1750, train loss 18.542301177978516, val loss None, lr 0.001111
iter 2000, train loss 18.43407440185547, val loss None, lr 0.001111
iter 2250, train loss 18.317161560058594, val loss None, lr 0.001111
best loss 18.233423233032227
layer7: mlp.gate_proj
iter 0, train loss 18571.685546875, val loss None, lr 0.01
iter 250, train loss 759.8697509765625, val loss None, lr 0.01
iter 500, train loss 714.131103515625, val loss None, lr 0.01
iter 750, train loss 690.8873901367188, val loss None, lr 0.01
iter 1000, train loss 673.3934326171875, val loss None, lr 0.003333
iter 1250, train loss 662.8745727539062, val loss None, lr 0.003333
iter 1500, train loss 656.6728515625, val loss None, lr 0.001111
iter 1750, train loss 654.1749877929688, val loss None, lr 0.001111
iter 2000, train loss 651.6318969726562, val loss None, lr 0.001111
iter 2250, train loss 649.2327880859375, val loss None, lr 0.00037
best loss 647.7867431640625
layer7: mlp.up_proj
iter 0, train loss 6088.94775390625, val loss None, lr 0.01
iter 250, train loss 510.7377014160156, val loss None, lr 0.01
iter 500, train loss 483.8292236328125, val loss None, lr 0.01
iter 750, train loss 479.33209228515625, val loss None, lr 0.01
iter 1000, train loss 470.791259765625, val loss None, lr 0.01
iter 1250, train loss 463.90380859375, val loss None, lr 0.003333
iter 1500, train loss 461.14202880859375, val loss None, lr 0.001111
iter 1750, train loss 459.66351318359375, val loss None, lr 0.001111
iter 2000, train loss 458.82830810546875, val loss None, lr 0.001111
iter 2250, train loss 457.5265197753906, val loss None, lr 0.001111
best loss 457.1191101074219
layer7: mlp.down_proj
iter 0, train loss 137.9766082763672, val loss None, lr 0.01
iter 250, train loss 9.099325180053711, val loss None, lr 0.01
iter 500, train loss 8.639110565185547, val loss None, lr 0.01
iter 750, train loss 8.458882331848145, val loss None, lr 0.01
iter 1000, train loss 8.14902400970459, val loss None, lr 0.01
iter 1250, train loss 7.870628356933594, val loss None, lr 0.003333
iter 1500, train loss 7.777312278747559, val loss None, lr 0.003333
iter 1750, train loss 7.750728607177734, val loss None, lr 0.003333
iter 2000, train loss 7.679766654968262, val loss None, lr 0.003333
iter 2250, train loss 7.6484575271606445, val loss None, lr 0.001111
best loss 7.621943473815918
38865 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:07,  4.00it/s]Inference:   6%|▋         | 2/32 [00:00<00:07,  3.77it/s]Inference:   9%|▉         | 3/32 [00:00<00:07,  3.68it/s]Inference:  12%|█▎        | 4/32 [00:01<00:07,  3.65it/s]Inference:  16%|█▌        | 5/32 [00:01<00:07,  3.63it/s]Inference:  19%|█▉        | 6/32 [00:01<00:07,  3.61it/s]Inference:  22%|██▏       | 7/32 [00:01<00:06,  3.63it/s]Inference:  25%|██▌       | 8/32 [00:02<00:06,  3.62it/s]Inference:  28%|██▊       | 9/32 [00:02<00:06,  3.63it/s]Inference:  31%|███▏      | 10/32 [00:02<00:06,  3.61it/s]Inference:  34%|███▍      | 11/32 [00:03<00:05,  3.61it/s]Inference:  38%|███▊      | 12/32 [00:03<00:05,  3.60it/s]Inference:  41%|████      | 13/32 [00:03<00:05,  3.60it/s]Inference:  44%|████▍     | 14/32 [00:03<00:04,  3.61it/s]Inference:  47%|████▋     | 15/32 [00:04<00:04,  3.62it/s]Inference:  50%|█████     | 16/32 [00:04<00:04,  3.61it/s]Inference:  53%|█████▎    | 17/32 [00:04<00:04,  3.62it/s]Inference:  56%|█████▋    | 18/32 [00:04<00:03,  3.60it/s]Inference:  59%|█████▉    | 19/32 [00:05<00:03,  3.60it/s]Inference:  62%|██████▎   | 20/32 [00:05<00:03,  3.60it/s]Inference:  66%|██████▌   | 21/32 [00:05<00:03,  3.58it/s]Inference:  69%|██████▉   | 22/32 [00:06<00:02,  3.58it/s]Inference:  72%|███████▏  | 23/32 [00:06<00:02,  3.59it/s]Inference:  75%|███████▌  | 24/32 [00:06<00:02,  3.60it/s]Inference:  78%|███████▊  | 25/32 [00:06<00:01,  3.60it/s]Inference:  81%|████████▏ | 26/32 [00:07<00:01,  3.61it/s]Inference:  84%|████████▍ | 27/32 [00:07<00:01,  3.61it/s]Inference:  88%|████████▊ | 28/32 [00:07<00:01,  3.61it/s]Inference:  91%|█████████ | 29/32 [00:08<00:00,  3.60it/s]Inference:  94%|█████████▍| 30/32 [00:08<00:00,  3.61it/s]Inference:  97%|█████████▋| 31/32 [00:08<00:00,  3.61it/s]Inference: 100%|██████████| 32/32 [00:08<00:00,  3.62it/s]Inference: 100%|██████████| 32/32 [00:08<00:00,  3.62it/s]
33561 MiB free out of 48676 MiB total
Saved layer 7 to ./models/meta-llama/Llama-2-7b-hf/tensorized/128/no_finetune2/layer_7.pt
after cast to cpu
37577 MiB free out of 48676 MiB total
Done with layer 7 total_time elapsed: 9072 estimated time left: 27217
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:16,  1.94it/s]Inference:   6%|▋         | 2/32 [00:01<00:15,  1.95it/s]Inference:   9%|▉         | 3/32 [00:01<00:15,  1.93it/s]Inference:  12%|█▎        | 4/32 [00:02<00:14,  1.93it/s]Inference:  16%|█▌        | 5/32 [00:02<00:14,  1.92it/s]Inference:  19%|█▉        | 6/32 [00:03<00:13,  1.93it/s]Inference:  22%|██▏       | 7/32 [00:03<00:13,  1.92it/s]Inference:  25%|██▌       | 8/32 [00:04<00:12,  1.94it/s]Inference:  28%|██▊       | 9/32 [00:04<00:11,  1.93it/s]Inference:  31%|███▏      | 10/32 [00:05<00:11,  1.93it/s]Inference:  34%|███▍      | 11/32 [00:05<00:10,  1.92it/s]Inference:  38%|███▊      | 12/32 [00:06<00:10,  1.93it/s]Inference:  41%|████      | 13/32 [00:06<00:09,  1.93it/s]Inference:  44%|████▍     | 14/32 [00:07<00:09,  1.92it/s]Inference:  47%|████▋     | 15/32 [00:07<00:08,  1.92it/s]Inference:  50%|█████     | 16/32 [00:08<00:08,  1.93it/s]Inference:  53%|█████▎    | 17/32 [00:08<00:07,  1.92it/s]Inference:  56%|█████▋    | 18/32 [00:09<00:07,  1.92it/s]Inference:  59%|█████▉    | 19/32 [00:09<00:06,  1.92it/s]Inference:  62%|██████▎   | 20/32 [00:10<00:06,  1.92it/s]Inference:  66%|██████▌   | 21/32 [00:10<00:05,  1.92it/s]Inference:  69%|██████▉   | 22/32 [00:11<00:05,  1.93it/s]Inference:  72%|███████▏  | 23/32 [00:11<00:04,  1.92it/s]Inference:  75%|███████▌  | 24/32 [00:12<00:04,  1.92it/s]Inference:  78%|███████▊  | 25/32 [00:12<00:03,  1.92it/s]Inference:  81%|████████▏ | 26/32 [00:13<00:03,  1.93it/s]Inference:  84%|████████▍ | 27/32 [00:14<00:02,  1.93it/s]Inference:  88%|████████▊ | 28/32 [00:14<00:02,  1.92it/s]Inference:  91%|█████████ | 29/32 [00:15<00:01,  1.92it/s]Inference:  94%|█████████▍| 30/32 [00:15<00:01,  1.92it/s]Inference:  97%|█████████▋| 31/32 [00:16<00:00,  1.93it/s]Inference: 100%|██████████| 32/32 [00:16<00:00,  1.93it/s]Inference: 100%|██████████| 32/32 [00:16<00:00,  1.93it/s]
layer8: self_attn.q_proj
iter 0, train loss 59611.453125, val loss None, lr 0.01
iter 250, train loss 1150.2904052734375, val loss None, lr 0.01
iter 500, train loss 930.7982177734375, val loss None, lr 0.003333
iter 750, train loss 893.3021240234375, val loss None, lr 0.001111
iter 1000, train loss 874.7803955078125, val loss None, lr 0.001111
iter 1250, train loss 859.2881469726562, val loss None, lr 0.001111
iter 1500, train loss 851.1497192382812, val loss None, lr 0.001111
iter 1750, train loss 842.3143310546875, val loss None, lr 0.001111
iter 2000, train loss 836.29345703125, val loss None, lr 0.001111
iter 2250, train loss 829.9048461914062, val loss None, lr 0.001111
best loss 823.7291259765625
layer8: self_attn.k_proj
iter 0, train loss 82680.4765625, val loss None, lr 0.01
iter 250, train loss 1078.1099853515625, val loss None, lr 0.01
iter 500, train loss 920.0302734375, val loss None, lr 0.003333
iter 750, train loss 912.0814819335938, val loss None, lr 0.003333
iter 1000, train loss 847.5543823242188, val loss None, lr 0.001111
iter 1250, train loss 831.2053833007812, val loss None, lr 0.001111
iter 1500, train loss 820.854248046875, val loss None, lr 0.001111
iter 1750, train loss 812.6642456054688, val loss None, lr 0.001111
iter 2000, train loss 808.9623413085938, val loss None, lr 0.001111
iter 2250, train loss 802.60693359375, val loss None, lr 0.001111
best loss 796.0570678710938
layer8: self_attn.v_proj
iter 0, train loss 2770.6376953125, val loss None, lr 0.01
iter 250, train loss 233.89230346679688, val loss None, lr 0.01
iter 500, train loss 219.00509643554688, val loss None, lr 0.01
iter 750, train loss 217.2220458984375, val loss None, lr 0.01
iter 1000, train loss 213.04080200195312, val loss None, lr 0.01
iter 1250, train loss 208.231689453125, val loss None, lr 0.003333
iter 1500, train loss 206.2279510498047, val loss None, lr 0.003333
iter 1750, train loss 204.21620178222656, val loss None, lr 0.001111
iter 2000, train loss 203.72781372070312, val loss None, lr 0.001111
iter 2250, train loss 203.20681762695312, val loss None, lr 0.001111
best loss 202.70838928222656
layer8: self_attn.o_proj
iter 0, train loss 889.4033813476562, val loss None, lr 0.01
iter 250, train loss 32.175086975097656, val loss None, lr 0.01
iter 500, train loss 27.231212615966797, val loss None, lr 0.01
iter 750, train loss 25.329021453857422, val loss None, lr 0.01
iter 1000, train loss 25.54096031188965, val loss None, lr 0.01
iter 1250, train loss 23.55278778076172, val loss None, lr 0.003333
iter 1500, train loss 23.2714900970459, val loss None, lr 0.003333
iter 1750, train loss 22.888282775878906, val loss None, lr 0.001111
iter 2000, train loss 22.662338256835938, val loss None, lr 0.001111
iter 2250, train loss 22.58904266357422, val loss None, lr 0.001111
best loss 22.496261596679688
layer8: mlp.gate_proj
iter 0, train loss 15748.220703125, val loss None, lr 0.01
iter 250, train loss 791.5279541015625, val loss None, lr 0.01
iter 500, train loss 786.06640625, val loss None, lr 0.01
iter 750, train loss 759.5343627929688, val loss None, lr 0.01
iter 1000, train loss 731.5233154296875, val loss None, lr 0.01
iter 1250, train loss 732.1903076171875, val loss None, lr 0.01
iter 1500, train loss 702.7769775390625, val loss None, lr 0.003333
iter 1750, train loss 695.5582275390625, val loss None, lr 0.001111
iter 2000, train loss 694.6171875, val loss None, lr 0.001111
iter 2250, train loss 691.2871704101562, val loss None, lr 0.001111
best loss 689.5782470703125
layer8: mlp.up_proj
iter 0, train loss 6632.13134765625, val loss None, lr 0.01
iter 250, train loss 593.6029052734375, val loss None, lr 0.01
iter 500, train loss 556.426513671875, val loss None, lr 0.01
iter 750, train loss 546.3309936523438, val loss None, lr 0.01
iter 1000, train loss 548.5247802734375, val loss None, lr 0.01
iter 1250, train loss 529.709716796875, val loss None, lr 0.003333
iter 1500, train loss 525.5960693359375, val loss None, lr 0.001111
iter 1750, train loss 524.181640625, val loss None, lr 0.001111
iter 2000, train loss 523.0872802734375, val loss None, lr 0.001111
iter 2250, train loss 522.2135009765625, val loss None, lr 0.001111
best loss 521.2169189453125
layer8: mlp.down_proj
iter 0, train loss 129.30560302734375, val loss None, lr 0.01
iter 250, train loss 10.366354942321777, val loss None, lr 0.01
iter 500, train loss 9.998952865600586, val loss None, lr 0.01
iter 750, train loss 9.392962455749512, val loss None, lr 0.003333
iter 1000, train loss 9.231401443481445, val loss None, lr 0.003333
iter 1250, train loss 9.103106498718262, val loss None, lr 0.001111
iter 1500, train loss 9.04205322265625, val loss None, lr 0.001111
iter 1750, train loss 9.009164810180664, val loss None, lr 0.001111
iter 2000, train loss 8.9808931350708, val loss None, lr 0.001111
iter 2250, train loss 8.954687118530273, val loss None, lr 0.001111
best loss 8.911386489868164
37577 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:06,  4.83it/s]Inference:   6%|▋         | 2/32 [00:00<00:06,  4.49it/s]Inference:   9%|▉         | 3/32 [00:00<00:06,  4.38it/s]Inference:  12%|█▎        | 4/32 [00:00<00:06,  4.29it/s]Inference:  16%|█▌        | 5/32 [00:01<00:06,  4.25it/s]Inference:  19%|█▉        | 6/32 [00:01<00:06,  4.22it/s]Inference:  22%|██▏       | 7/32 [00:01<00:05,  4.20it/s]Inference:  25%|██▌       | 8/32 [00:01<00:05,  4.18it/s]Inference:  28%|██▊       | 9/32 [00:02<00:05,  4.18it/s]Inference:  31%|███▏      | 10/32 [00:02<00:05,  4.18it/s]Inference:  34%|███▍      | 11/32 [00:02<00:04,  4.21it/s]Inference:  38%|███▊      | 12/32 [00:02<00:04,  4.22it/s]Inference:  41%|████      | 13/32 [00:03<00:04,  4.22it/s]Inference:  44%|████▍     | 14/32 [00:03<00:04,  4.21it/s]Inference:  47%|████▋     | 15/32 [00:03<00:04,  4.22it/s]Inference:  50%|█████     | 16/32 [00:03<00:03,  4.20it/s]Inference:  53%|█████▎    | 17/32 [00:04<00:03,  4.20it/s]Inference:  56%|█████▋    | 18/32 [00:04<00:03,  4.19it/s]Inference:  59%|█████▉    | 19/32 [00:04<00:03,  4.19it/s]Inference:  62%|██████▎   | 20/32 [00:04<00:02,  4.20it/s]Inference:  66%|██████▌   | 21/32 [00:04<00:02,  4.22it/s]Inference:  69%|██████▉   | 22/32 [00:05<00:02,  4.22it/s]Inference:  72%|███████▏  | 23/32 [00:05<00:02,  4.22it/s]Inference:  75%|███████▌  | 24/32 [00:05<00:01,  4.20it/s]Inference:  78%|███████▊  | 25/32 [00:05<00:01,  4.20it/s]Inference:  81%|████████▏ | 26/32 [00:06<00:01,  4.19it/s]Inference:  84%|████████▍ | 27/32 [00:06<00:01,  4.20it/s]Inference:  88%|████████▊ | 28/32 [00:06<00:00,  4.21it/s]Inference:  91%|█████████ | 29/32 [00:06<00:00,  4.21it/s]Inference:  94%|█████████▍| 30/32 [00:07<00:00,  4.21it/s]Inference:  97%|█████████▋| 31/32 [00:07<00:00,  4.22it/s]Inference: 100%|██████████| 32/32 [00:07<00:00,  4.21it/s]Inference: 100%|██████████| 32/32 [00:07<00:00,  4.22it/s]
32273 MiB free out of 48676 MiB total
Saved layer 8 to ./models/meta-llama/Llama-2-7b-hf/tensorized/128/no_finetune2/layer_8.pt
after cast to cpu
36289 MiB free out of 48676 MiB total
Done with layer 8 total_time elapsed: 9940 estimated time left: 25402
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:15,  2.04it/s]Inference:   6%|▋         | 2/32 [00:01<00:15,  1.98it/s]Inference:   9%|▉         | 3/32 [00:01<00:14,  1.95it/s]Inference:  12%|█▎        | 4/32 [00:02<00:14,  1.94it/s]Inference:  16%|█▌        | 5/32 [00:02<00:13,  1.93it/s]Inference:  19%|█▉        | 6/32 [00:03<00:13,  1.94it/s]Inference:  22%|██▏       | 7/32 [00:03<00:12,  1.93it/s]Inference:  25%|██▌       | 8/32 [00:04<00:12,  1.94it/s]Inference:  28%|██▊       | 9/32 [00:04<00:11,  1.93it/s]Inference:  31%|███▏      | 10/32 [00:05<00:11,  1.94it/s]Inference:  34%|███▍      | 11/32 [00:05<00:10,  1.93it/s]Inference:  38%|███▊      | 12/32 [00:06<00:10,  1.93it/s]Inference:  41%|████      | 13/32 [00:06<00:09,  1.92it/s]Inference:  44%|████▍     | 14/32 [00:07<00:09,  1.93it/s]Inference:  47%|████▋     | 15/32 [00:07<00:08,  1.92it/s]Inference:  50%|█████     | 16/32 [00:08<00:08,  1.92it/s]Inference:  53%|█████▎    | 17/32 [00:08<00:07,  1.92it/s]Inference:  56%|█████▋    | 18/32 [00:09<00:07,  1.93it/s]Inference:  59%|█████▉    | 19/32 [00:09<00:06,  1.92it/s]Inference:  62%|██████▎   | 20/32 [00:10<00:06,  1.92it/s]Inference:  66%|██████▌   | 21/32 [00:10<00:05,  1.92it/s]Inference:  69%|██████▉   | 22/32 [00:11<00:05,  1.93it/s]Inference:  72%|███████▏  | 23/32 [00:11<00:04,  1.92it/s]Inference:  75%|███████▌  | 24/32 [00:12<00:04,  1.93it/s]Inference:  78%|███████▊  | 25/32 [00:12<00:03,  1.92it/s]Inference:  81%|████████▏ | 26/32 [00:13<00:03,  1.93it/s]Inference:  84%|████████▍ | 27/32 [00:13<00:02,  1.93it/s]Inference:  88%|████████▊ | 28/32 [00:14<00:02,  1.92it/s]Inference:  91%|█████████ | 29/32 [00:15<00:01,  1.92it/s]Inference:  94%|█████████▍| 30/32 [00:15<00:01,  1.93it/s]Inference:  97%|█████████▋| 31/32 [00:16<00:00,  1.92it/s]Inference: 100%|██████████| 32/32 [00:16<00:00,  1.93it/s]Inference: 100%|██████████| 32/32 [00:16<00:00,  1.93it/s]
layer9: self_attn.q_proj
iter 0, train loss 66509.7890625, val loss None, lr 0.01
iter 250, train loss 1079.418212890625, val loss None, lr 0.01
iter 500, train loss 954.2353515625, val loss None, lr 0.003333
iter 750, train loss 930.5345458984375, val loss None, lr 0.003333
iter 1000, train loss 902.90966796875, val loss None, lr 0.003333
iter 1250, train loss 872.3322143554688, val loss None, lr 0.001111
iter 1500, train loss 860.847412109375, val loss None, lr 0.001111
iter 1750, train loss 853.5098876953125, val loss None, lr 0.001111
iter 2000, train loss 845.2968139648438, val loss None, lr 0.001111
iter 2250, train loss 841.9046630859375, val loss None, lr 0.001111
best loss 836.7276611328125
layer9: self_attn.k_proj
iter 0, train loss 93603.8046875, val loss None, lr 0.01
iter 250, train loss 1111.9512939453125, val loss None, lr 0.01
iter 500, train loss 1153.63330078125, val loss None, lr 0.01
iter 750, train loss 904.8792114257812, val loss None, lr 0.003333
iter 1000, train loss 901.59228515625, val loss None, lr 0.003333
iter 1250, train loss 855.225341796875, val loss None, lr 0.001111
iter 1500, train loss 842.2058715820312, val loss None, lr 0.001111
iter 1750, train loss 836.4891967773438, val loss None, lr 0.001111
iter 2000, train loss 827.5208129882812, val loss None, lr 0.001111
iter 2250, train loss 819.3108520507812, val loss None, lr 0.00037
best loss 814.4049072265625
layer9: self_attn.v_proj
iter 0, train loss 2566.02587890625, val loss None, lr 0.01
iter 250, train loss 234.76771545410156, val loss None, lr 0.01
iter 500, train loss 224.54904174804688, val loss None, lr 0.01
iter 750, train loss 218.15309143066406, val loss None, lr 0.01
iter 1000, train loss 212.02011108398438, val loss None, lr 0.01
iter 1250, train loss 210.37149047851562, val loss None, lr 0.003333
iter 1500, train loss 209.72508239746094, val loss None, lr 0.003333
iter 1750, train loss 207.38746643066406, val loss None, lr 0.001111
iter 2000, train loss 206.69216918945312, val loss None, lr 0.001111
iter 2250, train loss 206.20004272460938, val loss None, lr 0.001111
best loss 205.8131103515625
layer9: self_attn.o_proj
iter 0, train loss 872.5439453125, val loss None, lr 0.01
iter 250, train loss 34.21055603027344, val loss None, lr 0.01
iter 500, train loss 29.89741325378418, val loss None, lr 0.01
iter 750, train loss 28.26943588256836, val loss None, lr 0.01
iter 1000, train loss 25.911468505859375, val loss None, lr 0.003333
iter 1250, train loss 25.38144302368164, val loss None, lr 0.001111
iter 1500, train loss 25.13308334350586, val loss None, lr 0.001111
iter 1750, train loss 24.967483520507812, val loss None, lr 0.001111
iter 2000, train loss 24.867216110229492, val loss None, lr 0.001111
iter 2250, train loss 24.673683166503906, val loss None, lr 0.00037
best loss 24.531808853149414
layer9: mlp.gate_proj
iter 0, train loss 16673.93359375, val loss None, lr 0.01
iter 250, train loss 839.9195556640625, val loss None, lr 0.01
iter 500, train loss 788.0032348632812, val loss None, lr 0.01
iter 750, train loss 769.9725341796875, val loss None, lr 0.01
iter 1000, train loss 758.9302978515625, val loss None, lr 0.01
iter 1250, train loss 739.1177978515625, val loss None, lr 0.003333
iter 1500, train loss 731.6341552734375, val loss None, lr 0.003333
iter 1750, train loss 723.7304077148438, val loss None, lr 0.001111
iter 2000, train loss 721.5819091796875, val loss None, lr 0.001111
iter 2250, train loss 719.2584838867188, val loss None, lr 0.001111
best loss 717.9143676757812
layer9: mlp.up_proj
iter 0, train loss 7048.65625, val loss None, lr 0.01
iter 250, train loss 630.2047119140625, val loss None, lr 0.01
iter 500, train loss 609.6323852539062, val loss None, lr 0.01
iter 750, train loss 596.929931640625, val loss None, lr 0.01
iter 1000, train loss 596.1755981445312, val loss None, lr 0.01
iter 1250, train loss 587.9043579101562, val loss None, lr 0.01
iter 1500, train loss 578.156005859375, val loss None, lr 0.003333
iter 1750, train loss 574.071533203125, val loss None, lr 0.001111
iter 2000, train loss 572.4681396484375, val loss None, lr 0.001111
iter 2250, train loss 571.2774658203125, val loss None, lr 0.001111
best loss 570.1220703125
layer9: mlp.down_proj
iter 0, train loss 146.28736877441406, val loss None, lr 0.01
iter 250, train loss 12.119263648986816, val loss None, lr 0.01
iter 500, train loss 11.543793678283691, val loss None, lr 0.01
iter 750, train loss 10.71946907043457, val loss None, lr 0.01
iter 1000, train loss 10.191720962524414, val loss None, lr 0.003333
iter 1250, train loss 10.107759475708008, val loss None, lr 0.003333
iter 1500, train loss 10.011759757995605, val loss None, lr 0.003333
iter 1750, train loss 10.202719688415527, val loss None, lr 0.001111
iter 2000, train loss 9.895462036132812, val loss None, lr 0.001111
iter 2250, train loss 9.880586624145508, val loss None, lr 0.001111
best loss 9.852325439453125
36289 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:06,  4.89it/s]Inference:   6%|▋         | 2/32 [00:00<00:06,  4.46it/s]Inference:   9%|▉         | 3/32 [00:00<00:06,  4.36it/s]Inference:  12%|█▎        | 4/32 [00:00<00:06,  4.32it/s]Inference:  16%|█▌        | 5/32 [00:01<00:06,  4.28it/s]Inference:  19%|█▉        | 6/32 [00:01<00:06,  4.23it/s]Inference:  22%|██▏       | 7/32 [00:01<00:05,  4.24it/s]Inference:  25%|██▌       | 8/32 [00:01<00:05,  4.22it/s]Inference:  28%|██▊       | 9/32 [00:02<00:05,  4.20it/s]Inference:  31%|███▏      | 10/32 [00:02<00:05,  4.20it/s]Inference:  34%|███▍      | 11/32 [00:02<00:05,  4.19it/s]Inference:  38%|███▊      | 12/32 [00:02<00:04,  4.22it/s]Inference:  41%|████      | 13/32 [00:03<00:04,  4.23it/s]Inference:  44%|████▍     | 14/32 [00:03<00:04,  4.23it/s]Inference:  47%|████▋     | 15/32 [00:03<00:04,  4.21it/s]Inference:  50%|█████     | 16/32 [00:03<00:03,  4.22it/s]Inference:  53%|█████▎    | 17/32 [00:04<00:03,  4.20it/s]Inference:  56%|█████▋    | 18/32 [00:04<00:03,  4.41it/s]Inference:  59%|█████▉    | 19/32 [00:04<00:02,  4.36it/s]Inference:  62%|██████▎   | 20/32 [00:04<00:02,  4.31it/s]Inference:  66%|██████▌   | 21/32 [00:04<00:02,  4.30it/s]Inference:  69%|██████▉   | 22/32 [00:05<00:02,  4.28it/s]Inference:  72%|███████▏  | 23/32 [00:05<00:02,  4.27it/s]Inference:  75%|███████▌  | 24/32 [00:05<00:01,  4.24it/s]Inference:  78%|███████▊  | 25/32 [00:05<00:01,  4.23it/s]Inference:  81%|████████▏ | 26/32 [00:06<00:01,  4.23it/s]Inference:  84%|████████▍ | 27/32 [00:06<00:01,  4.21it/s]Inference:  88%|████████▊ | 28/32 [00:06<00:00,  4.21it/s]Inference:  91%|█████████ | 29/32 [00:06<00:00,  4.24it/s]Inference:  94%|█████████▍| 30/32 [00:07<00:00,  4.23it/s]Inference:  97%|█████████▋| 31/32 [00:07<00:00,  4.24it/s]Inference: 100%|██████████| 32/32 [00:07<00:00,  4.23it/s]Inference: 100%|██████████| 32/32 [00:07<00:00,  4.26it/s]
31049 MiB free out of 48676 MiB total
Saved layer 9 to ./models/meta-llama/Llama-2-7b-hf/tensorized/128/no_finetune2/layer_9.pt
after cast to cpu
35065 MiB free out of 48676 MiB total
Done with layer 9 total_time elapsed: 10806 estimated time left: 23774
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:15,  1.95it/s]Inference:   6%|▋         | 2/32 [00:01<00:15,  1.93it/s]Inference:   9%|▉         | 3/32 [00:01<00:14,  1.93it/s]Inference:  12%|█▎        | 4/32 [00:02<00:14,  1.93it/s]Inference:  16%|█▌        | 5/32 [00:02<00:14,  1.93it/s]Inference:  19%|█▉        | 6/32 [00:03<00:13,  1.92it/s]Inference:  22%|██▏       | 7/32 [00:03<00:13,  1.92it/s]Inference:  25%|██▌       | 8/32 [00:04<00:12,  1.93it/s]Inference:  28%|██▊       | 9/32 [00:04<00:11,  1.96it/s]Inference:  31%|███▏      | 10/32 [00:05<00:11,  1.95it/s]Inference:  34%|███▍      | 11/32 [00:05<00:10,  1.94it/s]Inference:  38%|███▊      | 12/32 [00:06<00:10,  1.94it/s]Inference:  41%|████      | 13/32 [00:06<00:09,  1.93it/s]Inference:  44%|████▍     | 14/32 [00:07<00:09,  1.93it/s]Inference:  47%|████▋     | 15/32 [00:07<00:08,  1.92it/s]Inference:  50%|█████     | 16/32 [00:08<00:08,  1.92it/s]Inference:  53%|█████▎    | 17/32 [00:08<00:07,  1.92it/s]Inference:  56%|█████▋    | 18/32 [00:09<00:07,  1.92it/s]Inference:  59%|█████▉    | 19/32 [00:09<00:06,  1.92it/s]Inference:  62%|██████▎   | 20/32 [00:10<00:06,  1.92it/s]Inference:  66%|██████▌   | 21/32 [00:10<00:05,  1.92it/s]Inference:  69%|██████▉   | 22/32 [00:11<00:05,  1.92it/s]Inference:  72%|███████▏  | 23/32 [00:11<00:04,  1.92it/s]Inference:  75%|███████▌  | 24/32 [00:12<00:04,  1.92it/s]Inference:  78%|███████▊  | 25/32 [00:12<00:03,  1.92it/s]Inference:  81%|████████▏ | 26/32 [00:13<00:03,  1.92it/s]Inference:  84%|████████▍ | 27/32 [00:14<00:02,  1.92it/s]Inference:  88%|████████▊ | 28/32 [00:14<00:02,  1.92it/s]Inference:  91%|█████████ | 29/32 [00:15<00:01,  1.92it/s]Inference:  94%|█████████▍| 30/32 [00:15<00:01,  1.92it/s]Inference:  97%|█████████▋| 31/32 [00:16<00:00,  1.91it/s]Inference: 100%|██████████| 32/32 [00:16<00:00,  1.92it/s]Inference: 100%|██████████| 32/32 [00:16<00:00,  1.92it/s]
layer10: self_attn.q_proj
iter 0, train loss 65129.4921875, val loss None, lr 0.01
iter 250, train loss 1067.8814697265625, val loss None, lr 0.01
iter 500, train loss 960.091552734375, val loss None, lr 0.01
iter 750, train loss 859.2396240234375, val loss None, lr 0.003333
iter 1000, train loss 850.448974609375, val loss None, lr 0.003333
iter 1250, train loss 870.4935913085938, val loss None, lr 0.003333
iter 1500, train loss 807.5845947265625, val loss None, lr 0.001111
iter 1750, train loss 799.885986328125, val loss None, lr 0.001111
iter 2000, train loss 795.8825073242188, val loss None, lr 0.001111
iter 2250, train loss 791.0569458007812, val loss None, lr 0.001111
best loss 787.34619140625
layer10: self_attn.k_proj
iter 0, train loss 91151.9453125, val loss None, lr 0.01
iter 250, train loss 1101.664794921875, val loss None, lr 0.01
iter 500, train loss 960.8428955078125, val loss None, lr 0.01
iter 750, train loss 887.892822265625, val loss None, lr 0.003333
iter 1000, train loss 842.620361328125, val loss None, lr 0.001111
iter 1250, train loss 826.530029296875, val loss None, lr 0.001111
iter 1500, train loss 814.0626220703125, val loss None, lr 0.001111
iter 1750, train loss 807.5167236328125, val loss None, lr 0.001111
iter 2000, train loss 803.5245361328125, val loss None, lr 0.001111
iter 2250, train loss 797.189453125, val loss None, lr 0.001111
best loss 790.110107421875
layer10: self_attn.v_proj
iter 0, train loss 2509.349853515625, val loss None, lr 0.01
iter 250, train loss 231.1309814453125, val loss None, lr 0.01
iter 500, train loss 218.8318634033203, val loss None, lr 0.01
iter 750, train loss 213.96871948242188, val loss None, lr 0.01
iter 1000, train loss 211.7483673095703, val loss None, lr 0.01
iter 1250, train loss 207.75357055664062, val loss None, lr 0.01
iter 1500, train loss 208.22164916992188, val loss None, lr 0.01
iter 1750, train loss 203.99217224121094, val loss None, lr 0.003333
iter 2000, train loss 203.6243896484375, val loss None, lr 0.003333
iter 2250, train loss 202.17117309570312, val loss None, lr 0.001111
best loss 201.87258911132812
layer10: self_attn.o_proj
iter 0, train loss 826.1358642578125, val loss None, lr 0.01
iter 250, train loss 35.37805938720703, val loss None, lr 0.01
iter 500, train loss 33.06005859375, val loss None, lr 0.01
iter 750, train loss 31.079177856445312, val loss None, lr 0.01
iter 1000, train loss 29.235319137573242, val loss None, lr 0.003333
iter 1250, train loss 28.75382423400879, val loss None, lr 0.003333
iter 1500, train loss 28.556900024414062, val loss None, lr 0.003333
iter 1750, train loss 28.82166290283203, val loss None, lr 0.001111
iter 2000, train loss 27.950302124023438, val loss None, lr 0.001111
iter 2250, train loss 27.858135223388672, val loss None, lr 0.001111
best loss 27.796112060546875
layer10: mlp.gate_proj
iter 0, train loss 15544.654296875, val loss None, lr 0.01
iter 250, train loss 808.228759765625, val loss None, lr 0.01
iter 500, train loss 773.672119140625, val loss None, lr 0.01
iter 750, train loss 736.3823852539062, val loss None, lr 0.003333
iter 1000, train loss 730.161376953125, val loss None, lr 0.003333
iter 1250, train loss 718.8757934570312, val loss None, lr 0.001111
iter 1500, train loss 715.5810546875, val loss None, lr 0.001111
iter 1750, train loss 712.1038818359375, val loss None, lr 0.001111
iter 2000, train loss 709.785888671875, val loss None, lr 0.001111
iter 2250, train loss 707.5968017578125, val loss None, lr 0.00037
best loss 706.0472412109375
layer10: mlp.up_proj
iter 0, train loss 7373.77001953125, val loss None, lr 0.01
iter 250, train loss 643.100830078125, val loss None, lr 0.01
iter 500, train loss 623.2412109375, val loss None, lr 0.01
iter 750, train loss 619.784423828125, val loss None, lr 0.01
iter 1000, train loss 603.578857421875, val loss None, lr 0.01
iter 1250, train loss 594.512939453125, val loss None, lr 0.003333
iter 1500, train loss 589.4327392578125, val loss None, lr 0.003333
iter 1750, train loss 587.378662109375, val loss None, lr 0.001111
iter 2000, train loss 586.0040283203125, val loss None, lr 0.001111
iter 2250, train loss 584.857177734375, val loss None, lr 0.001111
best loss 583.74267578125
layer10: mlp.down_proj
iter 0, train loss 149.99307250976562, val loss None, lr 0.01
iter 250, train loss 12.456830024719238, val loss None, lr 0.01
iter 500, train loss 12.223456382751465, val loss None, lr 0.01
iter 750, train loss 11.660374641418457, val loss None, lr 0.01
iter 1000, train loss 10.908463478088379, val loss None, lr 0.003333
iter 1250, train loss 10.846692085266113, val loss None, lr 0.003333
iter 1500, train loss 10.821048736572266, val loss None, lr 0.003333
iter 1750, train loss 10.66845989227295, val loss None, lr 0.003333
iter 2000, train loss 10.622512817382812, val loss None, lr 0.001111
iter 2250, train loss 10.591144561767578, val loss None, lr 0.001111
best loss 10.570353507995605
35065 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:06,  4.96it/s]Inference:   6%|▋         | 2/32 [00:00<00:06,  4.52it/s]Inference:   9%|▉         | 3/32 [00:00<00:06,  4.35it/s]Inference:  12%|█▎        | 4/32 [00:00<00:06,  4.30it/s]Inference:  16%|█▌        | 5/32 [00:01<00:06,  4.26it/s]Inference:  19%|█▉        | 6/32 [00:01<00:06,  4.29it/s]Inference:  22%|██▏       | 7/32 [00:01<00:05,  4.28it/s]Inference:  25%|██▌       | 8/32 [00:01<00:05,  4.27it/s]Inference:  28%|██▊       | 9/32 [00:02<00:05,  4.25it/s]Inference:  31%|███▏      | 10/32 [00:02<00:05,  4.26it/s]Inference:  34%|███▍      | 11/32 [00:02<00:04,  4.24it/s]Inference:  38%|███▊      | 12/32 [00:02<00:04,  4.25it/s]Inference:  41%|████      | 13/32 [00:02<00:04,  4.46it/s]Inference:  44%|████▍     | 14/32 [00:03<00:04,  4.38it/s]Inference:  47%|████▋     | 15/32 [00:03<00:03,  4.34it/s]Inference:  50%|█████     | 16/32 [00:03<00:03,  4.30it/s]Inference:  53%|█████▎    | 17/32 [00:03<00:03,  4.28it/s]Inference:  56%|█████▋    | 18/32 [00:04<00:03,  4.24it/s]Inference:  59%|█████▉    | 19/32 [00:04<00:03,  4.24it/s]Inference:  62%|██████▎   | 20/32 [00:04<00:02,  4.22it/s]Inference:  66%|██████▌   | 21/32 [00:04<00:02,  4.21it/s]Inference:  69%|██████▉   | 22/32 [00:05<00:02,  4.20it/s]Inference:  72%|███████▏  | 23/32 [00:05<00:02,  4.23it/s]Inference:  75%|███████▌  | 24/32 [00:05<00:01,  4.24it/s]Inference:  78%|███████▊  | 25/32 [00:05<00:01,  4.24it/s]Inference:  81%|████████▏ | 26/32 [00:06<00:01,  4.22it/s]Inference:  84%|████████▍ | 27/32 [00:06<00:01,  4.21it/s]Inference:  88%|████████▊ | 28/32 [00:06<00:00,  4.21it/s]Inference:  91%|█████████ | 29/32 [00:06<00:00,  4.19it/s]Inference:  94%|█████████▍| 30/32 [00:07<00:00,  4.21it/s]Inference:  97%|█████████▋| 31/32 [00:07<00:00,  4.19it/s]Inference: 100%|██████████| 32/32 [00:07<00:00,  4.22it/s]Inference: 100%|██████████| 32/32 [00:07<00:00,  4.26it/s]
29761 MiB free out of 48676 MiB total
Saved layer 10 to ./models/meta-llama/Llama-2-7b-hf/tensorized/128/no_finetune2/layer_10.pt
after cast to cpu
33777 MiB free out of 48676 MiB total
Done with layer 10 total_time elapsed: 11673 estimated time left: 22284
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:15,  1.95it/s]Inference:   6%|▋         | 2/32 [00:01<00:15,  1.94it/s]Inference:   9%|▉         | 3/32 [00:01<00:15,  1.93it/s]Inference:  12%|█▎        | 4/32 [00:02<00:14,  1.93it/s]Inference:  16%|█▌        | 5/32 [00:02<00:13,  1.93it/s]Inference:  19%|█▉        | 6/32 [00:03<00:13,  1.93it/s]Inference:  22%|██▏       | 7/32 [00:03<00:12,  1.93it/s]Inference:  25%|██▌       | 8/32 [00:04<00:12,  1.94it/s]Inference:  28%|██▊       | 9/32 [00:04<00:11,  1.94it/s]Inference:  31%|███▏      | 10/32 [00:05<00:11,  1.93it/s]Inference:  34%|███▍      | 11/32 [00:05<00:10,  1.93it/s]Inference:  38%|███▊      | 12/32 [00:06<00:10,  1.94it/s]Inference:  41%|████      | 13/32 [00:06<00:09,  1.94it/s]Inference:  44%|████▍     | 14/32 [00:07<00:09,  1.94it/s]Inference:  47%|████▋     | 15/32 [00:07<00:08,  1.93it/s]Inference:  50%|█████     | 16/32 [00:08<00:08,  1.93it/s]Inference:  53%|█████▎    | 17/32 [00:08<00:07,  1.93it/s]Inference:  56%|█████▋    | 18/32 [00:09<00:07,  1.93it/s]Inference:  59%|█████▉    | 19/32 [00:09<00:06,  1.93it/s]Inference:  62%|██████▎   | 20/32 [00:10<00:06,  1.92it/s]Inference:  66%|██████▌   | 21/32 [00:10<00:05,  1.92it/s]Inference:  69%|██████▉   | 22/32 [00:11<00:05,  1.92it/s]Inference:  72%|███████▏  | 23/32 [00:11<00:04,  1.92it/s]Inference:  75%|███████▌  | 24/32 [00:12<00:04,  1.92it/s]Inference:  78%|███████▊  | 25/32 [00:12<00:03,  1.92it/s]Inference:  81%|████████▏ | 26/32 [00:13<00:03,  1.92it/s]Inference:  84%|████████▍ | 27/32 [00:13<00:02,  1.93it/s]Inference:  88%|████████▊ | 28/32 [00:14<00:02,  1.93it/s]Inference:  91%|█████████ | 29/32 [00:15<00:01,  1.93it/s]Inference:  94%|█████████▍| 30/32 [00:15<00:01,  1.96it/s]Inference:  97%|█████████▋| 31/32 [00:16<00:00,  1.95it/s]Inference: 100%|██████████| 32/32 [00:16<00:00,  1.94it/s]Inference: 100%|██████████| 32/32 [00:16<00:00,  1.93it/s]
layer11: self_attn.q_proj
iter 0, train loss 63423.56640625, val loss None, lr 0.01
iter 250, train loss 1031.8397216796875, val loss None, lr 0.01
iter 500, train loss 945.1309814453125, val loss None, lr 0.01
iter 750, train loss 882.516357421875, val loss None, lr 0.003333
iter 1000, train loss 837.459716796875, val loss None, lr 0.003333
iter 1250, train loss 822.2403564453125, val loss None, lr 0.001111
iter 1500, train loss 813.0697021484375, val loss None, lr 0.001111
iter 1750, train loss 807.2752075195312, val loss None, lr 0.001111
iter 2000, train loss 801.1712036132812, val loss None, lr 0.001111
iter 2250, train loss 797.21923828125, val loss None, lr 0.001111
best loss 791.0926513671875
layer11: self_attn.k_proj
iter 0, train loss 89607.734375, val loss None, lr 0.01
iter 250, train loss 1070.7994384765625, val loss None, lr 0.01
iter 500, train loss 964.404541015625, val loss None, lr 0.01
iter 750, train loss 847.3091430664062, val loss None, lr 0.003333
iter 1000, train loss 839.9071044921875, val loss None, lr 0.003333
iter 1250, train loss 835.6253662109375, val loss None, lr 0.001111
iter 1500, train loss 789.6046752929688, val loss None, lr 0.001111
iter 1750, train loss 782.2446899414062, val loss None, lr 0.001111
iter 2000, train loss 773.8311157226562, val loss None, lr 0.001111
iter 2250, train loss 767.7433471679688, val loss None, lr 0.00037
best loss 763.5367431640625
layer11: self_attn.v_proj
iter 0, train loss 3490.74755859375, val loss None, lr 0.01
iter 250, train loss 290.1785583496094, val loss None, lr 0.01
iter 500, train loss 272.8846435546875, val loss None, lr 0.01
iter 750, train loss 263.7061767578125, val loss None, lr 0.003333
iter 1000, train loss 260.7273864746094, val loss None, lr 0.003333
iter 1250, train loss 259.3447265625, val loss None, lr 0.003333
iter 1500, train loss 256.74322509765625, val loss None, lr 0.001111
iter 1750, train loss 255.67457580566406, val loss None, lr 0.001111
iter 2000, train loss 254.73797607421875, val loss None, lr 0.001111
iter 2250, train loss 254.19276428222656, val loss None, lr 0.001111
best loss 253.4953155517578
layer11: self_attn.o_proj
iter 0, train loss 1260.888916015625, val loss None, lr 0.01
iter 250, train loss 50.56770324707031, val loss None, lr 0.01
iter 500, train loss 46.0197868347168, val loss None, lr 0.01
iter 750, train loss 43.46764373779297, val loss None, lr 0.01
iter 1000, train loss 40.276878356933594, val loss None, lr 0.003333
iter 1250, train loss 40.09689712524414, val loss None, lr 0.003333
iter 1500, train loss 39.858768463134766, val loss None, lr 0.003333
iter 1750, train loss 38.702274322509766, val loss None, lr 0.001111
iter 2000, train loss 38.42688751220703, val loss None, lr 0.001111
iter 2250, train loss 38.235992431640625, val loss None, lr 0.001111
best loss 38.10359573364258
layer11: mlp.gate_proj
iter 0, train loss 16423.3359375, val loss None, lr 0.01
iter 250, train loss 824.1976928710938, val loss None, lr 0.01
iter 500, train loss 769.562744140625, val loss None, lr 0.01
iter 750, train loss 754.4698486328125, val loss None, lr 0.01
iter 1000, train loss 730.8038330078125, val loss None, lr 0.003333
iter 1250, train loss 717.6619873046875, val loss None, lr 0.001111
iter 1500, train loss 714.13916015625, val loss None, lr 0.001111
iter 1750, train loss 711.2546997070312, val loss None, lr 0.001111
iter 2000, train loss 709.271728515625, val loss None, lr 0.001111
iter 2250, train loss 708.7896118164062, val loss None, lr 0.001111
best loss 705.40673828125
layer11: mlp.up_proj
iter 0, train loss 7717.7744140625, val loss None, lr 0.01
iter 250, train loss 662.9229736328125, val loss None, lr 0.01
iter 500, train loss 640.3565673828125, val loss None, lr 0.01
iter 750, train loss 625.5382080078125, val loss None, lr 0.01
iter 1000, train loss 623.973388671875, val loss None, lr 0.01
iter 1250, train loss 639.8104248046875, val loss None, lr 0.01
iter 1500, train loss 607.5423583984375, val loss None, lr 0.003333
iter 1750, train loss 604.0927734375, val loss None, lr 0.003333
iter 2000, train loss 601.6550903320312, val loss None, lr 0.001111
iter 2250, train loss 600.1165771484375, val loss None, lr 0.001111
best loss 599.1192016601562
layer11: mlp.down_proj
iter 0, train loss 153.06976318359375, val loss None, lr 0.01
iter 250, train loss 13.059242248535156, val loss None, lr 0.01
iter 500, train loss 12.272859573364258, val loss None, lr 0.01
iter 750, train loss 11.427921295166016, val loss None, lr 0.01
iter 1000, train loss 11.287524223327637, val loss None, lr 0.01
iter 1250, train loss 10.98770523071289, val loss None, lr 0.003333
iter 1500, train loss 11.04671573638916, val loss None, lr 0.003333
iter 1750, train loss 10.80799674987793, val loss None, lr 0.001111
iter 2000, train loss 10.759092330932617, val loss None, lr 0.001111
iter 2250, train loss 10.724493026733398, val loss None, lr 0.001111
best loss 10.69355583190918
33777 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:06,  4.88it/s]Inference:   6%|▋         | 2/32 [00:00<00:06,  4.53it/s]Inference:   9%|▉         | 3/32 [00:00<00:06,  4.38it/s]Inference:  12%|█▎        | 4/32 [00:00<00:06,  4.32it/s]Inference:  16%|█▌        | 5/32 [00:01<00:06,  4.31it/s]Inference:  19%|█▉        | 6/32 [00:01<00:06,  4.29it/s]Inference:  22%|██▏       | 7/32 [00:01<00:05,  4.29it/s]Inference:  25%|██▌       | 8/32 [00:01<00:05,  4.28it/s]Inference:  28%|██▊       | 9/32 [00:02<00:05,  4.29it/s]Inference:  31%|███▏      | 10/32 [00:02<00:05,  4.26it/s]Inference:  34%|███▍      | 11/32 [00:02<00:04,  4.27it/s]Inference:  38%|███▊      | 12/32 [00:02<00:04,  4.25it/s]Inference:  41%|████      | 13/32 [00:03<00:04,  4.26it/s]Inference:  44%|████▍     | 14/32 [00:03<00:04,  4.25it/s]Inference:  47%|████▋     | 15/32 [00:03<00:03,  4.26it/s]Inference:  50%|█████     | 16/32 [00:03<00:03,  4.26it/s]Inference:  53%|█████▎    | 17/32 [00:03<00:03,  4.25it/s]Inference:  56%|█████▋    | 18/32 [00:04<00:03,  4.24it/s]Inference:  59%|█████▉    | 19/32 [00:04<00:03,  4.21it/s]Inference:  62%|██████▎   | 20/32 [00:04<00:02,  4.22it/s]Inference:  66%|██████▌   | 21/32 [00:04<00:02,  4.22it/s]Inference:  69%|██████▉   | 22/32 [00:05<00:02,  4.22it/s]Inference:  72%|███████▏  | 23/32 [00:05<00:02,  4.21it/s]Inference:  75%|███████▌  | 24/32 [00:05<00:01,  4.23it/s]Inference:  78%|███████▊  | 25/32 [00:05<00:01,  4.23it/s]Inference:  81%|████████▏ | 26/32 [00:06<00:01,  4.24it/s]Inference:  84%|████████▍ | 27/32 [00:06<00:01,  4.22it/s]Inference:  88%|████████▊ | 28/32 [00:06<00:00,  4.22it/s]Inference:  91%|█████████ | 29/32 [00:06<00:00,  4.22it/s]Inference:  94%|█████████▍| 30/32 [00:07<00:00,  4.24it/s]Inference:  97%|█████████▋| 31/32 [00:07<00:00,  4.22it/s]Inference: 100%|██████████| 32/32 [00:07<00:00,  4.24it/s]Inference: 100%|██████████| 32/32 [00:07<00:00,  4.26it/s]
28473 MiB free out of 48676 MiB total
Saved layer 11 to ./models/meta-llama/Llama-2-7b-hf/tensorized/128/no_finetune2/layer_11.pt
after cast to cpu
32489 MiB free out of 48676 MiB total
Done with layer 11 total_time elapsed: 12539 estimated time left: 20899
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:15,  1.96it/s]Inference:   6%|▋         | 2/32 [00:01<00:15,  1.95it/s]Inference:   9%|▉         | 3/32 [00:01<00:14,  1.94it/s]Inference:  12%|█▎        | 4/32 [00:02<00:14,  1.94it/s]Inference:  16%|█▌        | 5/32 [00:02<00:13,  1.98it/s]Inference:  19%|█▉        | 6/32 [00:03<00:13,  1.96it/s]Inference:  22%|██▏       | 7/32 [00:03<00:12,  1.95it/s]Inference:  25%|██▌       | 8/32 [00:04<00:12,  1.95it/s]Inference:  28%|██▊       | 9/32 [00:04<00:11,  1.95it/s]Inference:  31%|███▏      | 10/32 [00:05<00:11,  1.94it/s]Inference:  34%|███▍      | 11/32 [00:05<00:10,  1.94it/s]Inference:  38%|███▊      | 12/32 [00:06<00:10,  1.94it/s]Inference:  41%|████      | 13/32 [00:06<00:09,  1.94it/s]Inference:  44%|████▍     | 14/32 [00:07<00:09,  1.94it/s]Inference:  47%|████▋     | 15/32 [00:07<00:08,  1.94it/s]Inference:  50%|█████     | 16/32 [00:08<00:08,  1.93it/s]Inference:  53%|█████▎    | 17/32 [00:08<00:07,  1.93it/s]Inference:  56%|█████▋    | 18/32 [00:09<00:07,  1.93it/s]Inference:  59%|█████▉    | 19/32 [00:09<00:06,  1.93it/s]Inference:  62%|██████▎   | 20/32 [00:10<00:06,  1.94it/s]Inference:  66%|██████▌   | 21/32 [00:10<00:05,  1.94it/s]Inference:  69%|██████▉   | 22/32 [00:11<00:05,  1.93it/s]Inference:  72%|███████▏  | 23/32 [00:11<00:04,  1.94it/s]Inference:  75%|███████▌  | 24/32 [00:12<00:04,  1.94it/s]Inference:  78%|███████▊  | 25/32 [00:12<00:03,  1.93it/s]Inference:  81%|████████▏ | 26/32 [00:13<00:03,  1.93it/s]Inference:  84%|████████▍ | 27/32 [00:13<00:02,  1.93it/s]Inference:  88%|████████▊ | 28/32 [00:14<00:02,  1.93it/s]Inference:  91%|█████████ | 29/32 [00:14<00:01,  1.93it/s]Inference:  94%|█████████▍| 30/32 [00:15<00:01,  1.93it/s]Inference:  97%|█████████▋| 31/32 [00:15<00:00,  1.93it/s]Inference: 100%|██████████| 32/32 [00:16<00:00,  1.94it/s]Inference: 100%|██████████| 32/32 [00:16<00:00,  1.94it/s]
layer12: self_attn.q_proj
iter 0, train loss 65026.20703125, val loss None, lr 0.01
iter 250, train loss 1004.1031494140625, val loss None, lr 0.01
iter 500, train loss 909.702392578125, val loss None, lr 0.01
iter 750, train loss 904.1268920898438, val loss None, lr 0.01
iter 1000, train loss 819.1566162109375, val loss None, lr 0.003333
iter 1250, train loss 799.00244140625, val loss None, lr 0.001111
iter 1500, train loss 787.396728515625, val loss None, lr 0.001111
iter 1750, train loss 782.4560546875, val loss None, lr 0.001111
iter 2000, train loss 780.0379638671875, val loss None, lr 0.001111
iter 2250, train loss 771.6423950195312, val loss None, lr 0.00037
best loss 768.1954345703125
layer12: self_attn.k_proj
iter 0, train loss 86602.46875, val loss None, lr 0.01
iter 250, train loss 1206.8419189453125, val loss None, lr 0.01
iter 500, train loss 927.117431640625, val loss None, lr 0.003333
iter 750, train loss 873.0074462890625, val loss None, lr 0.003333
iter 1000, train loss 839.8233642578125, val loss None, lr 0.003333
iter 1250, train loss 846.1094970703125, val loss None, lr 0.003333
iter 1500, train loss 814.4664916992188, val loss None, lr 0.001111
iter 1750, train loss 806.299560546875, val loss None, lr 0.001111
iter 2000, train loss 802.4254150390625, val loss None, lr 0.001111
iter 2250, train loss 793.143798828125, val loss None, lr 0.001111
best loss 785.8384399414062
layer12: self_attn.v_proj
iter 0, train loss 3240.560302734375, val loss None, lr 0.01
iter 250, train loss 263.4264831542969, val loss None, lr 0.01
iter 500, train loss 250.8111572265625, val loss None, lr 0.01
iter 750, train loss 244.00128173828125, val loss None, lr 0.01
iter 1000, train loss 239.380126953125, val loss None, lr 0.01
iter 1250, train loss 241.85662841796875, val loss None, lr 0.01
iter 1500, train loss 233.34060668945312, val loss None, lr 0.003333
iter 1750, train loss 232.48348999023438, val loss None, lr 0.003333
iter 2000, train loss 230.68563842773438, val loss None, lr 0.001111
iter 2250, train loss 230.24142456054688, val loss None, lr 0.001111
best loss 229.7672119140625
layer12: self_attn.o_proj
iter 0, train loss 1113.014404296875, val loss None, lr 0.01
iter 250, train loss 39.20796203613281, val loss None, lr 0.01
iter 500, train loss 39.070640563964844, val loss None, lr 0.01
iter 750, train loss 32.943180084228516, val loss None, lr 0.003333
iter 1000, train loss 32.37470245361328, val loss None, lr 0.001111
iter 1250, train loss 31.549198150634766, val loss None, lr 0.001111
iter 1500, train loss 31.216571807861328, val loss None, lr 0.001111
iter 1750, train loss 30.906944274902344, val loss None, lr 0.001111
iter 2000, train loss 30.809494018554688, val loss None, lr 0.001111
iter 2250, train loss 30.645183563232422, val loss None, lr 0.001111
best loss 30.30899429321289
layer12: mlp.gate_proj
iter 0, train loss 14877.76953125, val loss None, lr 0.01
iter 250, train loss 801.8236694335938, val loss None, lr 0.01
iter 500, train loss 752.49609375, val loss None, lr 0.01
iter 750, train loss 719.2347412109375, val loss None, lr 0.003333
iter 1000, train loss 706.7332153320312, val loss None, lr 0.003333
iter 1250, train loss 702.8465576171875, val loss None, lr 0.003333
iter 1500, train loss 695.8062133789062, val loss None, lr 0.001111
iter 1750, train loss 693.1053466796875, val loss None, lr 0.001111
iter 2000, train loss 691.1009521484375, val loss None, lr 0.001111
iter 2250, train loss 689.8831176757812, val loss None, lr 0.001111
best loss 687.3426513671875
layer12: mlp.up_proj
iter 0, train loss 7541.53173828125, val loss None, lr 0.01
iter 250, train loss 693.8172607421875, val loss None, lr 0.01
iter 500, train loss 666.576904296875, val loss None, lr 0.01
iter 750, train loss 646.3373413085938, val loss None, lr 0.01
iter 1000, train loss 632.860595703125, val loss None, lr 0.003333
iter 1250, train loss 625.9365234375, val loss None, lr 0.001111
iter 1500, train loss 624.0301513671875, val loss None, lr 0.001111
iter 1750, train loss 622.207275390625, val loss None, lr 0.001111
iter 2000, train loss 620.6199340820312, val loss None, lr 0.001111
iter 2250, train loss 619.7718505859375, val loss None, lr 0.001111
best loss 618.5677490234375
layer12: mlp.down_proj
iter 0, train loss 136.4637451171875, val loss None, lr 0.01
iter 250, train loss 12.306865692138672, val loss None, lr 0.01
iter 500, train loss 11.651962280273438, val loss None, lr 0.01
iter 750, train loss 11.739449501037598, val loss None, lr 0.01
iter 1000, train loss 10.95557689666748, val loss None, lr 0.003333
iter 1250, train loss 10.872356414794922, val loss None, lr 0.001111
iter 1500, train loss 10.825942993164062, val loss None, lr 0.001111
iter 1750, train loss 10.780027389526367, val loss None, lr 0.001111
iter 2000, train loss 10.756683349609375, val loss None, lr 0.001111
iter 2250, train loss 10.729975700378418, val loss None, lr 0.001111
best loss 10.696853637695312
32489 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:06,  4.72it/s]Inference:   6%|▋         | 2/32 [00:00<00:06,  4.37it/s]Inference:   9%|▉         | 3/32 [00:00<00:06,  4.25it/s]Inference:  12%|█▎        | 4/32 [00:00<00:06,  4.18it/s]Inference:  16%|█▌        | 5/32 [00:01<00:06,  4.17it/s]Inference:  19%|█▉        | 6/32 [00:01<00:06,  4.15it/s]Inference:  22%|██▏       | 7/32 [00:01<00:06,  4.14it/s]Inference:  25%|██▌       | 8/32 [00:01<00:05,  4.13it/s]Inference:  28%|██▊       | 9/32 [00:02<00:05,  4.13it/s]Inference:  31%|███▏      | 10/32 [00:02<00:05,  4.13it/s]Inference:  34%|███▍      | 11/32 [00:02<00:04,  4.33it/s]Inference:  38%|███▊      | 12/32 [00:02<00:04,  4.26it/s]Inference:  41%|████      | 13/32 [00:03<00:04,  4.24it/s]Inference:  44%|████▍     | 14/32 [00:03<00:04,  4.20it/s]Inference:  47%|████▋     | 15/32 [00:03<00:04,  4.20it/s]Inference:  50%|█████     | 16/32 [00:03<00:03,  4.20it/s]Inference:  53%|█████▎    | 17/32 [00:04<00:03,  4.20it/s]Inference:  56%|█████▋    | 18/32 [00:04<00:03,  4.21it/s]Inference:  59%|█████▉    | 19/32 [00:04<00:03,  4.19it/s]Inference:  62%|██████▎   | 20/32 [00:04<00:02,  4.17it/s]Inference:  66%|██████▌   | 21/32 [00:04<00:02,  4.17it/s]Inference:  69%|██████▉   | 22/32 [00:05<00:02,  4.37it/s]Inference:  72%|███████▏  | 23/32 [00:05<00:02,  4.29it/s]Inference:  75%|███████▌  | 24/32 [00:05<00:01,  4.23it/s]Inference:  78%|███████▊  | 25/32 [00:05<00:01,  4.19it/s]Inference:  81%|████████▏ | 26/32 [00:06<00:01,  4.17it/s]Inference:  84%|████████▍ | 27/32 [00:06<00:01,  4.17it/s]Inference:  88%|████████▊ | 28/32 [00:06<00:00,  4.15it/s]Inference:  91%|█████████ | 29/32 [00:06<00:00,  4.14it/s]Inference:  94%|█████████▍| 30/32 [00:07<00:00,  4.12it/s]Inference:  97%|█████████▋| 31/32 [00:07<00:00,  4.09it/s]Inference: 100%|██████████| 32/32 [00:07<00:00,  4.08it/s]Inference: 100%|██████████| 32/32 [00:07<00:00,  4.19it/s]
27249 MiB free out of 48676 MiB total
Saved layer 12 to ./models/meta-llama/Llama-2-7b-hf/tensorized/128/no_finetune2/layer_12.pt
after cast to cpu
31265 MiB free out of 48676 MiB total
Done with layer 12 total_time elapsed: 13408 estimated time left: 19596
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:16,  1.91it/s]Inference:   6%|▋         | 2/32 [00:01<00:15,  1.91it/s]Inference:   9%|▉         | 3/32 [00:01<00:15,  1.90it/s]Inference:  12%|█▎        | 4/32 [00:02<00:14,  1.89it/s]Inference:  16%|█▌        | 5/32 [00:02<00:14,  1.90it/s]Inference:  19%|█▉        | 6/32 [00:03<00:13,  1.89it/s]Inference:  22%|██▏       | 7/32 [00:03<00:13,  1.89it/s]Inference:  25%|██▌       | 8/32 [00:04<00:12,  1.90it/s]Inference:  28%|██▊       | 9/32 [00:04<00:11,  1.93it/s]Inference:  31%|███▏      | 10/32 [00:05<00:11,  1.92it/s]Inference:  34%|███▍      | 11/32 [00:05<00:11,  1.91it/s]Inference:  38%|███▊      | 12/32 [00:06<00:10,  1.91it/s]Inference:  41%|████      | 13/32 [00:06<00:09,  1.91it/s]Inference:  44%|████▍     | 14/32 [00:07<00:09,  1.91it/s]Inference:  47%|████▋     | 15/32 [00:07<00:08,  1.90it/s]Inference:  50%|█████     | 16/32 [00:08<00:08,  1.90it/s]Inference:  53%|█████▎    | 17/32 [00:08<00:07,  1.90it/s]Inference:  56%|█████▋    | 18/32 [00:09<00:07,  1.90it/s]Inference:  59%|█████▉    | 19/32 [00:09<00:06,  1.89it/s]Inference:  62%|██████▎   | 20/32 [00:10<00:06,  1.90it/s]Inference:  66%|██████▌   | 21/32 [00:11<00:05,  1.90it/s]Inference:  69%|██████▉   | 22/32 [00:11<00:05,  1.90it/s]Inference:  72%|███████▏  | 23/32 [00:12<00:04,  1.90it/s]Inference:  75%|███████▌  | 24/32 [00:12<00:04,  1.89it/s]Inference:  78%|███████▊  | 25/32 [00:13<00:03,  1.90it/s]Inference:  81%|████████▏ | 26/32 [00:13<00:03,  1.89it/s]Inference:  84%|████████▍ | 27/32 [00:14<00:02,  1.89it/s]Inference:  88%|████████▊ | 28/32 [00:14<00:02,  1.89it/s]Inference:  91%|█████████ | 29/32 [00:15<00:01,  1.89it/s]Inference:  94%|█████████▍| 30/32 [00:15<00:01,  1.89it/s]Inference:  97%|█████████▋| 31/32 [00:16<00:00,  1.89it/s]Inference: 100%|██████████| 32/32 [00:16<00:00,  1.91it/s]Inference: 100%|██████████| 32/32 [00:16<00:00,  1.90it/s]
layer13: self_attn.q_proj
iter 0, train loss 70477.34375, val loss None, lr 0.01
iter 250, train loss 1039.880859375, val loss None, lr 0.01
iter 500, train loss 956.0884399414062, val loss None, lr 0.01
iter 750, train loss 878.5675659179688, val loss None, lr 0.003333
iter 1000, train loss 839.2015380859375, val loss None, lr 0.001111
iter 1250, train loss 820.14111328125, val loss None, lr 0.001111
iter 1500, train loss 810.0791625976562, val loss None, lr 0.001111
iter 1750, train loss 807.8248291015625, val loss None, lr 0.001111
iter 2000, train loss 795.8043823242188, val loss None, lr 0.00037
iter 2250, train loss 792.0115966796875, val loss None, lr 0.00037
best loss 787.747802734375
layer13: self_attn.k_proj
iter 0, train loss 91908.140625, val loss None, lr 0.01
iter 250, train loss 1215.097412109375, val loss None, lr 0.003333
iter 500, train loss 898.3348388671875, val loss None, lr 0.003333
iter 750, train loss 897.9378662109375, val loss None, lr 0.003333
iter 1000, train loss 834.0509033203125, val loss None, lr 0.001111
iter 1250, train loss 818.1784057617188, val loss None, lr 0.001111
iter 1500, train loss 808.6861572265625, val loss None, lr 0.001111
iter 1750, train loss 798.250732421875, val loss None, lr 0.001111
iter 2000, train loss 791.5604248046875, val loss None, lr 0.001111
iter 2250, train loss 782.993896484375, val loss None, lr 0.00037
best loss 779.1554565429688
layer13: self_attn.v_proj
iter 0, train loss 3340.719970703125, val loss None, lr 0.01
iter 250, train loss 292.9705810546875, val loss None, lr 0.01
iter 500, train loss 276.11517333984375, val loss None, lr 0.01
iter 750, train loss 270.3030090332031, val loss None, lr 0.01
iter 1000, train loss 264.703125, val loss None, lr 0.01
iter 1250, train loss 259.7166442871094, val loss None, lr 0.003333
iter 1500, train loss 259.0596618652344, val loss None, lr 0.003333
iter 1750, train loss 256.46441650390625, val loss None, lr 0.001111
iter 2000, train loss 255.83995056152344, val loss None, lr 0.001111
iter 2250, train loss 255.0850372314453, val loss None, lr 0.001111
best loss 254.42431640625
layer13: self_attn.o_proj
iter 0, train loss 1183.8123779296875, val loss None, lr 0.01
iter 250, train loss 46.926700592041016, val loss None, lr 0.01
iter 500, train loss 41.529136657714844, val loss None, lr 0.01
iter 750, train loss 39.67935562133789, val loss None, lr 0.01
iter 1000, train loss 37.8604850769043, val loss None, lr 0.003333
iter 1250, train loss 36.79325866699219, val loss None, lr 0.003333
iter 1500, train loss 36.28129577636719, val loss None, lr 0.003333
iter 1750, train loss 36.15668869018555, val loss None, lr 0.003333
iter 2000, train loss 35.441009521484375, val loss None, lr 0.001111
iter 2250, train loss 35.28511428833008, val loss None, lr 0.001111
best loss 35.167747497558594
layer13: mlp.gate_proj
iter 0, train loss 15888.46875, val loss None, lr 0.01
iter 250, train loss 858.7545166015625, val loss None, lr 0.01
iter 500, train loss 774.617431640625, val loss None, lr 0.01
iter 750, train loss 733.8355712890625, val loss None, lr 0.003333
iter 1000, train loss 721.8408203125, val loss None, lr 0.003333
iter 1250, train loss 719.0516967773438, val loss None, lr 0.003333
iter 1500, train loss 712.4723510742188, val loss None, lr 0.003333
iter 1750, train loss 707.41015625, val loss None, lr 0.001111
iter 2000, train loss 704.9840087890625, val loss None, lr 0.001111
iter 2250, train loss 702.8448486328125, val loss None, lr 0.001111
best loss 700.6414794921875
layer13: mlp.up_proj
iter 0, train loss 7826.9052734375, val loss None, lr 0.01
iter 250, train loss 719.1552124023438, val loss None, lr 0.01
iter 500, train loss 688.2940673828125, val loss None, lr 0.01
iter 750, train loss 679.1802368164062, val loss None, lr 0.01
iter 1000, train loss 672.1522216796875, val loss None, lr 0.01
iter 1250, train loss 660.600341796875, val loss None, lr 0.003333
iter 1500, train loss 653.9860229492188, val loss None, lr 0.001111
iter 1750, train loss 652.2542114257812, val loss None, lr 0.001111
iter 2000, train loss 650.8397827148438, val loss None, lr 0.001111
iter 2250, train loss 649.4149780273438, val loss None, lr 0.00037
best loss 648.3258666992188
layer13: mlp.down_proj
iter 0, train loss 167.84747314453125, val loss None, lr 0.01
iter 250, train loss 15.25426959991455, val loss None, lr 0.01
iter 500, train loss 13.384806632995605, val loss None, lr 0.01
iter 750, train loss 13.200922012329102, val loss None, lr 0.01
iter 1000, train loss 12.556404113769531, val loss None, lr 0.003333
iter 1250, train loss 12.38154411315918, val loss None, lr 0.003333
iter 1500, train loss 12.38936996459961, val loss None, lr 0.003333
iter 1750, train loss 12.211446762084961, val loss None, lr 0.001111
iter 2000, train loss 12.184842109680176, val loss None, lr 0.001111
iter 2250, train loss 12.164647102355957, val loss None, lr 0.001111
best loss 12.110645294189453
31265 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:06,  4.74it/s]Inference:   6%|▋         | 2/32 [00:00<00:06,  4.40it/s]Inference:   9%|▉         | 3/32 [00:00<00:06,  4.29it/s]Inference:  12%|█▎        | 4/32 [00:00<00:06,  4.22it/s]Inference:  16%|█▌        | 5/32 [00:01<00:06,  4.18it/s]Inference:  19%|█▉        | 6/32 [00:01<00:06,  4.15it/s]Inference:  22%|██▏       | 7/32 [00:01<00:06,  4.12it/s]Inference:  25%|██▌       | 8/32 [00:01<00:05,  4.11it/s]Inference:  28%|██▊       | 9/32 [00:02<00:05,  4.10it/s]Inference:  31%|███▏      | 10/32 [00:02<00:05,  4.11it/s]Inference:  34%|███▍      | 11/32 [00:02<00:05,  4.10it/s]Inference:  38%|███▊      | 12/32 [00:02<00:04,  4.10it/s]Inference:  41%|████      | 13/32 [00:03<00:04,  4.09it/s]Inference:  44%|████▍     | 14/32 [00:03<00:04,  4.10it/s]Inference:  47%|████▋     | 15/32 [00:03<00:04,  4.08it/s]Inference:  50%|█████     | 16/32 [00:03<00:03,  4.30it/s]Inference:  53%|█████▎    | 17/32 [00:04<00:03,  4.21it/s]Inference:  56%|█████▋    | 18/32 [00:04<00:03,  4.16it/s]Inference:  59%|█████▉    | 19/32 [00:04<00:03,  4.13it/s]Inference:  62%|██████▎   | 20/32 [00:04<00:02,  4.14it/s]Inference:  66%|██████▌   | 21/32 [00:05<00:02,  4.12it/s]Inference:  69%|██████▉   | 22/32 [00:05<00:02,  4.12it/s]Inference:  72%|███████▏  | 23/32 [00:05<00:02,  4.09it/s]Inference:  75%|███████▌  | 24/32 [00:05<00:01,  4.08it/s]Inference:  78%|███████▊  | 25/32 [00:06<00:01,  4.07it/s]Inference:  81%|████████▏ | 26/32 [00:06<00:01,  4.07it/s]Inference:  84%|████████▍ | 27/32 [00:06<00:01,  4.09it/s]Inference:  88%|████████▊ | 28/32 [00:06<00:00,  4.09it/s]Inference:  91%|█████████ | 29/32 [00:07<00:00,  4.10it/s]Inference:  94%|█████████▍| 30/32 [00:07<00:00,  4.10it/s]Inference:  97%|█████████▋| 31/32 [00:07<00:00,  4.09it/s]Inference: 100%|██████████| 32/32 [00:07<00:00,  4.08it/s]Inference: 100%|██████████| 32/32 [00:07<00:00,  4.13it/s]
25961 MiB free out of 48676 MiB total
Saved layer 13 to ./models/meta-llama/Llama-2-7b-hf/tensorized/128/no_finetune2/layer_13.pt
after cast to cpu
29977 MiB free out of 48676 MiB total
Done with layer 13 total_time elapsed: 14274 estimated time left: 18353
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:16,  1.90it/s]Inference:   6%|▋         | 2/32 [00:01<00:15,  1.91it/s]Inference:   9%|▉         | 3/32 [00:01<00:15,  1.89it/s]Inference:  12%|█▎        | 4/32 [00:02<00:14,  1.89it/s]Inference:  16%|█▌        | 5/32 [00:02<00:14,  1.89it/s]Inference:  19%|█▉        | 6/32 [00:03<00:13,  1.90it/s]Inference:  22%|██▏       | 7/32 [00:03<00:13,  1.92it/s]Inference:  25%|██▌       | 8/32 [00:04<00:12,  1.92it/s]Inference:  28%|██▊       | 9/32 [00:04<00:12,  1.91it/s]Inference:  31%|███▏      | 10/32 [00:05<00:11,  1.91it/s]Inference:  34%|███▍      | 11/32 [00:05<00:11,  1.91it/s]Inference:  38%|███▊      | 12/32 [00:06<00:10,  1.90it/s]Inference:  41%|████      | 13/32 [00:06<00:10,  1.90it/s]Inference:  44%|████▍     | 14/32 [00:07<00:09,  1.90it/s]Inference:  47%|████▋     | 15/32 [00:07<00:08,  1.89it/s]Inference:  50%|█████     | 16/32 [00:08<00:08,  1.89it/s]Inference:  53%|█████▎    | 17/32 [00:08<00:07,  1.89it/s]Inference:  56%|█████▋    | 18/32 [00:09<00:07,  1.93it/s]Inference:  59%|█████▉    | 19/32 [00:09<00:06,  1.92it/s]Inference:  62%|██████▎   | 20/32 [00:10<00:06,  1.91it/s]Inference:  66%|██████▌   | 21/32 [00:11<00:05,  1.90it/s]Inference:  69%|██████▉   | 22/32 [00:11<00:05,  1.90it/s]Inference:  72%|███████▏  | 23/32 [00:12<00:04,  1.89it/s]Inference:  75%|███████▌  | 24/32 [00:12<00:04,  1.94it/s]Inference:  78%|███████▊  | 25/32 [00:13<00:03,  1.92it/s]Inference:  81%|████████▏ | 26/32 [00:13<00:03,  1.91it/s]Inference:  84%|████████▍ | 27/32 [00:14<00:02,  1.90it/s]Inference:  88%|████████▊ | 28/32 [00:14<00:02,  1.89it/s]Inference:  91%|█████████ | 29/32 [00:15<00:01,  1.90it/s]Inference:  94%|█████████▍| 30/32 [00:15<00:01,  1.90it/s]Inference:  97%|█████████▋| 31/32 [00:16<00:00,  1.89it/s]Inference: 100%|██████████| 32/32 [00:16<00:00,  1.89it/s]Inference: 100%|██████████| 32/32 [00:16<00:00,  1.90it/s]
layer14: self_attn.q_proj
iter 0, train loss 64577.98046875, val loss None, lr 0.01
iter 250, train loss 1011.4586181640625, val loss None, lr 0.01
iter 500, train loss 918.67822265625, val loss None, lr 0.01
iter 750, train loss 949.772216796875, val loss None, lr 0.01
iter 1000, train loss 830.0155029296875, val loss None, lr 0.003333
iter 1250, train loss 806.34228515625, val loss None, lr 0.001111
iter 1500, train loss 795.0494995117188, val loss None, lr 0.001111
iter 1750, train loss 787.199462890625, val loss None, lr 0.001111
iter 2000, train loss 784.0037231445312, val loss None, lr 0.001111
iter 2250, train loss 778.47119140625, val loss None, lr 0.001111
best loss 772.0318603515625
layer14: self_attn.k_proj
iter 0, train loss 94998.25, val loss None, lr 0.01
iter 250, train loss 1269.82080078125, val loss None, lr 0.01
iter 500, train loss 937.5542602539062, val loss None, lr 0.003333
iter 750, train loss 923.0519409179688, val loss None, lr 0.003333
iter 1000, train loss 879.4913330078125, val loss None, lr 0.003333
iter 1250, train loss 833.5841064453125, val loss None, lr 0.001111
iter 1500, train loss 818.31884765625, val loss None, lr 0.001111
iter 1750, train loss 810.7734985351562, val loss None, lr 0.001111
iter 2000, train loss 801.848388671875, val loss None, lr 0.001111
iter 2250, train loss 793.8135986328125, val loss None, lr 0.00037
best loss 788.377197265625
layer14: self_attn.v_proj
iter 0, train loss 3064.809326171875, val loss None, lr 0.01
iter 250, train loss 267.4727783203125, val loss None, lr 0.01
iter 500, train loss 254.90646362304688, val loss None, lr 0.01
iter 750, train loss 250.48716735839844, val loss None, lr 0.01
iter 1000, train loss 244.77960205078125, val loss None, lr 0.01
iter 1250, train loss 246.8243408203125, val loss None, lr 0.003333
iter 1500, train loss 239.08609008789062, val loss None, lr 0.003333
iter 1750, train loss 237.13558959960938, val loss None, lr 0.001111
iter 2000, train loss 236.37673950195312, val loss None, lr 0.001111
iter 2250, train loss 235.7712860107422, val loss None, lr 0.001111
best loss 235.2640838623047
layer14: self_attn.o_proj
iter 0, train loss 1071.775634765625, val loss None, lr 0.01
iter 250, train loss 44.92194366455078, val loss None, lr 0.01
iter 500, train loss 39.60173034667969, val loss None, lr 0.01
iter 750, train loss 39.03559494018555, val loss None, lr 0.01
iter 1000, train loss 36.64686584472656, val loss None, lr 0.01
iter 1250, train loss 34.0003547668457, val loss None, lr 0.003333
iter 1500, train loss 33.42578125, val loss None, lr 0.001111
iter 1750, train loss 33.152381896972656, val loss None, lr 0.001111
iter 2000, train loss 33.032039642333984, val loss None, lr 0.001111
iter 2250, train loss 32.91167449951172, val loss None, lr 0.001111
best loss 32.711570739746094
layer14: mlp.gate_proj
iter 0, train loss 14533.5537109375, val loss None, lr 0.01
iter 250, train loss 843.333984375, val loss None, lr 0.01
iter 500, train loss 811.381591796875, val loss None, lr 0.01
iter 750, train loss 775.3638305664062, val loss None, lr 0.003333
iter 1000, train loss 764.01123046875, val loss None, lr 0.003333
iter 1250, train loss 760.8200073242188, val loss None, lr 0.003333
iter 1500, train loss 752.719482421875, val loss None, lr 0.001111
iter 1750, train loss 753.7188720703125, val loss None, lr 0.001111
iter 2000, train loss 746.9827270507812, val loss None, lr 0.001111
iter 2250, train loss 745.9349975585938, val loss None, lr 0.001111
best loss 743.7283325195312
layer14: mlp.up_proj
iter 0, train loss 7992.833984375, val loss None, lr 0.01
iter 250, train loss 762.2081909179688, val loss None, lr 0.01
iter 500, train loss 734.70849609375, val loss None, lr 0.01
iter 750, train loss 731.3197021484375, val loss None, lr 0.01
iter 1000, train loss 715.9954833984375, val loss None, lr 0.01
iter 1250, train loss 712.7198486328125, val loss None, lr 0.003333
iter 1500, train loss 715.2296752929688, val loss None, lr 0.003333
iter 1750, train loss 698.8090209960938, val loss None, lr 0.001111
iter 2000, train loss 697.560302734375, val loss None, lr 0.001111
iter 2250, train loss 696.4107666015625, val loss None, lr 0.001111
best loss 695.25439453125
layer14: mlp.down_proj
iter 0, train loss 136.9129638671875, val loss None, lr 0.01
iter 250, train loss 14.277826309204102, val loss None, lr 0.01
iter 500, train loss 13.475793838500977, val loss None, lr 0.01
iter 750, train loss 13.575289726257324, val loss None, lr 0.01
iter 1000, train loss 12.968631744384766, val loss None, lr 0.01
iter 1250, train loss 13.093395233154297, val loss None, lr 0.01
iter 1500, train loss 12.571344375610352, val loss None, lr 0.003333
iter 1750, train loss 12.443276405334473, val loss None, lr 0.001111
iter 2000, train loss 12.406786918640137, val loss None, lr 0.001111
iter 2250, train loss 12.377246856689453, val loss None, lr 0.001111
best loss 12.353580474853516
29977 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:06,  4.77it/s]Inference:   6%|▋         | 2/32 [00:00<00:06,  4.33it/s]Inference:   9%|▉         | 3/32 [00:00<00:06,  4.19it/s]Inference:  12%|█▎        | 4/32 [00:00<00:06,  4.15it/s]Inference:  16%|█▌        | 5/32 [00:01<00:06,  4.11it/s]Inference:  19%|█▉        | 6/32 [00:01<00:06,  4.08it/s]Inference:  22%|██▏       | 7/32 [00:01<00:06,  4.08it/s]Inference:  25%|██▌       | 8/32 [00:01<00:05,  4.10it/s]Inference:  28%|██▊       | 9/32 [00:02<00:05,  4.08it/s]Inference:  31%|███▏      | 10/32 [00:02<00:05,  4.30it/s]Inference:  34%|███▍      | 11/32 [00:02<00:04,  4.20it/s]Inference:  38%|███▊      | 12/32 [00:02<00:04,  4.16it/s]Inference:  41%|████      | 13/32 [00:03<00:04,  4.12it/s]Inference:  44%|████▍     | 14/32 [00:03<00:04,  4.11it/s]Inference:  47%|████▋     | 15/32 [00:03<00:04,  4.08it/s]Inference:  50%|█████     | 16/32 [00:03<00:03,  4.09it/s]Inference:  53%|█████▎    | 17/32 [00:04<00:03,  4.07it/s]Inference:  56%|█████▋    | 18/32 [00:04<00:03,  4.07it/s]Inference:  59%|█████▉    | 19/32 [00:04<00:03,  4.05it/s]Inference:  62%|██████▎   | 20/32 [00:04<00:02,  4.06it/s]Inference:  66%|██████▌   | 21/32 [00:05<00:02,  4.05it/s]Inference:  69%|██████▉   | 22/32 [00:05<00:02,  4.03it/s]Inference:  72%|███████▏  | 23/32 [00:05<00:02,  4.05it/s]Inference:  75%|███████▌  | 24/32 [00:05<00:01,  4.05it/s]Inference:  78%|███████▊  | 25/32 [00:06<00:01,  4.07it/s]Inference:  81%|████████▏ | 26/32 [00:06<00:01,  4.27it/s]Inference:  84%|████████▍ | 27/32 [00:06<00:01,  4.19it/s]Inference:  88%|████████▊ | 28/32 [00:06<00:00,  4.14it/s]Inference:  91%|█████████ | 29/32 [00:07<00:00,  4.13it/s]Inference:  94%|█████████▍| 30/32 [00:07<00:00,  4.11it/s]Inference:  97%|█████████▋| 31/32 [00:07<00:00,  4.09it/s]Inference: 100%|██████████| 32/32 [00:07<00:00,  4.07it/s]Inference: 100%|██████████| 32/32 [00:07<00:00,  4.11it/s]
24673 MiB free out of 48676 MiB total
Saved layer 14 to ./models/meta-llama/Llama-2-7b-hf/tensorized/128/no_finetune2/layer_14.pt
after cast to cpu
28689 MiB free out of 48676 MiB total
Done with layer 14 total_time elapsed: 15143 estimated time left: 17162
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:16,  1.90it/s]Inference:   6%|▋         | 2/32 [00:01<00:15,  1.91it/s]Inference:   9%|▉         | 3/32 [00:01<00:15,  1.89it/s]Inference:  12%|█▎        | 4/32 [00:02<00:14,  1.89it/s]Inference:  16%|█▌        | 5/32 [00:02<00:14,  1.88it/s]Inference:  19%|█▉        | 6/32 [00:03<00:13,  1.93it/s]Inference:  22%|██▏       | 7/32 [00:03<00:13,  1.91it/s]Inference:  25%|██▌       | 8/32 [00:04<00:12,  1.91it/s]Inference:  28%|██▊       | 9/32 [00:04<00:12,  1.90it/s]Inference:  31%|███▏      | 10/32 [00:05<00:11,  1.89it/s]Inference:  34%|███▍      | 11/32 [00:05<00:11,  1.89it/s]Inference:  38%|███▊      | 12/32 [00:06<00:10,  1.88it/s]Inference:  41%|████      | 13/32 [00:06<00:10,  1.88it/s]Inference:  44%|████▍     | 14/32 [00:07<00:09,  1.88it/s]Inference:  47%|████▋     | 15/32 [00:07<00:09,  1.88it/s]Inference:  50%|█████     | 16/32 [00:08<00:08,  1.88it/s]Inference:  53%|█████▎    | 17/32 [00:08<00:07,  1.88it/s]Inference:  56%|█████▋    | 18/32 [00:09<00:07,  1.88it/s]Inference:  59%|█████▉    | 19/32 [00:10<00:06,  1.88it/s]Inference:  62%|██████▎   | 20/32 [00:10<00:06,  1.88it/s]Inference:  66%|██████▌   | 21/32 [00:11<00:05,  1.88it/s]Inference:  69%|██████▉   | 22/32 [00:11<00:05,  1.88it/s]Inference:  72%|███████▏  | 23/32 [00:12<00:04,  1.88it/s]Inference:  75%|███████▌  | 24/32 [00:12<00:04,  1.88it/s]Inference:  78%|███████▊  | 25/32 [00:13<00:03,  1.88it/s]Inference:  81%|████████▏ | 26/32 [00:13<00:03,  1.92it/s]Inference:  84%|████████▍ | 27/32 [00:14<00:02,  1.91it/s]Inference:  88%|████████▊ | 28/32 [00:14<00:02,  1.90it/s]Inference:  91%|█████████ | 29/32 [00:15<00:01,  1.89it/s]Inference:  94%|█████████▍| 30/32 [00:15<00:01,  1.89it/s]Inference:  97%|█████████▋| 31/32 [00:16<00:00,  1.88it/s]Inference: 100%|██████████| 32/32 [00:16<00:00,  1.88it/s]Inference: 100%|██████████| 32/32 [00:16<00:00,  1.89it/s]
layer15: self_attn.q_proj
iter 0, train loss 71424.6640625, val loss None, lr 0.01
iter 250, train loss 967.1052856445312, val loss None, lr 0.01
iter 500, train loss 1046.0833740234375, val loss None, lr 0.01
iter 750, train loss 839.2759399414062, val loss None, lr 0.003333
iter 1000, train loss 811.7513427734375, val loss None, lr 0.003333
iter 1250, train loss 783.1681518554688, val loss None, lr 0.001111
iter 1500, train loss 768.8701171875, val loss None, lr 0.001111
iter 1750, train loss 761.0128784179688, val loss None, lr 0.001111
iter 2000, train loss 755.6159057617188, val loss None, lr 0.001111
iter 2250, train loss 754.3094482421875, val loss None, lr 0.001111
best loss 748.7999877929688
layer15: self_attn.k_proj
iter 0, train loss 91142.359375, val loss None, lr 0.01
iter 250, train loss 1189.4678955078125, val loss None, lr 0.01
iter 500, train loss 911.7724609375, val loss None, lr 0.003333
iter 750, train loss 871.0325927734375, val loss None, lr 0.003333
iter 1000, train loss 826.6627807617188, val loss None, lr 0.001111
iter 1250, train loss 807.25830078125, val loss None, lr 0.001111
iter 1500, train loss 794.1636962890625, val loss None, lr 0.001111
iter 1750, train loss 787.9566650390625, val loss None, lr 0.001111
iter 2000, train loss 779.9935302734375, val loss None, lr 0.001111
iter 2250, train loss 775.1856689453125, val loss None, lr 0.001111
best loss 764.1551513671875
layer15: self_attn.v_proj
iter 0, train loss 3260.341552734375, val loss None, lr 0.01
iter 250, train loss 292.72857666015625, val loss None, lr 0.01
iter 500, train loss 275.7747497558594, val loss None, lr 0.01
iter 750, train loss 270.4281005859375, val loss None, lr 0.01
iter 1000, train loss 266.4473876953125, val loss None, lr 0.01
iter 1250, train loss 263.9808349609375, val loss None, lr 0.01
iter 1500, train loss 259.2989196777344, val loss None, lr 0.003333
iter 1750, train loss 258.40631103515625, val loss None, lr 0.003333
iter 2000, train loss 256.6448059082031, val loss None, lr 0.001111
iter 2250, train loss 255.99668884277344, val loss None, lr 0.001111
best loss 255.3927001953125
layer15: self_attn.o_proj
iter 0, train loss 1361.1602783203125, val loss None, lr 0.01
iter 250, train loss 57.18716049194336, val loss None, lr 0.01
iter 500, train loss 51.18879699707031, val loss None, lr 0.01
iter 750, train loss 49.68764877319336, val loss None, lr 0.01
iter 1000, train loss 51.7740592956543, val loss None, lr 0.01
iter 1250, train loss 48.36731719970703, val loss None, lr 0.01
iter 1500, train loss 45.45344161987305, val loss None, lr 0.003333
iter 1750, train loss 44.91351318359375, val loss None, lr 0.001111
iter 2000, train loss 44.08717346191406, val loss None, lr 0.001111
iter 2250, train loss 43.903602600097656, val loss None, lr 0.001111
best loss 43.751197814941406
layer15: mlp.gate_proj
iter 0, train loss 15159.337890625, val loss None, lr 0.01
iter 250, train loss 887.9035034179688, val loss None, lr 0.01
iter 500, train loss 845.4453735351562, val loss None, lr 0.01
iter 750, train loss 832.0452880859375, val loss None, lr 0.01
iter 1000, train loss 825.793701171875, val loss None, lr 0.01
iter 1250, train loss 803.2684326171875, val loss None, lr 0.003333
iter 1500, train loss 788.3697509765625, val loss None, lr 0.001111
iter 1750, train loss 785.5559692382812, val loss None, lr 0.001111
iter 2000, train loss 783.9962158203125, val loss None, lr 0.001111
iter 2250, train loss 781.5753784179688, val loss None, lr 0.001111
best loss 779.8299560546875
layer15: mlp.up_proj
iter 0, train loss 8627.623046875, val loss None, lr 0.01
iter 250, train loss 803.40380859375, val loss None, lr 0.01
iter 500, train loss 772.159912109375, val loss None, lr 0.01
iter 750, train loss 768.8121948242188, val loss None, lr 0.01
iter 1000, train loss 764.9160766601562, val loss None, lr 0.01
iter 1250, train loss 754.6937866210938, val loss None, lr 0.01
iter 1500, train loss 742.2906494140625, val loss None, lr 0.003333
iter 1750, train loss 737.1529541015625, val loss None, lr 0.001111
iter 2000, train loss 735.5929565429688, val loss None, lr 0.001111
iter 2250, train loss 734.4022827148438, val loss None, lr 0.001111
best loss 733.1641845703125
layer15: mlp.down_proj
iter 0, train loss 190.2920684814453, val loss None, lr 0.01
iter 250, train loss 18.222509384155273, val loss None, lr 0.01
iter 500, train loss 17.048194885253906, val loss None, lr 0.01
iter 750, train loss 16.72465705871582, val loss None, lr 0.01
iter 1000, train loss 16.452409744262695, val loss None, lr 0.01
iter 1250, train loss 15.99011516571045, val loss None, lr 0.003333
iter 1500, train loss 15.83465576171875, val loss None, lr 0.001111
iter 1750, train loss 15.773555755615234, val loss None, lr 0.001111
iter 2000, train loss 15.728310585021973, val loss None, lr 0.001111
iter 2250, train loss 15.784279823303223, val loss None, lr 0.001111
best loss 15.636831283569336
28689 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:06,  4.85it/s]Inference:   6%|▋         | 2/32 [00:00<00:06,  4.44it/s]Inference:   9%|▉         | 3/32 [00:00<00:06,  4.25it/s]Inference:  12%|█▎        | 4/32 [00:00<00:06,  4.19it/s]Inference:  16%|█▌        | 5/32 [00:01<00:06,  4.14it/s]Inference:  19%|█▉        | 6/32 [00:01<00:06,  4.13it/s]Inference:  22%|██▏       | 7/32 [00:01<00:06,  4.12it/s]Inference:  25%|██▌       | 8/32 [00:01<00:05,  4.16it/s]Inference:  28%|██▊       | 9/32 [00:02<00:05,  4.15it/s]Inference:  31%|███▏      | 10/32 [00:02<00:05,  4.14it/s]Inference:  34%|███▍      | 11/32 [00:02<00:05,  4.12it/s]Inference:  38%|███▊      | 12/32 [00:02<00:04,  4.36it/s]Inference:  41%|████      | 13/32 [00:03<00:04,  4.26it/s]Inference:  44%|████▍     | 14/32 [00:03<00:04,  4.22it/s]Inference:  47%|████▋     | 15/32 [00:03<00:04,  4.18it/s]Inference:  50%|█████     | 16/32 [00:03<00:03,  4.18it/s]Inference:  53%|█████▎    | 17/32 [00:04<00:03,  4.17it/s]Inference:  56%|█████▋    | 18/32 [00:04<00:03,  4.17it/s]Inference:  59%|█████▉    | 19/32 [00:04<00:03,  4.16it/s]Inference:  62%|██████▎   | 20/32 [00:04<00:02,  4.14it/s]Inference:  66%|██████▌   | 21/32 [00:05<00:02,  4.14it/s]Inference:  69%|██████▉   | 22/32 [00:05<00:02,  4.12it/s]Inference:  72%|███████▏  | 23/32 [00:05<00:02,  4.18it/s]Inference:  75%|███████▌  | 24/32 [00:05<00:01,  4.18it/s]Inference:  78%|███████▊  | 25/32 [00:05<00:01,  4.17it/s]Inference:  81%|████████▏ | 26/32 [00:06<00:01,  4.17it/s]Inference:  84%|████████▍ | 27/32 [00:06<00:01,  4.16it/s]Inference:  88%|████████▊ | 28/32 [00:06<00:00,  4.14it/s]Inference:  91%|█████████ | 29/32 [00:06<00:00,  4.14it/s]Inference:  94%|█████████▍| 30/32 [00:07<00:00,  4.12it/s]Inference:  97%|█████████▋| 31/32 [00:07<00:00,  4.13it/s]Inference: 100%|██████████| 32/32 [00:07<00:00,  4.13it/s]Inference: 100%|██████████| 32/32 [00:07<00:00,  4.17it/s]
23449 MiB free out of 48676 MiB total
Saved layer 15 to ./models/meta-llama/Llama-2-7b-hf/tensorized/128/no_finetune2/layer_15.pt
after cast to cpu
27465 MiB free out of 48676 MiB total
Done with layer 15 total_time elapsed: 16010 estimated time left: 16010
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:16,  1.91it/s]Inference:   6%|▋         | 2/32 [00:01<00:15,  1.92it/s]Inference:   9%|▉         | 3/32 [00:01<00:15,  1.90it/s]Inference:  12%|█▎        | 4/32 [00:02<00:14,  1.90it/s]Inference:  16%|█▌        | 5/32 [00:02<00:14,  1.90it/s]Inference:  19%|█▉        | 6/32 [00:03<00:13,  1.90it/s]Inference:  22%|██▏       | 7/32 [00:03<00:13,  1.90it/s]Inference:  25%|██▌       | 8/32 [00:04<00:12,  1.91it/s]Inference:  28%|██▊       | 9/32 [00:04<00:12,  1.90it/s]Inference:  31%|███▏      | 10/32 [00:05<00:11,  1.90it/s]Inference:  34%|███▍      | 11/32 [00:05<00:11,  1.89it/s]Inference:  38%|███▊      | 12/32 [00:06<00:10,  1.90it/s]Inference:  41%|████      | 13/32 [00:06<00:10,  1.90it/s]Inference:  44%|████▍     | 14/32 [00:07<00:09,  1.90it/s]Inference:  47%|████▋     | 15/32 [00:07<00:08,  1.90it/s]Inference:  50%|█████     | 16/32 [00:08<00:08,  1.91it/s]Inference:  53%|█████▎    | 17/32 [00:08<00:07,  1.90it/s]Inference:  56%|█████▋    | 18/32 [00:09<00:07,  1.90it/s]Inference:  59%|█████▉    | 19/32 [00:09<00:06,  1.90it/s]Inference:  62%|██████▎   | 20/32 [00:10<00:06,  1.90it/s]Inference:  66%|██████▌   | 21/32 [00:11<00:05,  1.90it/s]Inference:  69%|██████▉   | 22/32 [00:11<00:05,  1.90it/s]Inference:  72%|███████▏  | 23/32 [00:12<00:04,  1.90it/s]Inference:  75%|███████▌  | 24/32 [00:12<00:04,  1.90it/s]Inference:  78%|███████▊  | 25/32 [00:13<00:03,  1.90it/s]Inference:  81%|████████▏ | 26/32 [00:13<00:03,  1.94it/s]Inference:  84%|████████▍ | 27/32 [00:14<00:02,  1.92it/s]Inference:  88%|████████▊ | 28/32 [00:14<00:02,  1.92it/s]Inference:  91%|█████████ | 29/32 [00:15<00:01,  1.91it/s]Inference:  94%|█████████▍| 30/32 [00:15<00:01,  1.91it/s]Inference:  97%|█████████▋| 31/32 [00:16<00:00,  1.91it/s]Inference: 100%|██████████| 32/32 [00:16<00:00,  1.92it/s]Inference: 100%|██████████| 32/32 [00:16<00:00,  1.91it/s]
layer16: self_attn.q_proj
iter 0, train loss 68355.15625, val loss None, lr 0.01
iter 250, train loss 951.6940307617188, val loss None, lr 0.01
iter 500, train loss 814.37548828125, val loss None, lr 0.003333
iter 750, train loss 781.9945678710938, val loss None, lr 0.003333
iter 1000, train loss 758.8160400390625, val loss None, lr 0.003333
iter 1250, train loss 753.9500732421875, val loss None, lr 0.003333
iter 1500, train loss 726.5567626953125, val loss None, lr 0.001111
iter 1750, train loss 720.4646606445312, val loss None, lr 0.001111
iter 2000, train loss 715.1793212890625, val loss None, lr 0.001111
iter 2250, train loss 708.3380737304688, val loss None, lr 0.001111
best loss 702.4827270507812
layer16: self_attn.k_proj
iter 0, train loss 97686.5390625, val loss None, lr 0.01
iter 250, train loss 1029.56982421875, val loss None, lr 0.01
iter 500, train loss 853.7734375, val loss None, lr 0.003333
iter 750, train loss 827.5784301757812, val loss None, lr 0.003333
iter 1000, train loss 812.6112670898438, val loss None, lr 0.003333
iter 1250, train loss 756.11083984375, val loss None, lr 0.001111
iter 1500, train loss 745.9271240234375, val loss None, lr 0.001111
iter 1750, train loss 736.5281982421875, val loss None, lr 0.001111
iter 2000, train loss 730.0538330078125, val loss None, lr 0.001111
iter 2250, train loss 721.1664428710938, val loss None, lr 0.00037
best loss 716.1854248046875
layer16: self_attn.v_proj
iter 0, train loss 3392.1435546875, val loss None, lr 0.01
iter 250, train loss 295.1482238769531, val loss None, lr 0.01
iter 500, train loss 279.6329040527344, val loss None, lr 0.01
iter 750, train loss 274.78070068359375, val loss None, lr 0.01
iter 1000, train loss 272.34002685546875, val loss None, lr 0.01
iter 1250, train loss 267.3855895996094, val loss None, lr 0.01
iter 1500, train loss 263.3984069824219, val loss None, lr 0.003333
iter 1750, train loss 260.9774169921875, val loss None, lr 0.001111
iter 2000, train loss 260.41705322265625, val loss None, lr 0.001111
iter 2250, train loss 259.8370666503906, val loss None, lr 0.001111
best loss 259.2627258300781
layer16: self_attn.o_proj
iter 0, train loss 1868.35986328125, val loss None, lr 0.01
iter 250, train loss 69.41927337646484, val loss None, lr 0.01
iter 500, train loss 63.957645416259766, val loss None, lr 0.01
iter 750, train loss 68.63109588623047, val loss None, lr 0.01
iter 1000, train loss 61.11250686645508, val loss None, lr 0.01
iter 1250, train loss 54.97874450683594, val loss None, lr 0.003333
iter 1500, train loss 53.67955780029297, val loss None, lr 0.003333
iter 1750, train loss 52.884620666503906, val loss None, lr 0.001111
iter 2000, train loss 52.46803283691406, val loss None, lr 0.001111
iter 2250, train loss 52.31544494628906, val loss None, lr 0.001111
best loss 52.00226593017578
layer16: mlp.gate_proj
iter 0, train loss 16284.08984375, val loss None, lr 0.01
iter 250, train loss 914.3009643554688, val loss None, lr 0.01
iter 500, train loss 875.8729248046875, val loss None, lr 0.01
iter 750, train loss 859.8934326171875, val loss None, lr 0.01
iter 1000, train loss 845.7540283203125, val loss None, lr 0.01
iter 1250, train loss 822.3663330078125, val loss None, lr 0.003333
iter 1500, train loss 819.77685546875, val loss None, lr 0.003333
iter 1750, train loss 810.0517578125, val loss None, lr 0.001111
iter 2000, train loss 807.547119140625, val loss None, lr 0.001111
iter 2250, train loss 805.691162109375, val loss None, lr 0.001111
best loss 803.4366455078125
layer16: mlp.up_proj
iter 0, train loss 9311.8466796875, val loss None, lr 0.01
iter 250, train loss 817.193359375, val loss None, lr 0.01
iter 500, train loss 783.368896484375, val loss None, lr 0.01
iter 750, train loss 766.662109375, val loss None, lr 0.01
iter 1000, train loss 769.8587036132812, val loss None, lr 0.01
iter 1250, train loss 750.283935546875, val loss None, lr 0.001111
iter 1500, train loss 745.4378051757812, val loss None, lr 0.001111
iter 1750, train loss 743.2554931640625, val loss None, lr 0.001111
iter 2000, train loss 741.9524536132812, val loss None, lr 0.001111
iter 2250, train loss 740.4842529296875, val loss None, lr 0.001111
best loss 739.025146484375
layer16: mlp.down_proj
iter 0, train loss 255.59043884277344, val loss None, lr 0.01
iter 250, train loss 20.18865966796875, val loss None, lr 0.01
iter 500, train loss 18.969823837280273, val loss None, lr 0.01
iter 750, train loss 17.908987045288086, val loss None, lr 0.01
iter 1000, train loss 17.361064910888672, val loss None, lr 0.003333
iter 1250, train loss 17.156471252441406, val loss None, lr 0.003333
iter 1500, train loss 17.189350128173828, val loss None, lr 0.001111
iter 1750, train loss 16.89436149597168, val loss None, lr 0.001111
iter 2000, train loss 16.820459365844727, val loss None, lr 0.001111
iter 2250, train loss 16.769384384155273, val loss None, lr 0.001111
best loss 16.716646194458008
27465 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:06,  4.81it/s]Inference:   6%|▋         | 2/32 [00:00<00:06,  4.40it/s]Inference:   9%|▉         | 3/32 [00:00<00:06,  4.25it/s]Inference:  12%|█▎        | 4/32 [00:00<00:06,  4.22it/s]Inference:  16%|█▌        | 5/32 [00:01<00:06,  4.18it/s]Inference:  19%|█▉        | 6/32 [00:01<00:06,  4.16it/s]Inference:  22%|██▏       | 7/32 [00:01<00:06,  4.15it/s]Inference:  25%|██▌       | 8/32 [00:01<00:05,  4.13it/s]Inference:  28%|██▊       | 9/32 [00:02<00:05,  4.11it/s]Inference:  31%|███▏      | 10/32 [00:02<00:05,  4.10it/s]Inference:  34%|███▍      | 11/32 [00:02<00:05,  4.09it/s]Inference:  38%|███▊      | 12/32 [00:02<00:04,  4.11it/s]Inference:  41%|████      | 13/32 [00:03<00:04,  4.13it/s]Inference:  44%|████▍     | 14/32 [00:03<00:04,  4.13it/s]Inference:  47%|████▋     | 15/32 [00:03<00:04,  4.11it/s]Inference:  50%|█████     | 16/32 [00:03<00:03,  4.11it/s]Inference:  53%|█████▎    | 17/32 [00:04<00:03,  4.34it/s]Inference:  56%|█████▋    | 18/32 [00:04<00:03,  4.24it/s]Inference:  59%|█████▉    | 19/32 [00:04<00:03,  4.20it/s]Inference:  62%|██████▎   | 20/32 [00:04<00:02,  4.16it/s]Inference:  66%|██████▌   | 21/32 [00:05<00:02,  4.18it/s]Inference:  69%|██████▉   | 22/32 [00:05<00:02,  4.17it/s]Inference:  72%|███████▏  | 23/32 [00:05<00:02,  4.18it/s]Inference:  75%|███████▌  | 24/32 [00:05<00:01,  4.14it/s]Inference:  78%|███████▊  | 25/32 [00:05<00:01,  4.14it/s]Inference:  81%|████████▏ | 26/32 [00:06<00:01,  4.12it/s]Inference:  84%|████████▍ | 27/32 [00:06<00:01,  4.12it/s]Inference:  88%|████████▊ | 28/32 [00:06<00:00,  4.12it/s]Inference:  91%|█████████ | 29/32 [00:06<00:00,  4.15it/s]Inference:  94%|█████████▍| 30/32 [00:07<00:00,  4.15it/s]Inference:  97%|█████████▋| 31/32 [00:07<00:00,  4.15it/s]Inference: 100%|██████████| 32/32 [00:07<00:00,  4.14it/s]Inference: 100%|██████████| 32/32 [00:07<00:00,  4.16it/s]
22161 MiB free out of 48676 MiB total
Saved layer 16 to ./models/meta-llama/Llama-2-7b-hf/tensorized/128/no_finetune2/layer_16.pt
after cast to cpu
26177 MiB free out of 48676 MiB total
Done with layer 16 total_time elapsed: 16878 estimated time left: 14892
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:16,  1.91it/s]Inference:   6%|▋         | 2/32 [00:01<00:15,  1.91it/s]Inference:   9%|▉         | 3/32 [00:01<00:15,  1.91it/s]Inference:  12%|█▎        | 4/32 [00:02<00:14,  1.90it/s]Inference:  16%|█▌        | 5/32 [00:02<00:14,  1.90it/s]Inference:  19%|█▉        | 6/32 [00:03<00:13,  1.90it/s]Inference:  22%|██▏       | 7/32 [00:03<00:13,  1.90it/s]Inference:  25%|██▌       | 8/32 [00:04<00:12,  1.91it/s]Inference:  28%|██▊       | 9/32 [00:04<00:12,  1.90it/s]Inference:  31%|███▏      | 10/32 [00:05<00:11,  1.90it/s]Inference:  34%|███▍      | 11/32 [00:05<00:11,  1.90it/s]Inference:  38%|███▊      | 12/32 [00:06<00:10,  1.94it/s]Inference:  41%|████      | 13/32 [00:06<00:09,  1.92it/s]Inference:  44%|████▍     | 14/32 [00:07<00:09,  1.92it/s]Inference:  47%|████▋     | 15/32 [00:07<00:08,  1.91it/s]Inference:  50%|█████     | 16/32 [00:08<00:08,  1.95it/s]Inference:  53%|█████▎    | 17/32 [00:08<00:07,  1.93it/s]Inference:  56%|█████▋    | 18/32 [00:09<00:07,  1.92it/s]Inference:  59%|█████▉    | 19/32 [00:09<00:06,  1.91it/s]Inference:  62%|██████▎   | 20/32 [00:10<00:06,  1.86it/s]Inference:  66%|██████▌   | 21/32 [00:11<00:05,  1.83it/s]Inference:  69%|██████▉   | 22/32 [00:11<00:05,  1.81it/s]Inference:  72%|███████▏  | 23/32 [00:12<00:04,  1.80it/s]Inference:  75%|███████▌  | 24/32 [00:12<00:04,  1.79it/s]Inference:  78%|███████▊  | 25/32 [00:13<00:03,  1.79it/s]Inference:  81%|████████▏ | 26/32 [00:13<00:03,  1.78it/s]Inference:  84%|████████▍ | 27/32 [00:14<00:02,  1.78it/s]Inference:  88%|████████▊ | 28/32 [00:15<00:02,  1.78it/s]Inference:  91%|█████████ | 29/32 [00:15<00:01,  1.78it/s]Inference:  94%|█████████▍| 30/32 [00:16<00:01,  1.77it/s]Inference:  97%|█████████▋| 31/32 [00:16<00:00,  1.78it/s]Inference: 100%|██████████| 32/32 [00:17<00:00,  1.78it/s]Inference: 100%|██████████| 32/32 [00:17<00:00,  1.85it/s]
layer17: self_attn.q_proj
iter 0, train loss 67013.09375, val loss None, lr 0.01
iter 250, train loss 803.8457641601562, val loss None, lr 0.01
iter 500, train loss 753.0216674804688, val loss None, lr 0.01
iter 750, train loss 689.2916259765625, val loss None, lr 0.01
iter 1000, train loss 641.1795654296875, val loss None, lr 0.003333
iter 1250, train loss 674.4769287109375, val loss None, lr 0.001111
iter 1500, train loss 606.25146484375, val loss None, lr 0.001111
iter 1750, train loss 600.9317626953125, val loss None, lr 0.001111
iter 2000, train loss 596.8832397460938, val loss None, lr 0.001111
iter 2250, train loss 592.3671264648438, val loss None, lr 0.001111
best loss 587.0394287109375
layer17: self_attn.k_proj
iter 0, train loss 88686.6484375, val loss None, lr 0.01
iter 250, train loss 1063.54150390625, val loss None, lr 0.01
iter 500, train loss 702.2623901367188, val loss None, lr 0.003333
iter 750, train loss 650.3291015625, val loss None, lr 0.003333
iter 1000, train loss 677.8252563476562, val loss None, lr 0.003333
iter 1250, train loss 614.06640625, val loss None, lr 0.001111
iter 1500, train loss 606.5696411132812, val loss None, lr 0.001111
iter 1750, train loss 602.086181640625, val loss None, lr 0.001111
iter 2000, train loss 595.37939453125, val loss None, lr 0.001111
iter 2250, train loss 588.7745361328125, val loss None, lr 0.00037
best loss 583.57861328125
layer17: self_attn.v_proj
iter 0, train loss 3739.6201171875, val loss None, lr 0.01
iter 250, train loss 257.60498046875, val loss None, lr 0.01
iter 500, train loss 241.71194458007812, val loss None, lr 0.01
iter 750, train loss 237.8036651611328, val loss None, lr 0.01
iter 1000, train loss 232.29270935058594, val loss None, lr 0.01
iter 1250, train loss 230.4302215576172, val loss None, lr 0.003333
iter 1500, train loss 223.96893310546875, val loss None, lr 0.003333
iter 1750, train loss 221.8427734375, val loss None, lr 0.001111
iter 2000, train loss 220.7040557861328, val loss None, lr 0.001111
iter 2250, train loss 219.90382385253906, val loss None, lr 0.001111
best loss 219.2401123046875
layer17: self_attn.o_proj
iter 0, train loss 2008.315185546875, val loss None, lr 0.01
iter 250, train loss 63.864051818847656, val loss None, lr 0.01
iter 500, train loss 56.78907775878906, val loss None, lr 0.01
iter 750, train loss 52.40493392944336, val loss None, lr 0.01
iter 1000, train loss 52.150882720947266, val loss None, lr 0.01
iter 1250, train loss 48.22966766357422, val loss None, lr 0.01
iter 1500, train loss 46.381378173828125, val loss None, lr 0.003333
iter 1750, train loss 45.930213928222656, val loss None, lr 0.003333
iter 2000, train loss 45.254669189453125, val loss None, lr 0.001111
iter 2250, train loss 44.892921447753906, val loss None, lr 0.001111
best loss 44.7118034362793
layer17: mlp.gate_proj
iter 0, train loss 16662.625, val loss None, lr 0.01
iter 250, train loss 901.95556640625, val loss None, lr 0.01
iter 500, train loss 832.1373291015625, val loss None, lr 0.01
iter 750, train loss 824.14306640625, val loss None, lr 0.01
iter 1000, train loss 781.1076049804688, val loss None, lr 0.003333
iter 1250, train loss 765.8055419921875, val loss None, lr 0.001111
iter 1500, train loss 762.5604248046875, val loss None, lr 0.001111
iter 1750, train loss 760.0242919921875, val loss None, lr 0.001111
iter 2000, train loss 758.0540771484375, val loss None, lr 0.001111
iter 2250, train loss 755.737060546875, val loss None, lr 0.001111
best loss 753.584716796875
layer17: mlp.up_proj
iter 0, train loss 9188.67578125, val loss None, lr 0.01
iter 250, train loss 757.4675903320312, val loss None, lr 0.01
iter 500, train loss 725.0894775390625, val loss None, lr 0.01
iter 750, train loss 718.7332153320312, val loss None, lr 0.01
iter 1000, train loss 703.2520141601562, val loss None, lr 0.01
iter 1250, train loss 693.3887329101562, val loss None, lr 0.003333
iter 1500, train loss 695.4475708007812, val loss None, lr 0.003333
iter 1750, train loss 686.0582885742188, val loss None, lr 0.003333
iter 2000, train loss 683.0313110351562, val loss None, lr 0.001111
iter 2250, train loss 681.5325927734375, val loss None, lr 0.001111
best loss 680.336669921875
layer17: mlp.down_proj
iter 0, train loss 281.1656494140625, val loss None, lr 0.01
iter 250, train loss 17.891836166381836, val loss None, lr 0.01
iter 500, train loss 16.710145950317383, val loss None, lr 0.01
iter 750, train loss 16.147497177124023, val loss None, lr 0.003333
iter 1000, train loss 15.319218635559082, val loss None, lr 0.001111
iter 1250, train loss 15.159912109375, val loss None, lr 0.001111
iter 1500, train loss 15.06566333770752, val loss None, lr 0.001111
iter 1750, train loss 14.984739303588867, val loss None, lr 0.001111
iter 2000, train loss 14.945178985595703, val loss None, lr 0.001111
iter 2250, train loss 14.835990905761719, val loss None, lr 0.00037
best loss 14.786271095275879
26177 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:06,  4.84it/s]Inference:   6%|▋         | 2/32 [00:00<00:06,  4.34it/s]Inference:   9%|▉         | 3/32 [00:00<00:06,  4.19it/s]Inference:  12%|█▎        | 4/32 [00:00<00:06,  4.12it/s]Inference:  16%|█▌        | 5/32 [00:01<00:06,  4.12it/s]Inference:  19%|█▉        | 6/32 [00:01<00:06,  4.10it/s]Inference:  22%|██▏       | 7/32 [00:01<00:06,  4.10it/s]Inference:  25%|██▌       | 8/32 [00:01<00:05,  4.09it/s]Inference:  28%|██▊       | 9/32 [00:02<00:05,  4.33it/s]Inference:  31%|███▏      | 10/32 [00:02<00:05,  4.25it/s]Inference:  34%|███▍      | 11/32 [00:02<00:05,  4.19it/s]Inference:  38%|███▊      | 12/32 [00:02<00:04,  4.15it/s]Inference:  41%|████      | 13/32 [00:03<00:04,  4.12it/s]Inference:  44%|████▍     | 14/32 [00:03<00:04,  4.35it/s]Inference:  47%|████▋     | 15/32 [00:03<00:03,  4.25it/s]Inference:  50%|█████     | 16/32 [00:03<00:03,  4.21it/s]Inference:  53%|█████▎    | 17/32 [00:04<00:03,  4.16it/s]Inference:  56%|█████▋    | 18/32 [00:04<00:03,  4.14it/s]Inference:  59%|█████▉    | 19/32 [00:04<00:03,  4.11it/s]Inference:  62%|██████▎   | 20/32 [00:04<00:02,  4.09it/s]Inference:  66%|██████▌   | 21/32 [00:05<00:02,  4.08it/s]Inference:  69%|██████▉   | 22/32 [00:05<00:02,  4.07it/s]Inference:  72%|███████▏  | 23/32 [00:05<00:02,  4.06it/s]Inference:  75%|███████▌  | 24/32 [00:05<00:01,  4.07it/s]Inference:  78%|███████▊  | 25/32 [00:06<00:01,  4.07it/s]Inference:  81%|████████▏ | 26/32 [00:06<00:01,  4.09it/s]Inference:  84%|████████▍ | 27/32 [00:06<00:01,  4.08it/s]Inference:  88%|████████▊ | 28/32 [00:06<00:00,  4.07it/s]Inference:  91%|█████████ | 29/32 [00:07<00:00,  4.06it/s]Inference:  94%|█████████▍| 30/32 [00:07<00:00,  4.06it/s]Inference:  97%|█████████▋| 31/32 [00:07<00:00,  4.05it/s]Inference: 100%|██████████| 32/32 [00:07<00:00,  4.05it/s]Inference: 100%|██████████| 32/32 [00:07<00:00,  4.13it/s]
20873 MiB free out of 48676 MiB total
Saved layer 17 to ./models/meta-llama/Llama-2-7b-hf/tensorized/128/no_finetune2/layer_17.pt
after cast to cpu
24889 MiB free out of 48676 MiB total
Done with layer 17 total_time elapsed: 17747 estimated time left: 13803
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:16,  1.89it/s]Inference:   6%|▋         | 2/32 [00:01<00:15,  1.90it/s]Inference:   9%|▉         | 3/32 [00:01<00:15,  1.89it/s]Inference:  12%|█▎        | 4/32 [00:02<00:14,  1.89it/s]Inference:  16%|█▌        | 5/32 [00:02<00:14,  1.89it/s]Inference:  19%|█▉        | 6/32 [00:03<00:13,  1.88it/s]Inference:  22%|██▏       | 7/32 [00:03<00:13,  1.88it/s]Inference:  25%|██▌       | 8/32 [00:04<00:12,  1.89it/s]Inference:  28%|██▊       | 9/32 [00:04<00:12,  1.89it/s]Inference:  31%|███▏      | 10/32 [00:05<00:11,  1.89it/s]Inference:  34%|███▍      | 11/32 [00:05<00:11,  1.89it/s]Inference:  38%|███▊      | 12/32 [00:06<00:10,  1.89it/s]Inference:  41%|████      | 13/32 [00:06<00:10,  1.89it/s]Inference:  44%|████▍     | 14/32 [00:07<00:09,  1.93it/s]Inference:  47%|████▋     | 15/32 [00:07<00:08,  1.91it/s]Inference:  50%|█████     | 16/32 [00:08<00:08,  1.90it/s]Inference:  53%|█████▎    | 17/32 [00:08<00:07,  1.90it/s]Inference:  56%|█████▋    | 18/32 [00:09<00:07,  1.90it/s]Inference:  59%|█████▉    | 19/32 [00:09<00:06,  1.93it/s]Inference:  62%|██████▎   | 20/32 [00:10<00:06,  1.91it/s]Inference:  66%|██████▌   | 21/32 [00:11<00:05,  1.90it/s]Inference:  69%|██████▉   | 22/32 [00:11<00:05,  1.90it/s]Inference:  72%|███████▏  | 23/32 [00:12<00:04,  1.94it/s]Inference:  75%|███████▌  | 24/32 [00:12<00:04,  1.92it/s]Inference:  78%|███████▊  | 25/32 [00:13<00:03,  1.91it/s]Inference:  81%|████████▏ | 26/32 [00:13<00:03,  1.90it/s]Inference:  84%|████████▍ | 27/32 [00:14<00:02,  1.90it/s]Inference:  88%|████████▊ | 28/32 [00:14<00:02,  1.90it/s]Inference:  91%|█████████ | 29/32 [00:15<00:01,  1.89it/s]Inference:  94%|█████████▍| 30/32 [00:15<00:01,  1.89it/s]Inference:  97%|█████████▋| 31/32 [00:16<00:00,  1.89it/s]Inference: 100%|██████████| 32/32 [00:16<00:00,  1.90it/s]Inference: 100%|██████████| 32/32 [00:16<00:00,  1.90it/s]
layer18: self_attn.q_proj
iter 0, train loss 73160.3828125, val loss None, lr 0.01
iter 250, train loss 840.2667236328125, val loss None, lr 0.01
iter 500, train loss 751.6475830078125, val loss None, lr 0.01
iter 750, train loss 628.7049560546875, val loss None, lr 0.003333
iter 1000, train loss 602.6486206054688, val loss None, lr 0.001111
iter 1250, train loss 590.9660034179688, val loss None, lr 0.001111
iter 1500, train loss 579.7527465820312, val loss None, lr 0.001111
iter 1750, train loss 575.6873779296875, val loss None, lr 0.001111
iter 2000, train loss 572.21240234375, val loss None, lr 0.001111
iter 2250, train loss 567.42333984375, val loss None, lr 0.001111
best loss 560.1657104492188
layer18: self_attn.k_proj
iter 0, train loss 89568.1796875, val loss None, lr 0.01
iter 250, train loss 815.098876953125, val loss None, lr 0.01
iter 500, train loss 703.1734619140625, val loss None, lr 0.003333
iter 750, train loss 633.8109130859375, val loss None, lr 0.001111
iter 1000, train loss 610.4666748046875, val loss None, lr 0.001111
iter 1250, train loss 602.01611328125, val loss None, lr 0.001111
iter 1500, train loss 592.5038452148438, val loss None, lr 0.001111
iter 1750, train loss 583.5572509765625, val loss None, lr 0.001111
iter 2000, train loss 580.243896484375, val loss None, lr 0.001111
iter 2250, train loss 572.9896850585938, val loss None, lr 0.001111
best loss 568.3165893554688
layer18: self_attn.v_proj
iter 0, train loss 4885.36669921875, val loss None, lr 0.01
iter 250, train loss 290.9852294921875, val loss None, lr 0.01
iter 500, train loss 274.58203125, val loss None, lr 0.01
iter 750, train loss 263.5282897949219, val loss None, lr 0.01
iter 1000, train loss 253.81979370117188, val loss None, lr 0.003333
iter 1250, train loss 250.68545532226562, val loss None, lr 0.001111
iter 1500, train loss 249.36196899414062, val loss None, lr 0.001111
iter 1750, train loss 248.04238891601562, val loss None, lr 0.001111
iter 2000, train loss 247.13404846191406, val loss None, lr 0.001111
iter 2250, train loss 246.33689880371094, val loss None, lr 0.001111
best loss 245.605224609375
layer18: self_attn.o_proj
iter 0, train loss 2856.788330078125, val loss None, lr 0.01
iter 250, train loss 89.457275390625, val loss None, lr 0.01
iter 500, train loss 85.63512420654297, val loss None, lr 0.01
iter 750, train loss 73.67828369140625, val loss None, lr 0.01
iter 1000, train loss 71.92787170410156, val loss None, lr 0.01
iter 1250, train loss 64.41008758544922, val loss None, lr 0.003333
iter 1500, train loss 63.07773971557617, val loss None, lr 0.001111
iter 1750, train loss 62.60042190551758, val loss None, lr 0.001111
iter 2000, train loss 62.224525451660156, val loss None, lr 0.001111
iter 2250, train loss 61.91203308105469, val loss None, lr 0.001111
best loss 61.32100296020508
layer18: mlp.gate_proj
iter 0, train loss 19826.84375, val loss None, lr 0.01
iter 250, train loss 911.048583984375, val loss None, lr 0.01
iter 500, train loss 888.0958251953125, val loss None, lr 0.01
iter 750, train loss 880.3501586914062, val loss None, lr 0.01
iter 1000, train loss 831.7035522460938, val loss None, lr 0.003333
iter 1250, train loss 819.84521484375, val loss None, lr 0.001111
iter 1500, train loss 809.2811279296875, val loss None, lr 0.001111
iter 1750, train loss 806.2708740234375, val loss None, lr 0.001111
iter 2000, train loss 803.073486328125, val loss None, lr 0.001111
iter 2250, train loss 800.3960571289062, val loss None, lr 0.00037
best loss 798.8716430664062
layer18: mlp.up_proj
iter 0, train loss 10390.900390625, val loss None, lr 0.01
iter 250, train loss 789.98828125, val loss None, lr 0.01
iter 500, train loss 766.0166015625, val loss None, lr 0.01
iter 750, train loss 762.657470703125, val loss None, lr 0.01
iter 1000, train loss 735.9500122070312, val loss None, lr 0.01
iter 1250, train loss 718.7037353515625, val loss None, lr 0.003333
iter 1500, train loss 723.2599487304688, val loss None, lr 0.001111
iter 1750, train loss 710.6843872070312, val loss None, lr 0.001111
iter 2000, train loss 708.7799072265625, val loss None, lr 0.001111
iter 2250, train loss 707.1951904296875, val loss None, lr 0.001111
best loss 705.5557250976562
layer18: mlp.down_proj
iter 0, train loss 263.1689758300781, val loss None, lr 0.01
iter 250, train loss 17.713029861450195, val loss None, lr 0.01
iter 500, train loss 17.0987548828125, val loss None, lr 0.01
iter 750, train loss 15.980203628540039, val loss None, lr 0.01
iter 1000, train loss 15.599000930786133, val loss None, lr 0.01
iter 1250, train loss 15.609517097473145, val loss None, lr 0.01
iter 1500, train loss 14.85948371887207, val loss None, lr 0.003333
iter 1750, train loss 14.744529724121094, val loss None, lr 0.003333
iter 2000, train loss 14.629085540771484, val loss None, lr 0.001111
iter 2250, train loss 14.581188201904297, val loss None, lr 0.001111
best loss 14.555554389953613
24889 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:06,  4.81it/s]Inference:   6%|▋         | 2/32 [00:00<00:06,  4.43it/s]Inference:   9%|▉         | 3/32 [00:00<00:06,  4.26it/s]Inference:  12%|█▎        | 4/32 [00:00<00:06,  4.18it/s]Inference:  16%|█▌        | 5/32 [00:01<00:06,  4.13it/s]Inference:  19%|█▉        | 6/32 [00:01<00:06,  4.10it/s]Inference:  22%|██▏       | 7/32 [00:01<00:06,  4.08it/s]Inference:  25%|██▌       | 8/32 [00:01<00:05,  4.08it/s]Inference:  28%|██▊       | 9/32 [00:02<00:05,  4.06it/s]Inference:  31%|███▏      | 10/32 [00:02<00:05,  4.06it/s]Inference:  34%|███▍      | 11/32 [00:02<00:05,  4.07it/s]Inference:  38%|███▊      | 12/32 [00:02<00:04,  4.07it/s]Inference:  41%|████      | 13/32 [00:03<00:04,  4.06it/s]Inference:  44%|████▍     | 14/32 [00:03<00:04,  4.06it/s]Inference:  47%|████▋     | 15/32 [00:03<00:04,  4.05it/s]Inference:  50%|█████     | 16/32 [00:03<00:03,  4.06it/s]Inference:  53%|█████▎    | 17/32 [00:04<00:03,  4.05it/s]Inference:  56%|█████▋    | 18/32 [00:04<00:03,  4.06it/s]Inference:  59%|█████▉    | 19/32 [00:04<00:03,  4.07it/s]Inference:  62%|██████▎   | 20/32 [00:04<00:02,  4.08it/s]Inference:  66%|██████▌   | 21/32 [00:05<00:02,  4.06it/s]Inference:  69%|██████▉   | 22/32 [00:05<00:02,  4.07it/s]Inference:  72%|███████▏  | 23/32 [00:05<00:02,  4.05it/s]Inference:  75%|███████▌  | 24/32 [00:05<00:01,  4.06it/s]Inference:  78%|███████▊  | 25/32 [00:06<00:01,  4.04it/s]Inference:  81%|████████▏ | 26/32 [00:06<00:01,  4.04it/s]Inference:  84%|████████▍ | 27/32 [00:06<00:01,  4.05it/s]Inference:  88%|████████▊ | 28/32 [00:06<00:00,  4.04it/s]Inference:  91%|█████████ | 29/32 [00:07<00:00,  4.05it/s]Inference:  94%|█████████▍| 30/32 [00:07<00:00,  4.05it/s]Inference:  97%|█████████▋| 31/32 [00:07<00:00,  4.06it/s]Inference: 100%|██████████| 32/32 [00:07<00:00,  4.06it/s]Inference: 100%|██████████| 32/32 [00:07<00:00,  4.08it/s]
19649 MiB free out of 48676 MiB total
Saved layer 18 to ./models/meta-llama/Llama-2-7b-hf/tensorized/128/no_finetune2/layer_18.pt
after cast to cpu
23665 MiB free out of 48676 MiB total
Done with layer 18 total_time elapsed: 18615 estimated time left: 12736
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:16,  1.89it/s]Inference:   6%|▋         | 2/32 [00:01<00:15,  1.90it/s]Inference:   9%|▉         | 3/32 [00:01<00:15,  1.89it/s]Inference:  12%|█▎        | 4/32 [00:02<00:14,  1.88it/s]Inference:  16%|█▌        | 5/32 [00:02<00:14,  1.88it/s]Inference:  19%|█▉        | 6/32 [00:03<00:13,  1.88it/s]Inference:  22%|██▏       | 7/32 [00:03<00:13,  1.88it/s]Inference:  25%|██▌       | 8/32 [00:04<00:12,  1.89it/s]Inference:  28%|██▊       | 9/32 [00:04<00:12,  1.89it/s]Inference:  31%|███▏      | 10/32 [00:05<00:11,  1.89it/s]Inference:  34%|███▍      | 11/32 [00:05<00:11,  1.88it/s]Inference:  38%|███▊      | 12/32 [00:06<00:10,  1.89it/s]Inference:  41%|████      | 13/32 [00:06<00:10,  1.89it/s]Inference:  44%|████▍     | 14/32 [00:07<00:09,  1.89it/s]Inference:  47%|████▋     | 15/32 [00:07<00:09,  1.88it/s]Inference:  50%|█████     | 16/32 [00:08<00:08,  1.88it/s]Inference:  53%|█████▎    | 17/32 [00:09<00:07,  1.88it/s]Inference:  56%|█████▋    | 18/32 [00:09<00:07,  1.88it/s]Inference:  59%|█████▉    | 19/32 [00:10<00:06,  1.88it/s]Inference:  62%|██████▎   | 20/32 [00:10<00:06,  1.88it/s]Inference:  66%|██████▌   | 21/32 [00:11<00:05,  1.88it/s]Inference:  69%|██████▉   | 22/32 [00:11<00:05,  1.88it/s]Inference:  72%|███████▏  | 23/32 [00:12<00:04,  1.88it/s]Inference:  75%|███████▌  | 24/32 [00:12<00:04,  1.88it/s]Inference:  78%|███████▊  | 25/32 [00:13<00:03,  1.88it/s]Inference:  81%|████████▏ | 26/32 [00:13<00:03,  1.88it/s]Inference:  84%|████████▍ | 27/32 [00:14<00:02,  1.88it/s]Inference:  88%|████████▊ | 28/32 [00:14<00:02,  1.89it/s]Inference:  91%|█████████ | 29/32 [00:15<00:01,  1.89it/s]Inference:  94%|█████████▍| 30/32 [00:15<00:01,  1.88it/s]Inference:  97%|█████████▋| 31/32 [00:16<00:00,  1.88it/s]Inference: 100%|██████████| 32/32 [00:16<00:00,  1.89it/s]Inference: 100%|██████████| 32/32 [00:16<00:00,  1.88it/s]
layer19: self_attn.q_proj
iter 0, train loss 76575.3125, val loss None, lr 0.01
iter 250, train loss 731.24755859375, val loss None, lr 0.01
iter 500, train loss 671.668701171875, val loss None, lr 0.01
iter 750, train loss 593.269287109375, val loss None, lr 0.01
iter 1000, train loss 565.9306030273438, val loss None, lr 0.003333
iter 1250, train loss 537.845703125, val loss None, lr 0.001111
iter 1500, train loss 527.5997314453125, val loss None, lr 0.001111
iter 1750, train loss 523.1219482421875, val loss None, lr 0.001111
iter 2000, train loss 515.16015625, val loss None, lr 0.001111
iter 2250, train loss 511.18695068359375, val loss None, lr 0.00037
best loss 507.0129699707031
layer19: self_attn.k_proj
iter 0, train loss 102614.890625, val loss None, lr 0.01
iter 250, train loss 1062.57666015625, val loss None, lr 0.01
iter 500, train loss 641.1844482421875, val loss None, lr 0.003333
iter 750, train loss 618.554443359375, val loss None, lr 0.003333
iter 1000, train loss 575.901611328125, val loss None, lr 0.001111
iter 1250, train loss 548.71435546875, val loss None, lr 0.001111
iter 1500, train loss 537.43603515625, val loss None, lr 0.001111
iter 1750, train loss 528.0313720703125, val loss None, lr 0.001111
iter 2000, train loss 525.4918212890625, val loss None, lr 0.001111
iter 2250, train loss 519.3845825195312, val loss None, lr 0.001111
best loss 512.4083862304688
layer19: self_attn.v_proj
iter 0, train loss 5112.60693359375, val loss None, lr 0.01
iter 250, train loss 268.3119201660156, val loss None, lr 0.01
iter 500, train loss 251.27154541015625, val loss None, lr 0.01
iter 750, train loss 243.7482452392578, val loss None, lr 0.01
iter 1000, train loss 239.7426300048828, val loss None, lr 0.01
iter 1250, train loss 231.56845092773438, val loss None, lr 0.003333
iter 1500, train loss 228.9392547607422, val loss None, lr 0.003333
iter 1750, train loss 230.2859649658203, val loss None, lr 0.003333
iter 2000, train loss 225.59808349609375, val loss None, lr 0.001111
iter 2250, train loss 224.83401489257812, val loss None, lr 0.001111
best loss 224.03909301757812
layer19: self_attn.o_proj
iter 0, train loss 3404.710693359375, val loss None, lr 0.01
iter 250, train loss 95.20333099365234, val loss None, lr 0.01
iter 500, train loss 77.98017883300781, val loss None, lr 0.01
iter 750, train loss 72.86273956298828, val loss None, lr 0.01
iter 1000, train loss 64.9502182006836, val loss None, lr 0.003333
iter 1250, train loss 63.00438690185547, val loss None, lr 0.003333
iter 1500, train loss 61.15266418457031, val loss None, lr 0.001111
iter 1750, train loss 60.45533752441406, val loss None, lr 0.001111
iter 2000, train loss 60.108116149902344, val loss None, lr 0.001111
iter 2250, train loss 59.8571891784668, val loss None, lr 0.001111
best loss 59.11183166503906
layer19: mlp.gate_proj
iter 0, train loss 20328.755859375, val loss None, lr 0.01
iter 250, train loss 911.6024780273438, val loss None, lr 0.01
iter 500, train loss 837.299560546875, val loss None, lr 0.003333
iter 750, train loss 818.005615234375, val loss None, lr 0.003333
iter 1000, train loss 807.4751586914062, val loss None, lr 0.001111
iter 1250, train loss 801.0054931640625, val loss None, lr 0.001111
iter 1500, train loss 796.3519287109375, val loss None, lr 0.001111
iter 1750, train loss 794.0170288085938, val loss None, lr 0.001111
iter 2000, train loss 791.6953125, val loss None, lr 0.001111
iter 2250, train loss 789.0930786132812, val loss None, lr 0.001111
best loss 786.0052490234375
layer19: mlp.up_proj
iter 0, train loss 11118.162109375, val loss None, lr 0.01
iter 250, train loss 775.8619995117188, val loss None, lr 0.01
iter 500, train loss 742.8236083984375, val loss None, lr 0.01
iter 750, train loss 726.73291015625, val loss None, lr 0.01
iter 1000, train loss 715.4657592773438, val loss None, lr 0.01
iter 1250, train loss 708.5501098632812, val loss None, lr 0.01
iter 1500, train loss 694.3995361328125, val loss None, lr 0.003333
iter 1750, train loss 689.3328247070312, val loss None, lr 0.001111
iter 2000, train loss 687.7525634765625, val loss None, lr 0.001111
iter 2250, train loss 686.439453125, val loss None, lr 0.001111
best loss 685.138427734375
layer19: mlp.down_proj
iter 0, train loss 285.2220764160156, val loss None, lr 0.01
iter 250, train loss 16.980632781982422, val loss None, lr 0.01
iter 500, train loss 15.760640144348145, val loss None, lr 0.01
iter 750, train loss 15.628447532653809, val loss None, lr 0.01
iter 1000, train loss 15.373478889465332, val loss None, lr 0.01
iter 1250, train loss 14.886916160583496, val loss None, lr 0.01
iter 1500, train loss 14.533756256103516, val loss None, lr 0.01
iter 1750, train loss 14.322965621948242, val loss None, lr 0.003333
iter 2000, train loss 14.148675918579102, val loss None, lr 0.001111
iter 2250, train loss 14.082542419433594, val loss None, lr 0.001111
best loss 14.053873062133789
23665 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:06,  4.83it/s]Inference:   6%|▋         | 2/32 [00:00<00:06,  4.38it/s]Inference:   9%|▉         | 3/32 [00:00<00:06,  4.48it/s]Inference:  12%|█▎        | 4/32 [00:00<00:06,  4.32it/s]Inference:  16%|█▌        | 5/32 [00:01<00:06,  4.22it/s]Inference:  19%|█▉        | 6/32 [00:01<00:06,  4.17it/s]Inference:  22%|██▏       | 7/32 [00:01<00:06,  4.14it/s]Inference:  25%|██▌       | 8/32 [00:01<00:05,  4.16it/s]Inference:  28%|██▊       | 9/32 [00:02<00:05,  4.14it/s]Inference:  31%|███▏      | 10/32 [00:02<00:05,  4.13it/s]Inference:  34%|███▍      | 11/32 [00:02<00:05,  4.14it/s]Inference:  38%|███▊      | 12/32 [00:02<00:04,  4.13it/s]Inference:  41%|████      | 13/32 [00:03<00:04,  4.11it/s]Inference:  44%|████▍     | 14/32 [00:03<00:04,  4.35it/s]Inference:  47%|████▋     | 15/32 [00:03<00:03,  4.26it/s]Inference:  50%|█████     | 16/32 [00:03<00:03,  4.21it/s]Inference:  53%|█████▎    | 17/32 [00:04<00:03,  4.17it/s]Inference:  56%|█████▋    | 18/32 [00:04<00:03,  4.15it/s]Inference:  59%|█████▉    | 19/32 [00:04<00:03,  4.14it/s]Inference:  62%|██████▎   | 20/32 [00:04<00:02,  4.12it/s]Inference:  66%|██████▌   | 21/32 [00:05<00:02,  4.11it/s]Inference:  69%|██████▉   | 22/32 [00:05<00:02,  4.11it/s]Inference:  72%|███████▏  | 23/32 [00:05<00:02,  4.10it/s]Inference:  75%|███████▌  | 24/32 [00:05<00:01,  4.22it/s]Inference:  78%|███████▊  | 25/32 [00:05<00:01,  4.22it/s]Inference:  81%|████████▏ | 26/32 [00:06<00:01,  4.19it/s]Inference:  84%|████████▍ | 27/32 [00:06<00:01,  4.24it/s]Inference:  88%|████████▊ | 28/32 [00:06<00:00,  4.20it/s]Inference:  91%|█████████ | 29/32 [00:06<00:00,  4.17it/s]Inference:  94%|█████████▍| 30/32 [00:07<00:00,  4.14it/s]Inference:  97%|█████████▋| 31/32 [00:07<00:00,  4.13it/s]Inference: 100%|██████████| 32/32 [00:07<00:00,  4.12it/s]Inference: 100%|██████████| 32/32 [00:07<00:00,  4.18it/s]
18361 MiB free out of 48676 MiB total
Saved layer 19 to ./models/meta-llama/Llama-2-7b-hf/tensorized/128/no_finetune2/layer_19.pt
after cast to cpu
22377 MiB free out of 48676 MiB total
Done with layer 19 total_time elapsed: 19482 estimated time left: 11689
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:16,  1.92it/s]Inference:   6%|▋         | 2/32 [00:01<00:15,  1.91it/s]Inference:   9%|▉         | 3/32 [00:01<00:15,  1.90it/s]Inference:  12%|█▎        | 4/32 [00:02<00:14,  1.90it/s]Inference:  16%|█▌        | 5/32 [00:02<00:14,  1.89it/s]Inference:  19%|█▉        | 6/32 [00:03<00:13,  1.89it/s]Inference:  22%|██▏       | 7/32 [00:03<00:13,  1.89it/s]Inference:  25%|██▌       | 8/32 [00:04<00:12,  1.90it/s]Inference:  28%|██▊       | 9/32 [00:04<00:12,  1.89it/s]Inference:  31%|███▏      | 10/32 [00:05<00:11,  1.90it/s]Inference:  34%|███▍      | 11/32 [00:05<00:11,  1.89it/s]Inference:  38%|███▊      | 12/32 [00:06<00:10,  1.90it/s]Inference:  41%|████      | 13/32 [00:06<00:10,  1.90it/s]Inference:  44%|████▍     | 14/32 [00:07<00:09,  1.90it/s]Inference:  47%|████▋     | 15/32 [00:07<00:08,  1.89it/s]Inference:  50%|█████     | 16/32 [00:08<00:08,  1.89it/s]Inference:  53%|█████▎    | 17/32 [00:08<00:07,  1.90it/s]Inference:  56%|█████▋    | 18/32 [00:09<00:07,  1.90it/s]Inference:  59%|█████▉    | 19/32 [00:10<00:06,  1.89it/s]Inference:  62%|██████▎   | 20/32 [00:10<00:06,  1.90it/s]Inference:  66%|██████▌   | 21/32 [00:11<00:05,  1.90it/s]Inference:  69%|██████▉   | 22/32 [00:11<00:05,  1.90it/s]Inference:  72%|███████▏  | 23/32 [00:12<00:04,  1.89it/s]Inference:  75%|███████▌  | 24/32 [00:12<00:04,  1.89it/s]Inference:  78%|███████▊  | 25/32 [00:13<00:03,  1.89it/s]Inference:  81%|████████▏ | 26/32 [00:13<00:03,  1.89it/s]Inference:  84%|████████▍ | 27/32 [00:14<00:02,  1.89it/s]Inference:  88%|████████▊ | 28/32 [00:14<00:02,  1.90it/s]Inference:  91%|█████████ | 29/32 [00:15<00:01,  1.90it/s]Inference:  94%|█████████▍| 30/32 [00:15<00:01,  1.90it/s]Inference:  97%|█████████▋| 31/32 [00:16<00:00,  1.89it/s]Inference: 100%|██████████| 32/32 [00:16<00:00,  1.90it/s]Inference: 100%|██████████| 32/32 [00:16<00:00,  1.90it/s]
layer20: self_attn.q_proj
iter 0, train loss 86890.125, val loss None, lr 0.01
iter 250, train loss 892.9027709960938, val loss None, lr 0.01
iter 500, train loss 682.6166381835938, val loss None, lr 0.01
iter 750, train loss 580.1728515625, val loss None, lr 0.003333
iter 1000, train loss 536.3554077148438, val loss None, lr 0.001111
iter 1250, train loss 546.2988891601562, val loss None, lr 0.001111
iter 1500, train loss 518.2839965820312, val loss None, lr 0.001111
iter 1750, train loss 512.4520874023438, val loss None, lr 0.001111
iter 2000, train loss 506.872314453125, val loss None, lr 0.001111
iter 2250, train loss 503.41339111328125, val loss None, lr 0.001111
best loss 497.38165283203125
layer20: self_attn.k_proj
iter 0, train loss 112080.8515625, val loss None, lr 0.01
iter 250, train loss 821.2800903320312, val loss None, lr 0.01
iter 500, train loss 781.272705078125, val loss None, lr 0.01
iter 750, train loss 601.730712890625, val loss None, lr 0.003333
iter 1000, train loss 540.9253540039062, val loss None, lr 0.001111
iter 1250, train loss 525.3241577148438, val loss None, lr 0.001111
iter 1500, train loss 511.82073974609375, val loss None, lr 0.001111
iter 1750, train loss 508.345947265625, val loss None, lr 0.001111
iter 2000, train loss 502.5482482910156, val loss None, lr 0.001111
iter 2250, train loss 496.48333740234375, val loss None, lr 0.001111
best loss 490.159423828125
layer20: self_attn.v_proj
iter 0, train loss 4830.7880859375, val loss None, lr 0.01
iter 250, train loss 266.3006286621094, val loss None, lr 0.01
iter 500, train loss 242.3269805908203, val loss None, lr 0.01
iter 750, train loss 236.0552215576172, val loss None, lr 0.01
iter 1000, train loss 222.99205017089844, val loss None, lr 0.003333
iter 1250, train loss 221.83633422851562, val loss None, lr 0.003333
iter 1500, train loss 217.48275756835938, val loss None, lr 0.001111
iter 1750, train loss 216.436279296875, val loss None, lr 0.001111
iter 2000, train loss 215.57241821289062, val loss None, lr 0.001111
iter 2250, train loss 214.8081512451172, val loss None, lr 0.001111
best loss 213.91481018066406
layer20: self_attn.o_proj
iter 0, train loss 4182.53759765625, val loss None, lr 0.01
iter 250, train loss 103.67843627929688, val loss None, lr 0.01
iter 500, train loss 94.15296936035156, val loss None, lr 0.01
iter 750, train loss 69.84674072265625, val loss None, lr 0.003333
iter 1000, train loss 66.04739379882812, val loss None, lr 0.001111
iter 1250, train loss 64.14981079101562, val loss None, lr 0.001111
iter 1500, train loss 63.22380828857422, val loss None, lr 0.001111
iter 1750, train loss 62.78438949584961, val loss None, lr 0.001111
iter 2000, train loss 61.68616485595703, val loss None, lr 0.00037
iter 2250, train loss 61.04535675048828, val loss None, lr 0.00037
best loss 60.614952087402344
layer20: mlp.gate_proj
iter 0, train loss 22923.236328125, val loss None, lr 0.01
iter 250, train loss 998.876708984375, val loss None, lr 0.01
iter 500, train loss 935.1943359375, val loss None, lr 0.01
iter 750, train loss 932.57373046875, val loss None, lr 0.01
iter 1000, train loss 889.996337890625, val loss None, lr 0.001111
iter 1250, train loss 863.6475830078125, val loss None, lr 0.001111
iter 1500, train loss 858.16015625, val loss None, lr 0.001111
iter 1750, train loss 854.6783447265625, val loss None, lr 0.001111
iter 2000, train loss 853.472412109375, val loss None, lr 0.00037
iter 2250, train loss 848.432373046875, val loss None, lr 0.00037
best loss 845.7864990234375
layer20: mlp.up_proj
iter 0, train loss 12904.822265625, val loss None, lr 0.01
iter 250, train loss 824.0328369140625, val loss None, lr 0.01
iter 500, train loss 811.5963134765625, val loss None, lr 0.01
iter 750, train loss 774.5233154296875, val loss None, lr 0.01
iter 1000, train loss 780.1749267578125, val loss None, lr 0.01
iter 1250, train loss 741.8887329101562, val loss None, lr 0.003333
iter 1500, train loss 737.018310546875, val loss None, lr 0.001111
iter 1750, train loss 734.7017822265625, val loss None, lr 0.001111
iter 2000, train loss 732.6705932617188, val loss None, lr 0.001111
iter 2250, train loss 731.6110229492188, val loss None, lr 0.001111
best loss 729.9105224609375
layer20: mlp.down_proj
iter 0, train loss 331.572265625, val loss None, lr 0.01
iter 250, train loss 18.854625701904297, val loss None, lr 0.01
iter 500, train loss 17.221961975097656, val loss None, lr 0.01
iter 750, train loss 16.54051971435547, val loss None, lr 0.01
iter 1000, train loss 15.92735481262207, val loss None, lr 0.003333
iter 1250, train loss 15.49679183959961, val loss None, lr 0.003333
iter 1500, train loss 15.209078788757324, val loss None, lr 0.001111
iter 1750, train loss 15.15214729309082, val loss None, lr 0.001111
iter 2000, train loss 15.086687088012695, val loss None, lr 0.001111
iter 2250, train loss 15.038646697998047, val loss None, lr 0.001111
best loss 15.00168228149414
22377 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:06,  4.84it/s]Inference:   6%|▋         | 2/32 [00:00<00:06,  4.40it/s]Inference:   9%|▉         | 3/32 [00:00<00:06,  4.27it/s]Inference:  12%|█▎        | 4/32 [00:00<00:06,  4.18it/s]Inference:  16%|█▌        | 5/32 [00:01<00:06,  4.15it/s]Inference:  19%|█▉        | 6/32 [00:01<00:06,  4.11it/s]Inference:  22%|██▏       | 7/32 [00:01<00:06,  4.11it/s]Inference:  25%|██▌       | 8/32 [00:01<00:05,  4.08it/s]Inference:  28%|██▊       | 9/32 [00:02<00:05,  4.08it/s]Inference:  31%|███▏      | 10/32 [00:02<00:05,  4.07it/s]Inference:  34%|███▍      | 11/32 [00:02<00:05,  4.09it/s]Inference:  38%|███▊      | 12/32 [00:02<00:04,  4.10it/s]Inference:  41%|████      | 13/32 [00:03<00:04,  4.09it/s]Inference:  44%|████▍     | 14/32 [00:03<00:04,  4.10it/s]Inference:  47%|████▋     | 15/32 [00:03<00:04,  4.10it/s]Inference:  50%|█████     | 16/32 [00:03<00:03,  4.10it/s]Inference:  53%|█████▎    | 17/32 [00:04<00:03,  4.08it/s]Inference:  56%|█████▋    | 18/32 [00:04<00:03,  4.08it/s]Inference:  59%|█████▉    | 19/32 [00:04<00:03,  4.07it/s]Inference:  62%|██████▎   | 20/32 [00:04<00:02,  4.08it/s]Inference:  66%|██████▌   | 21/32 [00:05<00:02,  4.09it/s]Inference:  69%|██████▉   | 22/32 [00:05<00:02,  4.08it/s]Inference:  72%|███████▏  | 23/32 [00:05<00:02,  4.08it/s]Inference:  75%|███████▌  | 24/32 [00:05<00:01,  4.06it/s]Inference:  78%|███████▊  | 25/32 [00:06<00:01,  4.07it/s]Inference:  81%|████████▏ | 26/32 [00:06<00:01,  4.06it/s]Inference:  84%|████████▍ | 27/32 [00:06<00:01,  4.08it/s]Inference:  88%|████████▊ | 28/32 [00:06<00:00,  4.08it/s]Inference:  91%|█████████ | 29/32 [00:07<00:00,  4.10it/s]Inference:  94%|█████████▍| 30/32 [00:07<00:00,  4.08it/s]Inference:  97%|█████████▋| 31/32 [00:07<00:00,  4.08it/s]Inference: 100%|██████████| 32/32 [00:07<00:00,  4.06it/s]Inference: 100%|██████████| 32/32 [00:07<00:00,  4.10it/s]
17073 MiB free out of 48676 MiB total
Saved layer 20 to ./models/meta-llama/Llama-2-7b-hf/tensorized/128/no_finetune2/layer_20.pt
after cast to cpu
21089 MiB free out of 48676 MiB total
Done with layer 20 total_time elapsed: 20351 estimated time left: 10660
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:16,  1.90it/s]Inference:   6%|▋         | 2/32 [00:01<00:15,  1.91it/s]Inference:   9%|▉         | 3/32 [00:01<00:15,  1.90it/s]Inference:  12%|█▎        | 4/32 [00:02<00:14,  1.89it/s]Inference:  16%|█▌        | 5/32 [00:02<00:14,  1.88it/s]Inference:  19%|█▉        | 6/32 [00:03<00:13,  1.90it/s]Inference:  22%|██▏       | 7/32 [00:03<00:13,  1.90it/s]Inference:  25%|██▌       | 8/32 [00:04<00:12,  1.90it/s]Inference:  28%|██▊       | 9/32 [00:04<00:12,  1.90it/s]Inference:  31%|███▏      | 10/32 [00:05<00:11,  1.89it/s]Inference:  34%|███▍      | 11/32 [00:05<00:11,  1.89it/s]Inference:  38%|███▊      | 12/32 [00:06<00:10,  1.89it/s]Inference:  41%|████      | 13/32 [00:06<00:10,  1.89it/s]Inference:  44%|████▍     | 14/32 [00:07<00:09,  1.89it/s]Inference:  47%|████▋     | 15/32 [00:07<00:09,  1.89it/s]Inference:  50%|█████     | 16/32 [00:08<00:08,  1.89it/s]Inference:  53%|█████▎    | 17/32 [00:08<00:07,  1.89it/s]Inference:  56%|█████▋    | 18/32 [00:09<00:07,  1.89it/s]Inference:  59%|█████▉    | 19/32 [00:10<00:06,  1.89it/s]Inference:  62%|██████▎   | 20/32 [00:10<00:06,  1.89it/s]Inference:  66%|██████▌   | 21/32 [00:11<00:05,  1.89it/s]Inference:  69%|██████▉   | 22/32 [00:11<00:05,  1.90it/s]Inference:  72%|███████▏  | 23/32 [00:12<00:04,  1.89it/s]Inference:  75%|███████▌  | 24/32 [00:12<00:04,  1.89it/s]Inference:  78%|███████▊  | 25/32 [00:13<00:03,  1.89it/s]Inference:  81%|████████▏ | 26/32 [00:13<00:03,  1.89it/s]Inference:  84%|████████▍ | 27/32 [00:14<00:02,  1.89it/s]Inference:  88%|████████▊ | 28/32 [00:14<00:02,  1.88it/s]Inference:  91%|█████████ | 29/32 [00:15<00:01,  1.89it/s]Inference:  94%|█████████▍| 30/32 [00:15<00:01,  1.89it/s]Inference:  97%|█████████▋| 31/32 [00:16<00:00,  1.89it/s]Inference: 100%|██████████| 32/32 [00:16<00:00,  1.89it/s]Inference: 100%|██████████| 32/32 [00:16<00:00,  1.89it/s]
layer21: self_attn.q_proj
iter 0, train loss 101678.5625, val loss None, lr 0.01
iter 250, train loss 790.2091064453125, val loss None, lr 0.01
iter 500, train loss 607.6007690429688, val loss None, lr 0.003333
iter 750, train loss 593.6896362304688, val loss None, lr 0.003333
iter 1000, train loss 571.9508056640625, val loss None, lr 0.003333
iter 1250, train loss 538.244873046875, val loss None, lr 0.001111
iter 1500, train loss 524.6107788085938, val loss None, lr 0.001111
iter 1750, train loss 518.2727661132812, val loss None, lr 0.001111
iter 2000, train loss 509.40106201171875, val loss None, lr 0.00037
iter 2250, train loss 505.48333740234375, val loss None, lr 0.00037
best loss 501.1583251953125
layer21: self_attn.k_proj
iter 0, train loss 125640.265625, val loss None, lr 0.01
iter 250, train loss 742.9610595703125, val loss None, lr 0.01
iter 500, train loss 706.4674682617188, val loss None, lr 0.01
iter 750, train loss 581.5528564453125, val loss None, lr 0.003333
iter 1000, train loss 537.005615234375, val loss None, lr 0.003333
iter 1250, train loss 551.9766235351562, val loss None, lr 0.003333
iter 1500, train loss 506.6722412109375, val loss None, lr 0.001111
iter 1750, train loss 498.72216796875, val loss None, lr 0.001111
iter 2000, train loss 495.5380554199219, val loss None, lr 0.001111
iter 2250, train loss 488.280029296875, val loss None, lr 0.001111
best loss 485.30303955078125
layer21: self_attn.v_proj
iter 0, train loss 6143.10302734375, val loss None, lr 0.01
iter 250, train loss 293.33062744140625, val loss None, lr 0.01
iter 500, train loss 280.4850158691406, val loss None, lr 0.01
iter 750, train loss 268.55975341796875, val loss None, lr 0.01
iter 1000, train loss 266.7391357421875, val loss None, lr 0.01
iter 1250, train loss 250.380126953125, val loss None, lr 0.003333
iter 1500, train loss 251.1327667236328, val loss None, lr 0.003333
iter 1750, train loss 247.12771606445312, val loss None, lr 0.001111
iter 2000, train loss 244.78741455078125, val loss None, lr 0.001111
iter 2250, train loss 243.75894165039062, val loss None, lr 0.001111
best loss 242.82864379882812
layer21: self_attn.o_proj
iter 0, train loss 6057.2197265625, val loss None, lr 0.01
iter 250, train loss 155.46539306640625, val loss None, lr 0.01
iter 500, train loss 133.60989379882812, val loss None, lr 0.01
iter 750, train loss 114.39002990722656, val loss None, lr 0.01
iter 1000, train loss 100.33757019042969, val loss None, lr 0.003333
iter 1250, train loss 96.83541870117188, val loss None, lr 0.001111
iter 1500, train loss 94.9727554321289, val loss None, lr 0.001111
iter 1750, train loss 93.9560317993164, val loss None, lr 0.001111
iter 2000, train loss 93.09635925292969, val loss None, lr 0.001111
iter 2250, train loss 92.5059585571289, val loss None, lr 0.001111
best loss 91.39298248291016
layer21: mlp.gate_proj
iter 0, train loss 25416.35546875, val loss None, lr 0.01
iter 250, train loss 970.2840576171875, val loss None, lr 0.01
iter 500, train loss 979.626708984375, val loss None, lr 0.01
iter 750, train loss 883.622314453125, val loss None, lr 0.01
iter 1000, train loss 858.1068115234375, val loss None, lr 0.003333
iter 1250, train loss 850.7020263671875, val loss None, lr 0.003333
iter 1500, train loss 836.3955688476562, val loss None, lr 0.001111
iter 1750, train loss 832.87353515625, val loss None, lr 0.001111
iter 2000, train loss 828.7894897460938, val loss None, lr 0.00037
iter 2250, train loss 826.1759033203125, val loss None, lr 0.00037
best loss 823.8543701171875
layer21: mlp.up_proj
iter 0, train loss 13386.75, val loss None, lr 0.01
iter 250, train loss 799.238525390625, val loss None, lr 0.01
iter 500, train loss 776.563720703125, val loss None, lr 0.01
iter 750, train loss 744.5596923828125, val loss None, lr 0.01
iter 1000, train loss 730.9388427734375, val loss None, lr 0.003333
iter 1250, train loss 720.13134765625, val loss None, lr 0.001111
iter 1500, train loss 717.1058349609375, val loss None, lr 0.001111
iter 1750, train loss 716.10009765625, val loss None, lr 0.001111
iter 2000, train loss 712.390625, val loss None, lr 0.001111
iter 2250, train loss 710.9703369140625, val loss None, lr 0.001111
best loss 708.7156982421875
layer21: mlp.down_proj
iter 0, train loss 334.8794860839844, val loss None, lr 0.01
iter 250, train loss 17.779207229614258, val loss None, lr 0.01
iter 500, train loss 16.530006408691406, val loss None, lr 0.01
iter 750, train loss 15.784708023071289, val loss None, lr 0.01
iter 1000, train loss 15.323263168334961, val loss None, lr 0.01
iter 1250, train loss 15.085639953613281, val loss None, lr 0.01
iter 1500, train loss 14.876917839050293, val loss None, lr 0.003333
iter 1750, train loss 14.442352294921875, val loss None, lr 0.003333
iter 2000, train loss 14.283475875854492, val loss None, lr 0.001111
iter 2250, train loss 14.222005844116211, val loss None, lr 0.001111
best loss 14.186468124389648
21089 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:06,  4.90it/s]Inference:   6%|▋         | 2/32 [00:00<00:06,  4.38it/s]Inference:   9%|▉         | 3/32 [00:00<00:06,  4.24it/s]Inference:  12%|█▎        | 4/32 [00:00<00:06,  4.11it/s]Inference:  16%|█▌        | 5/32 [00:01<00:06,  4.09it/s]Inference:  19%|█▉        | 6/32 [00:01<00:06,  3.89it/s]Inference:  22%|██▏       | 7/32 [00:01<00:06,  4.14it/s]Inference:  25%|██▌       | 8/32 [00:01<00:05,  4.12it/s]Inference:  28%|██▊       | 9/32 [00:02<00:05,  4.13it/s]Inference:  31%|███▏      | 10/32 [00:02<00:05,  4.10it/s]Inference:  34%|███▍      | 11/32 [00:02<00:05,  4.09it/s]Inference:  38%|███▊      | 12/32 [00:02<00:04,  4.07it/s]Inference:  41%|████      | 13/32 [00:03<00:04,  4.09it/s]Inference:  44%|████▍     | 14/32 [00:03<00:04,  4.10it/s]Inference:  47%|████▋     | 15/32 [00:03<00:04,  4.10it/s]Inference:  50%|█████     | 16/32 [00:03<00:03,  4.08it/s]Inference:  53%|█████▎    | 17/32 [00:04<00:03,  4.10it/s]Inference:  56%|█████▋    | 18/32 [00:04<00:03,  4.10it/s]Inference:  59%|█████▉    | 19/32 [00:04<00:03,  4.10it/s]Inference:  62%|██████▎   | 20/32 [00:04<00:02,  4.08it/s]Inference:  66%|██████▌   | 21/32 [00:05<00:02,  4.09it/s]Inference:  69%|██████▉   | 22/32 [00:05<00:02,  4.08it/s]Inference:  72%|███████▏  | 23/32 [00:05<00:02,  4.09it/s]Inference:  75%|███████▌  | 24/32 [00:05<00:01,  4.07it/s]Inference:  78%|███████▊  | 25/32 [00:06<00:01,  4.11it/s]Inference:  81%|████████▏ | 26/32 [00:06<00:01,  4.11it/s]Inference:  84%|████████▍ | 27/32 [00:06<00:01,  4.11it/s]Inference:  88%|████████▊ | 28/32 [00:06<00:00,  4.09it/s]Inference:  91%|█████████ | 29/32 [00:07<00:00,  4.09it/s]Inference:  94%|█████████▍| 30/32 [00:07<00:00,  4.07it/s]Inference:  97%|█████████▋| 31/32 [00:07<00:00,  4.10it/s]Inference: 100%|██████████| 32/32 [00:07<00:00,  4.11it/s]Inference: 100%|██████████| 32/32 [00:07<00:00,  4.11it/s]
15849 MiB free out of 48676 MiB total
Saved layer 21 to ./models/meta-llama/Llama-2-7b-hf/tensorized/128/no_finetune2/layer_21.pt
after cast to cpu
19865 MiB free out of 48676 MiB total
Done with layer 21 total_time elapsed: 21219 estimated time left: 9645
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:16,  1.89it/s]Inference:   6%|▋         | 2/32 [00:01<00:15,  1.90it/s]Inference:   9%|▉         | 3/32 [00:01<00:15,  1.89it/s]Inference:  12%|█▎        | 4/32 [00:02<00:14,  1.90it/s]Inference:  16%|█▌        | 5/32 [00:02<00:14,  1.89it/s]Inference:  19%|█▉        | 6/32 [00:03<00:13,  1.89it/s]Inference:  22%|██▏       | 7/32 [00:03<00:13,  1.89it/s]Inference:  25%|██▌       | 8/32 [00:04<00:12,  1.90it/s]Inference:  28%|██▊       | 9/32 [00:04<00:12,  1.89it/s]Inference:  31%|███▏      | 10/32 [00:05<00:11,  1.90it/s]Inference:  34%|███▍      | 11/32 [00:05<00:11,  1.90it/s]Inference:  38%|███▊      | 12/32 [00:06<00:10,  1.90it/s]Inference:  41%|████      | 13/32 [00:06<00:09,  1.94it/s]Inference:  44%|████▍     | 14/32 [00:07<00:09,  1.92it/s]Inference:  47%|████▋     | 15/32 [00:07<00:08,  1.91it/s]Inference:  50%|█████     | 16/32 [00:08<00:08,  1.95it/s]Inference:  53%|█████▎    | 17/32 [00:08<00:07,  1.93it/s]Inference:  56%|█████▋    | 18/32 [00:09<00:07,  1.92it/s]Inference:  59%|█████▉    | 19/32 [00:09<00:06,  1.91it/s]Inference:  62%|██████▎   | 20/32 [00:10<00:06,  1.90it/s]Inference:  66%|██████▌   | 21/32 [00:11<00:05,  1.90it/s]Inference:  69%|██████▉   | 22/32 [00:11<00:05,  1.89it/s]Inference:  72%|███████▏  | 23/32 [00:12<00:04,  1.89it/s]Inference:  75%|███████▌  | 24/32 [00:12<00:04,  1.90it/s]Inference:  78%|███████▊  | 25/32 [00:13<00:03,  1.89it/s]Inference:  81%|████████▏ | 26/32 [00:13<00:03,  1.94it/s]Inference:  84%|████████▍ | 27/32 [00:14<00:02,  1.92it/s]Inference:  88%|████████▊ | 28/32 [00:14<00:02,  1.92it/s]Inference:  91%|█████████ | 29/32 [00:15<00:01,  1.91it/s]Inference:  94%|█████████▍| 30/32 [00:15<00:01,  1.95it/s]Inference:  97%|█████████▋| 31/32 [00:16<00:00,  1.92it/s]Inference: 100%|██████████| 32/32 [00:16<00:00,  1.92it/s]Inference: 100%|██████████| 32/32 [00:16<00:00,  1.91it/s]
layer22: self_attn.q_proj
iter 0, train loss 116345.7578125, val loss None, lr 0.01
iter 250, train loss 781.1444091796875, val loss None, lr 0.01
iter 500, train loss 700.630859375, val loss None, lr 0.01
iter 750, train loss 651.2640380859375, val loss None, lr 0.01
iter 1000, train loss 552.6724853515625, val loss None, lr 0.003333
iter 1250, train loss 550.314208984375, val loss None, lr 0.003333
iter 1500, train loss 520.5782470703125, val loss None, lr 0.001111
iter 1750, train loss 514.22705078125, val loss None, lr 0.001111
iter 2000, train loss 509.186279296875, val loss None, lr 0.001111
iter 2250, train loss 504.1506652832031, val loss None, lr 0.001111
best loss 495.8664245605469
layer22: self_attn.k_proj
iter 0, train loss 140451.3125, val loss None, lr 0.01
iter 250, train loss 828.0362548828125, val loss None, lr 0.01
iter 500, train loss 786.6749877929688, val loss None, lr 0.003333
iter 750, train loss 602.1484375, val loss None, lr 0.003333
iter 1000, train loss 541.80908203125, val loss None, lr 0.001111
iter 1250, train loss 533.1460571289062, val loss None, lr 0.001111
iter 1500, train loss 518.7808227539062, val loss None, lr 0.001111
iter 1750, train loss 507.5455017089844, val loss None, lr 0.00037
iter 2000, train loss 499.0037841796875, val loss None, lr 0.00037
iter 2250, train loss 493.188232421875, val loss None, lr 0.00037
best loss 488.2991027832031
layer22: self_attn.v_proj
iter 0, train loss 6625.78515625, val loss None, lr 0.01
iter 250, train loss 289.1001892089844, val loss None, lr 0.01
iter 500, train loss 265.5454406738281, val loss None, lr 0.01
iter 750, train loss 250.4639129638672, val loss None, lr 0.003333
iter 1000, train loss 246.13064575195312, val loss None, lr 0.003333
iter 1250, train loss 241.9570770263672, val loss None, lr 0.003333
iter 1500, train loss 243.40402221679688, val loss None, lr 0.003333
iter 1750, train loss 240.4701385498047, val loss None, lr 0.003333
iter 2000, train loss 235.37127685546875, val loss None, lr 0.001111
iter 2250, train loss 234.98464965820312, val loss None, lr 0.001111
best loss 233.22146606445312
layer22: self_attn.o_proj
iter 0, train loss 5919.4833984375, val loss None, lr 0.01
iter 250, train loss 147.82083129882812, val loss None, lr 0.01
iter 500, train loss 130.88119506835938, val loss None, lr 0.01
iter 750, train loss 120.84934997558594, val loss None, lr 0.01
iter 1000, train loss 122.68974304199219, val loss None, lr 0.003333
iter 1250, train loss 106.2052001953125, val loss None, lr 0.003333
iter 1500, train loss 105.0030746459961, val loss None, lr 0.003333
iter 1750, train loss 102.60861206054688, val loss None, lr 0.001111
iter 2000, train loss 101.47993469238281, val loss None, lr 0.001111
iter 2250, train loss 101.07135009765625, val loss None, lr 0.001111
best loss 100.51307678222656
layer22: mlp.gate_proj
iter 0, train loss 27550.39453125, val loss None, lr 0.01
iter 250, train loss 997.235595703125, val loss None, lr 0.01
iter 500, train loss 951.4317016601562, val loss None, lr 0.01
iter 750, train loss 950.7591552734375, val loss None, lr 0.01
iter 1000, train loss 865.6773681640625, val loss None, lr 0.003333
iter 1250, train loss 865.7520751953125, val loss None, lr 0.003333
iter 1500, train loss 848.2666015625, val loss None, lr 0.001111
iter 1750, train loss 844.0460815429688, val loss None, lr 0.001111
iter 2000, train loss 840.080810546875, val loss None, lr 0.00037
iter 2250, train loss 837.5712890625, val loss None, lr 0.00037
best loss 835.6298217773438
layer22: mlp.up_proj
iter 0, train loss 14180.11328125, val loss None, lr 0.01
iter 250, train loss 811.2063598632812, val loss None, lr 0.01
iter 500, train loss 789.5318603515625, val loss None, lr 0.01
iter 750, train loss 750.1644897460938, val loss None, lr 0.01
iter 1000, train loss 732.1505737304688, val loss None, lr 0.003333
iter 1250, train loss 723.536376953125, val loss None, lr 0.003333
iter 1500, train loss 714.2357177734375, val loss None, lr 0.001111
iter 1750, train loss 711.6533203125, val loss None, lr 0.001111
iter 2000, train loss 709.4153442382812, val loss None, lr 0.001111
iter 2250, train loss 707.3465576171875, val loss None, lr 0.00037
best loss 706.0469970703125
layer22: mlp.down_proj
iter 0, train loss 353.7975158691406, val loss None, lr 0.01
iter 250, train loss 18.315013885498047, val loss None, lr 0.01
iter 500, train loss 16.335115432739258, val loss None, lr 0.01
iter 750, train loss 16.34016990661621, val loss None, lr 0.01
iter 1000, train loss 15.520574569702148, val loss None, lr 0.01
iter 1250, train loss 14.916144371032715, val loss None, lr 0.003333
iter 1500, train loss 14.591753005981445, val loss None, lr 0.001111
iter 1750, train loss 14.503637313842773, val loss None, lr 0.001111
iter 2000, train loss 14.421175956726074, val loss None, lr 0.001111
iter 2250, train loss 14.384521484375, val loss None, lr 0.001111
best loss 14.334945678710938
19865 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:06,  4.99it/s]Inference:   6%|▋         | 2/32 [00:00<00:06,  4.46it/s]Inference:   9%|▉         | 3/32 [00:00<00:06,  4.28it/s]Inference:  12%|█▎        | 4/32 [00:00<00:06,  4.19it/s]Inference:  16%|█▌        | 5/32 [00:01<00:06,  4.17it/s]Inference:  19%|█▉        | 6/32 [00:01<00:06,  4.14it/s]Inference:  22%|██▏       | 7/32 [00:01<00:06,  4.15it/s]Inference:  25%|██▌       | 8/32 [00:01<00:05,  4.12it/s]Inference:  28%|██▊       | 9/32 [00:02<00:05,  4.13it/s]Inference:  31%|███▏      | 10/32 [00:02<00:05,  4.11it/s]Inference:  34%|███▍      | 11/32 [00:02<00:05,  4.11it/s]Inference:  38%|███▊      | 12/32 [00:02<00:04,  4.10it/s]Inference:  41%|████      | 13/32 [00:03<00:04,  4.10it/s]Inference:  44%|████▍     | 14/32 [00:03<00:04,  4.10it/s]Inference:  47%|████▋     | 15/32 [00:03<00:04,  4.17it/s]Inference:  50%|█████     | 16/32 [00:03<00:03,  4.16it/s]Inference:  53%|█████▎    | 17/32 [00:04<00:03,  4.14it/s]Inference:  56%|█████▋    | 18/32 [00:04<00:03,  4.13it/s]Inference:  59%|█████▉    | 19/32 [00:04<00:03,  4.12it/s]Inference:  62%|██████▎   | 20/32 [00:04<00:02,  4.10it/s]Inference:  66%|██████▌   | 21/32 [00:05<00:02,  4.10it/s]Inference:  69%|██████▉   | 22/32 [00:05<00:02,  4.09it/s]Inference:  72%|███████▏  | 23/32 [00:05<00:02,  4.11it/s]Inference:  75%|███████▌  | 24/32 [00:05<00:01,  4.11it/s]Inference:  78%|███████▊  | 25/32 [00:06<00:01,  4.11it/s]Inference:  81%|████████▏ | 26/32 [00:06<00:01,  4.10it/s]Inference:  84%|████████▍ | 27/32 [00:06<00:01,  4.10it/s]Inference:  88%|████████▊ | 28/32 [00:06<00:00,  4.09it/s]Inference:  91%|█████████ | 29/32 [00:07<00:00,  4.10it/s]Inference:  94%|█████████▍| 30/32 [00:07<00:00,  4.09it/s]Inference:  97%|█████████▋| 31/32 [00:07<00:00,  4.14it/s]Inference: 100%|██████████| 32/32 [00:07<00:00,  4.38it/s]Inference: 100%|██████████| 32/32 [00:07<00:00,  4.16it/s]
14561 MiB free out of 48676 MiB total
Saved layer 22 to ./models/meta-llama/Llama-2-7b-hf/tensorized/128/no_finetune2/layer_22.pt
after cast to cpu
18577 MiB free out of 48676 MiB total
Done with layer 22 total_time elapsed: 22087 estimated time left: 8643
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:16,  1.89it/s]Inference:   6%|▋         | 2/32 [00:01<00:15,  1.90it/s]Inference:   9%|▉         | 3/32 [00:01<00:15,  1.90it/s]Inference:  12%|█▎        | 4/32 [00:02<00:14,  1.90it/s]Inference:  16%|█▌        | 5/32 [00:02<00:14,  1.89it/s]Inference:  19%|█▉        | 6/32 [00:03<00:13,  1.89it/s]Inference:  22%|██▏       | 7/32 [00:03<00:13,  1.89it/s]Inference:  25%|██▌       | 8/32 [00:04<00:12,  1.89it/s]Inference:  28%|██▊       | 9/32 [00:04<00:12,  1.89it/s]Inference:  31%|███▏      | 10/32 [00:05<00:11,  1.90it/s]Inference:  34%|███▍      | 11/32 [00:05<00:11,  1.90it/s]Inference:  38%|███▊      | 12/32 [00:06<00:10,  1.89it/s]Inference:  41%|████      | 13/32 [00:06<00:10,  1.90it/s]Inference:  44%|████▍     | 14/32 [00:07<00:09,  1.94it/s]Inference:  47%|████▋     | 15/32 [00:07<00:08,  1.92it/s]Inference:  50%|█████     | 16/32 [00:08<00:08,  1.91it/s]Inference:  53%|█████▎    | 17/32 [00:08<00:07,  1.90it/s]Inference:  56%|█████▋    | 18/32 [00:09<00:07,  1.90it/s]Inference:  59%|█████▉    | 19/32 [00:09<00:06,  1.90it/s]Inference:  62%|██████▎   | 20/32 [00:10<00:06,  1.89it/s]Inference:  66%|██████▌   | 21/32 [00:11<00:05,  1.89it/s]Inference:  69%|██████▉   | 22/32 [00:11<00:05,  1.89it/s]Inference:  72%|███████▏  | 23/32 [00:12<00:04,  1.89it/s]Inference:  75%|███████▌  | 24/32 [00:12<00:04,  1.89it/s]Inference:  78%|███████▊  | 25/32 [00:13<00:03,  1.90it/s]Inference:  81%|████████▏ | 26/32 [00:13<00:03,  1.90it/s]Inference:  84%|████████▍ | 27/32 [00:14<00:02,  1.89it/s]Inference:  88%|████████▊ | 28/32 [00:14<00:02,  1.89it/s]Inference:  91%|█████████ | 29/32 [00:15<00:01,  1.89it/s]Inference:  94%|█████████▍| 30/32 [00:15<00:01,  1.89it/s]Inference:  97%|█████████▋| 31/32 [00:16<00:00,  1.89it/s]Inference: 100%|██████████| 32/32 [00:16<00:00,  1.90it/s]Inference: 100%|██████████| 32/32 [00:16<00:00,  1.90it/s]
layer23: self_attn.q_proj
iter 0, train loss 124374.1015625, val loss None, lr 0.01
iter 250, train loss 807.36376953125, val loss None, lr 0.01
iter 500, train loss 671.62158203125, val loss None, lr 0.01
iter 750, train loss 643.6669311523438, val loss None, lr 0.01
iter 1000, train loss 577.9134521484375, val loss None, lr 0.003333
iter 1250, train loss 562.6422119140625, val loss None, lr 0.003333
iter 1500, train loss 529.9647827148438, val loss None, lr 0.001111
iter 1750, train loss 521.9029541015625, val loss None, lr 0.001111
iter 2000, train loss 517.1591186523438, val loss None, lr 0.001111
iter 2250, train loss 511.95965576171875, val loss None, lr 0.001111
best loss 505.74505615234375
layer23: self_attn.k_proj
iter 0, train loss 150454.25, val loss None, lr 0.01
iter 250, train loss 951.7947387695312, val loss None, lr 0.01
iter 500, train loss 884.288330078125, val loss None, lr 0.01
iter 750, train loss 631.667236328125, val loss None, lr 0.003333
iter 1000, train loss 603.5634765625, val loss None, lr 0.003333
iter 1250, train loss 547.8828125, val loss None, lr 0.001111
iter 1500, train loss 536.43603515625, val loss None, lr 0.001111
iter 1750, train loss 526.0650634765625, val loss None, lr 0.001111
iter 2000, train loss 522.260498046875, val loss None, lr 0.001111
iter 2250, train loss 517.8011474609375, val loss None, lr 0.001111
best loss 511.66229248046875
layer23: self_attn.v_proj
iter 0, train loss 8405.3505859375, val loss None, lr 0.01
iter 250, train loss 342.54547119140625, val loss None, lr 0.01
iter 500, train loss 316.4943542480469, val loss None, lr 0.01
iter 750, train loss 300.77001953125, val loss None, lr 0.003333
iter 1000, train loss 293.031005859375, val loss None, lr 0.003333
iter 1250, train loss 287.3787536621094, val loss None, lr 0.001111
iter 1500, train loss 284.64788818359375, val loss None, lr 0.001111
iter 1750, train loss 282.8187255859375, val loss None, lr 0.001111
iter 2000, train loss 281.0642395019531, val loss None, lr 0.001111
iter 2250, train loss 279.93597412109375, val loss None, lr 0.001111
best loss 278.70709228515625
layer23: self_attn.o_proj
iter 0, train loss 10190.8193359375, val loss None, lr 0.01
iter 250, train loss 238.28494262695312, val loss None, lr 0.01
iter 500, train loss 243.60650634765625, val loss None, lr 0.01
iter 750, train loss 160.640625, val loss None, lr 0.003333
iter 1000, train loss 150.87327575683594, val loss None, lr 0.003333
iter 1250, train loss 146.71600341796875, val loss None, lr 0.001111
iter 1500, train loss 143.67044067382812, val loss None, lr 0.001111
iter 1750, train loss 142.3284912109375, val loss None, lr 0.001111
iter 2000, train loss 140.92080688476562, val loss None, lr 0.001111
iter 2250, train loss 139.83189392089844, val loss None, lr 0.001111
best loss 138.45355224609375
layer23: mlp.gate_proj
iter 0, train loss 27938.86328125, val loss None, lr 0.01
iter 250, train loss 1046.1424560546875, val loss None, lr 0.01
iter 500, train loss 989.0755004882812, val loss None, lr 0.01
iter 750, train loss 999.6834716796875, val loss None, lr 0.01
iter 1000, train loss 903.9251708984375, val loss None, lr 0.003333
iter 1250, train loss 892.8380737304688, val loss None, lr 0.003333
iter 1500, train loss 883.6860961914062, val loss None, lr 0.003333
iter 1750, train loss 874.3908081054688, val loss None, lr 0.001111
iter 2000, train loss 870.4324340820312, val loss None, lr 0.001111
iter 2250, train loss 868.0147705078125, val loss None, lr 0.001111
best loss 866.3036499023438
layer23: mlp.up_proj
iter 0, train loss 14783.314453125, val loss None, lr 0.01
iter 250, train loss 868.5247192382812, val loss None, lr 0.01
iter 500, train loss 819.1873779296875, val loss None, lr 0.01
iter 750, train loss 776.9940795898438, val loss None, lr 0.003333
iter 1000, train loss 764.0628662109375, val loss None, lr 0.003333
iter 1250, train loss 761.886474609375, val loss None, lr 0.003333
iter 1500, train loss 751.6997680664062, val loss None, lr 0.001111
iter 1750, train loss 748.3204345703125, val loss None, lr 0.001111
iter 2000, train loss 745.4608154296875, val loss None, lr 0.001111
iter 2250, train loss 743.974365234375, val loss None, lr 0.00037
best loss 741.8577880859375
layer23: mlp.down_proj
iter 0, train loss 462.72869873046875, val loss None, lr 0.01
iter 250, train loss 21.92535972595215, val loss None, lr 0.01
iter 500, train loss 18.85999298095703, val loss None, lr 0.01
iter 750, train loss 17.92417335510254, val loss None, lr 0.003333
iter 1000, train loss 17.47620391845703, val loss None, lr 0.001111
iter 1250, train loss 17.247983932495117, val loss None, lr 0.001111
iter 1500, train loss 17.11803436279297, val loss None, lr 0.001111
iter 1750, train loss 16.99216079711914, val loss None, lr 0.001111
iter 2000, train loss 16.926483154296875, val loss None, lr 0.001111
iter 2250, train loss 16.896644592285156, val loss None, lr 0.001111
best loss 16.71030616760254
18577 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:06,  4.89it/s]Inference:   6%|▋         | 2/32 [00:00<00:06,  4.40it/s]Inference:   9%|▉         | 3/32 [00:00<00:06,  4.27it/s]Inference:  12%|█▎        | 4/32 [00:00<00:06,  4.17it/s]Inference:  16%|█▌        | 5/32 [00:01<00:06,  4.14it/s]Inference:  19%|█▉        | 6/32 [00:01<00:06,  4.10it/s]Inference:  22%|██▏       | 7/32 [00:01<00:06,  4.10it/s]Inference:  25%|██▌       | 8/32 [00:01<00:05,  4.11it/s]Inference:  28%|██▊       | 9/32 [00:02<00:05,  4.13it/s]Inference:  31%|███▏      | 10/32 [00:02<00:05,  4.11it/s]Inference:  34%|███▍      | 11/32 [00:02<00:05,  4.11it/s]Inference:  38%|███▊      | 12/32 [00:02<00:04,  4.09it/s]Inference:  41%|████      | 13/32 [00:03<00:04,  4.10it/s]Inference:  44%|████▍     | 14/32 [00:03<00:04,  4.08it/s]Inference:  47%|████▋     | 15/32 [00:03<00:04,  4.07it/s]Inference:  50%|█████     | 16/32 [00:03<00:03,  4.07it/s]Inference:  53%|█████▎    | 17/32 [00:04<00:03,  4.11it/s]Inference:  56%|█████▋    | 18/32 [00:04<00:03,  4.13it/s]Inference:  59%|█████▉    | 19/32 [00:04<00:03,  4.12it/s]Inference:  62%|██████▎   | 20/32 [00:04<00:02,  4.12it/s]Inference:  66%|██████▌   | 21/32 [00:05<00:02,  4.09it/s]Inference:  69%|██████▉   | 22/32 [00:05<00:02,  4.09it/s]Inference:  72%|███████▏  | 23/32 [00:05<00:02,  4.08it/s]Inference:  75%|███████▌  | 24/32 [00:05<00:01,  4.09it/s]Inference:  78%|███████▊  | 25/32 [00:06<00:01,  4.10it/s]Inference:  81%|████████▏ | 26/32 [00:06<00:01,  4.11it/s]Inference:  84%|████████▍ | 27/32 [00:06<00:01,  4.12it/s]Inference:  88%|████████▊ | 28/32 [00:06<00:00,  4.13it/s]Inference:  91%|█████████ | 29/32 [00:07<00:00,  4.11it/s]Inference:  94%|█████████▍| 30/32 [00:07<00:00,  4.10it/s]Inference:  97%|█████████▋| 31/32 [00:07<00:00,  4.09it/s]Inference: 100%|██████████| 32/32 [00:07<00:00,  4.09it/s]Inference: 100%|██████████| 32/32 [00:07<00:00,  4.12it/s]
13273 MiB free out of 48676 MiB total
Saved layer 23 to ./models/meta-llama/Llama-2-7b-hf/tensorized/128/no_finetune2/layer_23.pt
after cast to cpu
17289 MiB free out of 48676 MiB total
Done with layer 23 total_time elapsed: 22956 estimated time left: 7652
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:16,  1.92it/s]Inference:   6%|▋         | 2/32 [00:01<00:15,  1.94it/s]Inference:   9%|▉         | 3/32 [00:01<00:15,  1.92it/s]Inference:  12%|█▎        | 4/32 [00:02<00:14,  1.90it/s]Inference:  16%|█▌        | 5/32 [00:02<00:14,  1.91it/s]Inference:  19%|█▉        | 6/32 [00:03<00:13,  1.91it/s]Inference:  22%|██▏       | 7/32 [00:03<00:13,  1.90it/s]Inference:  25%|██▌       | 8/32 [00:04<00:12,  1.90it/s]Inference:  28%|██▊       | 9/32 [00:04<00:12,  1.90it/s]Inference:  31%|███▏      | 10/32 [00:05<00:11,  1.90it/s]Inference:  34%|███▍      | 11/32 [00:05<00:11,  1.90it/s]Inference:  38%|███▊      | 12/32 [00:06<00:10,  1.90it/s]Inference:  41%|████      | 13/32 [00:06<00:10,  1.90it/s]Inference:  44%|████▍     | 14/32 [00:07<00:09,  1.90it/s]Inference:  47%|████▋     | 15/32 [00:07<00:08,  1.90it/s]Inference:  50%|█████     | 16/32 [00:08<00:08,  1.91it/s]Inference:  53%|█████▎    | 17/32 [00:08<00:07,  1.90it/s]Inference:  56%|█████▋    | 18/32 [00:09<00:07,  1.90it/s]Inference:  59%|█████▉    | 19/32 [00:09<00:06,  1.90it/s]Inference:  62%|██████▎   | 20/32 [00:10<00:06,  1.90it/s]Inference:  66%|██████▌   | 21/32 [00:11<00:05,  1.89it/s]Inference:  69%|██████▉   | 22/32 [00:11<00:05,  1.89it/s]Inference:  72%|███████▏  | 23/32 [00:12<00:04,  1.89it/s]Inference:  75%|███████▌  | 24/32 [00:12<00:04,  1.89it/s]Inference:  78%|███████▊  | 25/32 [00:13<00:03,  1.89it/s]Inference:  81%|████████▏ | 26/32 [00:13<00:03,  1.89it/s]Inference:  84%|████████▍ | 27/32 [00:14<00:02,  1.89it/s]Inference:  88%|████████▊ | 28/32 [00:14<00:02,  1.90it/s]Inference:  91%|█████████ | 29/32 [00:15<00:01,  1.92it/s]Inference:  94%|█████████▍| 30/32 [00:15<00:01,  1.91it/s]Inference:  97%|█████████▋| 31/32 [00:16<00:00,  1.91it/s]Inference: 100%|██████████| 32/32 [00:16<00:00,  1.91it/s]Inference: 100%|██████████| 32/32 [00:16<00:00,  1.90it/s]
layer24: self_attn.q_proj
iter 0, train loss 135420.84375, val loss None, lr 0.01
iter 250, train loss 745.9880981445312, val loss None, lr 0.01
iter 500, train loss 602.0361328125, val loss None, lr 0.003333
iter 750, train loss 561.4637451171875, val loss None, lr 0.003333
iter 1000, train loss 517.4989624023438, val loss None, lr 0.001111
iter 1250, train loss 504.0711975097656, val loss None, lr 0.001111
iter 1500, train loss 493.812255859375, val loss None, lr 0.001111
iter 1750, train loss 486.4681396484375, val loss None, lr 0.001111
iter 2000, train loss 479.519775390625, val loss None, lr 0.001111
iter 2250, train loss 468.0791015625, val loss None, lr 0.00037
best loss 462.8985595703125
layer24: self_attn.k_proj
iter 0, train loss 178350.9375, val loss None, lr 0.01
iter 250, train loss 1177.1077880859375, val loss None, lr 0.01
iter 500, train loss 594.60302734375, val loss None, lr 0.003333
iter 750, train loss 574.9921264648438, val loss None, lr 0.003333
iter 1000, train loss 506.4586486816406, val loss None, lr 0.001111
iter 1250, train loss 491.67596435546875, val loss None, lr 0.001111
iter 1500, train loss 485.2061767578125, val loss None, lr 0.001111
iter 1750, train loss 471.44964599609375, val loss None, lr 0.001111
iter 2000, train loss 469.5856018066406, val loss None, lr 0.001111
iter 2250, train loss 462.13885498046875, val loss None, lr 0.001111
best loss 455.49029541015625
layer24: self_attn.v_proj
iter 0, train loss 8287.900390625, val loss None, lr 0.01
iter 250, train loss 310.683349609375, val loss None, lr 0.01
iter 500, train loss 288.9913330078125, val loss None, lr 0.01
iter 750, train loss 272.1232604980469, val loss None, lr 0.003333
iter 1000, train loss 266.7939147949219, val loss None, lr 0.001111
iter 1250, train loss 261.4486083984375, val loss None, lr 0.001111
iter 1500, train loss 259.0049743652344, val loss None, lr 0.001111
iter 1750, train loss 257.4306640625, val loss None, lr 0.001111
iter 2000, train loss 255.92276000976562, val loss None, lr 0.001111
iter 2250, train loss 254.53285217285156, val loss None, lr 0.001111
best loss 253.10633850097656
layer24: self_attn.o_proj
iter 0, train loss 8773.47265625, val loss None, lr 0.01
iter 250, train loss 208.3618927001953, val loss None, lr 0.01
iter 500, train loss 167.97744750976562, val loss None, lr 0.003333
iter 750, train loss 156.83282470703125, val loss None, lr 0.003333
iter 1000, train loss 152.75747680664062, val loss None, lr 0.003333
iter 1250, train loss 147.46214294433594, val loss None, lr 0.003333
iter 1500, train loss 143.0867919921875, val loss None, lr 0.001111
iter 1750, train loss 141.0568084716797, val loss None, lr 0.001111
iter 2000, train loss 139.8902130126953, val loss None, lr 0.001111
iter 2250, train loss 138.88002014160156, val loss None, lr 0.001111
best loss 137.68942260742188
layer24: mlp.gate_proj
iter 0, train loss 31781.294921875, val loss None, lr 0.01
iter 250, train loss 1039.1854248046875, val loss None, lr 0.01
iter 500, train loss 969.209716796875, val loss None, lr 0.01
iter 750, train loss 915.8385620117188, val loss None, lr 0.01
iter 1000, train loss 895.6802978515625, val loss None, lr 0.003333
iter 1250, train loss 871.1975708007812, val loss None, lr 0.001111
iter 1500, train loss 866.7628173828125, val loss None, lr 0.001111
iter 1750, train loss 862.2648315429688, val loss None, lr 0.001111
iter 2000, train loss 859.021484375, val loss None, lr 0.001111
iter 2250, train loss 856.6219482421875, val loss None, lr 0.001111
best loss 851.9417724609375
layer24: mlp.up_proj
iter 0, train loss 15433.1044921875, val loss None, lr 0.01
iter 250, train loss 858.6600341796875, val loss None, lr 0.01
iter 500, train loss 838.718505859375, val loss None, lr 0.01
iter 750, train loss 780.9452514648438, val loss None, lr 0.01
iter 1000, train loss 764.92431640625, val loss None, lr 0.01
iter 1250, train loss 767.6895751953125, val loss None, lr 0.01
iter 1500, train loss 741.94775390625, val loss None, lr 0.003333
iter 1750, train loss 732.41162109375, val loss None, lr 0.001111
iter 2000, train loss 729.4852294921875, val loss None, lr 0.001111
iter 2250, train loss 727.1820068359375, val loss None, lr 0.001111
best loss 725.37060546875
layer24: mlp.down_proj
iter 0, train loss 399.21435546875, val loss None, lr 0.01
iter 250, train loss 20.319782257080078, val loss None, lr 0.01
iter 500, train loss 18.22154426574707, val loss None, lr 0.01
iter 750, train loss 17.728347778320312, val loss None, lr 0.01
iter 1000, train loss 16.629390716552734, val loss None, lr 0.003333
iter 1250, train loss 16.475997924804688, val loss None, lr 0.003333
iter 1500, train loss 16.260356903076172, val loss None, lr 0.003333
iter 1750, train loss 16.24539566040039, val loss None, lr 0.003333
iter 2000, train loss 15.982678413391113, val loss None, lr 0.001111
iter 2250, train loss 15.920494079589844, val loss None, lr 0.001111
best loss 15.841731071472168
17289 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:06,  4.90it/s]Inference:   6%|▋         | 2/32 [00:00<00:06,  4.48it/s]Inference:   9%|▉         | 3/32 [00:00<00:06,  4.30it/s]Inference:  12%|█▎        | 4/32 [00:00<00:06,  4.26it/s]Inference:  16%|█▌        | 5/32 [00:01<00:06,  4.23it/s]Inference:  19%|█▉        | 6/32 [00:01<00:06,  4.22it/s]Inference:  22%|██▏       | 7/32 [00:01<00:05,  4.18it/s]Inference:  25%|██▌       | 8/32 [00:01<00:05,  4.16it/s]Inference:  28%|██▊       | 9/32 [00:02<00:05,  4.14it/s]Inference:  31%|███▏      | 10/32 [00:02<00:05,  4.14it/s]Inference:  34%|███▍      | 11/32 [00:02<00:05,  4.11it/s]Inference:  38%|███▊      | 12/32 [00:02<00:04,  4.14it/s]Inference:  41%|████      | 13/32 [00:03<00:04,  4.13it/s]Inference:  44%|████▍     | 14/32 [00:03<00:04,  4.14it/s]Inference:  47%|████▋     | 15/32 [00:03<00:04,  4.15it/s]Inference:  50%|█████     | 16/32 [00:03<00:03,  4.15it/s]Inference:  53%|█████▎    | 17/32 [00:04<00:03,  4.13it/s]Inference:  56%|█████▋    | 18/32 [00:04<00:03,  4.13it/s]Inference:  59%|█████▉    | 19/32 [00:04<00:03,  4.11it/s]Inference:  62%|██████▎   | 20/32 [00:04<00:02,  4.11it/s]Inference:  66%|██████▌   | 21/32 [00:05<00:02,  4.12it/s]Inference:  69%|██████▉   | 22/32 [00:05<00:02,  4.12it/s]Inference:  72%|███████▏  | 23/32 [00:05<00:02,  4.13it/s]Inference:  75%|███████▌  | 24/32 [00:05<00:01,  4.16it/s]Inference:  78%|███████▊  | 25/32 [00:06<00:01,  4.15it/s]Inference:  81%|████████▏ | 26/32 [00:06<00:01,  4.14it/s]Inference:  84%|████████▍ | 27/32 [00:06<00:01,  4.37it/s]Inference:  88%|████████▊ | 28/32 [00:06<00:00,  4.28it/s]Inference:  91%|█████████ | 29/32 [00:06<00:00,  4.24it/s]Inference:  94%|█████████▍| 30/32 [00:07<00:00,  4.20it/s]Inference:  97%|█████████▋| 31/32 [00:07<00:00,  4.19it/s]Inference: 100%|██████████| 32/32 [00:07<00:00,  4.17it/s]Inference: 100%|██████████| 32/32 [00:07<00:00,  4.18it/s]
12049 MiB free out of 48676 MiB total
Saved layer 24 to ./models/meta-llama/Llama-2-7b-hf/tensorized/128/no_finetune2/layer_24.pt
after cast to cpu
16065 MiB free out of 48676 MiB total
Done with layer 24 total_time elapsed: 23824 estimated time left: 6671
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:16,  1.92it/s]Inference:   6%|▋         | 2/32 [00:01<00:15,  1.91it/s]Inference:   9%|▉         | 3/32 [00:01<00:15,  1.91it/s]Inference:  12%|█▎        | 4/32 [00:02<00:14,  1.91it/s]Inference:  16%|█▌        | 5/32 [00:02<00:14,  1.91it/s]Inference:  19%|█▉        | 6/32 [00:03<00:13,  1.90it/s]Inference:  22%|██▏       | 7/32 [00:03<00:13,  1.90it/s]Inference:  25%|██▌       | 8/32 [00:04<00:12,  1.90it/s]Inference:  28%|██▊       | 9/32 [00:04<00:12,  1.90it/s]Inference:  31%|███▏      | 10/32 [00:05<00:11,  1.90it/s]Inference:  34%|███▍      | 11/32 [00:05<00:11,  1.90it/s]Inference:  38%|███▊      | 12/32 [00:06<00:10,  1.90it/s]Inference:  41%|████      | 13/32 [00:06<00:09,  1.90it/s]Inference:  44%|████▍     | 14/32 [00:07<00:09,  1.90it/s]Inference:  47%|████▋     | 15/32 [00:07<00:08,  1.90it/s]Inference:  50%|█████     | 16/32 [00:08<00:08,  1.90it/s]Inference:  53%|█████▎    | 17/32 [00:08<00:07,  1.90it/s]Inference:  56%|█████▋    | 18/32 [00:09<00:07,  1.90it/s]Inference:  59%|█████▉    | 19/32 [00:09<00:06,  1.90it/s]Inference:  62%|██████▎   | 20/32 [00:10<00:06,  1.89it/s]Inference:  66%|██████▌   | 21/32 [00:11<00:05,  1.90it/s]Inference:  69%|██████▉   | 22/32 [00:11<00:05,  1.90it/s]Inference:  72%|███████▏  | 23/32 [00:12<00:04,  1.90it/s]Inference:  75%|███████▌  | 24/32 [00:12<00:04,  1.90it/s]Inference:  78%|███████▊  | 25/32 [00:13<00:03,  1.91it/s]Inference:  81%|████████▏ | 26/32 [00:13<00:03,  1.90it/s]Inference:  84%|████████▍ | 27/32 [00:14<00:02,  1.90it/s]Inference:  88%|████████▊ | 28/32 [00:14<00:02,  1.89it/s]Inference:  91%|█████████ | 29/32 [00:15<00:01,  1.89it/s]Inference:  94%|█████████▍| 30/32 [00:15<00:01,  1.89it/s]Inference:  97%|█████████▋| 31/32 [00:16<00:00,  1.90it/s]Inference: 100%|██████████| 32/32 [00:16<00:00,  1.90it/s]Inference: 100%|██████████| 32/32 [00:16<00:00,  1.90it/s]
layer25: self_attn.q_proj
iter 0, train loss 146120.453125, val loss None, lr 0.01
iter 250, train loss 1472.666748046875, val loss None, lr 0.01
iter 500, train loss 831.9974975585938, val loss None, lr 0.01
iter 750, train loss 557.70703125, val loss None, lr 0.003333
iter 1000, train loss 527.5958251953125, val loss None, lr 0.003333
iter 1250, train loss 507.5555419921875, val loss None, lr 0.003333
iter 1500, train loss 496.2537841796875, val loss None, lr 0.003333
iter 1750, train loss 487.9377746582031, val loss None, lr 0.003333
iter 2000, train loss 471.9972839355469, val loss None, lr 0.001111
iter 2250, train loss 467.0726318359375, val loss None, lr 0.001111
best loss 460.3076171875
layer25: self_attn.k_proj
iter 0, train loss 176151.921875, val loss None, lr 0.01
iter 250, train loss 1289.00927734375, val loss None, lr 0.01
iter 500, train loss 1203.1678466796875, val loss None, lr 0.003333
iter 750, train loss 597.7018432617188, val loss None, lr 0.003333
iter 1000, train loss 536.5888061523438, val loss None, lr 0.001111
iter 1250, train loss 507.0554504394531, val loss None, lr 0.001111
iter 1500, train loss 490.903564453125, val loss None, lr 0.001111
iter 1750, train loss 478.6351318359375, val loss None, lr 0.001111
iter 2000, train loss 469.74090576171875, val loss None, lr 0.001111
iter 2250, train loss 463.1222839355469, val loss None, lr 0.001111
best loss 456.90887451171875
layer25: self_attn.v_proj
iter 0, train loss 10028.41796875, val loss None, lr 0.01
iter 250, train loss 365.8042907714844, val loss None, lr 0.01
iter 500, train loss 345.44683837890625, val loss None, lr 0.01
iter 750, train loss 319.2333984375, val loss None, lr 0.01
iter 1000, train loss 313.994873046875, val loss None, lr 0.01
iter 1250, train loss 299.751953125, val loss None, lr 0.003333
iter 1500, train loss 294.06597900390625, val loss None, lr 0.003333
iter 1750, train loss 290.3751220703125, val loss None, lr 0.001111
iter 2000, train loss 288.8285827636719, val loss None, lr 0.001111
iter 2250, train loss 287.32720947265625, val loss None, lr 0.001111
best loss 285.871826171875
layer25: self_attn.o_proj
iter 0, train loss 12237.5986328125, val loss None, lr 0.01
iter 250, train loss 265.0496826171875, val loss None, lr 0.01
iter 500, train loss 221.42446899414062, val loss None, lr 0.01
iter 750, train loss 196.11386108398438, val loss None, lr 0.003333
iter 1000, train loss 190.602783203125, val loss None, lr 0.003333
iter 1250, train loss 182.38304138183594, val loss None, lr 0.001111
iter 1500, train loss 179.60494995117188, val loss None, lr 0.001111
iter 1750, train loss 177.90048217773438, val loss None, lr 0.001111
iter 2000, train loss 176.322509765625, val loss None, lr 0.001111
iter 2250, train loss 175.04013061523438, val loss None, lr 0.001111
best loss 173.72705078125
layer25: mlp.gate_proj
iter 0, train loss 40252.01953125, val loss None, lr 0.01
iter 250, train loss 1050.8944091796875, val loss None, lr 0.01
iter 500, train loss 1109.249267578125, val loss None, lr 0.01
iter 750, train loss 913.7115478515625, val loss None, lr 0.003333
iter 1000, train loss 898.1759643554688, val loss None, lr 0.003333
iter 1250, train loss 885.86669921875, val loss None, lr 0.003333
iter 1500, train loss 869.9578247070312, val loss None, lr 0.001111
iter 1750, train loss 864.781005859375, val loss None, lr 0.001111
iter 2000, train loss 862.7440185546875, val loss None, lr 0.001111
iter 2250, train loss 857.33740234375, val loss None, lr 0.001111
best loss 853.8397827148438
layer25: mlp.up_proj
iter 0, train loss 19127.494140625, val loss None, lr 0.01
iter 250, train loss 860.523193359375, val loss None, lr 0.01
iter 500, train loss 827.7731323242188, val loss None, lr 0.01
iter 750, train loss 895.1401977539062, val loss None, lr 0.01
iter 1000, train loss 763.100341796875, val loss None, lr 0.003333
iter 1250, train loss 751.2608642578125, val loss None, lr 0.001111
iter 1500, train loss 746.4475708007812, val loss None, lr 0.001111
iter 1750, train loss 743.5400390625, val loss None, lr 0.001111
iter 2000, train loss 740.8265380859375, val loss None, lr 0.001111
iter 2250, train loss 738.764404296875, val loss None, lr 0.001111
best loss 736.5367431640625
layer25: mlp.down_proj
iter 0, train loss 1041.0250244140625, val loss None, lr 0.01
iter 250, train loss 24.495088577270508, val loss None, lr 0.01
iter 500, train loss 22.317852020263672, val loss None, lr 0.01
iter 750, train loss 20.726322174072266, val loss None, lr 0.003333
iter 1000, train loss 19.68292999267578, val loss None, lr 0.003333
iter 1250, train loss 19.1768741607666, val loss None, lr 0.001111
iter 1500, train loss 19.44594383239746, val loss None, lr 0.001111
iter 1750, train loss 18.845443725585938, val loss None, lr 0.001111
iter 2000, train loss 18.696731567382812, val loss None, lr 0.001111
iter 2250, train loss 18.52081298828125, val loss None, lr 0.00037
best loss 18.375614166259766
16065 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:06,  4.84it/s]Inference:   6%|▋         | 2/32 [00:00<00:06,  4.35it/s]Inference:   9%|▉         | 3/32 [00:00<00:06,  4.24it/s]Inference:  12%|█▎        | 4/32 [00:00<00:06,  4.15it/s]Inference:  16%|█▌        | 5/32 [00:01<00:06,  4.13it/s]Inference:  19%|█▉        | 6/32 [00:01<00:06,  4.09it/s]Inference:  22%|██▏       | 7/32 [00:01<00:06,  4.13it/s]Inference:  25%|██▌       | 8/32 [00:01<00:05,  4.11it/s]Inference:  28%|██▊       | 9/32 [00:02<00:05,  4.11it/s]Inference:  31%|███▏      | 10/32 [00:02<00:05,  4.09it/s]Inference:  34%|███▍      | 11/32 [00:02<00:05,  4.09it/s]Inference:  38%|███▊      | 12/32 [00:02<00:04,  4.32it/s]Inference:  41%|████      | 13/32 [00:03<00:04,  4.25it/s]Inference:  44%|████▍     | 14/32 [00:03<00:04,  4.22it/s]Inference:  47%|████▋     | 15/32 [00:03<00:04,  4.19it/s]Inference:  50%|█████     | 16/32 [00:03<00:03,  4.18it/s]Inference:  53%|█████▎    | 17/32 [00:04<00:03,  4.16it/s]Inference:  56%|█████▋    | 18/32 [00:04<00:03,  4.14it/s]Inference:  59%|█████▉    | 19/32 [00:04<00:03,  4.12it/s]Inference:  62%|██████▎   | 20/32 [00:04<00:02,  4.10it/s]Inference:  66%|██████▌   | 21/32 [00:05<00:02,  4.08it/s]Inference:  69%|██████▉   | 22/32 [00:05<00:02,  4.08it/s]Inference:  72%|███████▏  | 23/32 [00:05<00:02,  4.09it/s]Inference:  75%|███████▌  | 24/32 [00:05<00:01,  4.11it/s]Inference:  78%|███████▊  | 25/32 [00:06<00:01,  4.12it/s]Inference:  81%|████████▏ | 26/32 [00:06<00:01,  4.13it/s]Inference:  84%|████████▍ | 27/32 [00:06<00:01,  4.10it/s]Inference:  88%|████████▊ | 28/32 [00:06<00:00,  4.10it/s]Inference:  91%|█████████ | 29/32 [00:07<00:00,  4.08it/s]Inference:  94%|█████████▍| 30/32 [00:07<00:00,  4.09it/s]Inference:  97%|█████████▋| 31/32 [00:07<00:00,  4.08it/s]Inference: 100%|██████████| 32/32 [00:07<00:00,  4.08it/s]Inference: 100%|██████████| 32/32 [00:07<00:00,  4.13it/s]
10761 MiB free out of 48676 MiB total
Saved layer 25 to ./models/meta-llama/Llama-2-7b-hf/tensorized/128/no_finetune2/layer_25.pt
after cast to cpu
14777 MiB free out of 48676 MiB total
Done with layer 25 total_time elapsed: 24693 estimated time left: 5698
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:16,  1.92it/s]Inference:   6%|▋         | 2/32 [00:01<00:15,  1.92it/s]Inference:   9%|▉         | 3/32 [00:01<00:15,  1.90it/s]Inference:  12%|█▎        | 4/32 [00:02<00:14,  1.90it/s]Inference:  16%|█▌        | 5/32 [00:02<00:14,  1.91it/s]Inference:  19%|█▉        | 6/32 [00:03<00:13,  1.91it/s]Inference:  22%|██▏       | 7/32 [00:03<00:13,  1.90it/s]Inference:  25%|██▌       | 8/32 [00:04<00:12,  1.90it/s]Inference:  28%|██▊       | 9/32 [00:04<00:12,  1.90it/s]Inference:  31%|███▏      | 10/32 [00:05<00:11,  1.90it/s]Inference:  34%|███▍      | 11/32 [00:05<00:11,  1.90it/s]Inference:  38%|███▊      | 12/32 [00:06<00:10,  1.90it/s]Inference:  41%|████      | 13/32 [00:06<00:10,  1.89it/s]Inference:  44%|████▍     | 14/32 [00:07<00:09,  1.90it/s]Inference:  47%|████▋     | 15/32 [00:07<00:08,  1.89it/s]Inference:  50%|█████     | 16/32 [00:08<00:08,  1.90it/s]Inference:  53%|█████▎    | 17/32 [00:08<00:07,  1.90it/s]Inference:  56%|█████▋    | 18/32 [00:09<00:07,  1.90it/s]Inference:  59%|█████▉    | 19/32 [00:09<00:06,  1.90it/s]Inference:  62%|██████▎   | 20/32 [00:10<00:06,  1.89it/s]Inference:  66%|██████▌   | 21/32 [00:11<00:05,  1.94it/s]Inference:  69%|██████▉   | 22/32 [00:11<00:05,  1.92it/s]Inference:  72%|███████▏  | 23/32 [00:12<00:04,  1.92it/s]Inference:  75%|███████▌  | 24/32 [00:12<00:04,  1.90it/s]Inference:  78%|███████▊  | 25/32 [00:13<00:03,  1.90it/s]Inference:  81%|████████▏ | 26/32 [00:13<00:03,  1.90it/s]Inference:  84%|████████▍ | 27/32 [00:14<00:02,  1.90it/s]Inference:  88%|████████▊ | 28/32 [00:14<00:02,  1.90it/s]Inference:  91%|█████████ | 29/32 [00:15<00:01,  1.90it/s]Inference:  94%|█████████▍| 30/32 [00:15<00:01,  1.89it/s]Inference:  97%|█████████▋| 31/32 [00:16<00:00,  1.90it/s]Inference: 100%|██████████| 32/32 [00:16<00:00,  1.90it/s]Inference: 100%|██████████| 32/32 [00:16<00:00,  1.90it/s]
layer26: self_attn.q_proj
iter 0, train loss 193450.375, val loss None, lr 0.01
iter 250, train loss 821.9660034179688, val loss None, lr 0.01
iter 500, train loss 612.3456420898438, val loss None, lr 0.003333
iter 750, train loss 586.7242431640625, val loss None, lr 0.003333
iter 1000, train loss 513.7182006835938, val loss None, lr 0.001111
iter 1250, train loss 495.302978515625, val loss None, lr 0.001111
iter 1500, train loss 481.4097595214844, val loss None, lr 0.001111
iter 1750, train loss 473.9981994628906, val loss None, lr 0.001111
iter 2000, train loss 468.7007141113281, val loss None, lr 0.001111
iter 2250, train loss 462.721435546875, val loss None, lr 0.001111
best loss 448.799072265625
layer26: self_attn.k_proj
iter 0, train loss 235723.921875, val loss None, lr 0.01
iter 250, train loss 1327.77880859375, val loss None, lr 0.01
iter 500, train loss 691.4384765625, val loss None, lr 0.003333
iter 750, train loss 623.4228515625, val loss None, lr 0.003333
iter 1000, train loss 531.4642944335938, val loss None, lr 0.001111
iter 1250, train loss 503.40069580078125, val loss None, lr 0.001111
iter 1500, train loss 491.31719970703125, val loss None, lr 0.001111
iter 1750, train loss 478.41680908203125, val loss None, lr 0.001111
iter 2000, train loss 460.4185791015625, val loss None, lr 0.00037
iter 2250, train loss 454.55224609375, val loss None, lr 0.00037
best loss 449.396728515625
layer26: self_attn.v_proj
iter 0, train loss 10399.8642578125, val loss None, lr 0.01
iter 250, train loss 352.3462829589844, val loss None, lr 0.01
iter 500, train loss 306.08251953125, val loss None, lr 0.01
iter 750, train loss 313.06402587890625, val loss None, lr 0.01
iter 1000, train loss 285.9720458984375, val loss None, lr 0.003333
iter 1250, train loss 275.8437805175781, val loss None, lr 0.003333
iter 1500, train loss 271.01318359375, val loss None, lr 0.003333
iter 1750, train loss 268.6251220703125, val loss None, lr 0.003333
iter 2000, train loss 267.14495849609375, val loss None, lr 0.003333
iter 2250, train loss 263.90020751953125, val loss None, lr 0.001111
best loss 262.359130859375
layer26: self_attn.o_proj
iter 0, train loss 13756.1015625, val loss None, lr 0.01
iter 250, train loss 320.2906799316406, val loss None, lr 0.01
iter 500, train loss 270.04705810546875, val loss None, lr 0.01
iter 750, train loss 237.80337524414062, val loss None, lr 0.003333
iter 1000, train loss 229.06129455566406, val loss None, lr 0.001111
iter 1250, train loss 223.96084594726562, val loss None, lr 0.001111
iter 1500, train loss 222.02365112304688, val loss None, lr 0.001111
iter 1750, train loss 220.19692993164062, val loss None, lr 0.001111
iter 2000, train loss 218.672607421875, val loss None, lr 0.001111
iter 2250, train loss 216.91445922851562, val loss None, lr 0.001111
best loss 215.98526000976562
layer26: mlp.gate_proj
iter 0, train loss 51059.5625, val loss None, lr 0.01
iter 250, train loss 1141.2579345703125, val loss None, lr 0.01
iter 500, train loss 1062.98779296875, val loss None, lr 0.01
iter 750, train loss 1079.89453125, val loss None, lr 0.01
iter 1000, train loss 934.5278930664062, val loss None, lr 0.003333
iter 1250, train loss 963.0623779296875, val loss None, lr 0.001111
iter 1500, train loss 897.6412353515625, val loss None, lr 0.001111
iter 1750, train loss 892.8529052734375, val loss None, lr 0.001111
iter 2000, train loss 886.249267578125, val loss None, lr 0.00037
iter 2250, train loss 882.3372192382812, val loss None, lr 0.00037
best loss 879.0518188476562
layer26: mlp.up_proj
iter 0, train loss 24381.28515625, val loss None, lr 0.01
iter 250, train loss 916.0822143554688, val loss None, lr 0.01
iter 500, train loss 842.8497314453125, val loss None, lr 0.01
iter 750, train loss 810.97216796875, val loss None, lr 0.003333
iter 1000, train loss 801.140625, val loss None, lr 0.003333
iter 1250, train loss 783.1448364257812, val loss None, lr 0.001111
iter 1500, train loss 778.4359130859375, val loss None, lr 0.001111
iter 1750, train loss 774.5829467773438, val loss None, lr 0.001111
iter 2000, train loss 771.0455322265625, val loss None, lr 0.00037
iter 2250, train loss 768.4461059570312, val loss None, lr 0.00037
best loss 766.531494140625
layer26: mlp.down_proj
iter 0, train loss 625.428466796875, val loss None, lr 0.01
iter 250, train loss 26.36090087890625, val loss None, lr 0.01
iter 500, train loss 23.19342803955078, val loss None, lr 0.01
iter 750, train loss 21.929916381835938, val loss None, lr 0.003333
iter 1000, train loss 20.863122940063477, val loss None, lr 0.003333
iter 1250, train loss 20.5924072265625, val loss None, lr 0.003333
iter 1500, train loss 20.624961853027344, val loss None, lr 0.003333
iter 1750, train loss 20.11928367614746, val loss None, lr 0.001111
iter 2000, train loss 20.057353973388672, val loss None, lr 0.001111
iter 2250, train loss 19.96508026123047, val loss None, lr 0.001111
best loss 19.874359130859375
14777 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:06,  4.81it/s]Inference:   6%|▋         | 2/32 [00:00<00:06,  4.40it/s]Inference:   9%|▉         | 3/32 [00:00<00:06,  4.27it/s]Inference:  12%|█▎        | 4/32 [00:00<00:06,  4.22it/s]Inference:  16%|█▌        | 5/32 [00:01<00:06,  4.18it/s]Inference:  19%|█▉        | 6/32 [00:01<00:06,  4.12it/s]Inference:  22%|██▏       | 7/32 [00:01<00:06,  4.11it/s]Inference:  25%|██▌       | 8/32 [00:01<00:05,  4.09it/s]Inference:  28%|██▊       | 9/32 [00:02<00:05,  4.10it/s]Inference:  31%|███▏      | 10/32 [00:02<00:05,  4.09it/s]Inference:  34%|███▍      | 11/32 [00:02<00:05,  4.09it/s]Inference:  38%|███▊      | 12/32 [00:02<00:04,  4.08it/s]Inference:  41%|████      | 13/32 [00:03<00:04,  4.09it/s]Inference:  44%|████▍     | 14/32 [00:03<00:04,  4.07it/s]Inference:  47%|████▋     | 15/32 [00:03<00:04,  4.08it/s]Inference:  50%|█████     | 16/32 [00:03<00:03,  4.06it/s]Inference:  53%|█████▎    | 17/32 [00:04<00:03,  4.08it/s]Inference:  56%|█████▋    | 18/32 [00:04<00:03,  4.07it/s]Inference:  59%|█████▉    | 19/32 [00:04<00:03,  4.08it/s]Inference:  62%|██████▎   | 20/32 [00:04<00:02,  4.10it/s]Inference:  66%|██████▌   | 21/32 [00:05<00:02,  4.12it/s]Inference:  69%|██████▉   | 22/32 [00:05<00:02,  4.11it/s]Inference:  72%|███████▏  | 23/32 [00:05<00:02,  4.10it/s]Inference:  75%|███████▌  | 24/32 [00:05<00:01,  4.10it/s]Inference:  78%|███████▊  | 25/32 [00:06<00:01,  4.09it/s]Inference:  81%|████████▏ | 26/32 [00:06<00:01,  4.09it/s]Inference:  84%|████████▍ | 27/32 [00:06<00:01,  4.09it/s]Inference:  88%|████████▊ | 28/32 [00:06<00:00,  4.11it/s]Inference:  91%|█████████ | 29/32 [00:07<00:00,  4.10it/s]Inference:  94%|█████████▍| 30/32 [00:07<00:00,  4.10it/s]Inference:  97%|█████████▋| 31/32 [00:07<00:00,  4.08it/s]Inference: 100%|██████████| 32/32 [00:07<00:00,  4.09it/s]Inference: 100%|██████████| 32/32 [00:07<00:00,  4.11it/s]
9473 MiB free out of 48676 MiB total
Saved layer 26 to ./models/meta-llama/Llama-2-7b-hf/tensorized/128/no_finetune2/layer_26.pt
after cast to cpu
13489 MiB free out of 48676 MiB total
Done with layer 26 total_time elapsed: 25562 estimated time left: 4734
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:16,  1.92it/s]Inference:   6%|▋         | 2/32 [00:01<00:15,  1.92it/s]Inference:   9%|▉         | 3/32 [00:01<00:15,  1.90it/s]Inference:  12%|█▎        | 4/32 [00:02<00:14,  1.90it/s]Inference:  16%|█▌        | 5/32 [00:02<00:13,  1.93it/s]Inference:  19%|█▉        | 6/32 [00:03<00:13,  1.94it/s]Inference:  22%|██▏       | 7/32 [00:03<00:12,  1.93it/s]Inference:  25%|██▌       | 8/32 [00:04<00:12,  1.97it/s]Inference:  28%|██▊       | 9/32 [00:04<00:11,  1.96it/s]Inference:  31%|███▏      | 10/32 [00:05<00:11,  1.95it/s]Inference:  34%|███▍      | 11/32 [00:05<00:10,  1.95it/s]Inference:  38%|███▊      | 12/32 [00:06<00:10,  1.95it/s]Inference:  41%|████      | 13/32 [00:06<00:09,  1.95it/s]Inference:  44%|████▍     | 14/32 [00:07<00:09,  1.95it/s]Inference:  47%|████▋     | 15/32 [00:07<00:08,  1.95it/s]Inference:  50%|█████     | 16/32 [00:08<00:08,  1.94it/s]Inference:  53%|█████▎    | 17/32 [00:08<00:07,  1.95it/s]Inference:  56%|█████▋    | 18/32 [00:09<00:07,  1.94it/s]Inference:  59%|█████▉    | 19/32 [00:09<00:06,  1.93it/s]Inference:  62%|██████▎   | 20/32 [00:10<00:06,  1.91it/s]Inference:  66%|██████▌   | 21/32 [00:10<00:05,  1.90it/s]Inference:  69%|██████▉   | 22/32 [00:11<00:05,  1.91it/s]Inference:  72%|███████▏  | 23/32 [00:11<00:04,  1.92it/s]Inference:  75%|███████▌  | 24/32 [00:12<00:04,  1.96it/s]Inference:  78%|███████▊  | 25/32 [00:12<00:03,  1.96it/s]Inference:  81%|████████▏ | 26/32 [00:13<00:03,  1.96it/s]Inference:  84%|████████▍ | 27/32 [00:13<00:02,  1.96it/s]Inference:  88%|████████▊ | 28/32 [00:14<00:02,  1.94it/s]Inference:  91%|█████████ | 29/32 [00:14<00:01,  1.93it/s]Inference:  94%|█████████▍| 30/32 [00:15<00:01,  1.93it/s]Inference:  97%|█████████▋| 31/32 [00:15<00:00,  1.96it/s]Inference: 100%|██████████| 32/32 [00:16<00:00,  1.98it/s]Inference: 100%|██████████| 32/32 [00:16<00:00,  1.94it/s]
layer27: self_attn.q_proj
iter 0, train loss 182233.90625, val loss None, lr 0.01
iter 250, train loss 810.91650390625, val loss None, lr 0.01
iter 500, train loss 611.0779418945312, val loss None, lr 0.01
iter 750, train loss 533.2015380859375, val loss None, lr 0.003333
iter 1000, train loss 494.4886474609375, val loss None, lr 0.003333
iter 1250, train loss 452.7923278808594, val loss None, lr 0.001111
iter 1500, train loss 442.553466796875, val loss None, lr 0.001111
iter 1750, train loss 427.9590759277344, val loss None, lr 0.00037
iter 2000, train loss 420.471435546875, val loss None, lr 0.00037
iter 2250, train loss 415.7628173828125, val loss None, lr 0.00037
best loss 411.4362487792969
layer27: self_attn.k_proj
iter 0, train loss 213037.6875, val loss None, lr 0.01
iter 250, train loss 798.288818359375, val loss None, lr 0.01
iter 500, train loss 628.7255249023438, val loss None, lr 0.01
iter 750, train loss 494.0120544433594, val loss None, lr 0.003333
iter 1000, train loss 513.9237670898438, val loss None, lr 0.003333
iter 1250, train loss 459.09735107421875, val loss None, lr 0.003333
iter 1500, train loss 456.241943359375, val loss None, lr 0.003333
iter 1750, train loss 429.850830078125, val loss None, lr 0.001111
iter 2000, train loss 421.0009765625, val loss None, lr 0.001111
iter 2250, train loss 418.207275390625, val loss None, lr 0.001111
best loss 413.82794189453125
layer27: self_attn.v_proj
iter 0, train loss 11484.390625, val loss None, lr 0.01
iter 250, train loss 324.0626220703125, val loss None, lr 0.01
iter 500, train loss 290.2951965332031, val loss None, lr 0.01
iter 750, train loss 260.4988098144531, val loss None, lr 0.003333
iter 1000, train loss 251.7157440185547, val loss None, lr 0.001111
iter 1250, train loss 247.09002685546875, val loss None, lr 0.001111
iter 1500, train loss 243.888671875, val loss None, lr 0.001111
iter 1750, train loss 241.378173828125, val loss None, lr 0.001111
iter 2000, train loss 239.39706420898438, val loss None, lr 0.001111
iter 2250, train loss 237.74984741210938, val loss None, lr 0.001111
best loss 236.34751892089844
layer27: self_attn.o_proj
iter 0, train loss 16317.3994140625, val loss None, lr 0.01
iter 250, train loss 306.1396484375, val loss None, lr 0.01
iter 500, train loss 250.7086944580078, val loss None, lr 0.003333
iter 750, train loss 235.70254516601562, val loss None, lr 0.003333
iter 1000, train loss 229.12869262695312, val loss None, lr 0.003333
iter 1250, train loss 220.5155487060547, val loss None, lr 0.003333
iter 1500, train loss 212.3588104248047, val loss None, lr 0.001111
iter 1750, train loss 217.42196655273438, val loss None, lr 0.001111
iter 2000, train loss 206.86203002929688, val loss None, lr 0.001111
iter 2250, train loss 205.61363220214844, val loss None, lr 0.001111
best loss 203.8419647216797
layer27: mlp.gate_proj
iter 0, train loss 64630.1171875, val loss None, lr 0.01
iter 250, train loss 1163.9141845703125, val loss None, lr 0.01
iter 500, train loss 983.707275390625, val loss None, lr 0.01
iter 750, train loss 982.137939453125, val loss None, lr 0.003333
