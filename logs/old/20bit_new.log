/home/lliu/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.23s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.29it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.18it/s]
Model loaded. LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
Loading tokenizer for huggyllama/llama-7b
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=True`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Token indices sequence length is longer than the specified maximum sequence length for this model (2824491 > 2048). Running this sequence through the model will result in indexing errors
Starting...
Ready.
initial layer
torch.Size([128, 2048, 4096])
torch.Size([128, 2048, 4096])
torch.Size([128, 2048, 4096])
0 self_attn.q_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
24 H_error tensor(5.5616, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.4308, device='cuda:6', grad_fn=<DivBackward0>)
40725 MiB free out of 48676 MiB total
0 self_attn.k_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
22 H_error tensor(5.5185, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.4217, device='cuda:6', grad_fn=<DivBackward0>)
40757 MiB free out of 48676 MiB total
0 self_attn.v_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
40 H_error tensor(0.2417, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3620, device='cuda:6', grad_fn=<DivBackward0>)
34266 MiB free out of 48676 MiB total
0 self_attn.o_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
60 H_error tensor(0.0347, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.4141, device='cuda:6', grad_fn=<DivBackward0>)
35104 MiB free out of 48676 MiB total
0 mlp.gate_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
103 H_error tensor(3.3279, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3362, device='cuda:6', grad_fn=<DivBackward0>)
35122 MiB free out of 48676 MiB total
0 mlp.up_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
102 H_error tensor(2.4411, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3279, device='cuda:6', grad_fn=<DivBackward0>)
35106 MiB free out of 48676 MiB total
0 mlp.down_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
140 H_error tensor(0.0546, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3357, device='cuda:6', grad_fn=<DivBackward0>)
34536 MiB free out of 48676 MiB total
inputs.shape torch.Size([128, 2048, 4096])
Iteration 0, loss 0.9782218933105469, time 11.623
Iteration 1, loss 0.6365470886230469, time 9.111
Iteration 2, loss 0.4821453094482422, time 8.058
Iteration 3, loss 0.3924903869628906, time 8.181
Iteration 4, loss 0.33377838134765625, time 8.272
Iteration 5, loss 0.292510986328125, time 7.939
Iteration 6, loss 0.2626047134399414, time 8.305
Iteration 7, loss 0.24071121215820312, time 8.276
Iteration 8, loss 0.22497081756591797, time 7.964
Iteration 9, loss 0.21408557891845703, time 8.386
595.9104917049408
Loading tokenizer for huggyllama/llama-7b
/home/lliu/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Token indices sequence length is longer than the specified maximum sequence length for this model (2824491 > 2048). Running this sequence through the model will result in indexing errors
Dataset: wikitext2
Evaluating ...
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
Perplexity: 5.944508
