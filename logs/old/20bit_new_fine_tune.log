/home/lliu/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.04s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.49it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.38it/s]
Model loaded. LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
Loading tokenizer for huggyllama/llama-7b
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=True`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Token indices sequence length is longer than the specified maximum sequence length for this model (2824491 > 2048). Running this sequence through the model will result in indexing errors
Starting...
Ready.
initial layer
torch.Size([128, 2048, 4096])
torch.Size([128, 2048, 4096])
torch.Size([128, 2048, 4096])
0 self_attn.q_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
24 H_error tensor(5.4297, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.4309, device='cuda:6', grad_fn=<DivBackward0>)
40725 MiB free out of 48676 MiB total
0 self_attn.k_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
22 H_error tensor(5.4716, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.4219, device='cuda:6', grad_fn=<DivBackward0>)
40757 MiB free out of 48676 MiB total
0 self_attn.v_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
20 H_error tensor(0.2412, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3623, device='cuda:6', grad_fn=<DivBackward0>)
40789 MiB free out of 48676 MiB total
0 self_attn.o_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
23 H_error tensor(0.0341, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.4162, device='cuda:6', grad_fn=<DivBackward0>)
40821 MiB free out of 48676 MiB total
0 mlp.gate_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
53 H_error tensor(3.3440, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3362, device='cuda:6', grad_fn=<DivBackward0>)
40807 MiB free out of 48676 MiB total
0 mlp.up_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
53 H_error tensor(2.4541, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3279, device='cuda:6', grad_fn=<DivBackward0>)
40761 MiB free out of 48676 MiB total
0 mlp.down_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
59 H_error tensor(0.0556, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3359, device='cuda:6', grad_fn=<DivBackward0>)
41103 MiB free out of 48676 MiB total
inputs.shape torch.Size([128, 2048, 4096])
saving because model is best
Iteration 0, loss 0.9960515554994345, regul_loss 0.0, reconstruct_loss 0.9960515554994345, time 16.732
Iteration 1, loss 47.7319133579731, regul_loss 468.3492126464844, reconstruct_loss 0.8969905064441264, time 16.776
Iteration 2, loss 12.799555979669094, regul_loss 119.48271179199219, reconstruct_loss 0.8512842399068177, time 16.863
Iteration 3, loss 8.777220159769058, regul_loss 79.29986572265625, reconstruct_loss 0.84723363770172, time 16.863
Iteration 4, loss 4.52337746694684, regul_loss 36.80023193359375, reconstruct_loss 0.8433542866259813, time 16.871
Iteration 5, loss 4.085939399898052, regul_loss 32.42963409423828, reconstruct_loss 0.8429758553393185, time 16.884
Iteration 6, loss 3.640416994690895, regul_loss 27.97814178466797, reconstruct_loss 0.8426028662361205, time 16.898
Iteration 7, loss 3.59531114064157, regul_loss 27.527450561523438, reconstruct_loss 0.8425659844651818, time 16.884
Iteration 8, loss 3.5497804433107376, regul_loss 27.072509765625, reconstruct_loss 0.8425293690524995, time 16.884
Iteration 9, loss 3.545195585116744, regul_loss 27.02669906616211, reconstruct_loss 0.8425257285125554, time 16.893
1 self_attn.q_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
22 H_error tensor(41.9322, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.4019, device='cuda:6', grad_fn=<DivBackward0>)
40159 MiB free out of 48676 MiB total
1 self_attn.k_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
21 H_error tensor(70.5200, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.4056, device='cuda:6', grad_fn=<DivBackward0>)
40095 MiB free out of 48676 MiB total
1 self_attn.v_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
reducing lr to  0.00020589113209464906
reducing lr to  0.00018530201888518417
21 H_error tensor(1.1919, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3581, device='cuda:6', grad_fn=<DivBackward0>)
40031 MiB free out of 48676 MiB total
1 self_attn.o_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
23 H_error tensor(0.1561, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3947, device='cuda:6', grad_fn=<DivBackward0>)
40031 MiB free out of 48676 MiB total
1 mlp.gate_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
53 H_error tensor(12.8850, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3246, device='cuda:6', grad_fn=<DivBackward0>)
40117 MiB free out of 48676 MiB total
1 mlp.up_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
53 H_error tensor(9.6465, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3181, device='cuda:6', grad_fn=<DivBackward0>)
40203 MiB free out of 48676 MiB total
1 mlp.down_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
59 H_error tensor(0.1423, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3247, device='cuda:6', grad_fn=<DivBackward0>)
40375 MiB free out of 48676 MiB total
inputs.shape torch.Size([128, 2048, 4096])
saving because model is best
Iteration 0, loss 32.96421991288662, regul_loss 0.0, reconstruct_loss 32.96421991288662, time 16.635
Iteration 1, loss 78.55799800157547, regul_loss 469.2637939453125, reconstruct_loss 31.6316180229187, time 16.829
Iteration 2, loss 43.946768403053284, regul_loss 126.68888854980469, reconstruct_loss 31.277879685163498, time 16.869
Iteration 3, loss 39.99981236457825, regul_loss 87.40460205078125, reconstruct_loss 31.259352296590805, time 16.875
Iteration 4, loss 35.87300994992256, regul_loss 46.266048431396484, reconstruct_loss 31.24640454351902, time 16.884
Iteration 5, loss 35.45532223582268, regul_loss 42.09901428222656, reconstruct_loss 31.245421558618546, time 16.895
Iteration 6, loss 35.03245134651661, regul_loss 37.87828826904297, reconstruct_loss 31.24462193250656, time 16.89
Iteration 7, loss 34.989822164177895, regul_loss 37.45268630981445, reconstruct_loss 31.24455328285694, time 16.899
Iteration 8, loss 34.94684226810932, regul_loss 37.02349090576172, reconstruct_loss 31.24449260532856, time 16.891
Iteration 9, loss 34.94251577556133, regul_loss 36.98029708862305, reconstruct_loss 31.24448636174202, time 16.894
2 self_attn.q_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
22 H_error tensor(65.7605, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3638, device='cuda:6', grad_fn=<DivBackward0>)
40159 MiB free out of 48676 MiB total
2 self_attn.k_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
21 H_error tensor(62.7166, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3694, device='cuda:6', grad_fn=<DivBackward0>)
40095 MiB free out of 48676 MiB total
2 self_attn.v_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
20 H_error tensor(3.1500, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3253, device='cuda:6', grad_fn=<DivBackward0>)
40095 MiB free out of 48676 MiB total
2 self_attn.o_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
20 H_error tensor(0.4842, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3345, device='cuda:6', grad_fn=<DivBackward0>)
40095 MiB free out of 48676 MiB total
2 mlp.gate_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
53 H_error tensor(32.4324, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3222, device='cuda:6', grad_fn=<DivBackward0>)
40221 MiB free out of 48676 MiB total
2 mlp.up_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
53 H_error tensor(20.6087, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3165, device='cuda:6', grad_fn=<DivBackward0>)
40175 MiB free out of 48676 MiB total
2 mlp.down_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
59 H_error tensor(-43.6733, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.4209, device='cuda:6', grad_fn=<DivBackward0>)
40347 MiB free out of 48676 MiB total
inputs.shape torch.Size([128, 2048, 4096])
saving because model is best
Iteration 0, loss 1.024567517451942, regul_loss 0.0, reconstruct_loss 1.024567517451942, time 16.622
Iteration 1, loss 47.8768355846405, regul_loss 469.5604248046875, reconstruct_loss 0.9207923454232514, time 16.816
Iteration 2, loss 12.982701234519482, regul_loss 120.41606140136719, reconstruct_loss 0.9410952581092715, time 16.841
Iteration 3, loss 8.960693277418613, regul_loss 80.16581726074219, reconstruct_loss 0.9441117513924837, time 16.863
Iteration 4, loss 4.705178800970316, regul_loss 37.57634735107422, reconstruct_loss 0.9475439274683595, time 16.874
Iteration 5, loss 4.267370734363794, regul_loss 33.19459533691406, reconstruct_loss 0.9479110618121922, time 16.873
Iteration 6, loss 3.8214212600141764, regul_loss 28.73128890991211, reconstruct_loss 0.9482923611067235, time 16.889
Iteration 7, loss 3.776268834248185, regul_loss 28.279373168945312, reconstruct_loss 0.9483314785175025, time 16.899
Iteration 8, loss 3.7306922543793917, regul_loss 27.823209762573242, reconstruct_loss 0.9483712818473577, time 16.913
Iteration 9, loss 3.7261028345674276, regul_loss 27.77727508544922, reconstruct_loss 0.9483752376399934, time 16.917
3 self_attn.q_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
21 H_error tensor(65.9162, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3513, device='cuda:6', grad_fn=<DivBackward0>)
40159 MiB free out of 48676 MiB total
3 self_attn.k_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
22 H_error tensor(81.5374, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3585, device='cuda:6', grad_fn=<DivBackward0>)
40095 MiB free out of 48676 MiB total
3 self_attn.v_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
reducing lr to  0.00020589113209464906
reducing lr to  0.00018530201888518417
reducing lr to  0.00016677181699666576
reducing lr to  0.0001500946352969992
reducing lr to  0.0001350851717672993
reducing lr to  0.00012157665459056936
reducing lr to  0.00010941898913151243
reducing lr to  9.847709021836118e-05
reducing lr to  8.862938119652506e-05
reducing lr to  7.976644307687256e-05
reducing lr to  7.17897987691853e-05
reducing lr to  6.461081889226677e-05
21 H_error tensor(12.9536, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3207, device='cuda:6', grad_fn=<DivBackward0>)
40095 MiB free out of 48676 MiB total
3 self_attn.o_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
21 H_error tensor(0.7073, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3369, device='cuda:6', grad_fn=<DivBackward0>)
40095 MiB free out of 48676 MiB total
3 mlp.gate_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
reducing lr to  0.00020589113209464906
reducing lr to  0.00018530201888518417
reducing lr to  0.00016677181699666576
reducing lr to  0.0001500946352969992
reducing lr to  0.0001350851717672993
54 H_error tensor(37.2777, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3190, device='cuda:6', grad_fn=<DivBackward0>)
40221 MiB free out of 48676 MiB total
3 mlp.up_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
reducing lr to  0.00020589113209464906
reducing lr to  0.00018530201888518417
reducing lr to  0.00016677181699666576
reducing lr to  0.0001500946352969992
reducing lr to  0.0001350851717672993
reducing lr to  0.00012157665459056936
reducing lr to  0.00010941898913151243
reducing lr to  9.847709021836118e-05
reducing lr to  8.862938119652506e-05
reducing lr to  7.976644307687256e-05
reducing lr to  7.17897987691853e-05
reducing lr to  6.461081889226677e-05
53 H_error tensor(28.9896, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3141, device='cuda:6', grad_fn=<DivBackward0>)
40175 MiB free out of 48676 MiB total
3 mlp.down_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
59 H_error tensor(0.4467, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3180, device='cuda:6', grad_fn=<DivBackward0>)
40347 MiB free out of 48676 MiB total
inputs.shape torch.Size([128, 2048, 4096])
saving because model is best
Iteration 0, loss 1.7492362754419446, regul_loss 0.0, reconstruct_loss 1.7492362754419446, time 16.63
Iteration 1, loss 48.446480840444565, regul_loss 467.5399169921875, reconstruct_loss 1.6924868831411004, time 16.832
Iteration 2, loss 13.625902585685253, regul_loss 119.51600646972656, reconstruct_loss 1.6743015432730317, time 16.863
Iteration 3, loss 9.613298386335373, regul_loss 79.40455627441406, reconstruct_loss 1.6728424848988652, time 16.874
Iteration 4, loss 5.364939630031586, regul_loss 36.93434143066406, reconstruct_loss 1.6715053897351027, time 16.878
Iteration 5, loss 4.927144832909107, regul_loss 32.55766296386719, reconstruct_loss 1.6713784588500857, time 16.898
Iteration 6, loss 4.480898052453995, regul_loss 28.09642791748047, reconstruct_loss 1.6712552774697542, time 16.899
Iteration 7, loss 4.435693044215441, regul_loss 27.64449691772461, reconstruct_loss 1.671243210323155, time 16.906
Iteration 8, loss 4.390059515833855, regul_loss 27.188282012939453, reconstruct_loss 1.6712313098832965, time 16.902
Iteration 9, loss 4.385463979095221, regul_loss 27.142337799072266, reconstruct_loss 1.6712300991639495, time 16.905
4 self_attn.q_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
21 H_error tensor(91.5033, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3535, device='cuda:6', grad_fn=<DivBackward0>)
40159 MiB free out of 48676 MiB total
4 self_attn.k_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
21 H_error tensor(105.8790, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3581, device='cuda:6', grad_fn=<DivBackward0>)
40095 MiB free out of 48676 MiB total
4 self_attn.v_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
reducing lr to  0.00020589113209464906
reducing lr to  0.00018530201888518417
reducing lr to  0.00016677181699666576
reducing lr to  0.0001500946352969992
21 H_error tensor(19.0708, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3246, device='cuda:6', grad_fn=<DivBackward0>)
40031 MiB free out of 48676 MiB total
4 self_attn.o_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
20 H_error tensor(0.9865, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3405, device='cuda:6', grad_fn=<DivBackward0>)
40031 MiB free out of 48676 MiB total
4 mlp.gate_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
53 H_error tensor(55.9510, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3221, device='cuda:6', grad_fn=<DivBackward0>)
40117 MiB free out of 48676 MiB total
4 mlp.up_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
reducing lr to  0.00020589113209464906
reducing lr to  0.00018530201888518417
reducing lr to  0.00016677181699666576
reducing lr to  0.0001500946352969992
reducing lr to  0.0001350851717672993
reducing lr to  0.00012157665459056936
reducing lr to  0.00010941898913151243
reducing lr to  9.847709021836118e-05
reducing lr to  8.862938119652506e-05
reducing lr to  7.976644307687256e-05
53 H_error tensor(39.6390, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3150, device='cuda:6', grad_fn=<DivBackward0>)
40203 MiB free out of 48676 MiB total
4 mlp.down_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
59 H_error tensor(0.7700, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3214, device='cuda:6', grad_fn=<DivBackward0>)
40375 MiB free out of 48676 MiB total
inputs.shape torch.Size([128, 2048, 4096])
saving because model is best
Iteration 0, loss 2.833852756768465, regul_loss 0.0, reconstruct_loss 2.833852756768465, time 16.635
Iteration 1, loss 49.39425724744797, regul_loss 468.206298828125, reconstruct_loss 2.5736281778663397, time 16.822
Iteration 2, loss 14.726290911436081, regul_loss 121.0789794921875, reconstruct_loss 2.6183931957930326, time 16.857
Iteration 3, loss 10.726070448756218, regul_loss 81.00653076171875, reconstruct_loss 2.625416789203882, time 16.861
Iteration 4, loss 6.491576254367828, regul_loss 38.58076477050781, reconstruct_loss 2.633499665185809, time 16.881
Iteration 5, loss 6.055750273168087, regul_loss 34.213829040527344, reconstruct_loss 2.634367384016514, time 16.869
Iteration 6, loss 5.611777674406767, regul_loss 29.765064239501953, reconstruct_loss 2.6352712213993073, time 16.88
Iteration 7, loss 5.56682350859046, regul_loss 29.314590454101562, reconstruct_loss 2.6353643108159304, time 16.873
Iteration 8, loss 5.521448988467455, regul_loss 28.85989761352539, reconstruct_loss 2.6354590840637684, time 16.879
Iteration 9, loss 5.516880203038454, regul_loss 28.814111709594727, reconstruct_loss 2.6354688815772533, time 16.872
5 self_attn.q_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
21 H_error tensor(124.3975, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3528, device='cuda:6', grad_fn=<DivBackward0>)
40159 MiB free out of 48676 MiB total
5 self_attn.k_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
21 H_error tensor(139.3696, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3554, device='cuda:6', grad_fn=<DivBackward0>)
40095 MiB free out of 48676 MiB total
5 self_attn.v_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
reducing lr to  0.00020589113209464906
reducing lr to  0.00018530201888518417
reducing lr to  0.00016677181699666576
reducing lr to  0.0001500946352969992
reducing lr to  0.0001350851717672993
reducing lr to  0.00012157665459056936
reducing lr to  0.00010941898913151243
reducing lr to  9.847709021836118e-05
reducing lr to  8.862938119652506e-05
reducing lr to  7.976644307687256e-05
reducing lr to  7.17897987691853e-05
reducing lr to  6.461081889226677e-05
reducing lr to  5.81497370030401e-05
reducing lr to  5.233476330273609e-05
reducing lr to  4.7101286972462485e-05
reducing lr to  4.239115827521624e-05
reducing lr to  3.8152042447694614e-05
reducing lr to  3.433683820292515e-05
reducing lr to  3.090315438263264e-05
reducing lr to  2.7812838944369376e-05
reducing lr to  2.503155504993244e-05
reducing lr to  2.2528399544939195e-05
reducing lr to  2.0275559590445276e-05
reducing lr to  1.8248003631400748e-05
reducing lr to  1.6423203268260675e-05
20 H_error tensor(29.7788, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3212, device='cuda:6', grad_fn=<DivBackward0>)
40095 MiB free out of 48676 MiB total
5 self_attn.o_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
20 H_error tensor(1.2391, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3351, device='cuda:6', grad_fn=<DivBackward0>)
40095 MiB free out of 48676 MiB total
5 mlp.gate_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
reducing lr to  0.00020589113209464906
reducing lr to  0.00018530201888518417
reducing lr to  0.00016677181699666576
53 H_error tensor(72.0202, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3212, device='cuda:6', grad_fn=<DivBackward0>)
40221 MiB free out of 48676 MiB total
5 mlp.up_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
reducing lr to  0.00020589113209464906
reducing lr to  0.00018530201888518417
reducing lr to  0.00016677181699666576
reducing lr to  0.0001500946352969992
reducing lr to  0.0001350851717672993
reducing lr to  0.00012157665459056936
reducing lr to  0.00010941898913151243
reducing lr to  9.847709021836118e-05
reducing lr to  8.862938119652506e-05
reducing lr to  7.976644307687256e-05
53 H_error tensor(52.4319, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3141, device='cuda:6', grad_fn=<DivBackward0>)
40175 MiB free out of 48676 MiB total
5 mlp.down_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
59 H_error tensor(0.9660, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3184, device='cuda:6', grad_fn=<DivBackward0>)
40347 MiB free out of 48676 MiB total
inputs.shape torch.Size([128, 2048, 4096])
saving because model is best
Iteration 0, loss 2.7651258297264576, regul_loss 0.0, reconstruct_loss 2.7651258297264576, time 16.642
Iteration 1, loss 49.33629396557808, regul_loss 467.640869140625, reconstruct_loss 2.5722064524888992, time 16.846
Iteration 2, loss 14.669525682926178, regul_loss 120.58210754394531, reconstruct_loss 2.611314356327057, time 16.885
Iteration 3, loss 10.670553244650364, regul_loss 80.53219604492188, reconstruct_loss 2.6173334140330553, time 16.875
Iteration 4, loss 6.437056731432676, regul_loss 38.128395080566406, reconstruct_loss 2.624217212200165, time 16.881
Iteration 5, loss 6.001179698854685, regul_loss 33.76225280761719, reconstruct_loss 2.6249544471502304, time 16.892
Iteration 6, loss 5.557070914655924, regul_loss 29.313508987426758, reconstruct_loss 2.6257200073450804, time 16.885
Iteration 7, loss 5.512095578014851, regul_loss 28.86297035217285, reconstruct_loss 2.6257985774427652, time 16.898
Iteration 8, loss 5.466698095202446, regul_loss 28.40819549560547, reconstruct_loss 2.6258784867823124, time 16.882
Iteration 9, loss 5.4621267803013325, regul_loss 28.36240005493164, reconstruct_loss 2.6258867289870977, time 16.897
6 self_attn.q_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
21 H_error tensor(122.0036, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3481, device='cuda:6', grad_fn=<DivBackward0>)
40159 MiB free out of 48676 MiB total
6 self_attn.k_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
21 H_error tensor(149.9337, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3534, device='cuda:6', grad_fn=<DivBackward0>)
40095 MiB free out of 48676 MiB total
6 self_attn.v_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
reducing lr to  0.00020589113209464906
reducing lr to  0.00018530201888518417
reducing lr to  0.00016677181699666576
reducing lr to  0.0001500946352969992
20 H_error tensor(32.2836, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3224, device='cuda:6', grad_fn=<DivBackward0>)
40031 MiB free out of 48676 MiB total
6 self_attn.o_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
20 H_error tensor(2.4940, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3382, device='cuda:6', grad_fn=<DivBackward0>)
40031 MiB free out of 48676 MiB total
6 mlp.gate_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
reducing lr to  0.00020589113209464906
reducing lr to  0.00018530201888518417
reducing lr to  0.00016677181699666576
reducing lr to  0.0001500946352969992
reducing lr to  0.0001350851717672993
53 H_error tensor(73.0621, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3209, device='cuda:6', grad_fn=<DivBackward0>)
40157 MiB free out of 48676 MiB total
6 mlp.up_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
reducing lr to  0.00020589113209464906
reducing lr to  0.00018530201888518417
reducing lr to  0.00016677181699666576
reducing lr to  0.0001500946352969992
reducing lr to  0.0001350851717672993
reducing lr to  0.00012157665459056936
reducing lr to  0.00010941898913151243
reducing lr to  9.847709021836118e-05
reducing lr to  8.862938119652506e-05
reducing lr to  7.976644307687256e-05
reducing lr to  7.17897987691853e-05
53 H_error tensor(55.9112, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3142, device='cuda:6', grad_fn=<DivBackward0>)
40111 MiB free out of 48676 MiB total
6 mlp.down_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
60 H_error tensor(1.1939, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3194, device='cuda:6', grad_fn=<DivBackward0>)
40283 MiB free out of 48676 MiB total
inputs.shape torch.Size([128, 2048, 4096])
saving because model is best
Iteration 0, loss 3.2936999145895243, regul_loss 0.0, reconstruct_loss 3.2936999145895243, time 16.634
Iteration 1, loss 49.62236714363098, regul_loss 466.328857421875, reconstruct_loss 2.9894820861518383, time 16.831
Iteration 2, loss 15.194845870137215, regul_loss 121.19453430175781, reconstruct_loss 3.0753924027085304, time 16.853
Iteration 3, loss 11.222087137401104, regul_loss 81.34214782714844, reconstruct_loss 3.087871827185154, time 16.863
Iteration 4, loss 7.019296009093523, regul_loss 39.173484802246094, reconstruct_loss 3.101947382092476, time 16.871
Iteration 5, loss 6.58725780621171, regul_loss 34.838134765625, reconstruct_loss 3.103444280102849, time 16.873
Iteration 6, loss 6.1473194397985935, regul_loss 30.423267364501953, reconstruct_loss 3.104992737993598, time 16.886
Iteration 7, loss 6.102785881608725, regul_loss 29.97634506225586, reconstruct_loss 3.1051512006670237, time 16.89
Iteration 8, loss 6.057838190346956, regul_loss 29.52526092529297, reconstruct_loss 3.1053121145814657, time 16.89
Iteration 9, loss 6.053312752395868, regul_loss 29.479841232299805, reconstruct_loss 3.1053285468369722, time 16.892
7 self_attn.q_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
22 H_error tensor(139.4368, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3529, device='cuda:6', grad_fn=<DivBackward0>)
40159 MiB free out of 48676 MiB total
7 self_attn.k_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
21 H_error tensor(143.2745, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3553, device='cuda:6', grad_fn=<DivBackward0>)
40159 MiB free out of 48676 MiB total
7 self_attn.v_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
reducing lr to  0.00020589113209464906
reducing lr to  0.00018530201888518417
reducing lr to  0.00016677181699666576
reducing lr to  0.0001500946352969992
reducing lr to  0.0001350851717672993
reducing lr to  0.00012157665459056936
reducing lr to  0.00010941898913151243
reducing lr to  9.847709021836118e-05
reducing lr to  8.862938119652506e-05
reducing lr to  7.976644307687256e-05
reducing lr to  7.17897987691853e-05
reducing lr to  6.461081889226677e-05
reducing lr to  5.81497370030401e-05
reducing lr to  5.233476330273609e-05
reducing lr to  4.7101286972462485e-05
reducing lr to  4.239115827521624e-05
reducing lr to  3.8152042447694614e-05
reducing lr to  3.433683820292515e-05
reducing lr to  3.090315438263264e-05
reducing lr to  2.7812838944369376e-05
reducing lr to  2.503155504993244e-05
reducing lr to  2.2528399544939195e-05
reducing lr to  2.0275559590445276e-05
reducing lr to  1.8248003631400748e-05
reducing lr to  1.6423203268260675e-05
reducing lr to  1.4780882941434607e-05
reducing lr to  1.3302794647291146e-05
20 H_error tensor(36.8451, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3236, device='cuda:6', grad_fn=<DivBackward0>)
40159 MiB free out of 48676 MiB total
7 self_attn.o_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
20 H_error tensor(2.7038, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3261, device='cuda:6', grad_fn=<DivBackward0>)
40159 MiB free out of 48676 MiB total
7 mlp.gate_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
53 H_error tensor(83.6092, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3238, device='cuda:6', grad_fn=<DivBackward0>)
40245 MiB free out of 48676 MiB total
7 mlp.up_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
reducing lr to  0.00020589113209464906
reducing lr to  0.00018530201888518417
reducing lr to  0.00016677181699666576
reducing lr to  0.0001500946352969992
reducing lr to  0.0001350851717672993
reducing lr to  0.00012157665459056936
53 H_error tensor(64.8905, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3151, device='cuda:6', grad_fn=<DivBackward0>)
40199 MiB free out of 48676 MiB total
7 mlp.down_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
59 H_error tensor(1.5836, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3212, device='cuda:6', grad_fn=<DivBackward0>)
40199 MiB free out of 48676 MiB total
inputs.shape torch.Size([128, 2048, 4096])
saving because model is best
Iteration 0, loss 3.5605533458292484, regul_loss 0.0, reconstruct_loss 3.5605533458292484, time 16.639
Iteration 1, loss 49.728436052799225, regul_loss 463.4638671875, reconstruct_loss 3.3820500504225492, time 16.819
Iteration 2, loss 15.229052737355232, regul_loss 118.91311645507812, reconstruct_loss 3.3377410359680653, time 16.852
Iteration 3, loss 11.251519314944744, regul_loss 79.1707763671875, reconstruct_loss 3.3344417605549097, time 16.878
Iteration 4, loss 7.03861116990447, regul_loss 37.07042694091797, reconstruct_loss 3.3315685018897057, time 16.882
Iteration 5, loss 6.604234203696251, regul_loss 32.72929000854492, reconstruct_loss 3.331305257976055, time 16.889
Iteration 6, loss 6.161389712244272, regul_loss 28.303333282470703, reconstruct_loss 3.331056224182248, time 16.885
Iteration 7, loss 6.116523019969463, regul_loss 27.854904174804688, reconstruct_loss 3.331032497808337, time 16.901
Iteration 8, loss 6.0712311416864395, regul_loss 27.4022159576416, reconstruct_loss 3.3310094233602285, time 16.907
Iteration 9, loss 6.06666998192668, regul_loss 27.356626510620117, reconstruct_loss 3.33100731857121, time 16.914
8 self_attn.q_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
22 H_error tensor(81.4693, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.4500, device='cuda:6', grad_fn=<DivBackward0>)
40159 MiB free out of 48676 MiB total
8 self_attn.k_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
21 H_error tensor(101.0417, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.4529, device='cuda:6', grad_fn=<DivBackward0>)
40095 MiB free out of 48676 MiB total
8 self_attn.v_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
20 H_error tensor(-75.8120, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.4368, device='cuda:6', grad_fn=<DivBackward0>)
40095 MiB free out of 48676 MiB total
8 self_attn.o_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
21 H_error tensor(4.5524, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3274, device='cuda:6', grad_fn=<DivBackward0>)
40095 MiB free out of 48676 MiB total
8 mlp.gate_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
reducing lr to  0.00020589113209464906
54 H_error tensor(88.8335, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3253, device='cuda:6', grad_fn=<DivBackward0>)
40353 MiB free out of 48676 MiB total
8 mlp.up_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
reducing lr to  0.00020589113209464906
reducing lr to  0.00018530201888518417
reducing lr to  0.00016677181699666576
reducing lr to  0.0001500946352969992
reducing lr to  0.0001350851717672993
reducing lr to  0.00012157665459056936
reducing lr to  0.00010941898913151243
reducing lr to  9.847709021836118e-05
53 H_error tensor(68.1986, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3147, device='cuda:6', grad_fn=<DivBackward0>)
40307 MiB free out of 48676 MiB total
8 mlp.down_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
59 H_error tensor(2.0318, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3225, device='cuda:6', grad_fn=<DivBackward0>)
40479 MiB free out of 48676 MiB total
inputs.shape torch.Size([128, 2048, 4096])
saving because model is best
Iteration 0, loss 3.6075745448470116, regul_loss 0.0, reconstruct_loss 3.6075745448470116, time 16.623
Iteration 1, loss 49.85290735960007, regul_loss 464.66351318359375, reconstruct_loss 3.3865568302571774, time 16.81
Iteration 2, loss 15.325195617973804, regul_loss 119.60945892333984, reconstruct_loss 3.364249527454376, time 16.85
Iteration 3, loss 11.34334360063076, regul_loss 79.79781341552734, reconstruct_loss 3.3635619897395372, time 16.871
Iteration 4, loss 7.127096362411976, regul_loss 37.63603210449219, reconstruct_loss 3.3634931426495314, time 16.878
Iteration 5, loss 6.692839823663235, regul_loss 33.29318618774414, reconstruct_loss 3.363521046936512, time 16.889
Iteration 6, loss 6.250386506319046, regul_loss 28.868160247802734, reconstruct_loss 3.363570487126708, time 16.891
Iteration 7, loss 6.2055814899504185, regul_loss 28.420042037963867, reconstruct_loss 3.363577175885439, time 16.902
Iteration 8, loss 6.160357549786568, regul_loss 27.967727661132812, reconstruct_loss 3.363584853708744, time 16.902
Iteration 9, loss 6.155803833156824, regul_loss 27.92218017578125, reconstruct_loss 3.363585889339447, time 16.9
9 self_attn.q_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
22 H_error tensor(162.7028, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3559, device='cuda:6', grad_fn=<DivBackward0>)
40159 MiB free out of 48676 MiB total
9 self_attn.k_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
21 H_error tensor(161.6620, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3568, device='cuda:6', grad_fn=<DivBackward0>)
40095 MiB free out of 48676 MiB total
9 self_attn.v_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
reducing lr to  0.00020589113209464906
reducing lr to  0.00018530201888518417
reducing lr to  0.00016677181699666576
reducing lr to  0.0001500946352969992
reducing lr to  0.0001350851717672993
reducing lr to  0.00012157665459056936
reducing lr to  0.00010941898913151243
reducing lr to  9.847709021836118e-05
reducing lr to  8.862938119652506e-05
reducing lr to  7.976644307687256e-05
reducing lr to  7.17897987691853e-05
reducing lr to  6.461081889226677e-05
reducing lr to  5.81497370030401e-05
reducing lr to  5.233476330273609e-05
reducing lr to  4.7101286972462485e-05
reducing lr to  4.239115827521624e-05
reducing lr to  3.8152042447694614e-05
reducing lr to  3.433683820292515e-05
reducing lr to  3.090315438263264e-05
reducing lr to  2.7812838944369376e-05
reducing lr to  2.503155504993244e-05
reducing lr to  2.2528399544939195e-05
reducing lr to  2.0275559590445276e-05
20 H_error tensor(47.3250, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3225, device='cuda:6', grad_fn=<DivBackward0>)
40095 MiB free out of 48676 MiB total
9 self_attn.o_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
20 H_error tensor(8.6205, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3394, device='cuda:6', grad_fn=<DivBackward0>)
40095 MiB free out of 48676 MiB total
9 mlp.gate_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
53 H_error tensor(91.5787, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3261, device='cuda:6', grad_fn=<DivBackward0>)
40353 MiB free out of 48676 MiB total
9 mlp.up_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
reducing lr to  0.00020589113209464906
reducing lr to  0.00018530201888518417
reducing lr to  0.00016677181699666576
53 H_error tensor(73.2013, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3159, device='cuda:6', grad_fn=<DivBackward0>)
40307 MiB free out of 48676 MiB total
9 mlp.down_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
59 H_error tensor(2.3205, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3268, device='cuda:6', grad_fn=<DivBackward0>)
40479 MiB free out of 48676 MiB total
inputs.shape torch.Size([128, 2048, 4096])
saving because model is best
Iteration 0, loss 3.1219516657292843, regul_loss 0.0, reconstruct_loss 3.1219516657292843, time 16.651
Iteration 1, loss 49.28724217414856, regul_loss 463.04095458984375, reconstruct_loss 2.9831460043787956, time 16.819
Iteration 2, loss 14.892465211451054, regul_loss 118.96803283691406, reconstruct_loss 2.9956614300608635, time 16.856
Iteration 3, loss 10.92575827986002, regul_loss 79.27596282958984, reconstruct_loss 2.9981616865843534, time 16.874
Iteration 4, loss 6.7242886535823345, regul_loss 37.23124694824219, reconstruct_loss 3.0011639036238194, time 16.889
Iteration 5, loss 6.291236888617277, regul_loss 32.89744186401367, reconstruct_loss 3.001492742449045, time 16.884
Iteration 6, loss 5.849848214536905, regul_loss 28.48009490966797, reconstruct_loss 3.0018386263400316, time 16.897
Iteration 7, loss 5.805137898772955, regul_loss 28.032634735107422, reconstruct_loss 3.001874450594187, time 16.926
Iteration 8, loss 5.760006617754698, regul_loss 27.580955505371094, reconstruct_loss 3.001911122351885, time 16.924
Iteration 9, loss 5.755462367087603, regul_loss 27.535472869873047, reconstruct_loss 3.001914966851473, time 16.921
10 self_attn.q_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
22 H_error tensor(161.3753, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3500, device='cuda:6', grad_fn=<DivBackward0>)
40159 MiB free out of 48676 MiB total
10 self_attn.k_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
21 H_error tensor(172.8987, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3526, device='cuda:6', grad_fn=<DivBackward0>)
40095 MiB free out of 48676 MiB total
10 self_attn.v_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
reducing lr to  0.00020589113209464906
reducing lr to  0.00018530201888518417
reducing lr to  0.00016677181699666576
reducing lr to  0.0001500946352969992
reducing lr to  0.0001350851717672993
reducing lr to  0.00012157665459056936
reducing lr to  0.00010941898913151243
reducing lr to  9.847709021836118e-05
reducing lr to  8.862938119652506e-05
reducing lr to  7.976644307687256e-05
reducing lr to  7.17897987691853e-05
reducing lr to  6.461081889226677e-05
reducing lr to  5.81497370030401e-05
reducing lr to  5.233476330273609e-05
reducing lr to  4.7101286972462485e-05
20 H_error tensor(53.7687, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3214, device='cuda:6', grad_fn=<DivBackward0>)
40095 MiB free out of 48676 MiB total
10 self_attn.o_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
20 H_error tensor(12.6337, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3367, device='cuda:6', grad_fn=<DivBackward0>)
40095 MiB free out of 48676 MiB total
10 mlp.gate_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
53 H_error tensor(96.3182, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3250, device='cuda:6', grad_fn=<DivBackward0>)
40353 MiB free out of 48676 MiB total
10 mlp.up_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
reducing lr to  0.00020589113209464906
reducing lr to  0.00018530201888518417
reducing lr to  0.00016677181699666576
reducing lr to  0.0001500946352969992
reducing lr to  0.0001350851717672993
53 H_error tensor(80.9762, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3158, device='cuda:6', grad_fn=<DivBackward0>)
40307 MiB free out of 48676 MiB total
10 mlp.down_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
59 H_error tensor(2.4693, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3245, device='cuda:6', grad_fn=<DivBackward0>)
40479 MiB free out of 48676 MiB total
inputs.shape torch.Size([128, 2048, 4096])
saving because model is best
Iteration 0, loss 4.959027849137783, regul_loss 0.0, reconstruct_loss 4.959027849137783, time 16.64
Iteration 1, loss 51.288859993219376, regul_loss 466.93780517578125, reconstruct_loss 4.595077209174633, time 16.801
Iteration 2, loss 16.84000340104103, regul_loss 121.56842041015625, reconstruct_loss 4.683161187916994, time 16.859
Iteration 3, loss 12.863146878778934, regul_loss 81.66877746582031, reconstruct_loss 4.696269147098064, time 16.865
Iteration 4, loss 8.654893830418587, regul_loss 39.43756103515625, reconstruct_loss 4.711137730628252, time 16.901
Iteration 5, loss 8.222160212695599, regul_loss 35.09436798095703, reconstruct_loss 4.7127234898507595, time 16.903
Iteration 6, loss 7.781472280621529, regul_loss 30.671051025390625, reconstruct_loss 4.7143671326339245, time 16.902
Iteration 7, loss 7.736859954893589, regul_loss 30.223243713378906, reconstruct_loss 4.714535541832447, time 16.897
Iteration 8, loss 7.691832348704338, regul_loss 29.771255493164062, reconstruct_loss 4.714706715196371, time 16.913
Iteration 9, loss 7.687298409640789, regul_loss 29.72574234008789, reconstruct_loss 4.714724183082581, time 16.896
11 self_attn.q_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
21 H_error tensor(140.5553, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3442, device='cuda:6', grad_fn=<DivBackward0>)
40159 MiB free out of 48676 MiB total
11 self_attn.k_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
21 H_error tensor(152.3922, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3498, device='cuda:6', grad_fn=<DivBackward0>)
40095 MiB free out of 48676 MiB total
11 self_attn.v_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
reducing lr to  0.00020589113209464906
reducing lr to  0.00018530201888518417
reducing lr to  0.00016677181699666576
reducing lr to  0.0001500946352969992
reducing lr to  0.0001350851717672993
reducing lr to  0.00012157665459056936
reducing lr to  0.00010941898913151243
reducing lr to  9.847709021836118e-05
reducing lr to  8.862938119652506e-05
reducing lr to  7.976644307687256e-05
reducing lr to  7.17897987691853e-05
reducing lr to  6.461081889226677e-05
reducing lr to  5.81497370030401e-05
reducing lr to  5.233476330273609e-05
reducing lr to  4.7101286972462485e-05
reducing lr to  4.239115827521624e-05
reducing lr to  3.8152042447694614e-05
20 H_error tensor(50.0514, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3208, device='cuda:6', grad_fn=<DivBackward0>)
40095 MiB free out of 48676 MiB total
11 self_attn.o_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
20 H_error tensor(9.7365, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3225, device='cuda:6', grad_fn=<DivBackward0>)
40095 MiB free out of 48676 MiB total
11 mlp.gate_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
53 H_error tensor(108.8297, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3292, device='cuda:6', grad_fn=<DivBackward0>)
40353 MiB free out of 48676 MiB total
11 mlp.up_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
52 H_error tensor(89.3869, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3166, device='cuda:6', grad_fn=<DivBackward0>)
40307 MiB free out of 48676 MiB total
11 mlp.down_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
58 H_error tensor(3.9825, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3257, device='cuda:6', grad_fn=<DivBackward0>)
40479 MiB free out of 48676 MiB total
inputs.shape torch.Size([128, 2048, 4096])
saving because model is best
Iteration 0, loss 3.7156496923416853, regul_loss 0.0, reconstruct_loss 3.7156496923416853, time 16.642
Iteration 1, loss 50.28601250052452, regul_loss 467.44244384765625, reconstruct_loss 3.5417688693851233, time 16.822
Iteration 2, loss 15.57236248999834, regul_loss 120.2313232421875, reconstruct_loss 3.5492301918566227, time 16.844
Iteration 3, loss 11.568319261074066, regul_loss 80.16738891601562, reconstruct_loss 3.551580363884568, time 16.855
Iteration 4, loss 7.327117722481489, regul_loss 37.725582122802734, reconstruct_loss 3.5545595102012157, time 16.864
Iteration 5, loss 6.889929220080376, regul_loss 33.35036087036133, reconstruct_loss 3.554893149062991, time 16.889
Iteration 6, loss 6.444285497069359, regul_loss 28.890377044677734, reconstruct_loss 3.555247638374567, time 16.894
Iteration 7, loss 6.399140976369381, regul_loss 28.438562393188477, reconstruct_loss 3.555284643545747, time 16.888
Iteration 8, loss 6.353569973260164, regul_loss 27.982473373413086, reconstruct_loss 3.555322628468275, time 16.901
Iteration 9, loss 6.348980985581875, regul_loss 27.93654441833496, reconstruct_loss 3.555326560512185, time 16.897
12 self_attn.q_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
21 H_error tensor(183.4674, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3505, device='cuda:6', grad_fn=<DivBackward0>)
40159 MiB free out of 48676 MiB total
12 self_attn.k_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
21 H_error tensor(194.3761, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3535, device='cuda:6', grad_fn=<DivBackward0>)
40095 MiB free out of 48676 MiB total
12 self_attn.v_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
reducing lr to  0.00020589113209464906
reducing lr to  0.00018530201888518417
reducing lr to  0.00016677181699666576
reducing lr to  0.0001500946352969992
reducing lr to  0.0001350851717672993
reducing lr to  0.00012157665459056936
reducing lr to  0.00010941898913151243
reducing lr to  9.847709021836118e-05
reducing lr to  8.862938119652506e-05
reducing lr to  7.976644307687256e-05
reducing lr to  7.17897987691853e-05
reducing lr to  6.461081889226677e-05
reducing lr to  5.81497370030401e-05
reducing lr to  5.233476330273609e-05
reducing lr to  4.7101286972462485e-05
reducing lr to  4.239115827521624e-05
reducing lr to  3.8152042447694614e-05
reducing lr to  3.433683820292515e-05
reducing lr to  3.090315438263264e-05
reducing lr to  2.7812838944369376e-05
20 H_error tensor(63.3909, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3193, device='cuda:6', grad_fn=<DivBackward0>)
40031 MiB free out of 48676 MiB total
12 self_attn.o_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
20 H_error tensor(15.6995, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3274, device='cuda:6', grad_fn=<DivBackward0>)
40031 MiB free out of 48676 MiB total
12 mlp.gate_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
53 H_error tensor(121.8267, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3284, device='cuda:6', grad_fn=<DivBackward0>)
40157 MiB free out of 48676 MiB total
12 mlp.up_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
reducing lr to  0.00020589113209464906
reducing lr to  0.00018530201888518417
reducing lr to  0.00016677181699666576
reducing lr to  0.0001500946352969992
52 H_error tensor(101.7105, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3156, device='cuda:6', grad_fn=<DivBackward0>)
40111 MiB free out of 48676 MiB total
12 mlp.down_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
59 H_error tensor(4.3884, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3236, device='cuda:6', grad_fn=<DivBackward0>)
40283 MiB free out of 48676 MiB total
inputs.shape torch.Size([128, 2048, 4096])
saving because model is best
Iteration 0, loss 6.018494214862585, regul_loss 0.0, reconstruct_loss 6.018494214862585, time 16.635
Iteration 1, loss 52.204318165779114, regul_loss 464.72601318359375, reconstruct_loss 5.7317154109478, time 16.807
Iteration 2, loss 17.748501524329185, regul_loss 120.05621337890625, reconstruct_loss 5.742879521101713, time 16.853
Iteration 3, loss 13.773064643144608, regul_loss 80.26344299316406, reconstruct_loss 5.74672033265233, time 16.856
Iteration 4, loss 9.563215412199497, regul_loss 38.1158332824707, reconstruct_loss 5.751632027328014, time 16.878
Iteration 5, loss 9.129661545157433, regul_loss 33.77477264404297, reconstruct_loss 5.752184014767408, time 16.886
Iteration 6, loss 8.687937535345554, regul_loss 29.351654052734375, reconstruct_loss 5.7527721635997295, time 16.894
Iteration 7, loss 8.643207609653473, regul_loss 28.90373992919922, reconstruct_loss 5.752833552658558, time 16.901
Iteration 8, loss 8.598058983683586, regul_loss 28.451622009277344, reconstruct_loss 5.752896726131439, time 16.893
Iteration 9, loss 8.593512885272503, regul_loss 28.406095504760742, reconstruct_loss 5.752903368324041, time 16.89
13 self_attn.q_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
22 H_error tensor(184.8743, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3481, device='cuda:6', grad_fn=<DivBackward0>)
40159 MiB free out of 48676 MiB total
13 self_attn.k_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
21 H_error tensor(204.3198, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3533, device='cuda:6', grad_fn=<DivBackward0>)
40095 MiB free out of 48676 MiB total
13 self_attn.v_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
reducing lr to  0.00020589113209464906
reducing lr to  0.00018530201888518417
reducing lr to  0.00016677181699666576
reducing lr to  0.0001500946352969992
reducing lr to  0.0001350851717672993
reducing lr to  0.00012157665459056936
reducing lr to  0.00010941898913151243
reducing lr to  9.847709021836118e-05
reducing lr to  8.862938119652506e-05
21 H_error tensor(76.8665, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3200, device='cuda:6', grad_fn=<DivBackward0>)
40095 MiB free out of 48676 MiB total
13 self_attn.o_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
23 H_error tensor(15.1524, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3259, device='cuda:6', grad_fn=<DivBackward0>)
40095 MiB free out of 48676 MiB total
13 mlp.gate_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
58 H_error tensor(131.5097, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3263, device='cuda:6', grad_fn=<DivBackward0>)
40353 MiB free out of 48676 MiB total
13 mlp.up_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
reducing lr to  0.00020589113209464906
reducing lr to  0.00018530201888518417
reducing lr to  0.00016677181699666576
reducing lr to  0.0001500946352969992
reducing lr to  0.0001350851717672993
reducing lr to  0.00012157665459056936
reducing lr to  0.00010941898913151243
reducing lr to  9.847709021836118e-05
reducing lr to  8.862938119652506e-05
reducing lr to  7.976644307687256e-05
59 H_error tensor(112.7624, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3153, device='cuda:6', grad_fn=<DivBackward0>)
40307 MiB free out of 48676 MiB total
13 mlp.down_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
64 H_error tensor(4.6282, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3227, device='cuda:6', grad_fn=<DivBackward0>)
40479 MiB free out of 48676 MiB total
inputs.shape torch.Size([128, 2048, 4096])
saving because model is best
Iteration 0, loss 6.78668612241745, regul_loss 0.0, reconstruct_loss 6.78668612241745, time 16.616
Iteration 1, loss 53.218222230672836, regul_loss 467.3048095703125, reconstruct_loss 6.487741928547621, time 16.815
Iteration 2, loss 18.67625293135643, regul_loss 121.13153076171875, reconstruct_loss 6.5630995482206345, time 16.854
Iteration 3, loss 14.688851565122604, regul_loss 81.14866638183594, reconstruct_loss 6.573984395712614, time 16.864
Iteration 4, loss 10.466653227806091, regul_loss 38.804115295410156, reconstruct_loss 6.586241580545902, time 16.883
Iteration 5, loss 10.031958170235157, regul_loss 34.44412612915039, reconstruct_loss 6.58754550293088, time 16.909
Iteration 6, loss 9.589095175266266, regul_loss 30.001995086669922, reconstruct_loss 6.588895697146654, time 16.899
Iteration 7, loss 9.54425124078989, regul_loss 29.552169799804688, reconstruct_loss 6.589034013450146, time 16.92
Iteration 8, loss 9.498987033963203, regul_loss 29.09812355041504, reconstruct_loss 6.589174546301365, time 16.919
Iteration 9, loss 9.494429722428322, regul_loss 29.052404403686523, reconstruct_loss 6.5891891829669476, time 16.915
14 self_attn.q_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
24 H_error tensor(184.4225, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3480, device='cuda:6', grad_fn=<DivBackward0>)
40159 MiB free out of 48676 MiB total
14 self_attn.k_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
23 H_error tensor(199.2641, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3507, device='cuda:6', grad_fn=<DivBackward0>)
40095 MiB free out of 48676 MiB total
14 self_attn.v_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
reducing lr to  0.00020589113209464906
reducing lr to  0.00018530201888518417
reducing lr to  0.00016677181699666576
reducing lr to  0.0001500946352969992
reducing lr to  0.0001350851717672993
reducing lr to  0.00012157665459056936
reducing lr to  0.00010941898913151243
reducing lr to  9.847709021836118e-05
reducing lr to  8.862938119652506e-05
reducing lr to  7.976644307687256e-05
reducing lr to  7.17897987691853e-05
reducing lr to  6.461081889226677e-05
reducing lr to  5.81497370030401e-05
reducing lr to  5.233476330273609e-05
reducing lr to  4.7101286972462485e-05
23 H_error tensor(75.4218, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3186, device='cuda:6', grad_fn=<DivBackward0>)
40095 MiB free out of 48676 MiB total
14 self_attn.o_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
23 H_error tensor(16.1640, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3221, device='cuda:6', grad_fn=<DivBackward0>)
40095 MiB free out of 48676 MiB total
14 mlp.gate_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
58 H_error tensor(145.4778, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3261, device='cuda:6', grad_fn=<DivBackward0>)
40353 MiB free out of 48676 MiB total
14 mlp.up_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
reducing lr to  0.00020589113209464906
reducing lr to  0.00018530201888518417
reducing lr to  0.00016677181699666576
reducing lr to  0.0001500946352969992
reducing lr to  0.0001350851717672993
reducing lr to  0.00012157665459056936
reducing lr to  0.00010941898913151243
59 H_error tensor(126.0775, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3150, device='cuda:6', grad_fn=<DivBackward0>)
40307 MiB free out of 48676 MiB total
14 mlp.down_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
65 H_error tensor(5.0903, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3240, device='cuda:6', grad_fn=<DivBackward0>)
40479 MiB free out of 48676 MiB total
inputs.shape torch.Size([128, 2048, 4096])
saving because model is best
Iteration 0, loss 6.519245903939009, regul_loss 0.0, reconstruct_loss 6.519245903939009, time 16.625
Iteration 1, loss 52.95429542660713, regul_loss 466.070556640625, reconstruct_loss 6.347239796072245, time 16.82
Iteration 2, loss 18.36145506799221, regul_loss 119.90934753417969, reconstruct_loss 6.370519682765007, time 16.847
Iteration 3, loss 14.370841957628727, regul_loss 79.96354675292969, reconstruct_loss 6.37448738142848, time 16.875
Iteration 4, loss 10.143191158771515, regul_loss 37.64066696166992, reconstruct_loss 6.379124477505684, time 16.889
Iteration 5, loss 9.707291685044765, regul_loss 33.276649475097656, reconstruct_loss 6.379626478999853, time 16.91
Iteration 6, loss 9.26293358951807, regul_loss 28.827816009521484, reconstruct_loss 6.380151756107807, time 16.911
Iteration 7, loss 9.21791736036539, regul_loss 28.37711524963379, reconstruct_loss 6.380205862224102, time 16.917
Iteration 8, loss 9.172477029263973, regul_loss 27.922157287597656, reconstruct_loss 6.380261216312647, time 16.901
Iteration 9, loss 9.167901620268822, regul_loss 27.8763427734375, reconstruct_loss 6.380267210304737, time 16.907
15 self_attn.q_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
24 H_error tensor(190.1577, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3485, device='cuda:6', grad_fn=<DivBackward0>)
40159 MiB free out of 48676 MiB total
15 self_attn.k_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
23 H_error tensor(206.3857, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3537, device='cuda:6', grad_fn=<DivBackward0>)
40095 MiB free out of 48676 MiB total
15 self_attn.v_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
reducing lr to  0.00020589113209464906
reducing lr to  0.00018530201888518417
reducing lr to  0.00016677181699666576
reducing lr to  0.0001500946352969992
reducing lr to  0.0001350851717672993
reducing lr to  0.00012157665459056936
reducing lr to  0.00010941898913151243
reducing lr to  9.847709021836118e-05
reducing lr to  8.862938119652506e-05
reducing lr to  7.976644307687256e-05
reducing lr to  7.17897987691853e-05
reducing lr to  6.461081889226677e-05
reducing lr to  5.81497370030401e-05
reducing lr to  5.233476330273609e-05
reducing lr to  4.7101286972462485e-05
reducing lr to  4.239115827521624e-05
reducing lr to  3.8152042447694614e-05
reducing lr to  3.433683820292515e-05
reducing lr to  3.090315438263264e-05
23 H_error tensor(76.3450, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3180, device='cuda:6', grad_fn=<DivBackward0>)
40095 MiB free out of 48676 MiB total
15 self_attn.o_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
23 H_error tensor(20.6606, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3224, device='cuda:6', grad_fn=<DivBackward0>)
40095 MiB free out of 48676 MiB total
15 mlp.gate_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
59 H_error tensor(157.1368, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3241, device='cuda:6', grad_fn=<DivBackward0>)
40353 MiB free out of 48676 MiB total
15 mlp.up_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
reducing lr to  0.00020589113209464906
reducing lr to  0.00018530201888518417
reducing lr to  0.00016677181699666576
reducing lr to  0.0001500946352969992
reducing lr to  0.0001350851717672993
reducing lr to  0.00012157665459056936
59 H_error tensor(136.8725, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3146, device='cuda:6', grad_fn=<DivBackward0>)
40307 MiB free out of 48676 MiB total
15 mlp.down_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
65 H_error tensor(6.4881, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3225, device='cuda:6', grad_fn=<DivBackward0>)
40479 MiB free out of 48676 MiB total
inputs.shape torch.Size([128, 2048, 4096])
saving because model is best
Iteration 0, loss 11.6897566691041, regul_loss 0.0, reconstruct_loss 11.6897566691041, time 16.623
Iteration 1, loss 58.2582573890686, regul_loss 470.81158447265625, reconstruct_loss 11.177099995315075, time 16.821
Iteration 2, loss 23.521563842892647, regul_loss 123.01654052734375, reconstruct_loss 11.219909995794296, time 16.85
Iteration 3, loss 19.513171017169952, regul_loss 82.83350372314453, reconstruct_loss 11.229820013046265, time 16.884
Iteration 4, loss 15.276011407375336, regul_loss 40.34068298339844, reconstruct_loss 11.241942577064037, time 16.915
Iteration 5, loss 14.841283656656742, regul_loss 35.980064392089844, reconstruct_loss 11.243277356028557, time 16.923
Iteration 6, loss 14.399075798690319, regul_loss 31.543901443481445, reconstruct_loss 11.244685471057892, time 16.911
Iteration 7, loss 14.354350194334984, regul_loss 31.095176696777344, reconstruct_loss 11.244832150638103, time 16.92
Iteration 8, loss 14.309218749403954, regul_loss 30.64236831665039, reconstruct_loss 11.244982086122036, time 16.928
Iteration 9, loss 14.304676838219166, regul_loss 30.59678077697754, reconstruct_loss 11.244998753070831, time 16.924
16 self_attn.q_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
24 H_error tensor(178.0699, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3423, device='cuda:6', grad_fn=<DivBackward0>)
40159 MiB free out of 48676 MiB total
16 self_attn.k_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
23 H_error tensor(201.9117, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3482, device='cuda:6', grad_fn=<DivBackward0>)
40095 MiB free out of 48676 MiB total
16 self_attn.v_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
reducing lr to  0.00020589113209464906
reducing lr to  0.00018530201888518417
reducing lr to  0.00016677181699666576
reducing lr to  0.0001500946352969992
reducing lr to  0.0001350851717672993
reducing lr to  0.00012157665459056936
reducing lr to  0.00010941898913151243
reducing lr to  9.847709021836118e-05
reducing lr to  8.862938119652506e-05
reducing lr to  7.976644307687256e-05
reducing lr to  7.17897987691853e-05
reducing lr to  6.461081889226677e-05
reducing lr to  5.81497370030401e-05
reducing lr to  5.233476330273609e-05
reducing lr to  4.7101286972462485e-05
23 H_error tensor(85.0683, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3169, device='cuda:6', grad_fn=<DivBackward0>)
40095 MiB free out of 48676 MiB total
16 self_attn.o_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
23 H_error tensor(16.7947, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3203, device='cuda:6', grad_fn=<DivBackward0>)
40095 MiB free out of 48676 MiB total
16 mlp.gate_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
59 H_error tensor(186.2636, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3260, device='cuda:6', grad_fn=<DivBackward0>)
40353 MiB free out of 48676 MiB total
16 mlp.up_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
reducing lr to  0.00020589113209464906
reducing lr to  0.00018530201888518417
reducing lr to  0.00016677181699666576
reducing lr to  0.0001500946352969992
reducing lr to  0.0001350851717672993
reducing lr to  0.00012157665459056936
reducing lr to  0.00010941898913151243
reducing lr to  9.847709021836118e-05
reducing lr to  8.862938119652506e-05
reducing lr to  7.976644307687256e-05
reducing lr to  7.17897987691853e-05
59 H_error tensor(157.1587, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3148, device='cuda:6', grad_fn=<DivBackward0>)
40307 MiB free out of 48676 MiB total
16 mlp.down_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
65 H_error tensor(9.1538, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3232, device='cuda:6', grad_fn=<DivBackward0>)
40479 MiB free out of 48676 MiB total
inputs.shape torch.Size([128, 2048, 4096])
saving because model is best
Iteration 0, loss 12.114340744912624, regul_loss 0.0, reconstruct_loss 12.114340744912624, time 16.627
Iteration 1, loss 58.41971105337143, regul_loss 468.3061828613281, reconstruct_loss 11.589091382920742, time 16.809
Iteration 2, loss 23.95164279639721, regul_loss 122.74346160888672, reconstruct_loss 11.677296444773674, time 16.87
Iteration 3, loss 19.973934456706047, regul_loss 82.81917572021484, reconstruct_loss 11.692016936838627, time 16.887
Iteration 4, loss 15.771929904818535, regul_loss 40.6279182434082, reconstruct_loss 11.709138087928295, time 16.9
Iteration 5, loss 15.341339647769928, regul_loss 36.303585052490234, reconstruct_loss 11.710981138050556, time 16.903
Iteration 6, loss 14.903535179793835, regul_loss 31.90629768371582, reconstruct_loss 11.712905257940292, time 16.91
Iteration 7, loss 14.859268628060818, regul_loss 31.461645126342773, reconstruct_loss 11.713103845715523, time 16.913
Iteration 8, loss 14.814603693783283, regul_loss 31.012975692749023, reconstruct_loss 11.71330627053976, time 16.924
Iteration 9, loss 14.81010865420103, regul_loss 30.967803955078125, reconstruct_loss 11.713327884674072, time 16.918
17 self_attn.q_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
24 H_error tensor(196.8758, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3442, device='cuda:6', grad_fn=<DivBackward0>)
40159 MiB free out of 48676 MiB total
17 self_attn.k_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
24 H_error tensor(217.7417, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3502, device='cuda:6', grad_fn=<DivBackward0>)
40095 MiB free out of 48676 MiB total
17 self_attn.v_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
reducing lr to  0.00020589113209464906
reducing lr to  0.00018530201888518417
reducing lr to  0.00016677181699666576
reducing lr to  0.0001500946352969992
reducing lr to  0.0001350851717672993
reducing lr to  0.00012157665459056936
reducing lr to  0.00010941898913151243
reducing lr to  9.847709021836118e-05
reducing lr to  8.862938119652506e-05
reducing lr to  7.976644307687256e-05
reducing lr to  7.17897987691853e-05
reducing lr to  6.461081889226677e-05
reducing lr to  5.81497370030401e-05
reducing lr to  5.233476330273609e-05
reducing lr to  4.7101286972462485e-05
reducing lr to  4.239115827521624e-05
reducing lr to  3.8152042447694614e-05
reducing lr to  3.433683820292515e-05
reducing lr to  3.090315438263264e-05
reducing lr to  2.7812838944369376e-05
reducing lr to  2.503155504993244e-05
23 H_error tensor(97.7649, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3171, device='cuda:6', grad_fn=<DivBackward0>)
40095 MiB free out of 48676 MiB total
17 self_attn.o_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
23 H_error tensor(21.1997, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3198, device='cuda:6', grad_fn=<DivBackward0>)
40095 MiB free out of 48676 MiB total
17 mlp.gate_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
59 H_error tensor(203.0182, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3233, device='cuda:6', grad_fn=<DivBackward0>)
40353 MiB free out of 48676 MiB total
17 mlp.up_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
reducing lr to  0.00020589113209464906
reducing lr to  0.00018530201888518417
reducing lr to  0.00016677181699666576
reducing lr to  0.0001500946352969992
reducing lr to  0.0001350851717672993
reducing lr to  0.00012157665459056936
59 H_error tensor(176.4451, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3144, device='cuda:6', grad_fn=<DivBackward0>)
40307 MiB free out of 48676 MiB total
17 mlp.down_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
65 H_error tensor(11.4095, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3221, device='cuda:6', grad_fn=<DivBackward0>)
40479 MiB free out of 48676 MiB total
inputs.shape torch.Size([128, 2048, 4096])
saving because model is best
Iteration 0, loss 14.025384202599525, regul_loss 0.0, reconstruct_loss 14.025384202599525, time 16.645
Iteration 1, loss 60.30051639676094, regul_loss 468.87701416015625, reconstruct_loss 13.412813417613506, time 16.834
Iteration 2, loss 25.904233038425446, regul_loss 123.55384063720703, reconstruct_loss 13.548849150538445, time 16.858
Iteration 3, loss 21.93214949965477, regul_loss 83.62361907958984, reconstruct_loss 13.569787666201591, time 16.891
Iteration 4, loss 17.736481562256813, regul_loss 41.427642822265625, reconstruct_loss 13.593716755509377, time 16.903
Iteration 5, loss 17.306797571480274, regul_loss 37.10521697998047, reconstruct_loss 13.596275925636292, time 16.936
Iteration 6, loss 16.87004227936268, regul_loss 32.71107482910156, reconstruct_loss 13.598935201764107, time 16.928
Iteration 7, loss 16.825891613960266, regul_loss 32.26683044433594, reconstruct_loss 13.599208675324917, time 16.919
Iteration 8, loss 16.78134685009718, regul_loss 31.818601608276367, reconstruct_loss 13.599486708641052, time 16.935
Iteration 9, loss 16.7768634557724, regul_loss 31.773473739624023, reconstruct_loss 13.599516093730927, time 16.923
18 self_attn.q_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
24 H_error tensor(206.4645, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3490, device='cuda:6', grad_fn=<DivBackward0>)
40159 MiB free out of 48676 MiB total
18 self_attn.k_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
24 H_error tensor(226.7879, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3538, device='cuda:6', grad_fn=<DivBackward0>)
40095 MiB free out of 48676 MiB total
18 self_attn.v_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
reducing lr to  0.00020589113209464906
reducing lr to  0.00018530201888518417
reducing lr to  0.00016677181699666576
reducing lr to  0.0001500946352969992
reducing lr to  0.0001350851717672993
reducing lr to  0.00012157665459056936
reducing lr to  0.00010941898913151243
reducing lr to  9.847709021836118e-05
reducing lr to  8.862938119652506e-05
reducing lr to  7.976644307687256e-05
reducing lr to  7.17897987691853e-05
reducing lr to  6.461081889226677e-05
reducing lr to  5.81497370030401e-05
reducing lr to  5.233476330273609e-05
reducing lr to  4.7101286972462485e-05
reducing lr to  4.239115827521624e-05
reducing lr to  3.8152042447694614e-05
reducing lr to  3.433683820292515e-05
reducing lr to  3.090315438263264e-05
23 H_error tensor(102.7405, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3162, device='cuda:6', grad_fn=<DivBackward0>)
40095 MiB free out of 48676 MiB total
18 self_attn.o_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
23 H_error tensor(26.7544, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3206, device='cuda:6', grad_fn=<DivBackward0>)
40095 MiB free out of 48676 MiB total
18 mlp.gate_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
59 H_error tensor(230.3335, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3224, device='cuda:6', grad_fn=<DivBackward0>)
40353 MiB free out of 48676 MiB total
18 mlp.up_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
reducing lr to  0.00020589113209464906
reducing lr to  0.00018530201888518417
reducing lr to  0.00016677181699666576
reducing lr to  0.0001500946352969992
reducing lr to  0.0001350851717672993
reducing lr to  0.00012157665459056936
reducing lr to  0.00010941898913151243
reducing lr to  9.847709021836118e-05
reducing lr to  8.862938119652506e-05
reducing lr to  7.976644307687256e-05
59 H_error tensor(195.8641, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3137, device='cuda:6', grad_fn=<DivBackward0>)
40307 MiB free out of 48676 MiB total
18 mlp.down_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
65 H_error tensor(12.6846, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3207, device='cuda:6', grad_fn=<DivBackward0>)
40479 MiB free out of 48676 MiB total
inputs.shape torch.Size([128, 2048, 4096])
saving because model is best
Iteration 0, loss 17.98683036863804, regul_loss 0.0, reconstruct_loss 17.98683036863804, time 16.637
Iteration 1, loss 63.97156283259392, regul_loss 465.40234375, reconstruct_loss 17.43132944405079, time 16.818
Iteration 2, loss 29.645666033029556, regul_loss 121.96449279785156, reconstruct_loss 17.44921676814556, time 16.854
Iteration 3, loss 25.690665021538734, regul_loss 82.33433532714844, reconstruct_loss 17.45723156630993, time 16.889
Iteration 4, loss 21.519760310649872, regul_loss 40.52098846435547, reconstruct_loss 17.467661134898663, time 16.898
Iteration 5, loss 21.093297973275185, regul_loss 36.244659423828125, reconstruct_loss 17.46883202344179, time 16.898
Iteration 6, loss 20.660027652978897, regul_loss 31.899446487426758, reconstruct_loss 17.470082461833954, time 16.9
Iteration 7, loss 20.61624190211296, regul_loss 31.460277557373047, reconstruct_loss 17.470213420689106, time 16.914
Iteration 8, loss 20.572069212794304, regul_loss 31.017213821411133, reconstruct_loss 17.470348343253136, time 16.907
Iteration 9, loss 20.56762497127056, regul_loss 30.972610473632812, reconstruct_loss 17.470363564789295, time 16.916
19 self_attn.q_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
24 H_error tensor(193.1414, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3466, device='cuda:6', grad_fn=<DivBackward0>)
40159 MiB free out of 48676 MiB total
19 self_attn.k_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
24 H_error tensor(213.0186, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3502, device='cuda:6', grad_fn=<DivBackward0>)
40095 MiB free out of 48676 MiB total
19 self_attn.v_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
reducing lr to  0.00020589113209464906
reducing lr to  0.00018530201888518417
reducing lr to  0.00016677181699666576
reducing lr to  0.0001500946352969992
reducing lr to  0.0001350851717672993
reducing lr to  0.00012157665459056936
reducing lr to  0.00010941898913151243
reducing lr to  9.847709021836118e-05
reducing lr to  8.862938119652506e-05
reducing lr to  7.976644307687256e-05
reducing lr to  7.17897987691853e-05
reducing lr to  6.461081889226677e-05
reducing lr to  5.81497370030401e-05
reducing lr to  5.233476330273609e-05
reducing lr to  4.7101286972462485e-05
reducing lr to  4.239115827521624e-05
reducing lr to  3.8152042447694614e-05
reducing lr to  3.433683820292515e-05
reducing lr to  3.090315438263264e-05
reducing lr to  2.7812838944369376e-05
23 H_error tensor(111.6695, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3175, device='cuda:6', grad_fn=<DivBackward0>)
40095 MiB free out of 48676 MiB total
19 self_attn.o_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
23 H_error tensor(29.8283, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3260, device='cuda:6', grad_fn=<DivBackward0>)
40095 MiB free out of 48676 MiB total
19 mlp.gate_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
59 H_error tensor(249.9054, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3217, device='cuda:6', grad_fn=<DivBackward0>)
40353 MiB free out of 48676 MiB total
19 mlp.up_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
reducing lr to  0.00020589113209464906
reducing lr to  0.00018530201888518417
reducing lr to  0.00016677181699666576
reducing lr to  0.0001500946352969992
reducing lr to  0.0001350851717672993
reducing lr to  0.00012157665459056936
reducing lr to  0.00010941898913151243
reducing lr to  9.847709021836118e-05
reducing lr to  8.862938119652506e-05
reducing lr to  7.976644307687256e-05
59 H_error tensor(209.8265, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3136, device='cuda:6', grad_fn=<DivBackward0>)
40307 MiB free out of 48676 MiB total
19 mlp.down_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
65 H_error tensor(14.4625, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3210, device='cuda:6', grad_fn=<DivBackward0>)
40479 MiB free out of 48676 MiB total
inputs.shape torch.Size([128, 2048, 4096])
saving because model is best
Iteration 0, loss 27.758555114269257, regul_loss 0.0, reconstruct_loss 27.758555114269257, time 16.627
Iteration 1, loss 73.75767743587494, regul_loss 468.8078918457031, reconstruct_loss 26.87688647210598, time 16.817
Iteration 2, loss 39.40466824173927, regul_loss 125.08866119384766, reconstruct_loss 26.895802408456802, time 16.87
Iteration 3, loss 35.454528629779816, regul_loss 85.45767211914062, reconstruct_loss 26.908760368824005, time 16.899
Iteration 4, loss 31.309356659650803, regul_loss 43.832862854003906, reconstruct_loss 26.92607043683529, time 16.893
Iteration 5, loss 30.889143362641335, regul_loss 39.611106872558594, reconstruct_loss 26.928032755851746, time 16.932
Iteration 6, loss 30.463719874620438, regul_loss 35.33587646484375, reconstruct_loss 26.930131569504738, time 16.929
Iteration 7, loss 30.420836195349693, regul_loss 34.90484619140625, reconstruct_loss 26.930351555347443, time 16.919
Iteration 8, loss 30.37760327756405, regul_loss 34.47026062011719, reconstruct_loss 26.93057781457901, time 16.918
Iteration 9, loss 30.373256772756577, regul_loss 34.42652893066406, reconstruct_loss 26.93060326576233, time 16.936
20 self_attn.q_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
24 H_error tensor(219.4113, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3437, device='cuda:6', grad_fn=<DivBackward0>)
40159 MiB free out of 48676 MiB total
20 self_attn.k_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
23 H_error tensor(238.2157, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3470, device='cuda:6', grad_fn=<DivBackward0>)
40095 MiB free out of 48676 MiB total
20 self_attn.v_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
reducing lr to  0.00020589113209464906
reducing lr to  0.00018530201888518417
reducing lr to  0.00016677181699666576
reducing lr to  0.0001500946352969992
reducing lr to  0.0001350851717672993
reducing lr to  0.00012157665459056936
reducing lr to  0.00010941898913151243
reducing lr to  9.847709021836118e-05
reducing lr to  8.862938119652506e-05
reducing lr to  7.976644307687256e-05
reducing lr to  7.17897987691853e-05
reducing lr to  6.461081889226677e-05
reducing lr to  5.81497370030401e-05
reducing lr to  5.233476330273609e-05
reducing lr to  4.7101286972462485e-05
reducing lr to  4.239115827521624e-05
reducing lr to  3.8152042447694614e-05
reducing lr to  3.433683820292515e-05
23 H_error tensor(127.4470, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3167, device='cuda:6', grad_fn=<DivBackward0>)
40095 MiB free out of 48676 MiB total
20 self_attn.o_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
23 H_error tensor(25.6195, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3204, device='cuda:6', grad_fn=<DivBackward0>)
40095 MiB free out of 48676 MiB total
20 mlp.gate_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
60 H_error tensor(274.1396, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3211, device='cuda:6', grad_fn=<DivBackward0>)
40353 MiB free out of 48676 MiB total
20 mlp.up_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
reducing lr to  0.00020589113209464906
reducing lr to  0.00018530201888518417
reducing lr to  0.00016677181699666576
reducing lr to  0.0001500946352969992
reducing lr to  0.0001350851717672993
reducing lr to  0.00012157665459056936
reducing lr to  0.00010941898913151243
reducing lr to  9.847709021836118e-05
reducing lr to  8.862938119652506e-05
59 H_error tensor(228.5408, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3135, device='cuda:6', grad_fn=<DivBackward0>)
40307 MiB free out of 48676 MiB total
20 mlp.down_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
65 H_error tensor(18.0668, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3212, device='cuda:6', grad_fn=<DivBackward0>)
40479 MiB free out of 48676 MiB total
inputs.shape torch.Size([128, 2048, 4096])
saving because model is best
Iteration 0, loss 30.59006956219673, regul_loss 0.0, reconstruct_loss 30.59006956219673, time 16.624
Iteration 1, loss 76.61241042613983, regul_loss 467.58172607421875, reconstruct_loss 29.85423567891121, time 16.804
Iteration 2, loss 42.11384245753288, regul_loss 123.93022155761719, reconstruct_loss 29.72082009911537, time 16.858
Iteration 3, loss 38.164174139499664, regul_loss 84.47085571289062, reconstruct_loss 29.717088297009468, time 16.881
Iteration 4, loss 34.02752396464348, regul_loss 43.109153747558594, reconstruct_loss 29.716607749462128, time 16.885
Iteration 5, loss 33.60794225335121, regul_loss 38.91227722167969, reconstruct_loss 29.716714560985565, time 16.902
Iteration 6, loss 33.18271279335022, regul_loss 34.65796661376953, reconstruct_loss 29.716915622353554, time 16.916
Iteration 7, loss 33.139808282256126, regul_loss 34.228660583496094, reconstruct_loss 29.716941714286804, time 16.912
Iteration 8, loss 33.09654612839222, regul_loss 33.795745849609375, reconstruct_loss 29.716972023248672, time 16.918
Iteration 9, loss 33.09219542145729, regul_loss 33.75217819213867, reconstruct_loss 29.71697773039341, time 16.923
21 self_attn.q_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
24 H_error tensor(210.4316, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3463, device='cuda:6', grad_fn=<DivBackward0>)
40159 MiB free out of 48676 MiB total
21 self_attn.k_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
23 H_error tensor(228.1779, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3505, device='cuda:6', grad_fn=<DivBackward0>)
40095 MiB free out of 48676 MiB total
21 self_attn.v_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
reducing lr to  0.00020589113209464906
reducing lr to  0.00018530201888518417
reducing lr to  0.00016677181699666576
reducing lr to  0.0001500946352969992
reducing lr to  0.0001350851717672993
reducing lr to  0.00012157665459056936
reducing lr to  0.00010941898913151243
reducing lr to  9.847709021836118e-05
reducing lr to  8.862938119652506e-05
reducing lr to  7.976644307687256e-05
reducing lr to  7.17897987691853e-05
reducing lr to  6.461081889226677e-05
reducing lr to  5.81497370030401e-05
reducing lr to  5.233476330273609e-05
reducing lr to  4.7101286972462485e-05
reducing lr to  4.239115827521624e-05
reducing lr to  3.8152042447694614e-05
23 H_error tensor(133.2788, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3158, device='cuda:6', grad_fn=<DivBackward0>)
40095 MiB free out of 48676 MiB total
21 self_attn.o_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
23 H_error tensor(42.3723, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3217, device='cuda:6', grad_fn=<DivBackward0>)
40095 MiB free out of 48676 MiB total
21 mlp.gate_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
reducing lr to  0.00020589113209464906
59 H_error tensor(304.1400, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3203, device='cuda:6', grad_fn=<DivBackward0>)
40353 MiB free out of 48676 MiB total
21 mlp.up_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
reducing lr to  0.00020589113209464906
reducing lr to  0.00018530201888518417
reducing lr to  0.00016677181699666576
reducing lr to  0.0001500946352969992
reducing lr to  0.0001350851717672993
reducing lr to  0.00012157665459056936
reducing lr to  0.00010941898913151243
reducing lr to  9.847709021836118e-05
reducing lr to  8.862938119652506e-05
reducing lr to  7.976644307687256e-05
reducing lr to  7.17897987691853e-05
reducing lr to  6.461081889226677e-05
reducing lr to  5.81497370030401e-05
reducing lr to  5.233476330273609e-05
58 H_error tensor(250.0654, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3135, device='cuda:6', grad_fn=<DivBackward0>)
40307 MiB free out of 48676 MiB total
21 mlp.down_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
65 H_error tensor(21.2196, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3200, device='cuda:6', grad_fn=<DivBackward0>)
40479 MiB free out of 48676 MiB total
inputs.shape torch.Size([128, 2048, 4096])
saving because model is best
Iteration 0, loss 32.636336758732796, regul_loss 0.0, reconstruct_loss 32.636336758732796, time 16.616
Iteration 1, loss 78.64013028144836, regul_loss 469.3079528808594, reconstruct_loss 31.709332659840584, time 16.796
Iteration 2, loss 44.27808555960655, regul_loss 126.41548156738281, reconstruct_loss 31.636537358164787, time 16.849
Iteration 3, loss 40.358282417058945, regul_loss 87.14814758300781, reconstruct_loss 31.643467590212822, time 16.886
Iteration 4, loss 36.28308446705341, regul_loss 46.27818298339844, reconstruct_loss 31.655265778303146, time 16.897
Iteration 5, loss 35.87389385700226, regul_loss 42.17207717895508, reconstruct_loss 31.6566853672266, time 16.902
Iteration 6, loss 35.46038344502449, regul_loss 38.02131652832031, reconstruct_loss 31.65825182199478, time 16.909
Iteration 7, loss 35.41873782873154, regul_loss 37.603179931640625, reconstruct_loss 31.658419355750084, time 16.918
Iteration 8, loss 35.376764580607414, regul_loss 37.18172073364258, reconstruct_loss 31.658592835068703, time 16.921
Iteration 9, loss 35.37254582345486, regul_loss 37.139320373535156, reconstruct_loss 31.658613801002502, time 16.927
22 self_attn.q_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
24 H_error tensor(232.6787, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3420, device='cuda:6', grad_fn=<DivBackward0>)
40159 MiB free out of 48676 MiB total
22 self_attn.k_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
24 H_error tensor(249.5445, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3452, device='cuda:6', grad_fn=<DivBackward0>)
40095 MiB free out of 48676 MiB total
22 self_attn.v_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
reducing lr to  0.00020589113209464906
reducing lr to  0.00018530201888518417
reducing lr to  0.00016677181699666576
reducing lr to  0.0001500946352969992
reducing lr to  0.0001350851717672993
reducing lr to  0.00012157665459056936
reducing lr to  0.00010941898913151243
reducing lr to  9.847709021836118e-05
reducing lr to  8.862938119652506e-05
reducing lr to  7.976644307687256e-05
reducing lr to  7.17897987691853e-05
reducing lr to  6.461081889226677e-05
reducing lr to  5.81497370030401e-05
reducing lr to  5.233476330273609e-05
reducing lr to  4.7101286972462485e-05
reducing lr to  4.239115827521624e-05
reducing lr to  3.8152042447694614e-05
reducing lr to  3.433683820292515e-05
23 H_error tensor(142.2718, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3155, device='cuda:6', grad_fn=<DivBackward0>)
40095 MiB free out of 48676 MiB total
22 self_attn.o_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
24 H_error tensor(80.7245, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3494, device='cuda:6', grad_fn=<DivBackward0>)
40095 MiB free out of 48676 MiB total
22 mlp.gate_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
59 H_error tensor(300.6912, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3181, device='cuda:6', grad_fn=<DivBackward0>)
40353 MiB free out of 48676 MiB total
22 mlp.up_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
reducing lr to  0.00020589113209464906
reducing lr to  0.00018530201888518417
reducing lr to  0.00016677181699666576
reducing lr to  0.0001500946352969992
reducing lr to  0.0001350851717672993
reducing lr to  0.00012157665459056936
reducing lr to  0.00010941898913151243
reducing lr to  9.847709021836118e-05
reducing lr to  8.862938119652506e-05
59 H_error tensor(259.0186, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3130, device='cuda:6', grad_fn=<DivBackward0>)
40307 MiB free out of 48676 MiB total
22 mlp.down_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
65 H_error tensor(20.2294, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3204, device='cuda:6', grad_fn=<DivBackward0>)
40479 MiB free out of 48676 MiB total
inputs.shape torch.Size([128, 2048, 4096])
saving because model is best
Iteration 0, loss 28.311262533068657, regul_loss 0.0, reconstruct_loss 28.311262533068657, time 16.64
Iteration 1, loss 74.38937509059906, regul_loss 466.2947082519531, reconstruct_loss 27.759903833270073, time 16.804
Iteration 2, loss 39.96115618944168, regul_loss 121.74142456054688, reconstruct_loss 27.787013441324234, time 16.852
Iteration 3, loss 35.98591536283493, regul_loss 81.90907287597656, reconstruct_loss 27.795008271932602, time 16.899
Iteration 4, loss 31.77862797677517, regul_loss 39.734703063964844, reconstruct_loss 27.805157840251923, time 16.897
Iteration 5, loss 31.346023589372635, regul_loss 35.39728927612305, reconstruct_loss 27.806294932961464, time 16.91
Iteration 6, loss 30.905567079782486, regul_loss 30.980613708496094, reconstruct_loss 27.80750635266304, time 16.906
Iteration 7, loss 30.860988467931747, regul_loss 30.533557891845703, reconstruct_loss 27.80763240158558, time 16.909
Iteration 8, loss 30.815997838974, regul_loss 30.082351684570312, reconstruct_loss 27.807762071490288, time 16.905
Iteration 9, loss 30.811469241976738, regul_loss 30.03692054748535, reconstruct_loss 27.807776793837547, time 16.909
23 self_attn.q_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
24 H_error tensor(223.8580, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3472, device='cuda:6', grad_fn=<DivBackward0>)
40159 MiB free out of 48676 MiB total
23 self_attn.k_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
24 H_error tensor(246.9836, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3502, device='cuda:6', grad_fn=<DivBackward0>)
40095 MiB free out of 48676 MiB total
23 self_attn.v_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
reducing lr to  0.00020589113209464906
reducing lr to  0.00018530201888518417
reducing lr to  0.00016677181699666576
reducing lr to  0.0001500946352969992
reducing lr to  0.0001350851717672993
reducing lr to  0.00012157665459056936
reducing lr to  0.00010941898913151243
reducing lr to  9.847709021836118e-05
reducing lr to  8.862938119652506e-05
reducing lr to  7.976644307687256e-05
reducing lr to  7.17897987691853e-05
reducing lr to  6.461081889226677e-05
reducing lr to  5.81497370030401e-05
reducing lr to  5.233476330273609e-05
reducing lr to  4.7101286972462485e-05
reducing lr to  4.239115827521624e-05
reducing lr to  3.8152042447694614e-05
reducing lr to  3.433683820292515e-05
reducing lr to  3.090315438263264e-05
23 H_error tensor(157.6425, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3159, device='cuda:6', grad_fn=<DivBackward0>)
40095 MiB free out of 48676 MiB total
23 self_attn.o_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
23 H_error tensor(92.5141, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3353, device='cuda:6', grad_fn=<DivBackward0>)
40095 MiB free out of 48676 MiB total
23 mlp.gate_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
59 H_error tensor(325.9438, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3176, device='cuda:6', grad_fn=<DivBackward0>)
40353 MiB free out of 48676 MiB total
23 mlp.up_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
reducing lr to  0.00020589113209464906
reducing lr to  0.00018530201888518417
reducing lr to  0.00016677181699666576
reducing lr to  0.0001500946352969992
reducing lr to  0.0001350851717672993
reducing lr to  0.00012157665459056936
reducing lr to  0.00010941898913151243
reducing lr to  9.847709021836118e-05
reducing lr to  8.862938119652506e-05
59 H_error tensor(282.8912, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3126, device='cuda:6', grad_fn=<DivBackward0>)
40307 MiB free out of 48676 MiB total
23 mlp.down_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
65 H_error tensor(23.4136, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3192, device='cuda:6', grad_fn=<DivBackward0>)
40479 MiB free out of 48676 MiB total
inputs.shape torch.Size([128, 2048, 4096])
saving because model is best
Iteration 0, loss 31.722981333732605, regul_loss 0.0, reconstruct_loss 31.722981333732605, time 16.642
Iteration 1, loss 77.83948349952698, regul_loss 471.1960144042969, reconstruct_loss 30.71988169848919, time 16.817
Iteration 2, loss 43.628969728946686, regul_loss 127.56100463867188, reconstruct_loss 30.8728696256876, time 16.866
Iteration 3, loss 39.69263607263565, regul_loss 87.89828491210938, reconstruct_loss 30.902807116508484, time 16.902
Iteration 4, loss 35.5642706155777, regul_loss 46.25871276855469, reconstruct_loss 30.93840040266514, time 16.903
Iteration 5, loss 35.146454617381096, regul_loss 42.04203796386719, reconstruct_loss 30.942250475287437, time 16.899
Iteration 6, loss 34.723725616931915, regul_loss 37.77446746826172, reconstruct_loss 30.946278899908066, time 16.905
Iteration 7, loss 34.681133419275284, regul_loss 37.344398498535156, reconstruct_loss 30.946693673729897, time 16.9
Iteration 8, loss 34.63819958269596, regul_loss 36.91081237792969, reconstruct_loss 30.947117894887924, time 16.905
Iteration 9, loss 34.633881002664566, regul_loss 36.8671875, reconstruct_loss 30.94716267287731, time 16.905
24 self_attn.q_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
24 H_error tensor(253.8277, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3446, device='cuda:6', grad_fn=<DivBackward0>)
40159 MiB free out of 48676 MiB total
24 self_attn.k_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
24 H_error tensor(280.4777, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3486, device='cuda:6', grad_fn=<DivBackward0>)
40095 MiB free out of 48676 MiB total
24 self_attn.v_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
reducing lr to  0.00020589113209464906
reducing lr to  0.00018530201888518417
reducing lr to  0.00016677181699666576
reducing lr to  0.0001500946352969992
reducing lr to  0.0001350851717672993
reducing lr to  0.00012157665459056936
reducing lr to  0.00010941898913151243
reducing lr to  9.847709021836118e-05
reducing lr to  8.862938119652506e-05
reducing lr to  7.976644307687256e-05
reducing lr to  7.17897987691853e-05
reducing lr to  6.461081889226677e-05
reducing lr to  5.81497370030401e-05
reducing lr to  5.233476330273609e-05
reducing lr to  4.7101286972462485e-05
reducing lr to  4.239115827521624e-05
reducing lr to  3.8152042447694614e-05
reducing lr to  3.433683820292515e-05
reducing lr to  3.090315438263264e-05
reducing lr to  2.7812838944369376e-05
reducing lr to  2.503155504993244e-05
reducing lr to  2.2528399544939195e-05
reducing lr to  2.0275559590445276e-05
23 H_error tensor(179.1259, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3157, device='cuda:6', grad_fn=<DivBackward0>)
40095 MiB free out of 48676 MiB total
24 self_attn.o_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
23 H_error tensor(115.7193, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3424, device='cuda:6', grad_fn=<DivBackward0>)
40095 MiB free out of 48676 MiB total
24 mlp.gate_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
59 H_error tensor(342.1393, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3177, device='cuda:6', grad_fn=<DivBackward0>)
40353 MiB free out of 48676 MiB total
24 mlp.up_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
reducing lr to  0.00020589113209464906
reducing lr to  0.00018530201888518417
reducing lr to  0.00016677181699666576
reducing lr to  0.0001500946352969992
reducing lr to  0.0001350851717672993
reducing lr to  0.00012157665459056936
reducing lr to  0.00010941898913151243
reducing lr to  9.847709021836118e-05
reducing lr to  8.862938119652506e-05
reducing lr to  7.976644307687256e-05
reducing lr to  7.17897987691853e-05
reducing lr to  6.461081889226677e-05
reducing lr to  5.81497370030401e-05
58 H_error tensor(297.1410, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3125, device='cuda:6', grad_fn=<DivBackward0>)
40307 MiB free out of 48676 MiB total
24 mlp.down_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
61 H_error tensor(24.7678, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3191, device='cuda:6', grad_fn=<DivBackward0>)
40479 MiB free out of 48676 MiB total
inputs.shape torch.Size([128, 2048, 4096])
saving because model is best
Iteration 0, loss 35.88868860900402, regul_loss 0.0, reconstruct_loss 35.88868860900402, time 16.653
Iteration 1, loss 81.83120596408844, regul_loss 464.95477294921875, reconstruct_loss 35.3357263058424, time 16.837
Iteration 2, loss 47.437080562114716, regul_loss 121.29418182373047, reconstruct_loss 35.307663068175316, time 16.882
Iteration 3, loss 43.470589995384216, regul_loss 81.60406494140625, reconstruct_loss 35.31018202006817, time 16.901
Iteration 4, loss 39.28050437569618, regul_loss 39.657169342041016, reconstruct_loss 35.31478716433048, time 16.922
Iteration 5, loss 38.85120941698551, regul_loss 35.358482360839844, reconstruct_loss 35.315362602472305, time 16.931
Iteration 6, loss 38.414874747395515, regul_loss 30.98866081237793, reconstruct_loss 35.316009148955345, time 16.934
Iteration 7, loss 38.3707689344883, regul_loss 30.546903610229492, reconstruct_loss 35.316077679395676, time 16.931
Iteration 8, loss 38.3262702524662, regul_loss 30.101205825805664, reconstruct_loss 35.316149801015854, time 16.931
Iteration 9, loss 38.321793019771576, regul_loss 30.056339263916016, reconstruct_loss 35.31615875661373, time 16.96
25 self_attn.q_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
22 H_error tensor(280.6597, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3391, device='cuda:6', grad_fn=<DivBackward0>)
40159 MiB free out of 48676 MiB total
25 self_attn.k_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
22 H_error tensor(305.8396, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3420, device='cuda:6', grad_fn=<DivBackward0>)
40095 MiB free out of 48676 MiB total
25 self_attn.v_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
reducing lr to  0.00020589113209464906
reducing lr to  0.00018530201888518417
reducing lr to  0.00016677181699666576
reducing lr to  0.0001500946352969992
reducing lr to  0.0001350851717672993
reducing lr to  0.00012157665459056936
reducing lr to  0.00010941898913151243
reducing lr to  9.847709021836118e-05
reducing lr to  8.862938119652506e-05
reducing lr to  7.976644307687256e-05
reducing lr to  7.17897987691853e-05
reducing lr to  6.461081889226677e-05
21 H_error tensor(191.1222, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3165, device='cuda:6', grad_fn=<DivBackward0>)
40095 MiB free out of 48676 MiB total
25 self_attn.o_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
21 H_error tensor(48.9425, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3269, device='cuda:6', grad_fn=<DivBackward0>)
40095 MiB free out of 48676 MiB total
25 mlp.gate_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
54 H_error tensor(381.7888, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3188, device='cuda:6', grad_fn=<DivBackward0>)
40353 MiB free out of 48676 MiB total
25 mlp.up_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
reducing lr to  0.00020589113209464906
reducing lr to  0.00018530201888518417
reducing lr to  0.00016677181699666576
reducing lr to  0.0001500946352969992
reducing lr to  0.0001350851717672993
reducing lr to  0.00012157665459056936
reducing lr to  0.00010941898913151243
reducing lr to  9.847709021836118e-05
54 H_error tensor(328.2845, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3126, device='cuda:6', grad_fn=<DivBackward0>)
40307 MiB free out of 48676 MiB total
25 mlp.down_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
reducing lr to  0.00020589113209464906
reducing lr to  0.00018530201888518417
reducing lr to  0.00016677181699666576
reducing lr to  0.0001500946352969992
reducing lr to  0.0001350851717672993
reducing lr to  0.00012157665459056936
reducing lr to  0.00010941898913151243
reducing lr to  9.847709021836118e-05
60 H_error tensor(29.3483, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3196, device='cuda:6', grad_fn=<DivBackward0>)
40479 MiB free out of 48676 MiB total
inputs.shape torch.Size([128, 2048, 4096])
saving because model is best
Iteration 0, loss 44.49306309223175, regul_loss 0.0, reconstruct_loss 44.49306309223175, time 16.648
Iteration 1, loss 90.64381605386734, regul_loss 467.9921569824219, reconstruct_loss 43.84459865093231, time 16.83
Iteration 2, loss 55.98421764373779, regul_loss 122.31980895996094, reconstruct_loss 43.75223711133003, time 16.866
Iteration 3, loss 51.992776334285736, regul_loss 82.43519592285156, reconstruct_loss 43.74925750494003, time 16.9
Iteration 4, loss 47.78692016005516, regul_loss 40.383262634277344, reconstruct_loss 43.748593896627426, time 16.918
Iteration 5, loss 47.357791751623154, regul_loss 36.09139633178711, reconstruct_loss 43.74865326285362, time 16.915
Iteration 6, loss 46.92233172059059, regul_loss 31.73541831970215, reconstruct_loss 43.74878969788551, time 16.906
Iteration 7, loss 46.87836113572121, regul_loss 31.2955379486084, reconstruct_loss 43.74880638718605, time 16.9
Iteration 8, loss 46.83401280641556, regul_loss 30.851871490478516, reconstruct_loss 43.748826801776886, time 16.906
Iteration 9, loss 46.82954865694046, regul_loss 30.80721664428711, reconstruct_loss 43.74882584810257, time 16.912
26 self_attn.q_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
22 H_error tensor(291.4487, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3371, device='cuda:6', grad_fn=<DivBackward0>)
40159 MiB free out of 48676 MiB total
26 self_attn.k_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
21 H_error tensor(310.2404, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3415, device='cuda:6', grad_fn=<DivBackward0>)
40095 MiB free out of 48676 MiB total
26 self_attn.v_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
reducing lr to  0.00020589113209464906
reducing lr to  0.00018530201888518417
reducing lr to  0.00016677181699666576
reducing lr to  0.0001500946352969992
reducing lr to  0.0001350851717672993
reducing lr to  0.00012157665459056936
reducing lr to  0.00010941898913151243
reducing lr to  9.847709021836118e-05
reducing lr to  8.862938119652506e-05
reducing lr to  7.976644307687256e-05
reducing lr to  7.17897987691853e-05
reducing lr to  6.461081889226677e-05
reducing lr to  5.81497370030401e-05
reducing lr to  5.233476330273609e-05
reducing lr to  4.7101286972462485e-05
reducing lr to  4.239115827521624e-05
reducing lr to  3.8152042447694614e-05
reducing lr to  3.433683820292515e-05
21 H_error tensor(220.4801, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3152, device='cuda:6', grad_fn=<DivBackward0>)
40095 MiB free out of 48676 MiB total
26 self_attn.o_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
21 H_error tensor(50.7128, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3283, device='cuda:6', grad_fn=<DivBackward0>)
40095 MiB free out of 48676 MiB total
26 mlp.gate_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
54 H_error tensor(406.3238, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3195, device='cuda:6', grad_fn=<DivBackward0>)
40353 MiB free out of 48676 MiB total
26 mlp.up_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
reducing lr to  0.00020589113209464906
reducing lr to  0.00018530201888518417
reducing lr to  0.00016677181699666576
reducing lr to  0.0001500946352969992
reducing lr to  0.0001350851717672993
reducing lr to  0.00012157665459056936
reducing lr to  0.00010941898913151243
reducing lr to  9.847709021836118e-05
54 H_error tensor(351.8042, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3128, device='cuda:6', grad_fn=<DivBackward0>)
40307 MiB free out of 48676 MiB total
26 mlp.down_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
reducing lr to  0.00020589113209464906
reducing lr to  0.00018530201888518417
reducing lr to  0.00016677181699666576
reducing lr to  0.0001500946352969992
reducing lr to  0.0001350851717672993
reducing lr to  0.00012157665459056936
reducing lr to  0.00010941898913151243
reducing lr to  9.847709021836118e-05
60 H_error tensor(31.8898, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3204, device='cuda:6', grad_fn=<DivBackward0>)
40479 MiB free out of 48676 MiB total
inputs.shape torch.Size([128, 2048, 4096])
saving because model is best
Iteration 0, loss 57.151512026786804, regul_loss 0.0, reconstruct_loss 57.151512026786804, time 16.646
Iteration 1, loss 102.78604060411453, regul_loss 467.1849365234375, reconstruct_loss 56.06754648685455, time 16.824
Iteration 2, loss 68.21114727854729, regul_loss 124.25074768066406, reconstruct_loss 55.78607374429703, time 16.859
Iteration 3, loss 64.26206487417221, regul_loss 84.92704772949219, reconstruct_loss 55.76935902237892, time 16.885
Iteration 4, loss 60.16368606686592, regul_loss 44.06832504272461, reconstruct_loss 55.756853103637695, time 16.874
Iteration 5, loss 59.75569644570351, regul_loss 39.99851989746094, reconstruct_loss 55.755844593048096, time 16.9
Iteration 6, loss 59.34552812576294, regul_loss 35.90546417236328, reconstruct_loss 55.754982352256775, time 16.925
Iteration 7, loss 59.304375410079956, regul_loss 35.49473571777344, reconstruct_loss 55.754902094602585, time 16.907
Iteration 8, loss 59.262946635484695, regul_loss 35.08118438720703, reconstruct_loss 55.754828840494156, time 16.92
Iteration 9, loss 59.25877687335014, regul_loss 35.03960418701172, reconstruct_loss 55.75481712818146, time 16.932
27 self_attn.q_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
22 H_error tensor(287.3137, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3388, device='cuda:6', grad_fn=<DivBackward0>)
40159 MiB free out of 48676 MiB total
27 self_attn.k_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
21 H_error tensor(316.6889, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3421, device='cuda:6', grad_fn=<DivBackward0>)
40095 MiB free out of 48676 MiB total
27 self_attn.v_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
reducing lr to  0.00020589113209464906
reducing lr to  0.00018530201888518417
reducing lr to  0.00016677181699666576
reducing lr to  0.0001500946352969992
reducing lr to  0.0001350851717672993
reducing lr to  0.00012157665459056936
reducing lr to  0.00010941898913151243
reducing lr to  9.847709021836118e-05
reducing lr to  8.862938119652506e-05
reducing lr to  7.976644307687256e-05
21 H_error tensor(229.7486, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3152, device='cuda:6', grad_fn=<DivBackward0>)
40095 MiB free out of 48676 MiB total
27 self_attn.o_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
21 H_error tensor(57.6236, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3395, device='cuda:6', grad_fn=<DivBackward0>)
40095 MiB free out of 48676 MiB total
27 mlp.gate_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
54 H_error tensor(441.4755, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3223, device='cuda:6', grad_fn=<DivBackward0>)
40353 MiB free out of 48676 MiB total
27 mlp.up_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
reducing lr to  0.00020589113209464906
reducing lr to  0.00018530201888518417
reducing lr to  0.00016677181699666576
reducing lr to  0.0001500946352969992
reducing lr to  0.0001350851717672993
reducing lr to  0.00012157665459056936
reducing lr to  0.00010941898913151243
reducing lr to  9.847709021836118e-05
reducing lr to  8.862938119652506e-05
reducing lr to  7.976644307687256e-05
54 H_error tensor(377.7394, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3136, device='cuda:6', grad_fn=<DivBackward0>)
40307 MiB free out of 48676 MiB total
27 mlp.down_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
61 H_error tensor(34.8291, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3226, device='cuda:6', grad_fn=<DivBackward0>)
40479 MiB free out of 48676 MiB total
inputs.shape torch.Size([128, 2048, 4096])
saving because model is best
Iteration 0, loss 48.831614911556244, regul_loss 0.0, reconstruct_loss 48.831614911556244, time 16.643
Iteration 1, loss 94.35450977087021, regul_loss 466.8751220703125, reconstruct_loss 47.66699853539467, time 16.844
Iteration 2, loss 60.53768536448479, regul_loss 126.85589599609375, reconstruct_loss 47.852096021175385, time 16.876
Iteration 3, loss 56.64363652467728, regul_loss 87.57366943359375, reconstruct_loss 47.88627016544342, time 16.895
Iteration 4, loss 52.56055045127869, regul_loss 46.340579986572266, reconstruct_loss 47.9264931678772, time 16.908
Iteration 5, loss 52.14728283882141, regul_loss 42.16450500488281, reconstruct_loss 47.93083310127258, time 16.916
Iteration 6, loss 51.72894665598869, regul_loss 37.93577575683594, reconstruct_loss 47.93536755442619, time 16.898
Iteration 7, loss 51.686772108078, regul_loss 37.509403228759766, reconstruct_loss 47.93583244085312, time 16.913
Iteration 8, loss 51.644253462553024, regul_loss 37.07947540283203, reconstruct_loss 47.936306685209274, time 16.916
Iteration 9, loss 51.63997480273247, regul_loss 37.03621292114258, reconstruct_loss 47.936353862285614, time 16.923
28 self_attn.q_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
22 H_error tensor(267.3907, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3476, device='cuda:6', grad_fn=<DivBackward0>)
40159 MiB free out of 48676 MiB total
28 self_attn.k_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
21 H_error tensor(305.6257, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3511, device='cuda:6', grad_fn=<DivBackward0>)
40095 MiB free out of 48676 MiB total
28 self_attn.v_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
21 H_error tensor(210.0646, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3475, device='cuda:6', grad_fn=<DivBackward0>)
40095 MiB free out of 48676 MiB total
28 self_attn.o_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
21 H_error tensor(83.9008, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3341, device='cuda:6', grad_fn=<DivBackward0>)
40095 MiB free out of 48676 MiB total
28 mlp.gate_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
53 H_error tensor(467.9616, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3235, device='cuda:6', grad_fn=<DivBackward0>)
40353 MiB free out of 48676 MiB total
28 mlp.up_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
reducing lr to  0.00020589113209464906
reducing lr to  0.00018530201888518417
reducing lr to  0.00016677181699666576
reducing lr to  0.0001500946352969992
reducing lr to  0.0001350851717672993
reducing lr to  0.00012157665459056936
reducing lr to  0.00010941898913151243
reducing lr to  9.847709021836118e-05
53 H_error tensor(412.3990, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3150, device='cuda:6', grad_fn=<DivBackward0>)
40307 MiB free out of 48676 MiB total
28 mlp.down_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
59 H_error tensor(40.9671, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3264, device='cuda:6', grad_fn=<DivBackward0>)
40479 MiB free out of 48676 MiB total
inputs.shape torch.Size([128, 2048, 4096])
saving because model is best
Iteration 0, loss 62.02332800626755, regul_loss 0.0, reconstruct_loss 62.02332800626755, time 16.621
Iteration 1, loss 108.302614569664, regul_loss 471.4176940917969, reconstruct_loss 61.16084513068199, time 16.816
Iteration 2, loss 73.3242909014225, regul_loss 123.892578125, reconstruct_loss 60.935032427310944, time 16.88
Iteration 3, loss 69.30825105309486, regul_loss 83.87004089355469, reconstruct_loss 60.92124906182289, time 16.893
Iteration 4, loss 65.09720295667648, regul_loss 41.86685562133789, reconstruct_loss 60.910518527030945, time 16.907
Iteration 5, loss 64.67090082168579, regul_loss 37.612911224365234, reconstruct_loss 60.90960970520973, time 16.913
Iteration 6, loss 64.23958545923233, regul_loss 33.30786895751953, reconstruct_loss 60.908798068761826, time 16.912
Iteration 7, loss 64.19611936807632, regul_loss 32.87399673461914, reconstruct_loss 60.90871977806091, time 16.922
Iteration 8, loss 64.15230840444565, regul_loss 32.436622619628906, reconstruct_loss 60.908646166324615, time 16.922
Iteration 9, loss 64.1478981077671, regul_loss 32.39262008666992, reconstruct_loss 60.90863487124443, time 16.919
29 self_attn.q_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
22 H_error tensor(277.0828, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3430, device='cuda:6', grad_fn=<DivBackward0>)
40159 MiB free out of 48676 MiB total
29 self_attn.k_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
21 H_error tensor(303.4376, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3468, device='cuda:6', grad_fn=<DivBackward0>)
40095 MiB free out of 48676 MiB total
29 self_attn.v_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
reducing lr to  0.00020589113209464906
reducing lr to  0.00018530201888518417
reducing lr to  0.00016677181699666576
reducing lr to  0.0001500946352969992
21 H_error tensor(248.7617, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3174, device='cuda:6', grad_fn=<DivBackward0>)
40095 MiB free out of 48676 MiB total
29 self_attn.o_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
21 H_error tensor(102.2568, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3375, device='cuda:6', grad_fn=<DivBackward0>)
40095 MiB free out of 48676 MiB total
29 mlp.gate_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
55 H_error tensor(491.7473, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3263, device='cuda:6', grad_fn=<DivBackward0>)
40353 MiB free out of 48676 MiB total
29 mlp.up_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
reducing lr to  0.00020589113209464906
reducing lr to  0.00018530201888518417
reducing lr to  0.00016677181699666576
reducing lr to  0.0001500946352969992
54 H_error tensor(426.6555, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3164, device='cuda:6', grad_fn=<DivBackward0>)
40307 MiB free out of 48676 MiB total
29 mlp.down_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
60 H_error tensor(48.4792, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3327, device='cuda:6', grad_fn=<DivBackward0>)
40479 MiB free out of 48676 MiB total
inputs.shape torch.Size([128, 2048, 4096])
saving because model is best
Iteration 0, loss 73.77001917362213, regul_loss 0.0, reconstruct_loss 73.77001917362213, time 16.629
Iteration 1, loss 118.3344561457634, regul_loss 468.18682861328125, reconstruct_loss 71.51577407121658, time 16.815
Iteration 2, loss 84.85301941633224, regul_loss 134.02651977539062, reconstruct_loss 71.4503703713417, time 16.871
Iteration 3, loss 81.04581034183502, regul_loss 95.71025085449219, reconstruct_loss 71.47478818893433, time 16.893
Iteration 4, loss 77.14107972383499, regul_loss 56.30729293823242, reconstruct_loss 71.51035010814667, time 16.895
Iteration 5, loss 76.75944685935974, regul_loss 52.449825286865234, reconstruct_loss 71.51446688175201, time 16.903
Iteration 6, loss 76.37803828716278, regul_loss 48.59115219116211, reconstruct_loss 71.51892274618149, time 16.923
Iteration 7, loss 76.33992463350296, regul_loss 48.20536804199219, reconstruct_loss 71.51939079165459, time 16.911
Iteration 8, loss 76.30159667134285, regul_loss 47.81721878051758, reconstruct_loss 71.51987370848656, time 16.907
Iteration 9, loss 76.29774260520935, regul_loss 47.77820587158203, reconstruct_loss 71.5199236869812, time 16.916
30 self_attn.q_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
22 H_error tensor(264.0538, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3430, device='cuda:6', grad_fn=<DivBackward0>)
40159 MiB free out of 48676 MiB total
30 self_attn.k_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
22 H_error tensor(307.4346, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3493, device='cuda:6', grad_fn=<DivBackward0>)
40095 MiB free out of 48676 MiB total
30 self_attn.v_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
reducing lr to  0.00020589113209464906
reducing lr to  0.00018530201888518417
reducing lr to  0.00016677181699666576
reducing lr to  0.0001500946352969992
reducing lr to  0.0001350851717672993
reducing lr to  0.00012157665459056936
reducing lr to  0.00010941898913151243
reducing lr to  9.847709021836118e-05
reducing lr to  8.862938119652506e-05
reducing lr to  7.976644307687256e-05
reducing lr to  7.17897987691853e-05
reducing lr to  6.461081889226677e-05
reducing lr to  5.81497370030401e-05
reducing lr to  5.233476330273609e-05
reducing lr to  4.7101286972462485e-05
21 H_error tensor(227.1164, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3175, device='cuda:6', grad_fn=<DivBackward0>)
40031 MiB free out of 48676 MiB total
30 self_attn.o_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
21 H_error tensor(131.5081, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3343, device='cuda:6', grad_fn=<DivBackward0>)
40031 MiB free out of 48676 MiB total
30 mlp.gate_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
54 H_error tensor(498.7764, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3273, device='cuda:6', grad_fn=<DivBackward0>)
40157 MiB free out of 48676 MiB total
30 mlp.up_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
54 H_error tensor(469.3125, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3204, device='cuda:6', grad_fn=<DivBackward0>)
40111 MiB free out of 48676 MiB total
30 mlp.down_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
60 H_error tensor(218.7906, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3570, device='cuda:6', grad_fn=<DivBackward0>)
40283 MiB free out of 48676 MiB total
inputs.shape torch.Size([128, 2048, 4096])
saving because model is best
Iteration 0, loss 142.5014407634735, regul_loss 0.0, reconstruct_loss 142.5014407634735, time 16.645
Iteration 1, loss 181.86220502853394, regul_loss 465.9727783203125, reconstruct_loss 135.26492953300476, time 16.834
Iteration 2, loss 147.23130255937576, regul_loss 155.95138549804688, reconstruct_loss 131.63616371154785, time 16.878
Iteration 3, loss 143.58881026506424, regul_loss 122.1228256225586, reconstruct_loss 131.37652957439423, time 16.901
saving because model is best
Iteration 4, loss 140.04071831703186, regul_loss 88.91300201416016, reconstruct_loss 131.14942103624344, time 16.908
saving because model is best
Iteration 5, loss 137.44103229045868, regul_loss 64.98978424072266, reconstruct_loss 130.942054271698, time 16.92
Iteration 6, loss 137.5086314678192, regul_loss 67.64741516113281, reconstruct_loss 130.74389111995697, time 16.916
Iteration 7, loss 137.54025548696518, regul_loss 68.1800537109375, reconstruct_loss 130.7222471833229, time 16.919
Iteration 8, loss 137.50449299812317, regul_loss 68.0560073852539, reconstruct_loss 130.6988903284073, time 16.918
Iteration 9, loss 137.4954707622528, regul_loss 67.99067687988281, reconstruct_loss 130.69640123844147, time 16.939
31 self_attn.q_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
22 H_error tensor(177.0078, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3441, device='cuda:6', grad_fn=<DivBackward0>)
40333 MiB free out of 48676 MiB total
31 self_attn.k_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
21 H_error tensor(217.2875, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3544, device='cuda:6', grad_fn=<DivBackward0>)
40333 MiB free out of 48676 MiB total
31 self_attn.v_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
reducing lr to  0.0009000000000000001
reducing lr to  0.0008100000000000001
reducing lr to  0.000729
reducing lr to  0.0006561000000000001
reducing lr to  0.00059049
reducing lr to  0.000531441
reducing lr to  0.0004782969
reducing lr to  0.00043046721
reducing lr to  0.000387420489
reducing lr to  0.0003486784401
reducing lr to  0.00031381059609000004
reducing lr to  0.00028242953648100003
reducing lr to  0.00025418658283290005
reducing lr to  0.00022876792454961005
reducing lr to  0.00020589113209464906
reducing lr to  0.00018530201888518417
reducing lr to  0.00016677181699666576
reducing lr to  0.0001500946352969992
reducing lr to  0.0001350851717672993
reducing lr to  0.00012157665459056936
reducing lr to  0.00010941898913151243
21 H_error tensor(123.8277, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3208, device='cuda:6', grad_fn=<DivBackward0>)
40333 MiB free out of 48676 MiB total
31 self_attn.o_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
21 H_error tensor(231.2608, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3458, device='cuda:6', grad_fn=<DivBackward0>)
40333 MiB free out of 48676 MiB total
31 mlp.gate_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
54 H_error tensor(485.3159, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3403, device='cuda:6', grad_fn=<DivBackward0>)
40287 MiB free out of 48676 MiB total
31 mlp.up_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
54 H_error tensor(694.3398, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.3369, device='cuda:6', grad_fn=<DivBackward0>)
40241 MiB free out of 48676 MiB total
31 mlp.down_proj
Pruning ...
using 0.0 of the hessian
using 0.0 of the magnitude
using all the weights
59 H_error tensor(-3579.7993, device='cuda:6', grad_fn=<ViewBackward0>) average_error tensor(0.5914, device='cuda:6', grad_fn=<DivBackward0>)
39777 MiB free out of 48676 MiB total
Total bits: 12952010752.0 Total params: 6476005376
average bits per value: 2.0
14608.988640069962
Loading tokenizer for huggyllama/llama-7b
/home/lliu/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Token indices sequence length is longer than the specified maximum sequence length for this model (2824491 > 2048). Running this sequence through the model will result in indexing errors
Dataset: wikitext2
Evaluating ...
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
Perplexity: 13.767429
