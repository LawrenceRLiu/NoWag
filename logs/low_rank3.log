/home/lliu/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.22s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.30it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.20it/s]
Model loaded. LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
4096
Loading tokenizer for meta-llama/Llama-2-7b-hf
Starting...
position_ids torch.Size([1, 4096])
attention_mask torch.Size([1, 1, 4096, 4096])
Ready.
Layer 0
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
total size:  202375168 total Megabytes:  tensor(272.3271, device='cuda:7') bits per value:  tensor(11.2882, device='cuda:7')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
weights.shape =  torch.Size([4096, 11008])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:7') bits per value:  tensor(0.8228, device='cuda:7')
Pre fine tuning: 24199 MiB free out of 48676 MiB total
Found 25 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1148624 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:7 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:7 torch.float32
----------
epoch=0
train loss=6.38e-05	
9839 MiB free out of 48676 MiB total
----------
epoch=1
train loss=4.48e-05	
9839 MiB free out of 48676 MiB total
----------
epoch=2
train loss=3.53e-05	
9839 MiB free out of 48676 MiB total
----------
epoch=3
train loss=3.10e-05	
9839 MiB free out of 48676 MiB total
----------
epoch=4
train loss=2.87e-05	
9839 MiB free out of 48676 MiB total
post_fine_tuning 9839 MiB free out of 48676 MiB total
9839 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:7') bits per value:  tensor(0.8228, device='cuda:7')
9833 MiB free out of 48676 MiB total
layer: 0
done in 0:09:26.979990 overall time: 0:09:26.980026 estimated time left: 4:52:56.380791
after cast to cpu
37325 MiB free out of 48676 MiB total
Layer 1
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
total size:  202375168 total Megabytes:  tensor(272.3271, device='cuda:7') bits per value:  tensor(11.2882, device='cuda:7')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
weights.shape =  torch.Size([4096, 11008])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:7') bits per value:  tensor(0.8228, device='cuda:7')
Pre fine tuning: 22981 MiB free out of 48676 MiB total
Found 25 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1148624 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:7 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:7 torch.float32
----------
epoch=0
train loss=3.73e-01	
10677 MiB free out of 48676 MiB total
----------
epoch=1
train loss=3.42e-01	
10677 MiB free out of 48676 MiB total
----------
epoch=2
train loss=3.07e-01	
10677 MiB free out of 48676 MiB total
----------
epoch=3
train loss=2.65e-01	
10677 MiB free out of 48676 MiB total
----------
epoch=4
train loss=2.25e-01	
10677 MiB free out of 48676 MiB total
post_fine_tuning 10677 MiB free out of 48676 MiB total
10677 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:7') bits per value:  tensor(0.8228, device='cuda:7')
10677 MiB free out of 48676 MiB total
layer: 1
done in 0:09:26.664923 overall time: 0:18:53.770551 estimated time left: 4:43:26.558272
after cast to cpu
37841 MiB free out of 48676 MiB total
Layer 2
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
total size:  202375168 total Megabytes:  tensor(272.3271, device='cuda:7') bits per value:  tensor(11.2882, device='cuda:7')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
weights.shape =  torch.Size([4096, 11008])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:7') bits per value:  tensor(0.8228, device='cuda:7')
Pre fine tuning: 23499 MiB free out of 48676 MiB total
Found 25 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1148624 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:7 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:7 torch.float32
----------
epoch=0
train loss=4.15e-04	
11193 MiB free out of 48676 MiB total
----------
epoch=1
train loss=3.24e-04	
11193 MiB free out of 48676 MiB total
----------
epoch=2
train loss=2.76e-04	
11193 MiB free out of 48676 MiB total
----------
epoch=3
train loss=2.45e-04	
11193 MiB free out of 48676 MiB total
----------
epoch=4
train loss=2.25e-04	
11193 MiB free out of 48676 MiB total
post_fine_tuning 11193 MiB free out of 48676 MiB total
11193 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:7') bits per value:  tensor(0.8228, device='cuda:7')
11193 MiB free out of 48676 MiB total
layer: 2
done in 0:09:26.533271 overall time: 0:28:20.460451 estimated time left: 4:33:57.784363
after cast to cpu
37841 MiB free out of 48676 MiB total
Layer 3
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
total size:  202375168 total Megabytes:  tensor(272.3271, device='cuda:7') bits per value:  tensor(11.2882, device='cuda:7')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
weights.shape =  torch.Size([4096, 11008])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:7') bits per value:  tensor(0.8228, device='cuda:7')
Pre fine tuning: 23497 MiB free out of 48676 MiB total
Found 25 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1148624 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:7 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:7 torch.float32
----------
epoch=0
train loss=1.16e-03	
11193 MiB free out of 48676 MiB total
----------
epoch=1
train loss=8.14e-04	
11193 MiB free out of 48676 MiB total
----------
epoch=2
train loss=6.41e-04	
11193 MiB free out of 48676 MiB total
----------
epoch=3
train loss=5.54e-04	
11193 MiB free out of 48676 MiB total
----------
epoch=4
train loss=5.11e-04	
11193 MiB free out of 48676 MiB total
post_fine_tuning 11193 MiB free out of 48676 MiB total
11193 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:7') bits per value:  tensor(0.8228, device='cuda:7')
11193 MiB free out of 48676 MiB total
layer: 3
done in 0:09:26.186056 overall time: 0:37:46.796887 estimated time left: 4:24:27.578208
after cast to cpu
37841 MiB free out of 48676 MiB total
Layer 4
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
total size:  202375168 total Megabytes:  tensor(272.3271, device='cuda:7') bits per value:  tensor(11.2882, device='cuda:7')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
weights.shape =  torch.Size([4096, 11008])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:7') bits per value:  tensor(0.8228, device='cuda:7')
Pre fine tuning: 23497 MiB free out of 48676 MiB total
Found 25 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1148624 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:7 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:7 torch.float32
----------
epoch=0
train loss=2.85e-03	
11193 MiB free out of 48676 MiB total
----------
epoch=1
train loss=2.19e-03	
11193 MiB free out of 48676 MiB total
----------
epoch=2
train loss=1.79e-03	
11193 MiB free out of 48676 MiB total
----------
epoch=3
train loss=1.53e-03	
11193 MiB free out of 48676 MiB total
----------
epoch=4
train loss=1.39e-03	
11193 MiB free out of 48676 MiB total
post_fine_tuning 11193 MiB free out of 48676 MiB total
11193 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:7') bits per value:  tensor(0.8228, device='cuda:7')
11193 MiB free out of 48676 MiB total
layer: 4
done in 0:09:25.472259 overall time: 0:47:12.418749 estimated time left: 4:14:55.061245
after cast to cpu
37841 MiB free out of 48676 MiB total
Layer 5
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
total size:  202375168 total Megabytes:  tensor(272.3271, device='cuda:7') bits per value:  tensor(11.2882, device='cuda:7')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
weights.shape =  torch.Size([4096, 11008])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:7') bits per value:  tensor(0.8228, device='cuda:7')
Pre fine tuning: 23497 MiB free out of 48676 MiB total
Found 25 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1148624 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:7 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:7 torch.float32
----------
epoch=0
train loss=3.60e-03	
11193 MiB free out of 48676 MiB total
----------
epoch=1
train loss=2.24e-03	
11193 MiB free out of 48676 MiB total
----------
epoch=2
train loss=1.51e-03	
11193 MiB free out of 48676 MiB total
----------
epoch=3
train loss=1.18e-03	
11193 MiB free out of 48676 MiB total
----------
epoch=4
train loss=1.07e-03	
11193 MiB free out of 48676 MiB total
post_fine_tuning 11193 MiB free out of 48676 MiB total
11193 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:7') bits per value:  tensor(0.8228, device='cuda:7')
11193 MiB free out of 48676 MiB total
layer: 5
done in 0:09:25.314377 overall time: 0:56:37.882946 estimated time left: 4:05:24.159435
after cast to cpu
37841 MiB free out of 48676 MiB total
Layer 6
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
total size:  202375168 total Megabytes:  tensor(272.3271, device='cuda:7') bits per value:  tensor(11.2882, device='cuda:7')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
weights.shape =  torch.Size([4096, 11008])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:7') bits per value:  tensor(0.8228, device='cuda:7')
Pre fine tuning: 23497 MiB free out of 48676 MiB total
Found 25 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1148624 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:7 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:7 torch.float32
----------
epoch=0
train loss=6.39e-03	
11193 MiB free out of 48676 MiB total
----------
epoch=1
train loss=3.44e-03	
11193 MiB free out of 48676 MiB total
----------
epoch=2
train loss=1.86e-03	
11193 MiB free out of 48676 MiB total
----------
epoch=3
train loss=1.25e-03	
11193 MiB free out of 48676 MiB total
----------
epoch=4
train loss=1.07e-03	
11193 MiB free out of 48676 MiB total
post_fine_tuning 11193 MiB free out of 48676 MiB total
11193 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:7') bits per value:  tensor(0.8228, device='cuda:7')
11193 MiB free out of 48676 MiB total
layer: 6
done in 0:09:25.276142 overall time: 1:06:03.318729 estimated time left: 3:55:54.709748
after cast to cpu
37841 MiB free out of 48676 MiB total
Layer 7
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
total size:  202375168 total Megabytes:  tensor(272.3271, device='cuda:7') bits per value:  tensor(11.2882, device='cuda:7')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
weights.shape =  torch.Size([4096, 11008])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:7') bits per value:  tensor(0.8228, device='cuda:7')
Pre fine tuning: 23497 MiB free out of 48676 MiB total
Found 25 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1148624 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:7 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:7 torch.float32
----------
epoch=0
train loss=8.93e-03	
11193 MiB free out of 48676 MiB total
----------
epoch=1
train loss=5.47e-03	
11193 MiB free out of 48676 MiB total
----------
epoch=2
train loss=2.95e-03	
11193 MiB free out of 48676 MiB total
----------
epoch=3
train loss=1.77e-03	
11193 MiB free out of 48676 MiB total
----------
epoch=4
train loss=1.40e-03	
11193 MiB free out of 48676 MiB total
post_fine_tuning 11193 MiB free out of 48676 MiB total
11193 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:7') bits per value:  tensor(0.8228, device='cuda:7')
11193 MiB free out of 48676 MiB total
layer: 7
done in 0:09:25.964233 overall time: 1:15:29.432997 estimated time left: 3:46:28.298990
after cast to cpu
37841 MiB free out of 48676 MiB total
Layer 8
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
total size:  202375168 total Megabytes:  tensor(272.3271, device='cuda:7') bits per value:  tensor(11.2882, device='cuda:7')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
weights.shape =  torch.Size([4096, 11008])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:7') bits per value:  tensor(0.8228, device='cuda:7')
Pre fine tuning: 23497 MiB free out of 48676 MiB total
Found 25 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1148624 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:7 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:7 torch.float32
----------
epoch=0
train loss=1.35e-02	
11193 MiB free out of 48676 MiB total
----------
epoch=1
train loss=7.62e-03	
11193 MiB free out of 48676 MiB total
----------
epoch=2
train loss=3.84e-03	
11193 MiB free out of 48676 MiB total
----------
epoch=3
train loss=2.26e-03	
11193 MiB free out of 48676 MiB total
----------
epoch=4
train loss=1.84e-03	
11193 MiB free out of 48676 MiB total
post_fine_tuning 11193 MiB free out of 48676 MiB total
11193 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:7') bits per value:  tensor(0.8228, device='cuda:7')
11193 MiB free out of 48676 MiB total
layer: 8
done in 0:09:24.787743 overall time: 1:24:54.370866 estimated time left: 3:36:58.947768
after cast to cpu
37841 MiB free out of 48676 MiB total
Layer 9
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
total size:  202375168 total Megabytes:  tensor(272.3271, device='cuda:7') bits per value:  tensor(11.2882, device='cuda:7')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
weights.shape =  torch.Size([4096, 11008])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:7') bits per value:  tensor(0.8228, device='cuda:7')
Pre fine tuning: 23497 MiB free out of 48676 MiB total
Found 25 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1148624 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:7 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:7 torch.float32
----------
epoch=0
train loss=1.19e-02	
11193 MiB free out of 48676 MiB total
----------
epoch=1
train loss=7.82e-03	
11193 MiB free out of 48676 MiB total
----------
epoch=2
train loss=4.77e-03	
11193 MiB free out of 48676 MiB total
----------
epoch=3
train loss=3.13e-03	
11193 MiB free out of 48676 MiB total
----------
epoch=4
train loss=2.47e-03	
11193 MiB free out of 48676 MiB total
post_fine_tuning 11193 MiB free out of 48676 MiB total
11193 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:7') bits per value:  tensor(0.8228, device='cuda:7')
11193 MiB free out of 48676 MiB total
layer: 9
done in 0:09:26.175062 overall time: 1:34:20.696445 estimated time left: 3:27:33.532180
after cast to cpu
37841 MiB free out of 48676 MiB total
Layer 10
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
total size:  202375168 total Megabytes:  tensor(272.3271, device='cuda:7') bits per value:  tensor(11.2882, device='cuda:7')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
weights.shape =  torch.Size([4096, 11008])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:7') bits per value:  tensor(0.8228, device='cuda:7')
Pre fine tuning: 23497 MiB free out of 48676 MiB total
Found 25 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1148624 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:7 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:7 torch.float32
----------
epoch=0
train loss=1.88e-02	
11193 MiB free out of 48676 MiB total
----------
epoch=1
train loss=1.10e-02	
11193 MiB free out of 48676 MiB total
----------
epoch=2
train loss=6.20e-03	
11193 MiB free out of 48676 MiB total
----------
epoch=3
train loss=3.85e-03	
11193 MiB free out of 48676 MiB total
----------
epoch=4
train loss=3.05e-03	
11193 MiB free out of 48676 MiB total
post_fine_tuning 11193 MiB free out of 48676 MiB total
11193 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:7') bits per value:  tensor(0.8228, device='cuda:7')
11193 MiB free out of 48676 MiB total
layer: 10
done in 0:09:26.082161 overall time: 1:43:46.928684 estimated time left: 3:18:07.772943
after cast to cpu
37841 MiB free out of 48676 MiB total
Layer 11
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
total size:  202375168 total Megabytes:  tensor(272.3271, device='cuda:7') bits per value:  tensor(11.2882, device='cuda:7')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
weights.shape =  torch.Size([4096, 11008])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:7') bits per value:  tensor(0.8228, device='cuda:7')
Pre fine tuning: 23497 MiB free out of 48676 MiB total
Found 25 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1148624 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:7 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:7 torch.float32
----------
epoch=0
train loss=3.62e-02	
11193 MiB free out of 48676 MiB total
----------
epoch=1
train loss=2.44e-02	
11193 MiB free out of 48676 MiB total
----------
epoch=2
train loss=1.46e-02	
11193 MiB free out of 48676 MiB total
----------
epoch=3
train loss=8.45e-03	
11193 MiB free out of 48676 MiB total
----------
epoch=4
train loss=5.55e-03	
11193 MiB free out of 48676 MiB total
post_fine_tuning 11193 MiB free out of 48676 MiB total
11193 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:7') bits per value:  tensor(0.8228, device='cuda:7')
11193 MiB free out of 48676 MiB total
layer: 11
done in 0:09:25.558833 overall time: 1:53:12.644250 estimated time left: 3:08:41.073750
after cast to cpu
37841 MiB free out of 48676 MiB total
Layer 12
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
total size:  202375168 total Megabytes:  tensor(272.3271, device='cuda:7') bits per value:  tensor(11.2882, device='cuda:7')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
weights.shape =  torch.Size([4096, 11008])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:7') bits per value:  tensor(0.8228, device='cuda:7')
Pre fine tuning: 23497 MiB free out of 48676 MiB total
Found 25 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1148624 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:7 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:7 torch.float32
----------
epoch=0
train loss=2.29e-02	
11193 MiB free out of 48676 MiB total
----------
epoch=1
train loss=1.49e-02	
11193 MiB free out of 48676 MiB total
----------
epoch=2
train loss=8.75e-03	
11193 MiB free out of 48676 MiB total
----------
epoch=3
train loss=5.09e-03	
11193 MiB free out of 48676 MiB total
----------
epoch=4
train loss=3.54e-03	
11193 MiB free out of 48676 MiB total
post_fine_tuning 11193 MiB free out of 48676 MiB total
11193 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:7') bits per value:  tensor(0.8228, device='cuda:7')
11193 MiB free out of 48676 MiB total
layer: 12
done in 0:09:24.516277 overall time: 2:02:37.310185 estimated time left: 2:59:12.991809
after cast to cpu
37841 MiB free out of 48676 MiB total
Layer 13
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
total size:  202375168 total Megabytes:  tensor(272.3271, device='cuda:7') bits per value:  tensor(11.2882, device='cuda:7')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
weights.shape =  torch.Size([4096, 11008])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:7') bits per value:  tensor(0.8228, device='cuda:7')
Pre fine tuning: 23497 MiB free out of 48676 MiB total
Found 25 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1148624 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:7 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:7 torch.float32
----------
epoch=0
train loss=2.07e-02	
11193 MiB free out of 48676 MiB total
----------
epoch=1
train loss=1.30e-02	
11193 MiB free out of 48676 MiB total
----------
epoch=2
train loss=7.73e-03	
11193 MiB free out of 48676 MiB total
----------
epoch=3
train loss=4.91e-03	
11193 MiB free out of 48676 MiB total
----------
epoch=4
train loss=3.81e-03	
11193 MiB free out of 48676 MiB total
post_fine_tuning 11193 MiB free out of 48676 MiB total
11193 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:7') bits per value:  tensor(0.8228, device='cuda:7')
11193 MiB free out of 48676 MiB total
layer: 13
done in 0:09:25.518798 overall time: 2:12:02.983296 estimated time left: 2:49:46.692809
after cast to cpu
37841 MiB free out of 48676 MiB total
Layer 14
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
total size:  202375168 total Megabytes:  tensor(272.3271, device='cuda:7') bits per value:  tensor(11.2882, device='cuda:7')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
weights.shape =  torch.Size([4096, 11008])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:7') bits per value:  tensor(0.8228, device='cuda:7')
Pre fine tuning: 23497 MiB free out of 48676 MiB total
Found 25 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1148624 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:7 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:7 torch.float32
----------
epoch=0
train loss=2.07e-02	
11193 MiB free out of 48676 MiB total
----------
epoch=1
train loss=1.19e-02	
11193 MiB free out of 48676 MiB total
----------
epoch=2
train loss=6.72e-03	
11193 MiB free out of 48676 MiB total
----------
epoch=3
train loss=4.39e-03	
11193 MiB free out of 48676 MiB total
----------
epoch=4
train loss=3.70e-03	
11193 MiB free out of 48676 MiB total
post_fine_tuning 11193 MiB free out of 48676 MiB total
11193 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:7') bits per value:  tensor(0.8228, device='cuda:7')
11193 MiB free out of 48676 MiB total
layer: 14
done in 0:09:25.825321 overall time: 2:21:28.958277 estimated time left: 2:40:20.819381
after cast to cpu
37841 MiB free out of 48676 MiB total
Layer 15
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
total size:  202375168 total Megabytes:  tensor(272.3271, device='cuda:7') bits per value:  tensor(11.2882, device='cuda:7')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
weights.shape =  torch.Size([4096, 11008])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:7') bits per value:  tensor(0.8228, device='cuda:7')
Pre fine tuning: 23497 MiB free out of 48676 MiB total
Found 25 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1148624 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:7 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:7 torch.float32
----------
epoch=0
train loss=2.87e-02	
11193 MiB free out of 48676 MiB total
----------
epoch=1
train loss=1.93e-02	
11193 MiB free out of 48676 MiB total
----------
epoch=2
train loss=1.21e-02	
11193 MiB free out of 48676 MiB total
----------
epoch=3
train loss=7.62e-03	
11193 MiB free out of 48676 MiB total
----------
epoch=4
train loss=5.55e-03	
11193 MiB free out of 48676 MiB total
post_fine_tuning 11193 MiB free out of 48676 MiB total
11193 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:7') bits per value:  tensor(0.8228, device='cuda:7')
11193 MiB free out of 48676 MiB total
layer: 15
done in 0:09:24.688830 overall time: 2:30:53.798819 estimated time left: 2:30:53.798819
after cast to cpu
37841 MiB free out of 48676 MiB total
Layer 16
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
total size:  202375168 total Megabytes:  tensor(272.3271, device='cuda:7') bits per value:  tensor(11.2882, device='cuda:7')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
weights.shape =  torch.Size([4096, 11008])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:7') bits per value:  tensor(0.8228, device='cuda:7')
Pre fine tuning: 23497 MiB free out of 48676 MiB total
Found 25 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1148624 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:7 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:7 torch.float32
----------
epoch=0
train loss=4.03e-02	
11193 MiB free out of 48676 MiB total
----------
epoch=1
train loss=2.57e-02	
11193 MiB free out of 48676 MiB total
----------
epoch=2
train loss=1.56e-02	
11193 MiB free out of 48676 MiB total
----------
epoch=3
train loss=9.25e-03	
11193 MiB free out of 48676 MiB total
----------
epoch=4
train loss=6.30e-03	
11193 MiB free out of 48676 MiB total
post_fine_tuning 11193 MiB free out of 48676 MiB total
11193 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:7') bits per value:  tensor(0.8228, device='cuda:7')
11193 MiB free out of 48676 MiB total
layer: 16
done in 0:09:24.029063 overall time: 2:40:17.978140 estimated time left: 2:21:26.451300
after cast to cpu
37841 MiB free out of 48676 MiB total
Layer 17
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
total size:  202375168 total Megabytes:  tensor(272.3271, device='cuda:7') bits per value:  tensor(11.2882, device='cuda:7')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
weights.shape =  torch.Size([4096, 11008])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:7') bits per value:  tensor(0.8228, device='cuda:7')
Pre fine tuning: 23497 MiB free out of 48676 MiB total
Found 25 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1148624 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:7 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:7 torch.float32
----------
epoch=0
train loss=3.97e-02	
11193 MiB free out of 48676 MiB total
----------
epoch=1
train loss=2.49e-02	
11193 MiB free out of 48676 MiB total
----------
epoch=2
train loss=1.47e-02	
11193 MiB free out of 48676 MiB total
----------
epoch=3
train loss=8.46e-03	
11193 MiB free out of 48676 MiB total
----------
epoch=4
train loss=5.74e-03	
11193 MiB free out of 48676 MiB total
post_fine_tuning 11193 MiB free out of 48676 MiB total
11193 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:7') bits per value:  tensor(0.8228, device='cuda:7')
11193 MiB free out of 48676 MiB total
layer: 17
done in 0:09:24.632922 overall time: 2:49:42.769930 estimated time left: 2:11:59.932168
after cast to cpu
37841 MiB free out of 48676 MiB total
Layer 18
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
total size:  202375168 total Megabytes:  tensor(272.3271, device='cuda:7') bits per value:  tensor(11.2882, device='cuda:7')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
weights.shape =  torch.Size([4096, 11008])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:7') bits per value:  tensor(0.8228, device='cuda:7')
Pre fine tuning: 23497 MiB free out of 48676 MiB total
Found 25 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1148624 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:7 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:7 torch.float32
----------
epoch=0
train loss=3.49e-02	
11193 MiB free out of 48676 MiB total
----------
epoch=1
train loss=1.91e-02	
11193 MiB free out of 48676 MiB total
----------
epoch=2
train loss=1.09e-02	
11193 MiB free out of 48676 MiB total
----------
epoch=3
train loss=6.99e-03	
11193 MiB free out of 48676 MiB total
----------
epoch=4
train loss=5.59e-03	
11193 MiB free out of 48676 MiB total
post_fine_tuning 11193 MiB free out of 48676 MiB total
11193 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:7') bits per value:  tensor(0.8228, device='cuda:7')
11193 MiB free out of 48676 MiB total
layer: 18
done in 0:09:24.608913 overall time: 2:59:07.528230 estimated time left: 2:02:33.571947
after cast to cpu
37841 MiB free out of 48676 MiB total
Layer 19
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
total size:  202375168 total Megabytes:  tensor(272.3271, device='cuda:7') bits per value:  tensor(11.2882, device='cuda:7')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
weights.shape =  torch.Size([4096, 11008])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:7') bits per value:  tensor(0.8228, device='cuda:7')
Pre fine tuning: 23497 MiB free out of 48676 MiB total
Found 25 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1148624 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:7 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:7 torch.float32
----------
epoch=0
train loss=3.64e-02	
11193 MiB free out of 48676 MiB total
----------
epoch=1
train loss=2.21e-02	
11193 MiB free out of 48676 MiB total
----------
epoch=2
train loss=1.34e-02	
11193 MiB free out of 48676 MiB total
----------
epoch=3
train loss=7.94e-03	
11193 MiB free out of 48676 MiB total
----------
epoch=4
train loss=5.44e-03	
11193 MiB free out of 48676 MiB total
post_fine_tuning 11193 MiB free out of 48676 MiB total
11193 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:7') bits per value:  tensor(0.8228, device='cuda:7')
11193 MiB free out of 48676 MiB total
layer: 19
done in 0:09:24.765667 overall time: 3:08:32.578660 estimated time left: 1:53:07.547196
after cast to cpu
37841 MiB free out of 48676 MiB total
Layer 20
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
total size:  202375168 total Megabytes:  tensor(272.3271, device='cuda:7') bits per value:  tensor(11.2882, device='cuda:7')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
weights.shape =  torch.Size([4096, 11008])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:7') bits per value:  tensor(0.8228, device='cuda:7')
Pre fine tuning: 23497 MiB free out of 48676 MiB total
Found 25 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1148624 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:7 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:7 torch.float32
----------
epoch=0
train loss=2.50e-02	
11193 MiB free out of 48676 MiB total
----------
epoch=1
train loss=1.56e-02	
11193 MiB free out of 48676 MiB total
----------
epoch=2
train loss=9.54e-03	
11193 MiB free out of 48676 MiB total
----------
epoch=3
train loss=5.97e-03	
11193 MiB free out of 48676 MiB total
----------
epoch=4
train loss=4.39e-03	
11193 MiB free out of 48676 MiB total
post_fine_tuning 11193 MiB free out of 48676 MiB total
11193 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:7') bits per value:  tensor(0.8228, device='cuda:7')
11193 MiB free out of 48676 MiB total
layer: 20
done in 0:09:24.860535 overall time: 3:17:57.716489 estimated time left: 1:43:41.661018
after cast to cpu
37841 MiB free out of 48676 MiB total
Layer 21
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
total size:  202375168 total Megabytes:  tensor(272.3271, device='cuda:7') bits per value:  tensor(11.2882, device='cuda:7')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
weights.shape =  torch.Size([4096, 11008])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:7') bits per value:  tensor(0.8228, device='cuda:7')
Pre fine tuning: 23497 MiB free out of 48676 MiB total
Found 25 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1148624 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:7 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:7 torch.float32
----------
epoch=0
train loss=4.95e-02	
11193 MiB free out of 48676 MiB total
----------
epoch=1
train loss=2.87e-02	
11193 MiB free out of 48676 MiB total
----------
epoch=2
train loss=1.53e-02	
11193 MiB free out of 48676 MiB total
----------
epoch=3
train loss=7.89e-03	
11193 MiB free out of 48676 MiB total
----------
epoch=4
train loss=5.17e-03	
11193 MiB free out of 48676 MiB total
post_fine_tuning 11193 MiB free out of 48676 MiB total
11193 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:7') bits per value:  tensor(0.8228, device='cuda:7')
11193 MiB free out of 48676 MiB total
layer: 21
done in 0:09:23.249806 overall time: 3:27:21.251635 estimated time left: 1:34:15.114380
after cast to cpu
37841 MiB free out of 48676 MiB total
Layer 22
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
total size:  202375168 total Megabytes:  tensor(272.3271, device='cuda:7') bits per value:  tensor(11.2882, device='cuda:7')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
weights.shape =  torch.Size([4096, 11008])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:7') bits per value:  tensor(0.8228, device='cuda:7')
Pre fine tuning: 23497 MiB free out of 48676 MiB total
Found 25 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1148624 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:7 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:7 torch.float32
----------
epoch=0
train loss=4.66e-02	
11193 MiB free out of 48676 MiB total
----------
epoch=1
train loss=2.53e-02	
11193 MiB free out of 48676 MiB total
----------
epoch=2
train loss=1.41e-02	
11193 MiB free out of 48676 MiB total
----------
epoch=3
train loss=7.63e-03	
11193 MiB free out of 48676 MiB total
----------
epoch=4
train loss=5.03e-03	
11193 MiB free out of 48676 MiB total
post_fine_tuning 11193 MiB free out of 48676 MiB total
11193 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:7') bits per value:  tensor(0.8228, device='cuda:7')
11193 MiB free out of 48676 MiB total
layer: 22
done in 0:09:24.228495 overall time: 3:36:45.766149 estimated time left: 1:24:49.212841
after cast to cpu
37841 MiB free out of 48676 MiB total
Layer 23
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
total size:  202375168 total Megabytes:  tensor(272.3271, device='cuda:7') bits per value:  tensor(11.2882, device='cuda:7')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
weights.shape =  torch.Size([4096, 11008])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:7') bits per value:  tensor(0.8228, device='cuda:7')
Pre fine tuning: 23497 MiB free out of 48676 MiB total
Found 25 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1148624 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:7 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:7 torch.float32
----------
epoch=0
train loss=4.15e-02	
11193 MiB free out of 48676 MiB total
----------
epoch=1
train loss=2.73e-02	
11193 MiB free out of 48676 MiB total
----------
epoch=2
train loss=1.65e-02	
11193 MiB free out of 48676 MiB total
----------
epoch=3
train loss=9.41e-03	
11193 MiB free out of 48676 MiB total
----------
epoch=4
train loss=5.90e-03	
11193 MiB free out of 48676 MiB total
post_fine_tuning 11193 MiB free out of 48676 MiB total
11193 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:7') bits per value:  tensor(0.8228, device='cuda:7')
11193 MiB free out of 48676 MiB total
layer: 23
done in 0:09:24.779896 overall time: 3:46:10.821939 estimated time left: 1:15:23.607313
after cast to cpu
37841 MiB free out of 48676 MiB total
Layer 24
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
total size:  202375168 total Megabytes:  tensor(272.3271, device='cuda:7') bits per value:  tensor(11.2882, device='cuda:7')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
weights.shape =  torch.Size([4096, 11008])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:7') bits per value:  tensor(0.8228, device='cuda:7')
Pre fine tuning: 23497 MiB free out of 48676 MiB total
Found 25 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1148624 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:7 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:7 torch.float32
----------
epoch=0
train loss=3.96e-02	
11193 MiB free out of 48676 MiB total
----------
epoch=1
train loss=2.43e-02	
11193 MiB free out of 48676 MiB total
----------
epoch=2
train loss=1.41e-02	
11193 MiB free out of 48676 MiB total
----------
epoch=3
train loss=7.76e-03	
11193 MiB free out of 48676 MiB total
----------
epoch=4
train loss=4.94e-03	
11193 MiB free out of 48676 MiB total
post_fine_tuning 11193 MiB free out of 48676 MiB total
11193 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:7') bits per value:  tensor(0.8228, device='cuda:7')
11193 MiB free out of 48676 MiB total
layer: 24
done in 0:09:24.706952 overall time: 3:55:35.797178 estimated time left: 1:05:58.023210
after cast to cpu
37841 MiB free out of 48676 MiB total
Layer 25
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
total size:  202375168 total Megabytes:  tensor(272.3271, device='cuda:7') bits per value:  tensor(11.2882, device='cuda:7')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
weights.shape =  torch.Size([4096, 11008])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:7') bits per value:  tensor(0.8228, device='cuda:7')
Pre fine tuning: 23497 MiB free out of 48676 MiB total
Found 25 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1148624 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:7 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:7 torch.float32
----------
epoch=0
train loss=5.85e-02	
11193 MiB free out of 48676 MiB total
----------
epoch=1
train loss=3.75e-02	
11193 MiB free out of 48676 MiB total
----------
epoch=2
train loss=2.18e-02	
11193 MiB free out of 48676 MiB total
----------
epoch=3
train loss=1.17e-02	
11193 MiB free out of 48676 MiB total
----------
epoch=4
train loss=6.89e-03	
11193 MiB free out of 48676 MiB total
post_fine_tuning 11193 MiB free out of 48676 MiB total
11193 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:7') bits per value:  tensor(0.8228, device='cuda:7')
11193 MiB free out of 48676 MiB total
layer: 25
done in 0:09:24.354057 overall time: 4:05:00.304137 estimated time left: 0:56:32.377878
after cast to cpu
37841 MiB free out of 48676 MiB total
Layer 26
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
total size:  202375168 total Megabytes:  tensor(272.3271, device='cuda:7') bits per value:  tensor(11.2882, device='cuda:7')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
weights.shape =  torch.Size([4096, 11008])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:7') bits per value:  tensor(0.8228, device='cuda:7')
Pre fine tuning: 23497 MiB free out of 48676 MiB total
Found 25 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1148624 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:7 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:7 torch.float32
----------
epoch=0
train loss=7.86e-02	
11193 MiB free out of 48676 MiB total
----------
epoch=1
train loss=4.94e-02	
11193 MiB free out of 48676 MiB total
----------
epoch=2
train loss=3.00e-02	
11193 MiB free out of 48676 MiB total
----------
epoch=3
train loss=1.63e-02	
11193 MiB free out of 48676 MiB total
----------
epoch=4
train loss=8.87e-03	
11193 MiB free out of 48676 MiB total
post_fine_tuning 11193 MiB free out of 48676 MiB total
11193 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:7') bits per value:  tensor(0.8228, device='cuda:7')
11193 MiB free out of 48676 MiB total
layer: 26
done in 0:09:24.385498 overall time: 4:14:24.962483 estimated time left: 0:47:06.844904
after cast to cpu
37841 MiB free out of 48676 MiB total
Layer 27
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
total size:  202375168 total Megabytes:  tensor(272.3271, device='cuda:7') bits per value:  tensor(11.2882, device='cuda:7')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
weights.shape =  torch.Size([4096, 11008])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:7') bits per value:  tensor(0.8228, device='cuda:7')
Pre fine tuning: 23497 MiB free out of 48676 MiB total
Found 25 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1148624 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:7 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:7 torch.float32
----------
epoch=0
train loss=3.59e-02	
11193 MiB free out of 48676 MiB total
----------
epoch=1
train loss=2.24e-02	
11193 MiB free out of 48676 MiB total
----------
epoch=2
train loss=1.30e-02	
11193 MiB free out of 48676 MiB total
----------
epoch=3
train loss=7.35e-03	
11193 MiB free out of 48676 MiB total
----------
epoch=4
train loss=4.76e-03	
11193 MiB free out of 48676 MiB total
post_fine_tuning 11193 MiB free out of 48676 MiB total
11193 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:7') bits per value:  tensor(0.8228, device='cuda:7')
11193 MiB free out of 48676 MiB total
layer: 27
done in 0:09:23.119010 overall time: 4:23:48.235793 estimated time left: 0:37:41.176542
after cast to cpu
37841 MiB free out of 48676 MiB total
Layer 28
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
total size:  202375168 total Megabytes:  tensor(272.3271, device='cuda:7') bits per value:  tensor(11.2882, device='cuda:7')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
weights.shape =  torch.Size([4096, 11008])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:7') bits per value:  tensor(0.8228, device='cuda:7')
Pre fine tuning: 23497 MiB free out of 48676 MiB total
Found 25 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1148624 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:7 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:7 torch.float32
----------
epoch=0
train loss=6.25e-02	
11193 MiB free out of 48676 MiB total
----------
epoch=1
train loss=3.70e-02	
11193 MiB free out of 48676 MiB total
----------
epoch=2
train loss=2.12e-02	
11193 MiB free out of 48676 MiB total
----------
epoch=3
train loss=1.11e-02	
11193 MiB free out of 48676 MiB total
----------
epoch=4
train loss=6.33e-03	
11193 MiB free out of 48676 MiB total
post_fine_tuning 11193 MiB free out of 48676 MiB total
11193 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:7') bits per value:  tensor(0.8228, device='cuda:7')
11193 MiB free out of 48676 MiB total
layer: 28
done in 0:09:24.272335 overall time: 4:33:12.658541 estimated time left: 0:28:15.792263
after cast to cpu
37841 MiB free out of 48676 MiB total
Layer 29
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
total size:  202375168 total Megabytes:  tensor(272.3271, device='cuda:7') bits per value:  tensor(11.2882, device='cuda:7')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
weights.shape =  torch.Size([4096, 11008])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:7') bits per value:  tensor(0.8228, device='cuda:7')
Pre fine tuning: 23497 MiB free out of 48676 MiB total
Found 25 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1148624 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:7 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:7 torch.float32
----------
epoch=0
train loss=1.00e-01	
11193 MiB free out of 48676 MiB total
----------
epoch=1
train loss=6.61e-02	
11193 MiB free out of 48676 MiB total
----------
epoch=2
train loss=4.22e-02	
11193 MiB free out of 48676 MiB total
----------
epoch=3
train loss=2.48e-02	
11193 MiB free out of 48676 MiB total
----------
epoch=4
train loss=1.52e-02	
11193 MiB free out of 48676 MiB total
post_fine_tuning 11193 MiB free out of 48676 MiB total
11193 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:7') bits per value:  tensor(0.8228, device='cuda:7')
11193 MiB free out of 48676 MiB total
layer: 29
done in 0:09:23.917765 overall time: 4:42:36.726665 estimated time left: 0:18:50.448444
after cast to cpu
37841 MiB free out of 48676 MiB total
Layer 30
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
total size:  202375168 total Megabytes:  tensor(272.3271, device='cuda:7') bits per value:  tensor(11.2882, device='cuda:7')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
weights.shape =  torch.Size([4096, 11008])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:7') bits per value:  tensor(0.8228, device='cuda:7')
Pre fine tuning: 23497 MiB free out of 48676 MiB total
Found 25 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1148624 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:7 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:7 torch.float32
----------
epoch=0
train loss=7.26e-01	
11193 MiB free out of 48676 MiB total
----------
epoch=1
train loss=6.51e-01	
11193 MiB free out of 48676 MiB total
----------
epoch=2
train loss=5.91e-01	
11193 MiB free out of 48676 MiB total
----------
epoch=3
train loss=5.40e-01	
11193 MiB free out of 48676 MiB total
----------
epoch=4
train loss=4.82e-01	
11193 MiB free out of 48676 MiB total
post_fine_tuning 11193 MiB free out of 48676 MiB total
11193 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:7') bits per value:  tensor(0.8228, device='cuda:7')
11193 MiB free out of 48676 MiB total
layer: 30
done in 0:09:23.654256 overall time: 4:52:00.531854 estimated time left: 0:09:25.178447
after cast to cpu
37841 MiB free out of 48676 MiB total
Layer 31
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
total size:  202375168 total Megabytes:  tensor(272.3271, device='cuda:7') bits per value:  tensor(11.2882, device='cuda:7')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
weights.shape =  torch.Size([4096, 11008])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:7') bits per value:  tensor(0.8228, device='cuda:7')
Pre fine tuning: 23497 MiB free out of 48676 MiB total
Found 25 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.sparse_weights1', 'self_attn.q_proj.sparse_weights2', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.sparse_weights1', 'self_attn.k_proj.sparse_weights2', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.sparse_weights1', 'self_attn.v_proj.sparse_weights2', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.sparse_weights1', 'self_attn.o_proj.sparse_weights2', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 1148624 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:7 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:7 torch.float32
----------
epoch=0
train loss=1.46e+00	
11193 MiB free out of 48676 MiB total
----------
epoch=1
train loss=3.90e-01	
11193 MiB free out of 48676 MiB total
----------
epoch=2
train loss=2.28e-01	
11193 MiB free out of 48676 MiB total
----------
epoch=3
train loss=1.71e-01	
11193 MiB free out of 48676 MiB total
----------
epoch=4
train loss=1.41e-01	
11193 MiB free out of 48676 MiB total
post_fine_tuning 11193 MiB free out of 48676 MiB total
11193 MiB free out of 48676 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:7') bits per value:  tensor(0.8228, device='cuda:7')
11193 MiB free out of 48676 MiB total
layer: 31
done in 0:09:25.525060 overall time: 5:01:26.207293 estimated time left: 0:00:00
after cast to cpu
37841 MiB free out of 48676 MiB total
Total bits: tensor(10656743424, device='cuda:7') Total params: 12952010752
average bits per value: tensor(0.8228, device='cuda:7')
18088.181819200516
Loading tokenizer for meta-llama/Llama-2-7b-hf
/home/lliu/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Dataset: wikitext2
Evaluating ...
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
Perplexity: 6049.253906
