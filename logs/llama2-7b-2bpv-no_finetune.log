2024-12-03 11:16:44.972171: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-12-03 11:16:44.988524: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-12-03 11:16:44.993456: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-12-03 11:16:45.006447: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-12-03 11:16:46.187586: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  4.57it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  5.02it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  4.94it/s]
Model loaded. LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
Starting...
getting inputs:   0%|          | 0/128 [00:00<?, ?it/s]getting inputs:   1%|          | 1/128 [00:00<01:45,  1.20it/s]getting inputs:   3%|▎         | 4/128 [00:00<00:22,  5.40it/s]getting inputs:   6%|▋         | 8/128 [00:01<00:11, 10.87it/s]getting inputs:   9%|▉         | 12/128 [00:01<00:07, 15.80it/s]getting inputs:  12%|█▏        | 15/128 [00:01<00:06, 18.69it/s]getting inputs:  14%|█▍        | 18/128 [00:01<00:06, 17.75it/s]getting inputs:  17%|█▋        | 22/128 [00:01<00:04, 21.64it/s]getting inputs:  20%|█▉        | 25/128 [00:01<00:04, 23.18it/s]getting inputs:  23%|██▎       | 29/128 [00:01<00:03, 25.91it/s]getting inputs:  25%|██▌       | 32/128 [00:01<00:03, 26.81it/s]getting inputs:  28%|██▊       | 36/128 [00:02<00:03, 28.11it/s]getting inputs:  31%|███▏      | 40/128 [00:02<00:02, 29.51it/s]getting inputs:  32%|███▏      | 41/128 [00:02<00:03, 22.58it/s]getting inputs:  35%|███▌      | 45/128 [00:02<00:03, 25.24it/s]getting inputs:  38%|███▊      | 48/128 [00:02<00:03, 26.42it/s]getting inputs:  41%|████      | 52/128 [00:02<00:02, 27.67it/s]getting inputs:  44%|████▍     | 56/128 [00:02<00:02, 28.52it/s]getting inputs:  47%|████▋     | 60/128 [00:02<00:02, 28.93it/s]getting inputs:  48%|████▊     | 62/128 [00:03<00:02, 23.17it/s]getting inputs:  52%|█████▏    | 66/128 [00:03<00:02, 25.52it/s]getting inputs:  55%|█████▍    | 70/128 [00:03<00:02, 26.97it/s]getting inputs:  58%|█████▊    | 74/128 [00:03<00:01, 28.07it/s]getting inputs:  61%|██████    | 78/128 [00:03<00:01, 28.74it/s]getting inputs:  64%|██████▍   | 82/128 [00:03<00:01, 29.19it/s]getting inputs:  66%|██████▌   | 84/128 [00:03<00:01, 23.43it/s]getting inputs:  69%|██████▉   | 88/128 [00:04<00:01, 25.21it/s]getting inputs:  72%|███████▏  | 92/128 [00:04<00:01, 26.42it/s]getting inputs:  75%|███████▌  | 96/128 [00:04<00:01, 27.26it/s]getting inputs:  78%|███████▊  | 100/128 [00:04<00:01, 27.96it/s]getting inputs:  81%|████████▏ | 104/128 [00:04<00:00, 28.47it/s]getting inputs:  84%|████████▍ | 108/128 [00:04<00:00, 28.95it/s]getting inputs:  88%|████████▊ | 112/128 [00:04<00:00, 29.09it/s]getting inputs:  91%|█████████ | 116/128 [00:04<00:00, 29.23it/s]getting inputs:  94%|█████████▍| 120/128 [00:05<00:00, 29.22it/s]getting inputs:  97%|█████████▋| 124/128 [00:05<00:00, 29.35it/s]getting inputs:  99%|█████████▉| 127/128 [00:05<00:00, 27.10it/s]getting inputs: 100%|██████████| 128/128 [00:05<00:00, 23.36it/s]
48323 MiB free out of 48676 MiB total
Ready.
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:27,  1.14it/s]Inference:   6%|▋         | 2/32 [00:01<00:22,  1.30it/s]Inference:   9%|▉         | 3/32 [00:02<00:20,  1.41it/s]Inference:  12%|█▎        | 4/32 [00:02<00:20,  1.39it/s]Inference:  16%|█▌        | 5/32 [00:03<00:18,  1.43it/s]Inference:  19%|█▉        | 6/32 [00:04<00:16,  1.57it/s]Inference:  22%|██▏       | 7/32 [00:04<00:16,  1.56it/s]Inference:  25%|██▌       | 8/32 [00:05<00:15,  1.51it/s]Inference:  28%|██▊       | 9/32 [00:06<00:15,  1.53it/s]Inference:  31%|███▏      | 10/32 [00:06<00:14,  1.49it/s]Inference:  34%|███▍      | 11/32 [00:07<00:14,  1.48it/s]Inference:  38%|███▊      | 12/32 [00:08<00:13,  1.46it/s]Inference:  41%|████      | 13/32 [00:09<00:14,  1.34it/s]Inference:  44%|████▍     | 14/32 [00:09<00:12,  1.45it/s]Inference:  47%|████▋     | 15/32 [00:10<00:11,  1.44it/s]Inference:  50%|█████     | 16/32 [00:11<00:11,  1.45it/s]Inference:  53%|█████▎    | 17/32 [00:11<00:10,  1.44it/s]Inference:  56%|█████▋    | 18/32 [00:12<00:09,  1.54it/s]Inference:  59%|█████▉    | 19/32 [00:12<00:08,  1.57it/s]Inference:  62%|██████▎   | 20/32 [00:13<00:07,  1.52it/s]Inference:  66%|██████▌   | 21/32 [00:14<00:07,  1.40it/s]Inference:  69%|██████▉   | 22/32 [00:14<00:06,  1.51it/s]Inference:  72%|███████▏  | 23/32 [00:15<00:05,  1.59it/s]Inference:  75%|███████▌  | 24/32 [00:16<00:04,  1.60it/s]Inference:  78%|███████▊  | 25/32 [00:16<00:04,  1.49it/s]Inference:  81%|████████▏ | 26/32 [00:17<00:04,  1.47it/s]Inference:  84%|████████▍ | 27/32 [00:18<00:03,  1.50it/s]Inference:  88%|████████▊ | 28/32 [00:18<00:02,  1.48it/s]Inference:  91%|█████████ | 29/32 [00:19<00:02,  1.42it/s]Inference:  94%|█████████▍| 30/32 [00:20<00:01,  1.34it/s]Inference:  97%|█████████▋| 31/32 [00:21<00:00,  1.33it/s]Inference: 100%|██████████| 32/32 [00:22<00:00,  1.28it/s]Inference: 100%|██████████| 32/32 [00:22<00:00,  1.44it/s]
layer0: self_attn.q_proj
256
val_hessian None
iter 0, train loss 5.630727767944336, val loss None, lr 0.001
iter 10, train loss 5.298004627227783, val loss None, lr 0.001
iter 20, train loss 5.439190864562988, val loss None, lr 0.001
iter 30, train loss 4.801451683044434, val loss None, lr 0.001
iter 40, train loss 4.345239639282227, val loss None, lr 0.001
iter 50, train loss 3.960052728652954, val loss None, lr 0.001
iter 60, train loss 3.872436046600342, val loss None, lr 0.001
iter 70, train loss 3.9522454738616943, val loss None, lr 0.001
iter 80, train loss 3.600914478302002, val loss None, lr 0.001
iter 90, train loss 3.586517095565796, val loss None, lr 0.001
best loss 3.55273699760437
layer0: self_attn.k_proj
256
val_hessian None
iter 0, train loss 4.003661632537842, val loss None, lr 0.001
iter 10, train loss 3.3744113445281982, val loss None, lr 0.001
iter 20, train loss 3.385298252105713, val loss None, lr 0.001
iter 30, train loss 3.071293830871582, val loss None, lr 0.001
iter 40, train loss 3.1148600578308105, val loss None, lr 0.001
iter 50, train loss 3.1034202575683594, val loss None, lr 0.001
iter 60, train loss 2.8815836906433105, val loss None, lr 0.001
iter 70, train loss 2.9677648544311523, val loss None, lr 0.001
iter 80, train loss 2.750091791152954, val loss None, lr 0.001
iter 90, train loss 2.746769905090332, val loss None, lr 0.001
best loss 2.611372947692871
layer0: self_attn.v_proj
256
val_hessian None
iter 0, train loss 0.38066941499710083, val loss None, lr 0.001
iter 10, train loss 0.323863685131073, val loss None, lr 0.001
iter 20, train loss 0.31036168336868286, val loss None, lr 0.001
iter 30, train loss 0.2890794277191162, val loss None, lr 0.001
iter 40, train loss 0.2805814743041992, val loss None, lr 0.001
iter 50, train loss 0.27681422233581543, val loss None, lr 0.001
iter 60, train loss 0.2714243531227112, val loss None, lr 0.001
iter 70, train loss 0.2658684253692627, val loss None, lr 0.001
iter 80, train loss 0.26798516511917114, val loss None, lr 0.001
iter 90, train loss 0.26219743490219116, val loss None, lr 0.001
best loss 0.25882863998413086
layer0: self_attn.o_proj
256
val_hessian None
iter 0, train loss 0.04648122936487198, val loss None, lr 0.001
iter 10, train loss 0.037801388651132584, val loss None, lr 0.001
iter 20, train loss 0.03513355553150177, val loss None, lr 0.001
iter 30, train loss 0.03344627469778061, val loss None, lr 0.001
iter 40, train loss 0.03152351826429367, val loss None, lr 0.001
iter 50, train loss 0.030765661969780922, val loss None, lr 0.001
iter 60, train loss 0.029814690351486206, val loss None, lr 0.001
iter 70, train loss 0.029256612062454224, val loss None, lr 0.001
iter 80, train loss 0.02937767654657364, val loss None, lr 0.001
iter 90, train loss 0.0285168644040823, val loss None, lr 0.001
best loss 0.028232302516698837
layer0: mlp.gate_proj
256
val_hessian None
iter 0, train loss 15.130496978759766, val loss None, lr 0.001
iter 10, train loss 16.063495635986328, val loss None, lr 0.001
iter 20, train loss 15.158061981201172, val loss None, lr 0.001
iter 30, train loss 15.087504386901855, val loss None, lr 0.001
iter 40, train loss 15.104338645935059, val loss None, lr 0.001
iter 50, train loss 15.121112823486328, val loss None, lr 0.001
iter 60, train loss 15.090415000915527, val loss None, lr 0.001
iter 70, train loss 15.12497615814209, val loss None, lr 0.001
iter 80, train loss 15.19987964630127, val loss None, lr 0.001
iter 90, train loss 15.1961669921875, val loss None, lr 0.001
best loss 14.286556243896484
layer0: mlp.up_proj
256
val_hessian None
iter 0, train loss 13.701522827148438, val loss None, lr 0.001
iter 10, train loss 14.56661319732666, val loss None, lr 0.001
iter 20, train loss 13.761611938476562, val loss None, lr 0.001
iter 30, train loss 13.747950553894043, val loss None, lr 0.001
iter 40, train loss 13.697179794311523, val loss None, lr 0.001
iter 50, train loss 13.717815399169922, val loss None, lr 0.001
iter 60, train loss 13.771162033081055, val loss None, lr 0.001
iter 70, train loss 13.772804260253906, val loss None, lr 0.001
iter 80, train loss 13.774887084960938, val loss None, lr 0.001
iter 90, train loss 13.776148796081543, val loss None, lr 0.001
best loss 12.97783088684082
layer0: mlp.down_proj
256
val_hessian None
iter 0, train loss 0.04737897217273712, val loss None, lr 0.001
iter 10, train loss 0.050926573574543, val loss None, lr 0.001
iter 20, train loss 0.04903682321310043, val loss None, lr 0.001
iter 30, train loss 0.048572126775979996, val loss None, lr 0.001
iter 40, train loss 0.047439418733119965, val loss None, lr 0.001
iter 50, train loss 0.046857886016368866, val loss None, lr 0.001
iter 60, train loss 0.046279799193143845, val loss None, lr 0.001
iter 70, train loss 0.04604697600007057, val loss None, lr 0.001
iter 80, train loss 0.04585379362106323, val loss None, lr 0.001
iter 90, train loss 0.04533783346414566, val loss None, lr 0.001
best loss 0.044252414256334305
48323 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:10,  2.94it/s]Inference:   6%|▋         | 2/32 [00:00<00:09,  3.02it/s]Inference:   9%|▉         | 3/32 [00:00<00:08,  3.32it/s]Inference:  12%|█▎        | 4/32 [00:01<00:08,  3.24it/s]Inference:  16%|█▌        | 5/32 [00:01<00:08,  3.09it/s]Inference:  19%|█▉        | 6/32 [00:01<00:08,  3.08it/s]Inference:  22%|██▏       | 7/32 [00:02<00:08,  2.80it/s]Inference:  25%|██▌       | 8/32 [00:02<00:07,  3.12it/s]Inference:  28%|██▊       | 9/32 [00:02<00:07,  3.02it/s]Inference:  31%|███▏      | 10/32 [00:03<00:07,  2.93it/s]Inference:  34%|███▍      | 11/32 [00:03<00:06,  3.17it/s]Inference:  38%|███▊      | 12/32 [00:03<00:06,  3.12it/s]Inference:  41%|████      | 13/32 [00:04<00:06,  3.09it/s]Inference:  44%|████▍     | 14/32 [00:04<00:05,  3.23it/s]Inference:  47%|████▋     | 15/32 [00:04<00:05,  3.12it/s]Inference:  50%|█████     | 16/32 [00:05<00:05,  3.05it/s]Inference:  53%|█████▎    | 17/32 [00:05<00:05,  2.96it/s]Inference:  56%|█████▋    | 18/32 [00:05<00:04,  2.97it/s]Inference:  59%|█████▉    | 19/32 [00:06<00:04,  3.17it/s]Inference:  62%|██████▎   | 20/32 [00:06<00:03,  3.09it/s]Inference:  66%|██████▌   | 21/32 [00:06<00:03,  3.03it/s]Inference:  69%|██████▉   | 22/32 [00:07<00:03,  3.20it/s]Inference:  72%|███████▏  | 23/32 [00:07<00:02,  3.39it/s]Inference:  75%|███████▌  | 24/32 [00:07<00:02,  3.21it/s]Inference:  78%|███████▊  | 25/32 [00:08<00:02,  3.12it/s]Inference:  81%|████████▏ | 26/32 [00:08<00:02,  2.83it/s]Inference:  84%|████████▍ | 27/32 [00:08<00:01,  3.02it/s]Inference:  88%|████████▊ | 28/32 [00:09<00:01,  2.96it/s]Inference:  91%|█████████ | 29/32 [00:09<00:01,  2.94it/s]Inference:  94%|█████████▍| 30/32 [00:09<00:00,  3.13it/s]Inference:  97%|█████████▋| 31/32 [00:10<00:00,  3.08it/s]Inference: 100%|██████████| 32/32 [00:10<00:00,  3.01it/s]Inference: 100%|██████████| 32/32 [00:10<00:00,  3.07it/s]
42485 MiB free out of 48676 MiB total
Saved layer 0 to ./models/meta-llama/Llama-2-7b-hf/2bpv/128/quantized/layer_0.pt
after cast to cpu
46465 MiB free out of 48676 MiB total
Done with layer 0 total_time elapsed: 494 estimated time left: 15323
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:18,  1.70it/s]Inference:   6%|▋         | 2/32 [00:01<00:20,  1.45it/s]Inference:   9%|▉         | 3/32 [00:02<00:21,  1.37it/s]Inference:  12%|█▎        | 4/32 [00:02<00:20,  1.34it/s]Inference:  16%|█▌        | 5/32 [00:03<00:21,  1.26it/s]Inference:  19%|█▉        | 6/32 [00:04<00:21,  1.22it/s]Inference:  22%|██▏       | 7/32 [00:05<00:19,  1.28it/s]Inference:  25%|██▌       | 8/32 [00:06<00:18,  1.29it/s]Inference:  28%|██▊       | 9/32 [00:06<00:17,  1.33it/s]Inference:  31%|███▏      | 10/32 [00:07<00:16,  1.37it/s]Inference:  34%|███▍      | 11/32 [00:08<00:15,  1.33it/s]Inference:  38%|███▊      | 12/32 [00:09<00:15,  1.32it/s]Inference:  41%|████      | 13/32 [00:09<00:14,  1.31it/s]Inference:  44%|████▍     | 14/32 [00:10<00:13,  1.29it/s]Inference:  47%|████▋     | 15/32 [00:11<00:13,  1.29it/s]Inference:  50%|█████     | 16/32 [00:12<00:11,  1.33it/s]Inference:  53%|█████▎    | 17/32 [00:12<00:10,  1.37it/s]Inference:  56%|█████▋    | 18/32 [00:13<00:10,  1.34it/s]Inference:  59%|█████▉    | 19/32 [00:14<00:09,  1.33it/s]Inference:  62%|██████▎   | 20/32 [00:15<00:09,  1.32it/s]Inference:  66%|██████▌   | 21/32 [00:15<00:08,  1.30it/s]Inference:  69%|██████▉   | 22/32 [00:16<00:08,  1.24it/s]Inference:  72%|███████▏  | 23/32 [00:17<00:07,  1.29it/s]Inference:  75%|███████▌  | 24/32 [00:18<00:06,  1.33it/s]Inference:  78%|███████▊  | 25/32 [00:18<00:05,  1.36it/s]Inference:  81%|████████▏ | 26/32 [00:19<00:04,  1.39it/s]Inference:  84%|████████▍ | 27/32 [00:20<00:03,  1.36it/s]Inference:  88%|████████▊ | 28/32 [00:21<00:02,  1.39it/s]Inference:  91%|█████████ | 29/32 [00:21<00:02,  1.45it/s]Inference:  94%|█████████▍| 30/32 [00:22<00:01,  1.45it/s]Inference:  97%|█████████▋| 31/32 [00:23<00:00,  1.45it/s]Inference: 100%|██████████| 32/32 [00:23<00:00,  1.46it/s]Inference: 100%|██████████| 32/32 [00:23<00:00,  1.35it/s]
layer1: self_attn.q_proj
256
val_hessian None
iter 0, train loss 67.15428161621094, val loss None, lr 0.001
iter 10, train loss 55.192237854003906, val loss None, lr 0.001
iter 20, train loss 65.4830093383789, val loss None, lr 0.001
iter 30, train loss 59.0758056640625, val loss None, lr 0.001
iter 40, train loss 56.10200500488281, val loss None, lr 0.001
iter 50, train loss 56.4341926574707, val loss None, lr 0.001
iter 60, train loss 54.233421325683594, val loss None, lr 0.001
iter 70, train loss 54.20838165283203, val loss None, lr 0.001
iter 80, train loss 52.119163513183594, val loss None, lr 0.001
iter 90, train loss 51.59923553466797, val loss None, lr 0.001
best loss 51.31187438964844
layer1: self_attn.k_proj
256
val_hessian None
iter 0, train loss 68.97442626953125, val loss None, lr 0.001
iter 10, train loss 59.10984802246094, val loss None, lr 0.001
iter 20, train loss 55.25965118408203, val loss None, lr 0.001
iter 30, train loss 55.86986541748047, val loss None, lr 0.001
iter 40, train loss 55.08503723144531, val loss None, lr 0.001
iter 50, train loss 58.464210510253906, val loss None, lr 0.001
iter 60, train loss 57.233787536621094, val loss None, lr 0.001
iter 70, train loss 54.42106628417969, val loss None, lr 0.001
iter 80, train loss 53.318050384521484, val loss None, lr 0.001
iter 90, train loss 53.24032211303711, val loss None, lr 0.001
best loss 53.093780517578125
layer1: self_attn.v_proj
256
val_hessian None
iter 0, train loss 3.547846555709839, val loss None, lr 0.001
iter 10, train loss 3.6655728816986084, val loss None, lr 0.001
iter 20, train loss 3.337527275085449, val loss None, lr 0.001
iter 30, train loss 3.2425241470336914, val loss None, lr 0.001
iter 40, train loss 3.1833271980285645, val loss None, lr 0.001
iter 50, train loss 3.1345725059509277, val loss None, lr 0.001
iter 60, train loss 3.125753402709961, val loss None, lr 0.001
iter 70, train loss 3.094261407852173, val loss None, lr 0.001
iter 80, train loss 3.0708611011505127, val loss None, lr 0.001
iter 90, train loss 3.068852424621582, val loss None, lr 0.001
best loss 3.0528054237365723
layer1: self_attn.o_proj
256
val_hessian None
iter 0, train loss 0.3364424705505371, val loss None, lr 0.001
iter 10, train loss 0.32505887746810913, val loss None, lr 0.001
iter 20, train loss 0.3123301863670349, val loss None, lr 0.001
iter 30, train loss 0.29934951663017273, val loss None, lr 0.001
iter 40, train loss 0.2921667695045471, val loss None, lr 0.001
iter 50, train loss 0.28445106744766235, val loss None, lr 0.001
iter 60, train loss 0.28954678773880005, val loss None, lr 0.001
iter 70, train loss 0.2827739715576172, val loss None, lr 0.001
iter 80, train loss 0.28435325622558594, val loss None, lr 0.001
iter 90, train loss 0.2832382321357727, val loss None, lr 0.001
best loss 0.28139376640319824
layer1: mlp.gate_proj
256
val_hessian None
iter 0, train loss 62.174407958984375, val loss None, lr 0.001
iter 10, train loss 68.53585815429688, val loss None, lr 0.001
iter 20, train loss 65.37348175048828, val loss None, lr 0.001
iter 30, train loss 65.959716796875, val loss None, lr 0.001
iter 40, train loss 65.94505310058594, val loss None, lr 0.001
iter 50, train loss 65.8071517944336, val loss None, lr 0.001
iter 60, train loss 65.71926879882812, val loss None, lr 0.001
iter 70, train loss 65.1777572631836, val loss None, lr 0.001
iter 80, train loss 65.08252716064453, val loss None, lr 0.001
iter 90, train loss 64.7030029296875, val loss None, lr 0.001
best loss 58.350433349609375
layer1: mlp.up_proj
256
val_hessian None
iter 0, train loss 50.604469299316406, val loss None, lr 0.001
iter 10, train loss 52.32768249511719, val loss None, lr 0.001
iter 20, train loss 51.737953186035156, val loss None, lr 0.001
iter 30, train loss 51.96686553955078, val loss None, lr 0.001
iter 40, train loss 52.06337356567383, val loss None, lr 0.001
iter 50, train loss 52.18192672729492, val loss None, lr 0.001
iter 60, train loss 52.21543884277344, val loss None, lr 0.001
iter 70, train loss 52.284446716308594, val loss None, lr 0.001
iter 80, train loss 52.39141845703125, val loss None, lr 0.001
iter 90, train loss 52.399070739746094, val loss None, lr 0.001
best loss 49.95830535888672
layer1: mlp.down_proj
256
val_hessian None
iter 0, train loss 0.3628951907157898, val loss None, lr 0.001
iter 10, train loss 1.5186983346939087, val loss None, lr 0.001
iter 20, train loss 1.3098626136779785, val loss None, lr 0.001
iter 30, train loss 1.4109439849853516, val loss None, lr 0.001
iter 40, train loss 2.1997430324554443, val loss None, lr 0.001
iter 50, train loss 18.658321380615234, val loss None, lr 0.001
iter 60, train loss 2.2937119007110596, val loss None, lr 0.001
iter 70, train loss 6.347288131713867, val loss None, lr 0.001
iter 80, train loss 3.552591562271118, val loss None, lr 0.001
iter 90, train loss 2.145350933074951, val loss None, lr 0.001
best loss 0.3628951907157898
46465 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:11,  2.82it/s]Inference:   6%|▋         | 2/32 [00:00<00:10,  2.97it/s]Inference:   9%|▉         | 3/32 [00:01<00:09,  3.01it/s]Inference:  12%|█▎        | 4/32 [00:01<00:09,  3.01it/s]Inference:  16%|█▌        | 5/32 [00:01<00:09,  2.94it/s]Inference:  19%|█▉        | 6/32 [00:02<00:09,  2.73it/s]Inference:  22%|██▏       | 7/32 [00:02<00:08,  2.84it/s]Inference:  25%|██▌       | 8/32 [00:02<00:08,  2.89it/s]Inference:  28%|██▊       | 9/32 [00:03<00:07,  2.91it/s]Inference:  31%|███▏      | 10/32 [00:03<00:07,  2.86it/s]Inference:  34%|███▍      | 11/32 [00:03<00:07,  2.85it/s]Inference:  38%|███▊      | 12/32 [00:04<00:06,  3.07it/s]Inference:  41%|████      | 13/32 [00:04<00:06,  3.06it/s]Inference:  44%|████▍     | 14/32 [00:04<00:06,  2.79it/s]Inference:  47%|████▋     | 15/32 [00:05<00:06,  2.82it/s]Inference:  50%|█████     | 16/32 [00:05<00:05,  3.03it/s]Inference:  53%|█████▎    | 17/32 [00:05<00:04,  3.03it/s]Inference:  56%|█████▋    | 18/32 [00:06<00:04,  2.98it/s]Inference:  59%|█████▉    | 19/32 [00:06<00:04,  2.98it/s]Inference:  62%|██████▎   | 20/32 [00:06<00:03,  3.23it/s]Inference:  66%|██████▌   | 21/32 [00:07<00:03,  3.17it/s]Inference:  69%|██████▉   | 22/32 [00:07<00:03,  2.86it/s]Inference:  72%|███████▏  | 23/32 [00:07<00:02,  3.08it/s]Inference:  75%|███████▌  | 24/32 [00:08<00:02,  2.98it/s]Inference:  78%|███████▊  | 25/32 [00:08<00:02,  2.93it/s]Inference:  81%|████████▏ | 26/32 [00:08<00:01,  3.12it/s]Inference:  84%|████████▍ | 27/32 [00:08<00:01,  3.36it/s]Inference:  88%|████████▊ | 28/32 [00:09<00:01,  3.19it/s]Inference:  91%|█████████ | 29/32 [00:09<00:00,  3.36it/s]Inference:  94%|█████████▍| 30/32 [00:10<00:00,  2.99it/s]Inference:  97%|█████████▋| 31/32 [00:10<00:00,  3.27it/s]Inference: 100%|██████████| 32/32 [00:10<00:00,  3.20it/s]Inference: 100%|██████████| 32/32 [00:10<00:00,  3.02it/s]
41175 MiB free out of 48676 MiB total
Saved layer 1 to ./models/meta-llama/Llama-2-7b-hf/2bpv/128/quantized/layer_1.pt
after cast to cpu
45177 MiB free out of 48676 MiB total
Done with layer 1 total_time elapsed: 1005 estimated time left: 15080
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:22,  1.40it/s]Inference:   6%|▋         | 2/32 [00:01<00:21,  1.40it/s]Inference:   9%|▉         | 3/32 [00:02<00:19,  1.48it/s]Inference:  12%|█▎        | 4/32 [00:02<00:19,  1.44it/s]Inference:  16%|█▌        | 5/32 [00:03<00:18,  1.43it/s]Inference:  19%|█▉        | 6/32 [00:04<00:18,  1.37it/s]Inference:  22%|██▏       | 7/32 [00:05<00:18,  1.32it/s]Inference:  25%|██▌       | 8/32 [00:05<00:17,  1.39it/s]Inference:  28%|██▊       | 9/32 [00:06<00:16,  1.40it/s]Inference:  31%|███▏      | 10/32 [00:07<00:15,  1.40it/s]Inference:  34%|███▍      | 11/32 [00:07<00:15,  1.35it/s]Inference:  38%|███▊      | 12/32 [00:08<00:15,  1.32it/s]Inference:  41%|████      | 13/32 [00:09<00:14,  1.34it/s]Inference:  44%|████▍     | 14/32 [00:10<00:12,  1.41it/s]Inference:  47%|████▋     | 15/32 [00:10<00:12,  1.41it/s]Inference:  50%|█████     | 16/32 [00:11<00:11,  1.41it/s]Inference:  53%|█████▎    | 17/32 [00:12<00:09,  1.51it/s]Inference:  56%|█████▋    | 18/32 [00:12<00:09,  1.53it/s]Inference:  59%|█████▉    | 19/32 [00:13<00:08,  1.49it/s]Inference:  62%|██████▎   | 20/32 [00:14<00:08,  1.37it/s]Inference:  66%|██████▌   | 21/32 [00:14<00:07,  1.43it/s]Inference:  69%|██████▉   | 22/32 [00:15<00:07,  1.37it/s]Inference:  72%|███████▏  | 23/32 [00:16<00:06,  1.34it/s]Inference:  75%|███████▌  | 24/32 [00:17<00:05,  1.33it/s]Inference:  78%|███████▊  | 25/32 [00:17<00:05,  1.35it/s]Inference:  81%|████████▏ | 26/32 [00:18<00:04,  1.28it/s]Inference:  84%|████████▍ | 27/32 [00:19<00:03,  1.41it/s]Inference:  88%|████████▊ | 28/32 [00:20<00:02,  1.39it/s]Inference:  91%|█████████ | 29/32 [00:20<00:02,  1.38it/s]Inference:  94%|█████████▍| 30/32 [00:21<00:01,  1.38it/s]Inference:  97%|█████████▋| 31/32 [00:22<00:00,  1.37it/s]Inference: 100%|██████████| 32/32 [00:23<00:00,  1.29it/s]Inference: 100%|██████████| 32/32 [00:23<00:00,  1.38it/s]
layer2: self_attn.q_proj
256
val_hessian None
iter 0, train loss 242.15902709960938, val loss None, lr 0.001
iter 10, train loss 238.893798828125, val loss None, lr 0.001
iter 20, train loss 240.91842651367188, val loss None, lr 0.001
iter 30, train loss 237.03717041015625, val loss None, lr 0.001
iter 40, train loss 235.74432373046875, val loss None, lr 0.001
iter 50, train loss 235.4357147216797, val loss None, lr 0.001
iter 60, train loss 237.35679626464844, val loss None, lr 0.001
iter 70, train loss 238.8445281982422, val loss None, lr 0.001
iter 80, train loss 236.30776977539062, val loss None, lr 0.001
iter 90, train loss 236.42715454101562, val loss None, lr 0.001
best loss 219.45547485351562
layer2: self_attn.k_proj
256
val_hessian None
iter 0, train loss 285.54852294921875, val loss None, lr 0.001
iter 10, train loss 271.839599609375, val loss None, lr 0.001
iter 20, train loss 282.3299255371094, val loss None, lr 0.001
iter 30, train loss 279.36395263671875, val loss None, lr 0.001
iter 40, train loss 276.6763916015625, val loss None, lr 0.001
iter 50, train loss 275.99151611328125, val loss None, lr 0.001
iter 60, train loss 275.97503662109375, val loss None, lr 0.001
iter 70, train loss 276.3828430175781, val loss None, lr 0.001
iter 80, train loss 274.7234802246094, val loss None, lr 0.001
iter 90, train loss 272.98516845703125, val loss None, lr 0.001
best loss 255.02560424804688
layer2: self_attn.v_proj
256
val_hessian None
iter 0, train loss 52.704566955566406, val loss None, lr 0.001
iter 10, train loss 55.03094482421875, val loss None, lr 0.001
iter 20, train loss 53.21977233886719, val loss None, lr 0.001
iter 30, train loss 53.01145935058594, val loss None, lr 0.001
iter 40, train loss 52.80987548828125, val loss None, lr 0.001
iter 50, train loss 52.45452117919922, val loss None, lr 0.001
iter 60, train loss 52.19829177856445, val loss None, lr 0.001
iter 70, train loss 52.20732116699219, val loss None, lr 0.001
iter 80, train loss 51.88524627685547, val loss None, lr 0.001
iter 90, train loss 51.72978210449219, val loss None, lr 0.001
best loss 51.52144241333008
layer2: self_attn.o_proj
256
val_hessian None
iter 0, train loss 0.46640968322753906, val loss None, lr 0.001
iter 10, train loss 0.46600019931793213, val loss None, lr 0.001
iter 20, train loss 0.46021169424057007, val loss None, lr 0.001
iter 30, train loss 0.4623118042945862, val loss None, lr 0.001
iter 40, train loss 0.46218329668045044, val loss None, lr 0.001
iter 50, train loss 0.4653751254081726, val loss None, lr 0.001
iter 60, train loss 0.4629427492618561, val loss None, lr 0.001
iter 70, train loss 0.4621826708316803, val loss None, lr 0.001
iter 80, train loss 0.4613528847694397, val loss None, lr 0.001
iter 90, train loss 0.4618692398071289, val loss None, lr 0.001
best loss 0.4578160345554352
layer2: mlp.gate_proj
256
val_hessian None
iter 0, train loss 128.9696044921875, val loss None, lr 0.001
iter 10, train loss 132.7128143310547, val loss None, lr 0.001
iter 20, train loss 131.11911010742188, val loss None, lr 0.001
iter 30, train loss 131.32272338867188, val loss None, lr 0.001
iter 40, train loss 132.33213806152344, val loss None, lr 0.001
iter 50, train loss 132.86798095703125, val loss None, lr 0.001
iter 60, train loss 133.5552215576172, val loss None, lr 0.001
iter 70, train loss 133.95391845703125, val loss None, lr 0.001
iter 80, train loss 134.25196838378906, val loss None, lr 0.001
iter 90, train loss 134.47540283203125, val loss None, lr 0.001
best loss 127.17742156982422
layer2: mlp.up_proj
256
val_hessian None
iter 0, train loss 106.00586700439453, val loss None, lr 0.001
iter 10, train loss 106.7220458984375, val loss None, lr 0.001
iter 20, train loss 107.3392105102539, val loss None, lr 0.001
iter 30, train loss 108.0605239868164, val loss None, lr 0.001
iter 40, train loss 108.52909088134766, val loss None, lr 0.001
iter 50, train loss 108.67752075195312, val loss None, lr 0.001
iter 60, train loss 108.86678314208984, val loss None, lr 0.001
iter 70, train loss 109.05506896972656, val loss None, lr 0.001
iter 80, train loss 109.17581176757812, val loss None, lr 0.001
iter 90, train loss 109.18641662597656, val loss None, lr 0.001
best loss 105.76575469970703
layer2: mlp.down_proj
256
val_hessian None
iter 0, train loss 0.6177161931991577, val loss None, lr 0.001
iter 10, train loss 0.6190352439880371, val loss None, lr 0.001
iter 20, train loss 0.617524266242981, val loss None, lr 0.001
iter 30, train loss 0.6154789328575134, val loss None, lr 0.001
iter 40, train loss 0.6166890859603882, val loss None, lr 0.001
iter 50, train loss 0.6163896322250366, val loss None, lr 0.001
iter 60, train loss 0.6170900464057922, val loss None, lr 0.001
iter 70, train loss 0.6185276508331299, val loss None, lr 0.001
iter 80, train loss 0.6193902492523193, val loss None, lr 0.001
iter 90, train loss 0.620076060295105, val loss None, lr 0.001
best loss 0.6154789328575134
45177 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:14,  2.20it/s]Inference:   6%|▋         | 2/32 [00:00<00:13,  2.23it/s]Inference:   9%|▉         | 3/32 [00:01<00:12,  2.23it/s]Inference:  12%|█▎        | 4/32 [00:01<00:12,  2.22it/s]Inference:  16%|█▌        | 5/32 [00:02<00:12,  2.21it/s]Inference:  19%|█▉        | 6/32 [00:02<00:12,  2.04it/s]Inference:  22%|██▏       | 7/32 [00:03<00:11,  2.26it/s]Inference:  25%|██▌       | 8/32 [00:03<00:10,  2.29it/s]Inference:  28%|██▊       | 9/32 [00:03<00:09,  2.42it/s]Inference:  31%|███▏      | 10/32 [00:04<00:08,  2.53it/s]Inference:  34%|███▍      | 11/32 [00:04<00:08,  2.45it/s]Inference:  38%|███▊      | 12/32 [00:05<00:09,  2.21it/s]Inference:  41%|████      | 13/32 [00:05<00:08,  2.29it/s]Inference:  44%|████▍     | 14/32 [00:06<00:07,  2.36it/s]Inference:  47%|████▋     | 15/32 [00:06<00:07,  2.39it/s]Inference:  50%|█████     | 16/32 [00:06<00:06,  2.45it/s]Inference:  53%|█████▎    | 17/32 [00:07<00:06,  2.49it/s]Inference:  56%|█████▋    | 18/32 [00:07<00:05,  2.69it/s]Inference:  59%|█████▉    | 19/32 [00:07<00:04,  2.65it/s]Inference:  62%|██████▎   | 20/32 [00:08<00:04,  2.61it/s]Inference:  66%|██████▌   | 21/32 [00:08<00:04,  2.63it/s]Inference:  69%|██████▉   | 22/32 [00:09<00:03,  2.61it/s]Inference:  72%|███████▏  | 23/32 [00:09<00:03,  2.71it/s]Inference:  75%|███████▌  | 24/32 [00:09<00:02,  2.77it/s]Inference:  78%|███████▊  | 25/32 [00:10<00:02,  2.45it/s]Inference:  81%|████████▏ | 26/32 [00:10<00:02,  2.40it/s]Inference:  84%|████████▍ | 27/32 [00:11<00:02,  2.34it/s]Inference:  88%|████████▊ | 28/32 [00:11<00:01,  2.30it/s]Inference:  91%|█████████ | 29/32 [00:12<00:01,  2.41it/s]Inference:  94%|█████████▍| 30/32 [00:12<00:00,  2.40it/s]Inference:  97%|█████████▋| 31/32 [00:12<00:00,  2.57it/s]Inference: 100%|██████████| 32/32 [00:13<00:00,  2.64it/s]Inference: 100%|██████████| 32/32 [00:13<00:00,  2.44it/s]
39887 MiB free out of 48676 MiB total
Saved layer 2 to ./models/meta-llama/Llama-2-7b-hf/2bpv/128/quantized/layer_2.pt
after cast to cpu
43889 MiB free out of 48676 MiB total
Done with layer 2 total_time elapsed: 1523 estimated time left: 14727
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:21,  1.41it/s]Inference:   6%|▋         | 2/32 [00:01<00:23,  1.25it/s]Inference:   9%|▉         | 3/32 [00:02<00:22,  1.32it/s]Inference:  12%|█▎        | 4/32 [00:03<00:20,  1.34it/s]Inference:  16%|█▌        | 5/32 [00:03<00:20,  1.32it/s]Inference:  19%|█▉        | 6/32 [00:04<00:20,  1.29it/s]Inference:  22%|██▏       | 7/32 [00:05<00:19,  1.26it/s]Inference:  25%|██▌       | 8/32 [00:06<00:19,  1.22it/s]Inference:  28%|██▊       | 9/32 [00:07<00:19,  1.16it/s]Inference:  31%|███▏      | 10/32 [00:07<00:18,  1.22it/s]Inference:  34%|███▍      | 11/32 [00:08<00:16,  1.30it/s]Inference:  38%|███▊      | 12/32 [00:09<00:16,  1.22it/s]Inference:  41%|████      | 13/32 [00:10<00:14,  1.27it/s]Inference:  44%|████▍     | 14/32 [00:10<00:13,  1.37it/s]Inference:  47%|████▋     | 15/32 [00:11<00:12,  1.33it/s]Inference:  50%|█████     | 16/32 [00:12<00:11,  1.36it/s]Inference:  53%|█████▎    | 17/32 [00:13<00:10,  1.37it/s]Inference:  56%|█████▋    | 18/32 [00:13<00:10,  1.39it/s]Inference:  59%|█████▉    | 19/32 [00:14<00:09,  1.37it/s]Inference:  62%|██████▎   | 20/32 [00:15<00:09,  1.29it/s]Inference:  66%|██████▌   | 21/32 [00:16<00:08,  1.23it/s]Inference:  69%|██████▉   | 22/32 [00:17<00:08,  1.20it/s]Inference:  72%|███████▏  | 23/32 [00:18<00:07,  1.18it/s]Inference:  75%|███████▌  | 24/32 [00:18<00:06,  1.16it/s]Inference:  78%|███████▊  | 25/32 [00:19<00:06,  1.16it/s]Inference:  81%|████████▏ | 26/32 [00:20<00:05,  1.14it/s]Inference:  84%|████████▍ | 27/32 [00:21<00:04,  1.15it/s]Inference:  88%|████████▊ | 28/32 [00:22<00:03,  1.15it/s]Inference:  91%|█████████ | 29/32 [00:23<00:02,  1.19it/s]Inference:  94%|█████████▍| 30/32 [00:24<00:01,  1.22it/s]Inference:  97%|█████████▋| 31/32 [00:25<00:00,  1.16it/s]Inference: 100%|██████████| 32/32 [00:25<00:00,  1.24it/s]Inference: 100%|██████████| 32/32 [00:25<00:00,  1.25it/s]
layer3: self_attn.q_proj
256
val_hessian None
iter 0, train loss 576.1444702148438, val loss None, lr 0.001
iter 10, train loss 610.6903076171875, val loss None, lr 0.001
iter 20, train loss 583.1142578125, val loss None, lr 0.001
iter 30, train loss 572.5296020507812, val loss None, lr 0.001
iter 40, train loss 575.342041015625, val loss None, lr 0.001
iter 50, train loss 569.2188110351562, val loss None, lr 0.001
iter 60, train loss 570.492431640625, val loss None, lr 0.001
iter 70, train loss 572.3441772460938, val loss None, lr 0.001
iter 80, train loss 573.9999389648438, val loss None, lr 0.001
iter 90, train loss 577.2935791015625, val loss None, lr 0.001
best loss 547.1146240234375
layer3: self_attn.k_proj
256
val_hessian None
iter 0, train loss 666.9169921875, val loss None, lr 0.001
iter 10, train loss 681.5728759765625, val loss None, lr 0.001
iter 20, train loss 685.5526733398438, val loss None, lr 0.001
iter 30, train loss 660.9891357421875, val loss None, lr 0.001
iter 40, train loss 657.0201416015625, val loss None, lr 0.001
iter 50, train loss 659.8738403320312, val loss None, lr 0.001
iter 60, train loss 654.8114013671875, val loss None, lr 0.001
iter 70, train loss 651.3279418945312, val loss None, lr 0.001
iter 80, train loss 657.10205078125, val loss None, lr 0.001
iter 90, train loss 651.9066772460938, val loss None, lr 0.001
best loss 615.24072265625
layer3: self_attn.v_proj
256
val_hessian None
iter 0, train loss 139.50892639160156, val loss None, lr 0.001
iter 10, train loss 140.4497833251953, val loss None, lr 0.001
iter 20, train loss 139.93905639648438, val loss None, lr 0.001
iter 30, train loss 139.30784606933594, val loss None, lr 0.001
iter 40, train loss 138.78257751464844, val loss None, lr 0.001
iter 50, train loss 138.49105834960938, val loss None, lr 0.001
iter 60, train loss 138.54458618164062, val loss None, lr 0.001
iter 70, train loss 138.27159118652344, val loss None, lr 0.001
iter 80, train loss 138.19793701171875, val loss None, lr 0.001
iter 90, train loss 138.12088012695312, val loss None, lr 0.001
best loss 138.07965087890625
layer3: self_attn.o_proj
256
val_hessian None
iter 0, train loss 0.9929072856903076, val loss None, lr 0.001
iter 10, train loss 0.9244133830070496, val loss None, lr 0.001
iter 20, train loss 0.8854951858520508, val loss None, lr 0.001
iter 30, train loss 0.8919284343719482, val loss None, lr 0.001
iter 40, train loss 0.8793987035751343, val loss None, lr 0.001
iter 50, train loss 0.8730955123901367, val loss None, lr 0.001
iter 60, train loss 0.8722261786460876, val loss None, lr 0.001
iter 70, train loss 0.8731042742729187, val loss None, lr 0.001
iter 80, train loss 0.8743454813957214, val loss None, lr 0.001
iter 90, train loss 0.879440426826477, val loss None, lr 0.001
best loss 0.8710570335388184
layer3: mlp.gate_proj
256
val_hessian None
iter 0, train loss 213.8888702392578, val loss None, lr 0.001
iter 10, train loss 215.77871704101562, val loss None, lr 0.001
iter 20, train loss 217.10305786132812, val loss None, lr 0.001
iter 30, train loss 218.96485900878906, val loss None, lr 0.001
iter 40, train loss 220.34075927734375, val loss None, lr 0.001
iter 50, train loss 221.29481506347656, val loss None, lr 0.001
iter 60, train loss 221.95391845703125, val loss None, lr 0.001
iter 70, train loss 222.38299560546875, val loss None, lr 0.001
iter 80, train loss 222.90103149414062, val loss None, lr 0.001
iter 90, train loss 222.92333984375, val loss None, lr 0.001
best loss 213.02597045898438
layer3: mlp.up_proj
256
val_hessian None
iter 0, train loss 178.21519470214844, val loss None, lr 0.001
iter 10, train loss 178.8379669189453, val loss None, lr 0.001
iter 20, train loss 179.50904846191406, val loss None, lr 0.001
iter 30, train loss 180.45059204101562, val loss None, lr 0.001
iter 40, train loss 180.86439514160156, val loss None, lr 0.001
iter 50, train loss 181.38015747070312, val loss None, lr 0.001
iter 60, train loss 181.3963623046875, val loss None, lr 0.001
iter 70, train loss 181.42147827148438, val loss None, lr 0.001
iter 80, train loss 181.40145874023438, val loss None, lr 0.001
iter 90, train loss 181.40782165527344, val loss None, lr 0.001
best loss 178.21519470214844
layer3: mlp.down_proj
256
val_hessian None
iter 0, train loss 1.2460498809814453, val loss None, lr 0.001
iter 10, train loss 1.2518775463104248, val loss None, lr 0.001
iter 20, train loss 1.2449746131896973, val loss None, lr 0.001
iter 30, train loss 1.2427749633789062, val loss None, lr 0.001
iter 40, train loss 1.2444933652877808, val loss None, lr 0.001
iter 50, train loss 1.2463455200195312, val loss None, lr 0.001
iter 60, train loss 1.2494094371795654, val loss None, lr 0.001
iter 70, train loss 1.2523765563964844, val loss None, lr 0.001
iter 80, train loss 1.255817174911499, val loss None, lr 0.001
iter 90, train loss 1.2592263221740723, val loss None, lr 0.001
best loss 1.2423670291900635
43889 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:14,  2.21it/s]Inference:   6%|▋         | 2/32 [00:01<00:15,  1.94it/s]Inference:   9%|▉         | 3/32 [00:01<00:14,  2.07it/s]Inference:  12%|█▎        | 4/32 [00:02<00:14,  1.97it/s]Inference:  16%|█▌        | 5/32 [00:02<00:13,  1.95it/s]Inference:  19%|█▉        | 6/32 [00:03<00:13,  1.93it/s]Inference:  22%|██▏       | 7/32 [00:03<00:12,  1.99it/s]Inference:  25%|██▌       | 8/32 [00:03<00:11,  2.15it/s]Inference:  28%|██▊       | 9/32 [00:04<00:09,  2.34it/s]Inference:  31%|███▏      | 10/32 [00:04<00:10,  2.15it/s]Inference:  34%|███▍      | 11/32 [00:05<00:09,  2.12it/s]Inference:  38%|███▊      | 12/32 [00:05<00:08,  2.36it/s]Inference:  41%|████      | 13/32 [00:06<00:08,  2.18it/s]Inference:  44%|████▍     | 14/32 [00:06<00:08,  2.20it/s]Inference:  47%|████▋     | 15/32 [00:06<00:07,  2.30it/s]Inference:  50%|█████     | 16/32 [00:07<00:07,  2.21it/s]Inference:  53%|█████▎    | 17/32 [00:07<00:07,  2.12it/s]Inference:  56%|█████▋    | 18/32 [00:08<00:06,  2.30it/s]Inference:  59%|█████▉    | 19/32 [00:08<00:05,  2.47it/s]Inference:  62%|██████▎   | 20/32 [00:09<00:04,  2.58it/s]Inference:  66%|██████▌   | 21/32 [00:09<00:03,  2.79it/s]Inference:  69%|██████▉   | 22/32 [00:09<00:03,  2.83it/s]Inference:  72%|███████▏  | 23/32 [00:10<00:03,  2.74it/s]Inference:  75%|███████▌  | 24/32 [00:10<00:02,  2.76it/s]Inference:  78%|███████▊  | 25/32 [00:10<00:02,  2.83it/s]Inference:  81%|████████▏ | 26/32 [00:11<00:02,  2.45it/s]Inference:  84%|████████▍ | 27/32 [00:11<00:01,  2.52it/s]Inference:  88%|████████▊ | 28/32 [00:12<00:01,  2.42it/s]Inference:  91%|█████████ | 29/32 [00:12<00:01,  2.59it/s]Inference:  94%|█████████▍| 30/32 [00:12<00:00,  2.49it/s]Inference:  97%|█████████▋| 31/32 [00:13<00:00,  2.63it/s]Inference: 100%|██████████| 32/32 [00:13<00:00,  2.42it/s]Inference: 100%|██████████| 32/32 [00:13<00:00,  2.34it/s]
38663 MiB free out of 48676 MiB total
Saved layer 3 to ./models/meta-llama/Llama-2-7b-hf/2bpv/128/quantized/layer_3.pt
after cast to cpu
42665 MiB free out of 48676 MiB total
Done with layer 3 total_time elapsed: 2048 estimated time left: 14338
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:24,  1.27it/s]Inference:   6%|▋         | 2/32 [00:01<00:20,  1.50it/s]Inference:   9%|▉         | 3/32 [00:02<00:19,  1.47it/s]Inference:  12%|█▎        | 4/32 [00:02<00:20,  1.39it/s]Inference:  16%|█▌        | 5/32 [00:03<00:19,  1.42it/s]Inference:  19%|█▉        | 6/32 [00:04<00:17,  1.49it/s]Inference:  22%|██▏       | 7/32 [00:04<00:16,  1.50it/s]Inference:  25%|██▌       | 8/32 [00:05<00:16,  1.43it/s]Inference:  28%|██▊       | 9/32 [00:06<00:16,  1.41it/s]Inference:  31%|███▏      | 10/32 [00:06<00:15,  1.44it/s]Inference:  34%|███▍      | 11/32 [00:07<00:14,  1.43it/s]Inference:  38%|███▊      | 12/32 [00:08<00:13,  1.44it/s]Inference:  41%|████      | 13/32 [00:09<00:13,  1.45it/s]Inference:  44%|████▍     | 14/32 [00:09<00:12,  1.46it/s]Inference:  47%|████▋     | 15/32 [00:10<00:11,  1.45it/s]Inference:  50%|█████     | 16/32 [00:11<00:11,  1.41it/s]Inference:  53%|█████▎    | 17/32 [00:11<00:11,  1.33it/s]Inference:  56%|█████▋    | 18/32 [00:12<00:09,  1.46it/s]Inference:  59%|█████▉    | 19/32 [00:13<00:09,  1.41it/s]Inference:  62%|██████▎   | 20/32 [00:13<00:08,  1.42it/s]Inference:  66%|██████▌   | 21/32 [00:14<00:07,  1.43it/s]Inference:  69%|██████▉   | 22/32 [00:15<00:06,  1.48it/s]Inference:  72%|███████▏  | 23/32 [00:15<00:06,  1.47it/s]Inference:  75%|███████▌  | 24/32 [00:16<00:05,  1.48it/s]Inference:  78%|███████▊  | 25/32 [00:17<00:04,  1.47it/s]Inference:  81%|████████▏ | 26/32 [00:17<00:04,  1.48it/s]Inference:  84%|████████▍ | 27/32 [00:18<00:03,  1.48it/s]Inference:  88%|████████▊ | 28/32 [00:19<00:02,  1.44it/s]Inference:  91%|█████████ | 29/32 [00:20<00:02,  1.40it/s]Inference:  94%|█████████▍| 30/32 [00:20<00:01,  1.43it/s]Inference:  97%|█████████▋| 31/32 [00:21<00:00,  1.49it/s]Inference: 100%|██████████| 32/32 [00:22<00:00,  1.53it/s]Inference: 100%|██████████| 32/32 [00:22<00:00,  1.45it/s]
layer4: self_attn.q_proj
256
val_hessian None
iter 0, train loss 539.106201171875, val loss None, lr 0.001
iter 10, train loss 578.7080688476562, val loss None, lr 0.001
iter 20, train loss 551.7687377929688, val loss None, lr 0.001
iter 30, train loss 544.4220581054688, val loss None, lr 0.001
iter 40, train loss 548.373291015625, val loss None, lr 0.001
iter 50, train loss 548.2609252929688, val loss None, lr 0.001
iter 60, train loss 545.0744018554688, val loss None, lr 0.001
iter 70, train loss 547.1878662109375, val loss None, lr 0.001
iter 80, train loss 545.5617065429688, val loss None, lr 0.001
iter 90, train loss 544.952880859375, val loss None, lr 0.001
best loss 521.179931640625
layer4: self_attn.k_proj
256
val_hessian None
iter 0, train loss 595.8878173828125, val loss None, lr 0.001
iter 10, train loss 629.0302734375, val loss None, lr 0.001
iter 20, train loss 621.5426025390625, val loss None, lr 0.001
iter 30, train loss 606.4774780273438, val loss None, lr 0.001
iter 40, train loss 607.8436889648438, val loss None, lr 0.001
iter 50, train loss 611.8900756835938, val loss None, lr 0.001
iter 60, train loss 613.2857666015625, val loss None, lr 0.001
iter 70, train loss 611.0938110351562, val loss None, lr 0.001
iter 80, train loss 614.427978515625, val loss None, lr 0.001
iter 90, train loss 613.4365844726562, val loss None, lr 0.001
best loss 571.871826171875
layer4: self_attn.v_proj
256
val_hessian None
iter 0, train loss 138.57211303710938, val loss None, lr 0.001
iter 10, train loss 139.47586059570312, val loss None, lr 0.001
iter 20, train loss 138.9403839111328, val loss None, lr 0.001
iter 30, train loss 137.96359252929688, val loss None, lr 0.001
iter 40, train loss 137.66082763671875, val loss None, lr 0.001
iter 50, train loss 137.36524963378906, val loss None, lr 0.001
iter 60, train loss 137.29747009277344, val loss None, lr 0.001
iter 70, train loss 136.95968627929688, val loss None, lr 0.001
iter 80, train loss 136.9288330078125, val loss None, lr 0.001
iter 90, train loss 136.61050415039062, val loss None, lr 0.001
best loss 136.4007568359375
layer4: self_attn.o_proj
256
val_hessian None
iter 0, train loss 2.8522841930389404, val loss None, lr 0.001
iter 10, train loss 2.552844524383545, val loss None, lr 0.001
iter 20, train loss 2.391847848892212, val loss None, lr 0.001
iter 30, train loss 2.3389434814453125, val loss None, lr 0.001
iter 40, train loss 2.2694265842437744, val loss None, lr 0.001
iter 50, train loss 2.237846851348877, val loss None, lr 0.001
iter 60, train loss 2.1994094848632812, val loss None, lr 0.001
iter 70, train loss 2.1809611320495605, val loss None, lr 0.001
iter 80, train loss 2.1642909049987793, val loss None, lr 0.001
iter 90, train loss 2.170175075531006, val loss None, lr 0.001
best loss 2.1552481651306152
layer4: mlp.gate_proj
256
val_hessian None
iter 0, train loss 310.86370849609375, val loss None, lr 0.001
iter 10, train loss 317.68072509765625, val loss None, lr 0.001
iter 20, train loss 316.8358154296875, val loss None, lr 0.001
iter 30, train loss 317.43829345703125, val loss None, lr 0.001
iter 40, train loss 318.1015319824219, val loss None, lr 0.001
iter 50, train loss 317.9920654296875, val loss None, lr 0.001
iter 60, train loss 318.33447265625, val loss None, lr 0.001
iter 70, train loss 318.31927490234375, val loss None, lr 0.001
iter 80, train loss 318.2720947265625, val loss None, lr 0.001
iter 90, train loss 318.5965270996094, val loss None, lr 0.001
best loss 308.38861083984375
layer4: mlp.up_proj
256
val_hessian None
iter 0, train loss 244.0445556640625, val loss None, lr 0.001
iter 10, train loss 244.6910858154297, val loss None, lr 0.001
iter 20, train loss 245.7698211669922, val loss None, lr 0.001
iter 30, train loss 246.76113891601562, val loss None, lr 0.001
iter 40, train loss 247.2895965576172, val loss None, lr 0.001
iter 50, train loss 247.7505645751953, val loss None, lr 0.001
iter 60, train loss 248.09222412109375, val loss None, lr 0.001
iter 70, train loss 248.35618591308594, val loss None, lr 0.001
iter 80, train loss 248.7686767578125, val loss None, lr 0.001
iter 90, train loss 249.0474395751953, val loss None, lr 0.001
best loss 244.0445556640625
layer4: mlp.down_proj
256
val_hessian None
iter 0, train loss 2.4977753162384033, val loss None, lr 0.001
iter 10, train loss 2.5253939628601074, val loss None, lr 0.001
iter 20, train loss 2.524198532104492, val loss None, lr 0.001
iter 30, train loss 2.5143344402313232, val loss None, lr 0.001
iter 40, train loss 2.5262439250946045, val loss None, lr 0.001
iter 50, train loss 2.527780771255493, val loss None, lr 0.001
iter 60, train loss 2.5219461917877197, val loss None, lr 0.001
iter 70, train loss 2.519289970397949, val loss None, lr 0.001
iter 80, train loss 2.524942398071289, val loss None, lr 0.001
iter 90, train loss 2.528751850128174, val loss None, lr 0.001
best loss 2.4977753162384033
42665 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:11,  2.70it/s]Inference:   6%|▋         | 2/32 [00:00<00:11,  2.56it/s]Inference:   9%|▉         | 3/32 [00:01<00:11,  2.47it/s]Inference:  12%|█▎        | 4/32 [00:01<00:11,  2.49it/s]Inference:  16%|█▌        | 5/32 [00:02<00:11,  2.32it/s]Inference:  19%|█▉        | 6/32 [00:02<00:10,  2.57it/s]Inference:  22%|██▏       | 7/32 [00:02<00:09,  2.67it/s]Inference:  25%|██▌       | 8/32 [00:03<00:08,  2.77it/s]Inference:  28%|██▊       | 9/32 [00:03<00:08,  2.60it/s]Inference:  31%|███▏      | 10/32 [00:03<00:08,  2.69it/s]Inference:  34%|███▍      | 11/32 [00:04<00:07,  2.70it/s]Inference:  38%|███▊      | 12/32 [00:04<00:08,  2.37it/s]Inference:  41%|████      | 13/32 [00:05<00:08,  2.31it/s]Inference:  44%|████▍     | 14/32 [00:05<00:07,  2.47it/s]Inference:  47%|████▋     | 15/32 [00:05<00:06,  2.48it/s]Inference:  50%|█████     | 16/32 [00:06<00:06,  2.66it/s]Inference:  53%|█████▎    | 17/32 [00:06<00:05,  2.71it/s]Inference:  56%|█████▋    | 18/32 [00:07<00:05,  2.55it/s]Inference:  59%|█████▉    | 19/32 [00:07<00:05,  2.56it/s]Inference:  62%|██████▎   | 20/32 [00:07<00:04,  2.73it/s]Inference:  66%|██████▌   | 21/32 [00:08<00:03,  2.90it/s]Inference:  69%|██████▉   | 22/32 [00:08<00:03,  2.75it/s]Inference:  72%|███████▏  | 23/32 [00:08<00:03,  2.72it/s]Inference:  75%|███████▌  | 24/32 [00:09<00:03,  2.58it/s]Inference:  78%|███████▊  | 25/32 [00:09<00:02,  2.59it/s]Inference:  81%|████████▏ | 26/32 [00:10<00:02,  2.59it/s]Inference:  84%|████████▍ | 27/32 [00:10<00:01,  2.73it/s]Inference:  88%|████████▊ | 28/32 [00:10<00:01,  2.55it/s]Inference:  91%|█████████ | 29/32 [00:11<00:01,  2.87it/s]Inference:  94%|█████████▍| 30/32 [00:11<00:00,  2.88it/s]Inference:  97%|█████████▋| 31/32 [00:11<00:00,  2.77it/s]Inference: 100%|██████████| 32/32 [00:12<00:00,  2.88it/s]Inference: 100%|██████████| 32/32 [00:12<00:00,  2.64it/s]
37375 MiB free out of 48676 MiB total
Saved layer 4 to ./models/meta-llama/Llama-2-7b-hf/2bpv/128/quantized/layer_4.pt
after cast to cpu
41377 MiB free out of 48676 MiB total
Done with layer 4 total_time elapsed: 2564 estimated time left: 13848
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:20,  1.48it/s]Inference:   6%|▋         | 2/32 [00:01<00:18,  1.58it/s]Inference:   9%|▉         | 3/32 [00:01<00:19,  1.51it/s]Inference:  12%|█▎        | 4/32 [00:02<00:19,  1.43it/s]Inference:  16%|█▌        | 5/32 [00:03<00:19,  1.37it/s]Inference:  19%|█▉        | 6/32 [00:04<00:18,  1.40it/s]Inference:  22%|██▏       | 7/32 [00:04<00:17,  1.44it/s]Inference:  25%|██▌       | 8/32 [00:05<00:17,  1.38it/s]Inference:  28%|██▊       | 9/32 [00:06<00:17,  1.32it/s]Inference:  31%|███▏      | 10/32 [00:07<00:15,  1.39it/s]Inference:  34%|███▍      | 11/32 [00:07<00:14,  1.42it/s]Inference:  38%|███▊      | 12/32 [00:08<00:13,  1.43it/s]Inference:  41%|████      | 13/32 [00:09<00:13,  1.38it/s]Inference:  44%|████▍     | 14/32 [00:09<00:13,  1.37it/s]Inference:  47%|████▋     | 15/32 [00:10<00:12,  1.39it/s]Inference:  50%|█████     | 16/32 [00:11<00:11,  1.41it/s]Inference:  53%|█████▎    | 17/32 [00:12<00:10,  1.43it/s]Inference:  56%|█████▋    | 18/32 [00:12<00:09,  1.45it/s]Inference:  59%|█████▉    | 19/32 [00:13<00:08,  1.47it/s]Inference:  62%|██████▎   | 20/32 [00:14<00:08,  1.47it/s]Inference:  66%|██████▌   | 21/32 [00:14<00:07,  1.46it/s]Inference:  69%|██████▉   | 22/32 [00:15<00:06,  1.51it/s]Inference:  72%|███████▏  | 23/32 [00:15<00:05,  1.54it/s]Inference:  75%|███████▌  | 24/32 [00:16<00:05,  1.45it/s]Inference:  78%|███████▊  | 25/32 [00:17<00:04,  1.49it/s]Inference:  81%|████████▏ | 26/32 [00:18<00:03,  1.53it/s]Inference:  84%|████████▍ | 27/32 [00:18<00:03,  1.53it/s]Inference:  88%|████████▊ | 28/32 [00:19<00:02,  1.52it/s]Inference:  91%|█████████ | 29/32 [00:20<00:02,  1.39it/s]Inference:  94%|█████████▍| 30/32 [00:20<00:01,  1.45it/s]Inference:  97%|█████████▋| 31/32 [00:21<00:00,  1.41it/s]Inference: 100%|██████████| 32/32 [00:22<00:00,  1.39it/s]Inference: 100%|██████████| 32/32 [00:22<00:00,  1.43it/s]
layer5: self_attn.q_proj
256
val_hessian None
iter 0, train loss 600.4291381835938, val loss None, lr 0.001
iter 10, train loss 642.9212036132812, val loss None, lr 0.001
iter 20, train loss 613.500732421875, val loss None, lr 0.001
iter 30, train loss 604.2713012695312, val loss None, lr 0.001
iter 40, train loss 607.1732177734375, val loss None, lr 0.001
iter 50, train loss 601.6356201171875, val loss None, lr 0.001
iter 60, train loss 602.8002319335938, val loss None, lr 0.001
iter 70, train loss 601.8296508789062, val loss None, lr 0.001
iter 80, train loss 599.510009765625, val loss None, lr 0.001
iter 90, train loss 601.1856689453125, val loss None, lr 0.001
best loss 580.65966796875
layer5: self_attn.k_proj
256
val_hessian None
iter 0, train loss 701.5072021484375, val loss None, lr 0.001
iter 10, train loss 724.6710815429688, val loss None, lr 0.001
iter 20, train loss 729.6944580078125, val loss None, lr 0.001
iter 30, train loss 709.2473754882812, val loss None, lr 0.001
iter 40, train loss 704.6526489257812, val loss None, lr 0.001
iter 50, train loss 704.2745971679688, val loss None, lr 0.001
iter 60, train loss 699.0799560546875, val loss None, lr 0.001
iter 70, train loss 700.8139038085938, val loss None, lr 0.001
iter 80, train loss 704.2719116210938, val loss None, lr 0.001
iter 90, train loss 701.9818115234375, val loss None, lr 0.001
best loss 662.5693359375
layer5: self_attn.v_proj
256
val_hessian None
iter 0, train loss 160.12844848632812, val loss None, lr 0.001
iter 10, train loss 160.03338623046875, val loss None, lr 0.001
iter 20, train loss 159.4983673095703, val loss None, lr 0.001
iter 30, train loss 159.0052490234375, val loss None, lr 0.001
iter 40, train loss 158.54812622070312, val loss None, lr 0.001
iter 50, train loss 158.13314819335938, val loss None, lr 0.001
iter 60, train loss 157.77621459960938, val loss None, lr 0.001
iter 70, train loss 157.48931884765625, val loss None, lr 0.001
iter 80, train loss 157.5106201171875, val loss None, lr 0.001
iter 90, train loss 157.38626098632812, val loss None, lr 0.001
best loss 157.2648468017578
layer5: self_attn.o_proj
256
val_hessian None
iter 0, train loss 3.8886184692382812, val loss None, lr 0.001
iter 10, train loss 3.828911781311035, val loss None, lr 0.001
iter 20, train loss 3.808116912841797, val loss None, lr 0.001
iter 30, train loss 3.795808792114258, val loss None, lr 0.001
iter 40, train loss 3.7864925861358643, val loss None, lr 0.001
iter 50, train loss 3.77711820602417, val loss None, lr 0.001
iter 60, train loss 3.7761218547821045, val loss None, lr 0.001
iter 70, train loss 3.769277572631836, val loss None, lr 0.001
iter 80, train loss 3.768089532852173, val loss None, lr 0.001
iter 90, train loss 3.766361713409424, val loss None, lr 0.001
best loss 3.7639544010162354
layer5: mlp.gate_proj
256
val_hessian None
iter 0, train loss 390.33843994140625, val loss None, lr 0.001
iter 10, train loss 406.01287841796875, val loss None, lr 0.001
iter 20, train loss 401.7781982421875, val loss None, lr 0.001
iter 30, train loss 400.93023681640625, val loss None, lr 0.001
iter 40, train loss 400.6277160644531, val loss None, lr 0.001
iter 50, train loss 400.4297790527344, val loss None, lr 0.001
iter 60, train loss 400.5122985839844, val loss None, lr 0.001
iter 70, train loss 400.7811279296875, val loss None, lr 0.001
iter 80, train loss 400.58819580078125, val loss None, lr 0.001
iter 90, train loss 400.0581359863281, val loss None, lr 0.001
best loss 386.83575439453125
layer5: mlp.up_proj
256
val_hessian None
iter 0, train loss 301.3662109375, val loss None, lr 0.001
iter 10, train loss 302.6322937011719, val loss None, lr 0.001
iter 20, train loss 304.00927734375, val loss None, lr 0.001
iter 30, train loss 305.3023376464844, val loss None, lr 0.001
iter 40, train loss 305.8912658691406, val loss None, lr 0.001
iter 50, train loss 306.2900390625, val loss None, lr 0.001
iter 60, train loss 306.5691223144531, val loss None, lr 0.001
iter 70, train loss 306.7286682128906, val loss None, lr 0.001
iter 80, train loss 306.97381591796875, val loss None, lr 0.001
iter 90, train loss 307.39990234375, val loss None, lr 0.001
best loss 301.3662109375
layer5: mlp.down_proj
256
val_hessian None
iter 0, train loss 3.7007570266723633, val loss None, lr 0.001
iter 10, train loss 3.7133283615112305, val loss None, lr 0.001
iter 20, train loss 3.693960189819336, val loss None, lr 0.001
iter 30, train loss 3.6889219284057617, val loss None, lr 0.001
iter 40, train loss 3.697605609893799, val loss None, lr 0.001
iter 50, train loss 3.7101306915283203, val loss None, lr 0.001
iter 60, train loss 3.7086493968963623, val loss None, lr 0.001
iter 70, train loss 3.722639560699463, val loss None, lr 0.001
iter 80, train loss 3.746150493621826, val loss None, lr 0.001
iter 90, train loss 3.750671625137329, val loss None, lr 0.001
best loss 3.687002182006836
41377 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:13,  2.35it/s]Inference:   6%|▋         | 2/32 [00:00<00:11,  2.70it/s]Inference:   9%|▉         | 3/32 [00:01<00:11,  2.44it/s]Inference:  12%|█▎        | 4/32 [00:01<00:10,  2.60it/s]Inference:  16%|█▌        | 5/32 [00:01<00:10,  2.51it/s]Inference:  19%|█▉        | 6/32 [00:02<00:10,  2.46it/s]Inference:  22%|██▏       | 7/32 [00:02<00:10,  2.41it/s]Inference:  25%|██▌       | 8/32 [00:03<00:09,  2.43it/s]Inference:  28%|██▊       | 9/32 [00:03<00:08,  2.59it/s]Inference:  31%|███▏      | 10/32 [00:03<00:08,  2.51it/s]Inference:  34%|███▍      | 11/32 [00:04<00:08,  2.44it/s]Inference:  38%|███▊      | 12/32 [00:04<00:08,  2.47it/s]Inference:  41%|████      | 13/32 [00:05<00:07,  2.49it/s]Inference:  44%|████▍     | 14/32 [00:05<00:07,  2.56it/s]Inference:  47%|████▋     | 15/32 [00:05<00:06,  2.61it/s]Inference:  50%|█████     | 16/32 [00:06<00:06,  2.44it/s]Inference:  53%|█████▎    | 17/32 [00:06<00:05,  2.52it/s]Inference:  56%|█████▋    | 18/32 [00:07<00:05,  2.63it/s]Inference:  59%|█████▉    | 19/32 [00:07<00:05,  2.49it/s]Inference:  62%|██████▎   | 20/32 [00:07<00:04,  2.57it/s]Inference:  66%|██████▌   | 21/32 [00:08<00:04,  2.49it/s]Inference:  69%|██████▉   | 22/32 [00:08<00:03,  2.59it/s]Inference:  72%|███████▏  | 23/32 [00:09<00:03,  2.54it/s]Inference:  75%|███████▌  | 24/32 [00:09<00:02,  2.78it/s]Inference:  78%|███████▊  | 25/32 [00:09<00:02,  2.94it/s]Inference:  81%|████████▏ | 26/32 [00:10<00:02,  2.81it/s]Inference:  84%|████████▍ | 27/32 [00:10<00:01,  2.94it/s]Inference:  88%|████████▊ | 28/32 [00:10<00:01,  2.83it/s]Inference:  91%|█████████ | 29/32 [00:11<00:01,  2.93it/s]Inference:  94%|█████████▍| 30/32 [00:11<00:00,  2.60it/s]Inference:  97%|█████████▋| 31/32 [00:11<00:00,  2.63it/s]Inference: 100%|██████████| 32/32 [00:12<00:00,  2.78it/s]Inference: 100%|██████████| 32/32 [00:12<00:00,  2.61it/s]
36087 MiB free out of 48676 MiB total
Saved layer 5 to ./models/meta-llama/Llama-2-7b-hf/2bpv/128/quantized/layer_5.pt
after cast to cpu
40089 MiB free out of 48676 MiB total
Done with layer 5 total_time elapsed: 3087 estimated time left: 13377
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:18,  1.64it/s]Inference:   6%|▋         | 2/32 [00:01<00:20,  1.44it/s]Inference:   9%|▉         | 3/32 [00:01<00:19,  1.51it/s]Inference:  12%|█▎        | 4/32 [00:02<00:18,  1.49it/s]Inference:  16%|█▌        | 5/32 [00:03<00:18,  1.47it/s]Inference:  19%|█▉        | 6/32 [00:04<00:18,  1.40it/s]Inference:  22%|██▏       | 7/32 [00:04<00:17,  1.42it/s]Inference:  25%|██▌       | 8/32 [00:05<00:16,  1.45it/s]Inference:  28%|██▊       | 9/32 [00:06<00:15,  1.44it/s]Inference:  31%|███▏      | 10/32 [00:06<00:15,  1.41it/s]Inference:  34%|███▍      | 11/32 [00:07<00:14,  1.49it/s]Inference:  38%|███▊      | 12/32 [00:08<00:12,  1.55it/s]Inference:  41%|████      | 13/32 [00:08<00:12,  1.53it/s]Inference:  44%|████▍     | 14/32 [00:09<00:11,  1.54it/s]Inference:  47%|████▋     | 15/32 [00:10<00:11,  1.46it/s]Inference:  50%|█████     | 16/32 [00:10<00:11,  1.42it/s]Inference:  53%|█████▎    | 17/32 [00:11<00:10,  1.44it/s]Inference:  56%|█████▋    | 18/32 [00:12<00:10,  1.39it/s]Inference:  59%|█████▉    | 19/32 [00:13<00:08,  1.44it/s]Inference:  62%|██████▎   | 20/32 [00:13<00:08,  1.44it/s]Inference:  66%|██████▌   | 21/32 [00:14<00:07,  1.44it/s]Inference:  69%|██████▉   | 22/32 [00:15<00:06,  1.45it/s]Inference:  72%|███████▏  | 23/32 [00:15<00:06,  1.48it/s]Inference:  75%|███████▌  | 24/32 [00:16<00:05,  1.48it/s]Inference:  78%|███████▊  | 25/32 [00:17<00:04,  1.46it/s]Inference:  81%|████████▏ | 26/32 [00:17<00:04,  1.41it/s]Inference:  84%|████████▍ | 27/32 [00:18<00:03,  1.49it/s]Inference:  88%|████████▊ | 28/32 [00:19<00:02,  1.52it/s]Inference:  91%|█████████ | 29/32 [00:19<00:02,  1.46it/s]Inference:  94%|█████████▍| 30/32 [00:20<00:01,  1.45it/s]Inference:  97%|█████████▋| 31/32 [00:21<00:00,  1.50it/s]Inference: 100%|██████████| 32/32 [00:21<00:00,  1.48it/s]Inference: 100%|██████████| 32/32 [00:21<00:00,  1.47it/s]
layer6: self_attn.q_proj
256
val_hessian None
iter 0, train loss 924.8988037109375, val loss None, lr 0.001
iter 10, train loss 996.7658081054688, val loss None, lr 0.001
iter 20, train loss 958.0756225585938, val loss None, lr 0.001
iter 30, train loss 923.6236572265625, val loss None, lr 0.001
iter 40, train loss 929.8292846679688, val loss None, lr 0.001
iter 50, train loss 928.7119750976562, val loss None, lr 0.001
iter 60, train loss 924.000244140625, val loss None, lr 0.001
iter 70, train loss 918.1896362304688, val loss None, lr 0.001
iter 80, train loss 919.173583984375, val loss None, lr 0.001
iter 90, train loss 924.6424560546875, val loss None, lr 0.001
best loss 849.337646484375
layer6: self_attn.k_proj
256
val_hessian None
iter 0, train loss 1020.3526611328125, val loss None, lr 0.001
iter 10, train loss 1068.6639404296875, val loss None, lr 0.001
iter 20, train loss 1044.993896484375, val loss None, lr 0.001
iter 30, train loss 1016.9633178710938, val loss None, lr 0.001
iter 40, train loss 1001.2094116210938, val loss None, lr 0.001
iter 50, train loss 997.5262451171875, val loss None, lr 0.001
iter 60, train loss 1001.6588134765625, val loss None, lr 0.001
iter 70, train loss 988.0261840820312, val loss None, lr 0.001
iter 80, train loss 987.454345703125, val loss None, lr 0.001
iter 90, train loss 982.618408203125, val loss None, lr 0.001
best loss 906.5343017578125
layer6: self_attn.v_proj
256
val_hessian None
iter 0, train loss 232.69891357421875, val loss None, lr 0.001
iter 10, train loss 233.92324829101562, val loss None, lr 0.001
iter 20, train loss 233.68951416015625, val loss None, lr 0.001
iter 30, train loss 232.49954223632812, val loss None, lr 0.001
iter 40, train loss 231.9288330078125, val loss None, lr 0.001
iter 50, train loss 231.61605834960938, val loss None, lr 0.001
iter 60, train loss 231.63494873046875, val loss None, lr 0.001
iter 70, train loss 231.18942260742188, val loss None, lr 0.001
iter 80, train loss 231.63555908203125, val loss None, lr 0.001
iter 90, train loss 231.99832153320312, val loss None, lr 0.001
best loss 231.1729736328125
layer6: self_attn.o_proj
256
val_hessian None
iter 0, train loss 6.673110485076904, val loss None, lr 0.001
iter 10, train loss 6.235259056091309, val loss None, lr 0.001
iter 20, train loss 6.034271240234375, val loss None, lr 0.001
iter 30, train loss 5.970623016357422, val loss None, lr 0.001
iter 40, train loss 5.875919342041016, val loss None, lr 0.001
iter 50, train loss 5.8129448890686035, val loss None, lr 0.001
iter 60, train loss 5.794074058532715, val loss None, lr 0.001
iter 70, train loss 5.791027069091797, val loss None, lr 0.001
iter 80, train loss 5.78875207901001, val loss None, lr 0.001
iter 90, train loss 5.791900157928467, val loss None, lr 0.001
best loss 5.776197910308838
layer6: mlp.gate_proj
256
val_hessian None
iter 0, train loss 498.09521484375, val loss None, lr 0.001
iter 10, train loss 520.3346557617188, val loss None, lr 0.001
iter 20, train loss 511.6473388671875, val loss None, lr 0.001
iter 30, train loss 508.61566162109375, val loss None, lr 0.001
iter 40, train loss 507.9736328125, val loss None, lr 0.001
iter 50, train loss 507.41424560546875, val loss None, lr 0.001
iter 60, train loss 508.2457580566406, val loss None, lr 0.001
iter 70, train loss 507.537841796875, val loss None, lr 0.001
iter 80, train loss 508.2226867675781, val loss None, lr 0.001
iter 90, train loss 507.94903564453125, val loss None, lr 0.001
best loss 493.1831970214844
layer6: mlp.up_proj
256
val_hessian None
iter 0, train loss 369.05914306640625, val loss None, lr 0.001
iter 10, train loss 369.9482421875, val loss None, lr 0.001
iter 20, train loss 371.7976379394531, val loss None, lr 0.001
iter 30, train loss 373.13458251953125, val loss None, lr 0.001
iter 40, train loss 373.9037780761719, val loss None, lr 0.001
iter 50, train loss 374.5044860839844, val loss None, lr 0.001
iter 60, train loss 374.86212158203125, val loss None, lr 0.001
iter 70, train loss 375.40692138671875, val loss None, lr 0.001
iter 80, train loss 375.48876953125, val loss None, lr 0.001
iter 90, train loss 376.2801513671875, val loss None, lr 0.001
best loss 369.05914306640625
layer6: mlp.down_proj
256
val_hessian None
iter 0, train loss 5.388860702514648, val loss None, lr 0.001
iter 10, train loss 5.372554779052734, val loss None, lr 0.001
iter 20, train loss 5.330428123474121, val loss None, lr 0.001
iter 30, train loss 5.308760643005371, val loss None, lr 0.001
iter 40, train loss 5.315733909606934, val loss None, lr 0.001
iter 50, train loss 5.319385528564453, val loss None, lr 0.001
iter 60, train loss 5.322138786315918, val loss None, lr 0.001
iter 70, train loss 5.324160575866699, val loss None, lr 0.001
iter 80, train loss 5.335405349731445, val loss None, lr 0.001
iter 90, train loss 5.342093467712402, val loss None, lr 0.001
best loss 5.303460597991943
40089 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:13,  2.25it/s]Inference:   6%|▋         | 2/32 [00:00<00:14,  2.05it/s]Inference:   9%|▉         | 3/32 [00:01<00:15,  1.91it/s]Inference:  12%|█▎        | 4/32 [00:01<00:14,  1.99it/s]Inference:  16%|█▌        | 5/32 [00:02<00:13,  2.05it/s]Inference:  19%|█▉        | 6/32 [00:02<00:12,  2.08it/s]Inference:  22%|██▏       | 7/32 [00:03<00:11,  2.24it/s]Inference:  25%|██▌       | 8/32 [00:03<00:10,  2.24it/s]Inference:  28%|██▊       | 9/32 [00:04<00:09,  2.33it/s]Inference:  31%|███▏      | 10/32 [00:04<00:09,  2.29it/s]Inference:  34%|███▍      | 11/32 [00:05<00:09,  2.30it/s]Inference:  38%|███▊      | 12/32 [00:05<00:08,  2.39it/s]Inference:  41%|████      | 13/32 [00:05<00:07,  2.69it/s]Inference:  44%|████▍     | 14/32 [00:05<00:06,  2.81it/s]Inference:  47%|████▋     | 15/32 [00:06<00:06,  2.72it/s]Inference:  50%|█████     | 16/32 [00:06<00:05,  2.83it/s]Inference:  53%|█████▎    | 17/32 [00:07<00:05,  2.90it/s]Inference:  56%|█████▋    | 18/32 [00:07<00:04,  2.97it/s]Inference:  59%|█████▉    | 19/32 [00:07<00:04,  3.02it/s]Inference:  62%|██████▎   | 20/32 [00:07<00:03,  3.19it/s]Inference:  66%|██████▌   | 21/32 [00:08<00:03,  3.12it/s]Inference:  69%|██████▉   | 22/32 [00:08<00:03,  3.09it/s]Inference:  72%|███████▏  | 23/32 [00:08<00:02,  3.26it/s]Inference:  75%|███████▌  | 24/32 [00:09<00:02,  3.13it/s]Inference:  78%|███████▊  | 25/32 [00:09<00:02,  3.11it/s]Inference:  81%|████████▏ | 26/32 [00:09<00:02,  2.85it/s]Inference:  84%|████████▍ | 27/32 [00:10<00:01,  3.10it/s]Inference:  88%|████████▊ | 28/32 [00:10<00:01,  3.06it/s]Inference:  91%|█████████ | 29/32 [00:10<00:01,  2.99it/s]Inference:  94%|█████████▍| 30/32 [00:11<00:00,  3.14it/s]Inference:  97%|█████████▋| 31/32 [00:11<00:00,  3.14it/s]Inference: 100%|██████████| 32/32 [00:11<00:00,  2.95it/s]Inference: 100%|██████████| 32/32 [00:11<00:00,  2.69it/s]
34863 MiB free out of 48676 MiB total
Saved layer 6 to ./models/meta-llama/Llama-2-7b-hf/2bpv/128/quantized/layer_6.pt
after cast to cpu
38865 MiB free out of 48676 MiB total
Done with layer 6 total_time elapsed: 3607 estimated time left: 12882
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:24,  1.26it/s]Inference:   6%|▋         | 2/32 [00:01<00:19,  1.57it/s]Inference:   9%|▉         | 3/32 [00:02<00:19,  1.45it/s]Inference:  12%|█▎        | 4/32 [00:02<00:20,  1.40it/s]Inference:  16%|█▌        | 5/32 [00:03<00:20,  1.34it/s]Inference:  19%|█▉        | 6/32 [00:04<00:18,  1.43it/s]Inference:  22%|██▏       | 7/32 [00:04<00:17,  1.45it/s]Inference:  25%|██▌       | 8/32 [00:05<00:16,  1.42it/s]Inference:  28%|██▊       | 9/32 [00:06<00:16,  1.42it/s]Inference:  31%|███▏      | 10/32 [00:07<00:15,  1.42it/s]Inference:  34%|███▍      | 11/32 [00:07<00:14,  1.45it/s]Inference:  38%|███▊      | 12/32 [00:08<00:13,  1.47it/s]Inference:  41%|████      | 13/32 [00:08<00:12,  1.58it/s]Inference:  44%|████▍     | 14/32 [00:09<00:12,  1.43it/s]Inference:  47%|████▋     | 15/32 [00:10<00:11,  1.43it/s]Inference:  50%|█████     | 16/32 [00:11<00:11,  1.39it/s]Inference:  53%|█████▎    | 17/32 [00:11<00:10,  1.48it/s]Inference:  56%|█████▋    | 18/32 [00:12<00:09,  1.44it/s]Inference:  59%|█████▉    | 19/32 [00:13<00:09,  1.39it/s]Inference:  62%|██████▎   | 20/32 [00:13<00:08,  1.41it/s]Inference:  66%|██████▌   | 21/32 [00:14<00:07,  1.47it/s]Inference:  69%|██████▉   | 22/32 [00:15<00:06,  1.51it/s]Inference:  72%|███████▏  | 23/32 [00:15<00:06,  1.43it/s]Inference:  75%|███████▌  | 24/32 [00:16<00:05,  1.35it/s]Inference:  78%|███████▊  | 25/32 [00:17<00:04,  1.41it/s]Inference:  81%|████████▏ | 26/32 [00:18<00:04,  1.45it/s]Inference:  84%|████████▍ | 27/32 [00:18<00:03,  1.45it/s]Inference:  88%|████████▊ | 28/32 [00:19<00:02,  1.47it/s]Inference:  91%|█████████ | 29/32 [00:20<00:01,  1.52it/s]Inference:  94%|█████████▍| 30/32 [00:20<00:01,  1.52it/s]Inference:  97%|█████████▋| 31/32 [00:21<00:00,  1.46it/s]Inference: 100%|██████████| 32/32 [00:22<00:00,  1.43it/s]Inference: 100%|██████████| 32/32 [00:22<00:00,  1.44it/s]
layer7: self_attn.q_proj
256
val_hessian None
iter 0, train loss 980.852783203125, val loss None, lr 0.001
iter 10, train loss 1067.884521484375, val loss None, lr 0.001
iter 20, train loss 1034.4117431640625, val loss None, lr 0.001
iter 30, train loss 997.3204956054688, val loss None, lr 0.001
iter 40, train loss 1004.148193359375, val loss None, lr 0.001
iter 50, train loss 996.3020629882812, val loss None, lr 0.001
iter 60, train loss 991.2045288085938, val loss None, lr 0.001
iter 70, train loss 997.359375, val loss None, lr 0.001
iter 80, train loss 994.6971435546875, val loss None, lr 0.001
iter 90, train loss 994.4185791015625, val loss None, lr 0.001
best loss 911.4361572265625
layer7: self_attn.k_proj
256
val_hessian None
iter 0, train loss 1033.0712890625, val loss None, lr 0.001
iter 10, train loss 1095.0240478515625, val loss None, lr 0.001
iter 20, train loss 1076.56982421875, val loss None, lr 0.001
iter 30, train loss 1030.145263671875, val loss None, lr 0.001
iter 40, train loss 1033.0780029296875, val loss None, lr 0.001
iter 50, train loss 1025.0771484375, val loss None, lr 0.001
iter 60, train loss 1019.37451171875, val loss None, lr 0.001
iter 70, train loss 1027.6727294921875, val loss None, lr 0.001
iter 80, train loss 1024.773193359375, val loss None, lr 0.001
iter 90, train loss 1018.86376953125, val loss None, lr 0.001
best loss 943.9862670898438
layer7: self_attn.v_proj
256
val_hessian None
iter 0, train loss 258.1228332519531, val loss None, lr 0.001
iter 10, train loss 258.60260009765625, val loss None, lr 0.001
iter 20, train loss 257.8340759277344, val loss None, lr 0.001
iter 30, train loss 257.1387634277344, val loss None, lr 0.001
iter 40, train loss 256.9125671386719, val loss None, lr 0.001
iter 50, train loss 256.20965576171875, val loss None, lr 0.001
iter 60, train loss 256.1065673828125, val loss None, lr 0.001
iter 70, train loss 256.00970458984375, val loss None, lr 0.001
iter 80, train loss 256.02874755859375, val loss None, lr 0.001
iter 90, train loss 255.9575653076172, val loss None, lr 0.001
best loss 255.876220703125
layer7: self_attn.o_proj
256
val_hessian None
iter 0, train loss 9.316085815429688, val loss None, lr 0.001
iter 10, train loss 8.8792142868042, val loss None, lr 0.001
iter 20, train loss 8.708736419677734, val loss None, lr 0.001
iter 30, train loss 8.661497116088867, val loss None, lr 0.001
iter 40, train loss 8.620262145996094, val loss None, lr 0.001
iter 50, train loss 8.609800338745117, val loss None, lr 0.001
iter 60, train loss 8.62061882019043, val loss None, lr 0.001
iter 70, train loss 8.620402336120605, val loss None, lr 0.001
iter 80, train loss 8.620075225830078, val loss None, lr 0.001
iter 90, train loss 8.62358570098877, val loss None, lr 0.001
best loss 8.599859237670898
layer7: mlp.gate_proj
256
val_hessian None
iter 0, train loss 562.2950439453125, val loss None, lr 0.001
iter 10, train loss 591.68359375, val loss None, lr 0.001
iter 20, train loss 577.7174682617188, val loss None, lr 0.001
iter 30, train loss 575.52294921875, val loss None, lr 0.001
iter 40, train loss 574.6345825195312, val loss None, lr 0.001
iter 50, train loss 571.7019653320312, val loss None, lr 0.001
iter 60, train loss 570.9562377929688, val loss None, lr 0.001
iter 70, train loss 571.8110961914062, val loss None, lr 0.001
iter 80, train loss 569.747802734375, val loss None, lr 0.001
iter 90, train loss 571.386962890625, val loss None, lr 0.001
best loss 556.7510375976562
layer7: mlp.up_proj
256
val_hessian None
iter 0, train loss 420.5890808105469, val loss None, lr 0.001
iter 10, train loss 421.7688903808594, val loss None, lr 0.001
iter 20, train loss 423.06158447265625, val loss None, lr 0.001
iter 30, train loss 423.4262390136719, val loss None, lr 0.001
iter 40, train loss 424.65576171875, val loss None, lr 0.001
iter 50, train loss 424.91357421875, val loss None, lr 0.001
iter 60, train loss 425.3577880859375, val loss None, lr 0.001
iter 70, train loss 424.9426574707031, val loss None, lr 0.001
iter 80, train loss 425.6254577636719, val loss None, lr 0.001
iter 90, train loss 425.994873046875, val loss None, lr 0.001
best loss 420.5890808105469
layer7: mlp.down_proj
256
val_hessian None
iter 0, train loss 6.972016334533691, val loss None, lr 0.001
iter 10, train loss 6.994124889373779, val loss None, lr 0.001
iter 20, train loss 6.962613105773926, val loss None, lr 0.001
iter 30, train loss 6.940273284912109, val loss None, lr 0.001
iter 40, train loss 6.937139987945557, val loss None, lr 0.001
iter 50, train loss 6.959317207336426, val loss None, lr 0.001
iter 60, train loss 6.964702606201172, val loss None, lr 0.001
iter 70, train loss 6.960226058959961, val loss None, lr 0.001
iter 80, train loss 6.973077297210693, val loss None, lr 0.001
iter 90, train loss 6.98619270324707, val loss None, lr 0.001
best loss 6.9350433349609375
38865 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:09,  3.28it/s]Inference:   6%|▋         | 2/32 [00:00<00:08,  3.36it/s]Inference:   9%|▉         | 3/32 [00:00<00:08,  3.32it/s]Inference:  12%|█▎        | 4/32 [00:01<00:08,  3.50it/s]Inference:  16%|█▌        | 5/32 [00:01<00:07,  3.48it/s]Inference:  19%|█▉        | 6/32 [00:01<00:07,  3.53it/s]Inference:  22%|██▏       | 7/32 [00:01<00:06,  3.77it/s]Inference:  25%|██▌       | 8/32 [00:02<00:06,  3.72it/s]Inference:  28%|██▊       | 9/32 [00:02<00:06,  3.31it/s]Inference:  31%|███▏      | 10/32 [00:02<00:06,  3.50it/s]Inference:  34%|███▍      | 11/32 [00:03<00:05,  3.60it/s]Inference:  38%|███▊      | 12/32 [00:03<00:05,  3.45it/s]Inference:  41%|████      | 13/32 [00:03<00:05,  3.61it/s]Inference:  44%|████▍     | 14/32 [00:03<00:05,  3.53it/s]Inference:  47%|████▋     | 15/32 [00:04<00:05,  3.19it/s]Inference:  50%|█████     | 16/32 [00:04<00:04,  3.37it/s]Inference:  53%|█████▎    | 17/32 [00:04<00:04,  3.41it/s]Inference:  56%|█████▋    | 18/32 [00:05<00:04,  3.17it/s]Inference:  59%|█████▉    | 19/32 [00:05<00:03,  3.33it/s]Inference:  62%|██████▎   | 20/32 [00:05<00:03,  3.51it/s]Inference:  66%|██████▌   | 21/32 [00:06<00:03,  3.36it/s]Inference:  69%|██████▉   | 22/32 [00:06<00:02,  3.42it/s]Inference:  72%|███████▏  | 23/32 [00:06<00:02,  3.57it/s]Inference:  75%|███████▌  | 24/32 [00:06<00:02,  3.50it/s]Inference:  78%|███████▊  | 25/32 [00:07<00:02,  3.45it/s]Inference:  81%|████████▏ | 26/32 [00:07<00:01,  3.68it/s]Inference:  84%|████████▍ | 27/32 [00:07<00:01,  3.60it/s]Inference:  88%|████████▊ | 28/32 [00:08<00:01,  3.56it/s]Inference:  91%|█████████ | 29/32 [00:08<00:00,  3.56it/s]Inference:  94%|█████████▍| 30/32 [00:08<00:00,  3.51it/s]Inference:  97%|█████████▋| 31/32 [00:08<00:00,  3.38it/s]Inference: 100%|██████████| 32/32 [00:09<00:00,  3.55it/s]Inference: 100%|██████████| 32/32 [00:09<00:00,  3.48it/s]
33575 MiB free out of 48676 MiB total
Saved layer 7 to ./models/meta-llama/Llama-2-7b-hf/2bpv/128/quantized/layer_7.pt
after cast to cpu
37577 MiB free out of 48676 MiB total
Done with layer 7 total_time elapsed: 4127 estimated time left: 12382
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:23,  1.31it/s]Inference:   6%|▋         | 2/32 [00:01<00:19,  1.51it/s]Inference:   9%|▉         | 3/32 [00:02<00:20,  1.44it/s]Inference:  12%|█▎        | 4/32 [00:02<00:19,  1.44it/s]Inference:  16%|█▌        | 5/32 [00:03<00:18,  1.48it/s]Inference:  19%|█▉        | 6/32 [00:04<00:17,  1.50it/s]Inference:  22%|██▏       | 7/32 [00:04<00:16,  1.50it/s]Inference:  25%|██▌       | 8/32 [00:05<00:16,  1.43it/s]Inference:  28%|██▊       | 9/32 [00:06<00:14,  1.54it/s]Inference:  31%|███▏      | 10/32 [00:06<00:14,  1.53it/s]Inference:  34%|███▍      | 11/32 [00:07<00:13,  1.52it/s]Inference:  38%|███▊      | 12/32 [00:08<00:13,  1.50it/s]Inference:  41%|████      | 13/32 [00:08<00:12,  1.53it/s]Inference:  44%|████▍     | 14/32 [00:09<00:11,  1.52it/s]Inference:  47%|████▋     | 15/32 [00:10<00:11,  1.52it/s]Inference:  50%|█████     | 16/32 [00:10<00:10,  1.48it/s]Inference:  53%|█████▎    | 17/32 [00:11<00:09,  1.58it/s]Inference:  56%|█████▋    | 18/32 [00:11<00:08,  1.61it/s]Inference:  59%|█████▉    | 19/32 [00:12<00:08,  1.49it/s]Inference:  62%|██████▎   | 20/32 [00:13<00:08,  1.47it/s]Inference:  66%|██████▌   | 21/32 [00:13<00:06,  1.58it/s]Inference:  69%|██████▉   | 22/32 [00:14<00:06,  1.51it/s]Inference:  72%|███████▏  | 23/32 [00:15<00:05,  1.50it/s]Inference:  75%|███████▌  | 24/32 [00:15<00:05,  1.51it/s]Inference:  78%|███████▊  | 25/32 [00:16<00:04,  1.48it/s]Inference:  81%|████████▏ | 26/32 [00:17<00:04,  1.49it/s]Inference:  84%|████████▍ | 27/32 [00:17<00:03,  1.47it/s]Inference:  88%|████████▊ | 28/32 [00:18<00:02,  1.52it/s]Inference:  91%|█████████ | 29/32 [00:19<00:01,  1.51it/s]Inference:  94%|█████████▍| 30/32 [00:19<00:01,  1.48it/s]Inference:  97%|█████████▋| 31/32 [00:20<00:00,  1.43it/s]Inference: 100%|██████████| 32/32 [00:21<00:00,  1.45it/s]Inference: 100%|██████████| 32/32 [00:21<00:00,  1.49it/s]
layer8: self_attn.q_proj
256
val_hessian None
iter 0, train loss 953.0484619140625, val loss None, lr 0.001
iter 10, train loss 1016.8880615234375, val loss None, lr 0.001
iter 20, train loss 973.8204345703125, val loss None, lr 0.001
iter 30, train loss 957.1888427734375, val loss None, lr 0.001
iter 40, train loss 970.8558349609375, val loss None, lr 0.001
iter 50, train loss 959.1259765625, val loss None, lr 0.001
iter 60, train loss 962.8824462890625, val loss None, lr 0.001
iter 70, train loss 960.110107421875, val loss None, lr 0.001
iter 80, train loss 966.381591796875, val loss None, lr 0.001
iter 90, train loss 959.76953125, val loss None, lr 0.001
best loss 896.240234375
layer8: self_attn.k_proj
256
val_hessian None
iter 0, train loss 990.182373046875, val loss None, lr 0.001
iter 10, train loss 1054.274169921875, val loss None, lr 0.001
iter 20, train loss 1026.632568359375, val loss None, lr 0.001
iter 30, train loss 996.7122802734375, val loss None, lr 0.001
iter 40, train loss 1002.398681640625, val loss None, lr 0.001
iter 50, train loss 994.694091796875, val loss None, lr 0.001
iter 60, train loss 991.1510009765625, val loss None, lr 0.001
iter 70, train loss 989.2313232421875, val loss None, lr 0.001
iter 80, train loss 989.7116088867188, val loss None, lr 0.001
iter 90, train loss 991.166748046875, val loss None, lr 0.001
best loss 941.3018188476562
layer8: self_attn.v_proj
256
val_hessian None
iter 0, train loss 264.74566650390625, val loss None, lr 0.001
iter 10, train loss 266.0761413574219, val loss None, lr 0.001
iter 20, train loss 264.9778137207031, val loss None, lr 0.001
iter 30, train loss 263.63916015625, val loss None, lr 0.001
iter 40, train loss 262.76220703125, val loss None, lr 0.001
iter 50, train loss 261.9857177734375, val loss None, lr 0.001
iter 60, train loss 261.5118713378906, val loss None, lr 0.001
iter 70, train loss 261.58935546875, val loss None, lr 0.001
iter 80, train loss 261.61956787109375, val loss None, lr 0.001
iter 90, train loss 261.196044921875, val loss None, lr 0.001
best loss 260.94854736328125
layer8: self_attn.o_proj
256
val_hessian None
iter 0, train loss 15.121217727661133, val loss None, lr 0.001
iter 10, train loss 14.285822868347168, val loss None, lr 0.001
iter 20, train loss 13.648994445800781, val loss None, lr 0.001
iter 30, train loss 13.276347160339355, val loss None, lr 0.001
iter 40, train loss 13.029705047607422, val loss None, lr 0.001
iter 50, train loss 12.862438201904297, val loss None, lr 0.001
iter 60, train loss 12.793916702270508, val loss None, lr 0.001
iter 70, train loss 12.70425796508789, val loss None, lr 0.001
iter 80, train loss 12.65422534942627, val loss None, lr 0.001
iter 90, train loss 12.631078720092773, val loss None, lr 0.001
best loss 12.631078720092773
layer8: mlp.gate_proj
256
val_hessian None
iter 0, train loss 579.2767333984375, val loss None, lr 0.001
iter 10, train loss 604.5374145507812, val loss None, lr 0.001
iter 20, train loss 590.7861328125, val loss None, lr 0.001
iter 30, train loss 588.0382690429688, val loss None, lr 0.001
iter 40, train loss 588.0185546875, val loss None, lr 0.001
iter 50, train loss 586.494140625, val loss None, lr 0.001
iter 60, train loss 585.5790405273438, val loss None, lr 0.001
iter 70, train loss 585.8162841796875, val loss None, lr 0.001
iter 80, train loss 585.939208984375, val loss None, lr 0.001
iter 90, train loss 586.3425903320312, val loss None, lr 0.001
best loss 574.6531982421875
layer8: mlp.up_proj
256
val_hessian None
iter 0, train loss 461.652099609375, val loss None, lr 0.001
iter 10, train loss 463.9326171875, val loss None, lr 0.001
iter 20, train loss 464.5771789550781, val loss None, lr 0.001
iter 30, train loss 465.5458984375, val loss None, lr 0.001
iter 40, train loss 465.34844970703125, val loss None, lr 0.001
iter 50, train loss 465.8939208984375, val loss None, lr 0.001
iter 60, train loss 465.7584228515625, val loss None, lr 0.001
iter 70, train loss 465.76593017578125, val loss None, lr 0.001
iter 80, train loss 465.7689208984375, val loss None, lr 0.001
iter 90, train loss 465.84197998046875, val loss None, lr 0.001
best loss 461.652099609375
layer8: mlp.down_proj
256
val_hessian None
iter 0, train loss 8.461380958557129, val loss None, lr 0.001
iter 10, train loss 8.484837532043457, val loss None, lr 0.001
iter 20, train loss 8.434828758239746, val loss None, lr 0.001
iter 30, train loss 8.421480178833008, val loss None, lr 0.001
iter 40, train loss 8.411787986755371, val loss None, lr 0.001
iter 50, train loss 8.426057815551758, val loss None, lr 0.001
iter 60, train loss 8.443897247314453, val loss None, lr 0.001
iter 70, train loss 8.453521728515625, val loss None, lr 0.001
iter 80, train loss 8.456186294555664, val loss None, lr 0.001
iter 90, train loss 8.47421646118164, val loss None, lr 0.001
best loss 8.411787986755371
37577 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:11,  2.63it/s]Inference:   6%|▋         | 2/32 [00:00<00:10,  2.88it/s]Inference:   9%|▉         | 3/32 [00:01<00:11,  2.56it/s]Inference:  12%|█▎        | 4/32 [00:01<00:10,  2.72it/s]Inference:  16%|█▌        | 5/32 [00:01<00:09,  2.85it/s]Inference:  19%|█▉        | 6/32 [00:02<00:09,  2.79it/s]Inference:  22%|██▏       | 7/32 [00:02<00:09,  2.77it/s]Inference:  25%|██▌       | 8/32 [00:02<00:08,  2.75it/s]Inference:  28%|██▊       | 9/32 [00:03<00:08,  2.84it/s]Inference:  31%|███▏      | 10/32 [00:03<00:07,  2.83it/s]Inference:  34%|███▍      | 11/32 [00:03<00:07,  2.77it/s]Inference:  38%|███▊      | 12/32 [00:04<00:07,  2.84it/s]Inference:  41%|████      | 13/32 [00:04<00:06,  2.75it/s]Inference:  44%|████▍     | 14/32 [00:04<00:06,  2.95it/s]Inference:  47%|████▋     | 15/32 [00:05<00:05,  2.91it/s]Inference:  50%|█████     | 16/32 [00:05<00:05,  2.92it/s]Inference:  53%|█████▎    | 17/32 [00:05<00:04,  3.12it/s]Inference:  56%|█████▋    | 18/32 [00:06<00:04,  3.06it/s]Inference:  59%|█████▉    | 19/32 [00:06<00:04,  2.94it/s]Inference:  62%|██████▎   | 20/32 [00:06<00:04,  2.92it/s]Inference:  66%|██████▌   | 21/32 [00:07<00:03,  2.88it/s]Inference:  69%|██████▉   | 22/32 [00:07<00:03,  2.88it/s]Inference:  72%|███████▏  | 23/32 [00:08<00:03,  2.87it/s]Inference:  75%|███████▌  | 24/32 [00:08<00:02,  2.77it/s]Inference:  78%|███████▊  | 25/32 [00:08<00:02,  2.94it/s]Inference:  81%|████████▏ | 26/32 [00:09<00:02,  2.74it/s]Inference:  84%|████████▍ | 27/32 [00:09<00:01,  2.96it/s]Inference:  88%|████████▊ | 28/32 [00:09<00:01,  2.89it/s]Inference:  91%|█████████ | 29/32 [00:10<00:01,  2.86it/s]Inference:  94%|█████████▍| 30/32 [00:10<00:00,  2.81it/s]Inference:  97%|█████████▋| 31/32 [00:10<00:00,  2.63it/s]Inference: 100%|██████████| 32/32 [00:11<00:00,  2.88it/s]Inference: 100%|██████████| 32/32 [00:11<00:00,  2.85it/s]
32287 MiB free out of 48676 MiB total
Saved layer 8 to ./models/meta-llama/Llama-2-7b-hf/2bpv/128/quantized/layer_8.pt
after cast to cpu
36289 MiB free out of 48676 MiB total
Done with layer 8 total_time elapsed: 4628 estimated time left: 11828
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:17,  1.73it/s]Inference:   6%|▋         | 2/32 [00:01<00:19,  1.55it/s]Inference:   9%|▉         | 3/32 [00:01<00:18,  1.53it/s]Inference:  12%|█▎        | 4/32 [00:02<00:19,  1.45it/s]Inference:  16%|█▌        | 5/32 [00:03<00:18,  1.47it/s]Inference:  19%|█▉        | 6/32 [00:04<00:17,  1.45it/s]Inference:  22%|██▏       | 7/32 [00:04<00:17,  1.44it/s]Inference:  25%|██▌       | 8/32 [00:05<00:16,  1.44it/s]Inference:  28%|██▊       | 9/32 [00:06<00:15,  1.52it/s]Inference:  31%|███▏      | 10/32 [00:06<00:15,  1.46it/s]Inference:  34%|███▍      | 11/32 [00:07<00:14,  1.42it/s]Inference:  38%|███▊      | 12/32 [00:08<00:13,  1.50it/s]Inference:  41%|████      | 13/32 [00:08<00:11,  1.63it/s]Inference:  44%|████▍     | 14/32 [00:09<00:11,  1.60it/s]Inference:  47%|████▋     | 15/32 [00:10<00:11,  1.51it/s]Inference:  50%|█████     | 16/32 [00:10<00:10,  1.49it/s]Inference:  53%|█████▎    | 17/32 [00:11<00:10,  1.48it/s]Inference:  56%|█████▋    | 18/32 [00:12<00:09,  1.47it/s]Inference:  59%|█████▉    | 19/32 [00:12<00:09,  1.40it/s]Inference:  62%|██████▎   | 20/32 [00:13<00:08,  1.41it/s]Inference:  66%|██████▌   | 21/32 [00:14<00:07,  1.49it/s]Inference:  69%|██████▉   | 22/32 [00:14<00:06,  1.50it/s]Inference:  72%|███████▏  | 23/32 [00:15<00:05,  1.56it/s]Inference:  75%|███████▌  | 24/32 [00:15<00:04,  1.60it/s]Inference:  78%|███████▊  | 25/32 [00:16<00:04,  1.62it/s]Inference:  81%|████████▏ | 26/32 [00:17<00:03,  1.53it/s]Inference:  84%|████████▍ | 27/32 [00:17<00:03,  1.52it/s]Inference:  88%|████████▊ | 28/32 [00:18<00:02,  1.52it/s]Inference:  91%|█████████ | 29/32 [00:19<00:01,  1.52it/s]Inference:  94%|█████████▍| 30/32 [00:20<00:01,  1.46it/s]Inference:  97%|█████████▋| 31/32 [00:20<00:00,  1.46it/s]Inference: 100%|██████████| 32/32 [00:21<00:00,  1.48it/s]Inference: 100%|██████████| 32/32 [00:21<00:00,  1.50it/s]
layer9: self_attn.q_proj
256
val_hessian None
iter 0, train loss 975.9083251953125, val loss None, lr 0.001
iter 10, train loss 1055.330078125, val loss None, lr 0.001
iter 20, train loss 994.294921875, val loss None, lr 0.001
iter 30, train loss 987.6470947265625, val loss None, lr 0.001
iter 40, train loss 991.4786376953125, val loss None, lr 0.001
iter 50, train loss 990.4075927734375, val loss None, lr 0.001
iter 60, train loss 989.8369750976562, val loss None, lr 0.001
iter 70, train loss 988.7567138671875, val loss None, lr 0.001
iter 80, train loss 992.9359130859375, val loss None, lr 0.001
iter 90, train loss 997.1085205078125, val loss None, lr 0.001
best loss 932.17138671875
layer9: self_attn.k_proj
256
val_hessian None
iter 0, train loss 1066.3756103515625, val loss None, lr 0.001
iter 10, train loss 1140.7635498046875, val loss None, lr 0.001
iter 20, train loss 1108.0059814453125, val loss None, lr 0.001
iter 30, train loss 1076.538818359375, val loss None, lr 0.001
iter 40, train loss 1083.1046142578125, val loss None, lr 0.001
iter 50, train loss 1069.0628662109375, val loss None, lr 0.001
iter 60, train loss 1071.6451416015625, val loss None, lr 0.001
iter 70, train loss 1067.726318359375, val loss None, lr 0.001
iter 80, train loss 1077.46728515625, val loss None, lr 0.001
iter 90, train loss 1081.7000732421875, val loss None, lr 0.001
best loss 1020.276123046875
layer9: self_attn.v_proj
256
val_hessian None
iter 0, train loss 286.8939208984375, val loss None, lr 0.001
iter 10, train loss 287.1507873535156, val loss None, lr 0.001
iter 20, train loss 286.71710205078125, val loss None, lr 0.001
iter 30, train loss 285.1768493652344, val loss None, lr 0.001
iter 40, train loss 284.27685546875, val loss None, lr 0.001
iter 50, train loss 283.8096923828125, val loss None, lr 0.001
iter 60, train loss 283.528564453125, val loss None, lr 0.001
iter 70, train loss 283.5600891113281, val loss None, lr 0.001
iter 80, train loss 283.1418151855469, val loss None, lr 0.001
iter 90, train loss 282.91650390625, val loss None, lr 0.001
best loss 282.83111572265625
layer9: self_attn.o_proj
256
val_hessian None
iter 0, train loss 19.0450439453125, val loss None, lr 0.001
iter 10, train loss 18.109315872192383, val loss None, lr 0.001
iter 20, train loss 17.702102661132812, val loss None, lr 0.001
iter 30, train loss 17.559268951416016, val loss None, lr 0.001
iter 40, train loss 17.39459800720215, val loss None, lr 0.001
iter 50, train loss 17.355693817138672, val loss None, lr 0.001
iter 60, train loss 17.29645538330078, val loss None, lr 0.001
iter 70, train loss 17.288829803466797, val loss None, lr 0.001
iter 80, train loss 17.26373291015625, val loss None, lr 0.001
iter 90, train loss 17.285600662231445, val loss None, lr 0.001
best loss 17.232818603515625
layer9: mlp.gate_proj
256
val_hessian None
iter 0, train loss 601.3176879882812, val loss None, lr 0.001
iter 10, train loss 628.4332275390625, val loss None, lr 0.001
iter 20, train loss 617.6611328125, val loss None, lr 0.001
iter 30, train loss 613.8861083984375, val loss None, lr 0.001
iter 40, train loss 611.3363037109375, val loss None, lr 0.001
iter 50, train loss 609.8262939453125, val loss None, lr 0.001
iter 60, train loss 608.0604248046875, val loss None, lr 0.001
iter 70, train loss 607.7943115234375, val loss None, lr 0.001
iter 80, train loss 607.772216796875, val loss None, lr 0.001
iter 90, train loss 609.0690307617188, val loss None, lr 0.001
best loss 595.2918090820312
layer9: mlp.up_proj
256
val_hessian None
iter 0, train loss 497.706298828125, val loss None, lr 0.001
iter 10, train loss 500.16046142578125, val loss None, lr 0.001
iter 20, train loss 501.02801513671875, val loss None, lr 0.001
iter 30, train loss 501.606689453125, val loss None, lr 0.001
iter 40, train loss 502.48626708984375, val loss None, lr 0.001
iter 50, train loss 503.1923828125, val loss None, lr 0.001
iter 60, train loss 503.5379638671875, val loss None, lr 0.001
iter 70, train loss 503.553466796875, val loss None, lr 0.001
iter 80, train loss 503.3865051269531, val loss None, lr 0.001
iter 90, train loss 503.3798828125, val loss None, lr 0.001
best loss 497.6328430175781
layer9: mlp.down_proj
256
val_hessian None
iter 0, train loss 10.140253067016602, val loss None, lr 0.001
iter 10, train loss 10.130356788635254, val loss None, lr 0.001
iter 20, train loss 10.103240966796875, val loss None, lr 0.001
iter 30, train loss 10.067334175109863, val loss None, lr 0.001
iter 40, train loss 10.052083969116211, val loss None, lr 0.001
iter 50, train loss 10.050868034362793, val loss None, lr 0.001
iter 60, train loss 10.055335998535156, val loss None, lr 0.001
iter 70, train loss 10.064258575439453, val loss None, lr 0.001
iter 80, train loss 10.073734283447266, val loss None, lr 0.001
iter 90, train loss 10.097075462341309, val loss None, lr 0.001
best loss 10.035181999206543
36289 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:09,  3.30it/s]Inference:   6%|▋         | 2/32 [00:00<00:08,  3.41it/s]Inference:   9%|▉         | 3/32 [00:00<00:08,  3.61it/s]Inference:  12%|█▎        | 4/32 [00:01<00:07,  3.51it/s]Inference:  16%|█▌        | 5/32 [00:01<00:08,  3.16it/s]Inference:  19%|█▉        | 6/32 [00:01<00:07,  3.64it/s]Inference:  22%|██▏       | 7/32 [00:01<00:06,  3.66it/s]Inference:  25%|██▌       | 8/32 [00:02<00:07,  3.27it/s]Inference:  28%|██▊       | 9/32 [00:02<00:06,  3.57it/s]Inference:  31%|███▏      | 10/32 [00:02<00:06,  3.55it/s]Inference:  34%|███▍      | 11/32 [00:03<00:05,  3.53it/s]Inference:  38%|███▊      | 12/32 [00:03<00:05,  3.54it/s]Inference:  41%|████      | 13/32 [00:03<00:04,  3.81it/s]Inference:  44%|████▍     | 14/32 [00:03<00:04,  4.01it/s]Inference:  47%|████▋     | 15/32 [00:04<00:04,  3.46it/s]Inference:  50%|█████     | 16/32 [00:04<00:04,  3.71it/s]Inference:  53%|█████▎    | 17/32 [00:04<00:04,  3.64it/s]Inference:  56%|█████▋    | 18/32 [00:05<00:04,  3.19it/s]Inference:  59%|█████▉    | 19/32 [00:05<00:03,  3.47it/s]Inference:  62%|██████▎   | 20/32 [00:05<00:03,  3.72it/s]Inference:  66%|██████▌   | 21/32 [00:05<00:03,  3.52it/s]Inference:  69%|██████▉   | 22/32 [00:06<00:02,  3.85it/s]Inference:  72%|███████▏  | 23/32 [00:06<00:02,  4.02it/s]Inference:  75%|███████▌  | 24/32 [00:06<00:02,  3.88it/s]Inference:  78%|███████▊  | 25/32 [00:06<00:01,  3.81it/s]Inference:  81%|████████▏ | 26/32 [00:07<00:01,  3.98it/s]Inference:  84%|████████▍ | 27/32 [00:07<00:01,  3.83it/s]Inference:  88%|████████▊ | 28/32 [00:07<00:01,  3.79it/s]Inference:  91%|█████████ | 29/32 [00:07<00:00,  3.71it/s]Inference:  94%|█████████▍| 30/32 [00:08<00:00,  3.67it/s]Inference:  97%|█████████▋| 31/32 [00:08<00:00,  3.62it/s]Inference: 100%|██████████| 32/32 [00:08<00:00,  3.60it/s]Inference: 100%|██████████| 32/32 [00:08<00:00,  3.63it/s]
31063 MiB free out of 48676 MiB total
Saved layer 9 to ./models/meta-llama/Llama-2-7b-hf/2bpv/128/quantized/layer_9.pt
after cast to cpu
35065 MiB free out of 48676 MiB total
Done with layer 9 total_time elapsed: 5116 estimated time left: 11256
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:17,  1.74it/s]Inference:   6%|▋         | 2/32 [00:01<00:20,  1.49it/s]Inference:   9%|▉         | 3/32 [00:01<00:19,  1.48it/s]Inference:  12%|█▎        | 4/32 [00:02<00:18,  1.55it/s]Inference:  16%|█▌        | 5/32 [00:03<00:17,  1.53it/s]Inference:  19%|█▉        | 6/32 [00:04<00:17,  1.46it/s]Inference:  22%|██▏       | 7/32 [00:04<00:17,  1.41it/s]Inference:  25%|██▌       | 8/32 [00:05<00:16,  1.50it/s]Inference:  28%|██▊       | 9/32 [00:06<00:15,  1.44it/s]Inference:  31%|███▏      | 10/32 [00:06<00:15,  1.46it/s]Inference:  34%|███▍      | 11/32 [00:07<00:14,  1.48it/s]Inference:  38%|███▊      | 12/32 [00:08<00:13,  1.49it/s]Inference:  41%|████      | 13/32 [00:08<00:12,  1.49it/s]Inference:  44%|████▍     | 14/32 [00:09<00:12,  1.44it/s]Inference:  47%|████▋     | 15/32 [00:10<00:11,  1.46it/s]Inference:  50%|█████     | 16/32 [00:10<00:10,  1.48it/s]Inference:  53%|█████▎    | 17/32 [00:11<00:10,  1.43it/s]Inference:  56%|█████▋    | 18/32 [00:12<00:09,  1.51it/s]Inference:  59%|█████▉    | 19/32 [00:12<00:08,  1.51it/s]Inference:  62%|██████▎   | 20/32 [00:13<00:07,  1.57it/s]Inference:  66%|██████▌   | 21/32 [00:14<00:07,  1.55it/s]Inference:  69%|██████▉   | 22/32 [00:14<00:06,  1.54it/s]Inference:  72%|███████▏  | 23/32 [00:15<00:05,  1.53it/s]Inference:  75%|███████▌  | 24/32 [00:16<00:05,  1.47it/s]Inference:  78%|███████▊  | 25/32 [00:16<00:04,  1.48it/s]Inference:  81%|████████▏ | 26/32 [00:17<00:04,  1.44it/s]Inference:  84%|████████▍ | 27/32 [00:18<00:03,  1.41it/s]Inference:  88%|████████▊ | 28/32 [00:18<00:02,  1.44it/s]Inference:  91%|█████████ | 29/32 [00:19<00:02,  1.46it/s]Inference:  94%|█████████▍| 30/32 [00:20<00:01,  1.51it/s]Inference:  97%|█████████▋| 31/32 [00:21<00:00,  1.42it/s]Inference: 100%|██████████| 32/32 [00:21<00:00,  1.50it/s]Inference: 100%|██████████| 32/32 [00:21<00:00,  1.48it/s]
layer10: self_attn.q_proj
256
val_hessian None
iter 0, train loss 997.0827026367188, val loss None, lr 0.001
iter 10, train loss 1064.3193359375, val loss None, lr 0.001
iter 20, train loss 1011.3009643554688, val loss None, lr 0.001
iter 30, train loss 1005.1730346679688, val loss None, lr 0.001
iter 40, train loss 1005.3856201171875, val loss None, lr 0.001
iter 50, train loss 1000.7341918945312, val loss None, lr 0.001
iter 60, train loss 999.30419921875, val loss None, lr 0.001
iter 70, train loss 1000.3583984375, val loss None, lr 0.001
iter 80, train loss 1001.702392578125, val loss None, lr 0.001
iter 90, train loss 999.9853515625, val loss None, lr 0.001
best loss 953.2152099609375
layer10: self_attn.k_proj
256
val_hessian None
iter 0, train loss 1099.90673828125, val loss None, lr 0.001
iter 10, train loss 1180.0841064453125, val loss None, lr 0.001
iter 20, train loss 1163.264892578125, val loss None, lr 0.001
iter 30, train loss 1132.229736328125, val loss None, lr 0.001
iter 40, train loss 1131.23779296875, val loss None, lr 0.001
iter 50, train loss 1128.44384765625, val loss None, lr 0.001
iter 60, train loss 1128.061767578125, val loss None, lr 0.001
iter 70, train loss 1126.3763427734375, val loss None, lr 0.001
iter 80, train loss 1130.177490234375, val loss None, lr 0.001
iter 90, train loss 1130.6890869140625, val loss None, lr 0.001
best loss 1059.5
layer10: self_attn.v_proj
256
val_hessian None
iter 0, train loss 287.72955322265625, val loss None, lr 0.001
iter 10, train loss 287.4472351074219, val loss None, lr 0.001
iter 20, train loss 286.5433044433594, val loss None, lr 0.001
iter 30, train loss 285.31365966796875, val loss None, lr 0.001
iter 40, train loss 284.7380065917969, val loss None, lr 0.001
iter 50, train loss 284.2856140136719, val loss None, lr 0.001
iter 60, train loss 283.556396484375, val loss None, lr 0.001
iter 70, train loss 283.0770568847656, val loss None, lr 0.001
iter 80, train loss 282.84222412109375, val loss None, lr 0.001
iter 90, train loss 282.6800537109375, val loss None, lr 0.001
best loss 282.39862060546875
layer10: self_attn.o_proj
256
val_hessian None
iter 0, train loss 29.628131866455078, val loss None, lr 0.001
iter 10, train loss 27.709117889404297, val loss None, lr 0.001
iter 20, train loss 26.66727638244629, val loss None, lr 0.001
iter 30, train loss 25.61127281188965, val loss None, lr 0.001
iter 40, train loss 25.21856689453125, val loss None, lr 0.001
iter 50, train loss 24.824440002441406, val loss None, lr 0.001
iter 60, train loss 24.68805694580078, val loss None, lr 0.001
iter 70, train loss 24.637462615966797, val loss None, lr 0.001
iter 80, train loss 24.55906867980957, val loss None, lr 0.001
iter 90, train loss 24.566137313842773, val loss None, lr 0.001
best loss 24.514667510986328
layer10: mlp.gate_proj
256
val_hessian None
iter 0, train loss 627.9559936523438, val loss None, lr 0.001
iter 10, train loss 660.673828125, val loss None, lr 0.001
iter 20, train loss 646.16015625, val loss None, lr 0.001
iter 30, train loss 641.3118896484375, val loss None, lr 0.001
iter 40, train loss 639.5523681640625, val loss None, lr 0.001
iter 50, train loss 638.6263427734375, val loss None, lr 0.001
iter 60, train loss 637.9701538085938, val loss None, lr 0.001
iter 70, train loss 636.5975952148438, val loss None, lr 0.001
iter 80, train loss 635.8079833984375, val loss None, lr 0.001
iter 90, train loss 636.5018310546875, val loss None, lr 0.001
best loss 621.6219482421875
layer10: mlp.up_proj
256
val_hessian None
iter 0, train loss 530.8599853515625, val loss None, lr 0.001
iter 10, train loss 533.6143798828125, val loss None, lr 0.001
iter 20, train loss 535.6551513671875, val loss None, lr 0.001
iter 30, train loss 535.40478515625, val loss None, lr 0.001
iter 40, train loss 535.6774291992188, val loss None, lr 0.001
iter 50, train loss 535.4432373046875, val loss None, lr 0.001
iter 60, train loss 535.9517211914062, val loss None, lr 0.001
iter 70, train loss 535.5146484375, val loss None, lr 0.001
iter 80, train loss 536.38525390625, val loss None, lr 0.001
iter 90, train loss 536.2684936523438, val loss None, lr 0.001
best loss 529.9600830078125
layer10: mlp.down_proj
256
val_hessian None
iter 0, train loss 12.47974967956543, val loss None, lr 0.001
iter 10, train loss 12.44484806060791, val loss None, lr 0.001
iter 20, train loss 12.348968505859375, val loss None, lr 0.001
iter 30, train loss 12.253414154052734, val loss None, lr 0.001
iter 40, train loss 12.199763298034668, val loss None, lr 0.001
iter 50, train loss 12.227548599243164, val loss None, lr 0.001
iter 60, train loss 12.196765899658203, val loss None, lr 0.001
iter 70, train loss 12.16537857055664, val loss None, lr 0.001
iter 80, train loss 12.17568302154541, val loss None, lr 0.001
iter 90, train loss 12.155407905578613, val loss None, lr 0.001
best loss 12.145200729370117
35065 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:11,  2.66it/s]Inference:   6%|▋         | 2/32 [00:00<00:11,  2.60it/s]Inference:   9%|▉         | 3/32 [00:01<00:11,  2.61it/s]Inference:  12%|█▎        | 4/32 [00:01<00:09,  2.93it/s]Inference:  16%|█▌        | 5/32 [00:01<00:09,  2.77it/s]Inference:  19%|█▉        | 6/32 [00:02<00:09,  2.89it/s]Inference:  22%|██▏       | 7/32 [00:02<00:09,  2.72it/s]Inference:  25%|██▌       | 8/32 [00:02<00:08,  2.69it/s]Inference:  28%|██▊       | 9/32 [00:03<00:08,  2.87it/s]Inference:  31%|███▏      | 10/32 [00:03<00:07,  2.83it/s]Inference:  34%|███▍      | 11/32 [00:03<00:07,  2.77it/s]Inference:  38%|███▊      | 12/32 [00:04<00:06,  2.98it/s]Inference:  41%|████      | 13/32 [00:04<00:06,  3.00it/s]Inference:  44%|████▍     | 14/32 [00:04<00:05,  3.18it/s]Inference:  47%|████▋     | 15/32 [00:05<00:05,  3.13it/s]Inference:  50%|█████     | 16/32 [00:05<00:05,  2.99it/s]Inference:  53%|█████▎    | 17/32 [00:05<00:04,  3.17it/s]Inference:  56%|█████▋    | 18/32 [00:06<00:04,  2.88it/s]Inference:  59%|█████▉    | 19/32 [00:06<00:04,  3.10it/s]Inference:  62%|██████▎   | 20/32 [00:06<00:04,  3.00it/s]Inference:  66%|██████▌   | 21/32 [00:07<00:03,  2.96it/s]Inference:  69%|██████▉   | 22/32 [00:07<00:03,  2.90it/s]Inference:  72%|███████▏  | 23/32 [00:07<00:03,  2.77it/s]Inference:  75%|███████▌  | 24/32 [00:08<00:02,  2.97it/s]Inference:  78%|███████▊  | 25/32 [00:08<00:02,  2.98it/s]Inference:  81%|████████▏ | 26/32 [00:08<00:02,  2.95it/s]Inference:  84%|████████▍ | 27/32 [00:09<00:01,  2.93it/s]Inference:  88%|████████▊ | 28/32 [00:09<00:01,  3.16it/s]Inference:  91%|█████████ | 29/32 [00:09<00:00,  3.11it/s]Inference:  94%|█████████▍| 30/32 [00:10<00:00,  3.27it/s]Inference:  97%|█████████▋| 31/32 [00:10<00:00,  3.20it/s]Inference: 100%|██████████| 32/32 [00:10<00:00,  3.00it/s]Inference: 100%|██████████| 32/32 [00:10<00:00,  2.95it/s]
29775 MiB free out of 48676 MiB total
Saved layer 10 to ./models/meta-llama/Llama-2-7b-hf/2bpv/128/quantized/layer_10.pt
after cast to cpu
33777 MiB free out of 48676 MiB total
Done with layer 10 total_time elapsed: 5616 estimated time left: 10721
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:19,  1.59it/s]Inference:   6%|▋         | 2/32 [00:01<00:19,  1.56it/s]Inference:   9%|▉         | 3/32 [00:02<00:20,  1.45it/s]Inference:  12%|█▎        | 4/32 [00:02<00:19,  1.47it/s]Inference:  16%|█▌        | 5/32 [00:03<00:18,  1.49it/s]Inference:  19%|█▉        | 6/32 [00:04<00:18,  1.44it/s]Inference:  22%|██▏       | 7/32 [00:04<00:17,  1.47it/s]Inference:  25%|██▌       | 8/32 [00:05<00:15,  1.55it/s]Inference:  28%|██▊       | 9/32 [00:05<00:14,  1.54it/s]Inference:  31%|███▏      | 10/32 [00:06<00:15,  1.46it/s]Inference:  34%|███▍      | 11/32 [00:07<00:13,  1.54it/s]Inference:  38%|███▊      | 12/32 [00:07<00:13,  1.53it/s]Inference:  41%|████      | 13/32 [00:08<00:12,  1.47it/s]Inference:  44%|████▍     | 14/32 [00:09<00:12,  1.43it/s]Inference:  47%|████▋     | 15/32 [00:10<00:12,  1.41it/s]Inference:  50%|█████     | 16/32 [00:10<00:11,  1.44it/s]Inference:  53%|█████▎    | 17/32 [00:11<00:10,  1.40it/s]Inference:  56%|█████▋    | 18/32 [00:12<00:09,  1.42it/s]Inference:  59%|█████▉    | 19/32 [00:12<00:09,  1.44it/s]Inference:  62%|██████▎   | 20/32 [00:13<00:08,  1.41it/s]Inference:  66%|██████▌   | 21/32 [00:14<00:08,  1.33it/s]Inference:  69%|██████▉   | 22/32 [00:15<00:07,  1.38it/s]Inference:  72%|███████▏  | 23/32 [00:15<00:06,  1.42it/s]Inference:  75%|███████▌  | 24/32 [00:16<00:05,  1.40it/s]Inference:  78%|███████▊  | 25/32 [00:17<00:05,  1.37it/s]Inference:  81%|████████▏ | 26/32 [00:17<00:04,  1.45it/s]Inference:  84%|████████▍ | 27/32 [00:18<00:03,  1.45it/s]Inference:  88%|████████▊ | 28/32 [00:19<00:02,  1.42it/s]Inference:  91%|█████████ | 29/32 [00:20<00:02,  1.44it/s]Inference:  94%|█████████▍| 30/32 [00:20<00:01,  1.46it/s]Inference:  97%|█████████▋| 31/32 [00:21<00:00,  1.47it/s]Inference: 100%|██████████| 32/32 [00:22<00:00,  1.42it/s]Inference: 100%|██████████| 32/32 [00:22<00:00,  1.44it/s]
layer11: self_attn.q_proj
256
val_hessian None
iter 0, train loss 1145.3641357421875, val loss None, lr 0.001
iter 10, train loss 1235.996337890625, val loss None, lr 0.001
iter 20, train loss 1176.3443603515625, val loss None, lr 0.001
iter 30, train loss 1160.9268798828125, val loss None, lr 0.001
iter 40, train loss 1164.4326171875, val loss None, lr 0.001
iter 50, train loss 1156.920654296875, val loss None, lr 0.001
iter 60, train loss 1161.0177001953125, val loss None, lr 0.001
iter 70, train loss 1154.48779296875, val loss None, lr 0.001
iter 80, train loss 1153.116943359375, val loss None, lr 0.001
iter 90, train loss 1151.8369140625, val loss None, lr 0.001
best loss 1076.8095703125
layer11: self_attn.k_proj
256
val_hessian None
iter 0, train loss 1161.535888671875, val loss None, lr 0.001
iter 10, train loss 1253.2203369140625, val loss None, lr 0.001
iter 20, train loss 1221.501220703125, val loss None, lr 0.001
iter 30, train loss 1191.3214111328125, val loss None, lr 0.001
iter 40, train loss 1180.660888671875, val loss None, lr 0.001
iter 50, train loss 1186.15234375, val loss None, lr 0.001
iter 60, train loss 1173.0985107421875, val loss None, lr 0.001
iter 70, train loss 1179.24658203125, val loss None, lr 0.001
iter 80, train loss 1184.1944580078125, val loss None, lr 0.001
iter 90, train loss 1178.9798583984375, val loss None, lr 0.001
best loss 1100.4281005859375
layer11: self_attn.v_proj
256
val_hessian None
iter 0, train loss 391.25091552734375, val loss None, lr 0.001
iter 10, train loss 391.3003234863281, val loss None, lr 0.001
iter 20, train loss 389.7767028808594, val loss None, lr 0.001
iter 30, train loss 388.207763671875, val loss None, lr 0.001
iter 40, train loss 386.6304016113281, val loss None, lr 0.001
iter 50, train loss 386.35675048828125, val loss None, lr 0.001
iter 60, train loss 386.02435302734375, val loss None, lr 0.001
iter 70, train loss 385.3745422363281, val loss None, lr 0.001
iter 80, train loss 385.0481872558594, val loss None, lr 0.001
iter 90, train loss 384.4694519042969, val loss None, lr 0.001
best loss 384.1479797363281
layer11: self_attn.o_proj
256
val_hessian None
iter 0, train loss 32.66191482543945, val loss None, lr 0.001
iter 10, train loss 31.898086547851562, val loss None, lr 0.001
iter 20, train loss 32.07837677001953, val loss None, lr 0.001
iter 30, train loss 31.59583854675293, val loss None, lr 0.001
iter 40, train loss 31.357595443725586, val loss None, lr 0.001
iter 50, train loss 31.23402214050293, val loss None, lr 0.001
iter 60, train loss 31.15213966369629, val loss None, lr 0.001
iter 70, train loss 31.096771240234375, val loss None, lr 0.001
iter 80, train loss 30.955387115478516, val loss None, lr 0.001
iter 90, train loss 30.934316635131836, val loss None, lr 0.001
best loss 30.81524658203125
layer11: mlp.gate_proj
256
val_hessian None
iter 0, train loss 680.3488159179688, val loss None, lr 0.001
iter 10, train loss 717.8389282226562, val loss None, lr 0.001
iter 20, train loss 703.9691772460938, val loss None, lr 0.001
iter 30, train loss 697.56640625, val loss None, lr 0.001
iter 40, train loss 694.3029174804688, val loss None, lr 0.001
iter 50, train loss 691.72265625, val loss None, lr 0.001
iter 60, train loss 689.4574584960938, val loss None, lr 0.001
iter 70, train loss 688.6253662109375, val loss None, lr 0.001
iter 80, train loss 685.808837890625, val loss None, lr 0.001
iter 90, train loss 686.5720825195312, val loss None, lr 0.001
best loss 672.85595703125
layer11: mlp.up_proj
256
val_hessian None
iter 0, train loss 589.4500122070312, val loss None, lr 0.001
iter 10, train loss 593.1817016601562, val loss None, lr 0.001
iter 20, train loss 595.1239624023438, val loss None, lr 0.001
iter 30, train loss 595.5595703125, val loss None, lr 0.001
iter 40, train loss 595.397705078125, val loss None, lr 0.001
iter 50, train loss 595.6617431640625, val loss None, lr 0.001
iter 60, train loss 595.1793823242188, val loss None, lr 0.001
iter 70, train loss 595.376708984375, val loss None, lr 0.001
iter 80, train loss 595.7215576171875, val loss None, lr 0.001
iter 90, train loss 595.7027587890625, val loss None, lr 0.001
best loss 589.4500122070312
layer11: mlp.down_proj
256
val_hessian None
iter 0, train loss 13.759103775024414, val loss None, lr 0.001
iter 10, train loss 13.74321460723877, val loss None, lr 0.001
iter 20, train loss 13.660219192504883, val loss None, lr 0.001
iter 30, train loss 13.648326873779297, val loss None, lr 0.001
iter 40, train loss 13.625955581665039, val loss None, lr 0.001
iter 50, train loss 13.590718269348145, val loss None, lr 0.001
iter 60, train loss 13.592996597290039, val loss None, lr 0.001
iter 70, train loss 13.612428665161133, val loss None, lr 0.001
iter 80, train loss 13.63212776184082, val loss None, lr 0.001
iter 90, train loss 13.673468589782715, val loss None, lr 0.001
best loss 13.568208694458008
33777 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:10,  3.00it/s]Inference:   6%|▋         | 2/32 [00:00<00:09,  3.06it/s]Inference:   9%|▉         | 3/32 [00:00<00:09,  3.11it/s]Inference:  12%|█▎        | 4/32 [00:01<00:10,  2.78it/s]Inference:  16%|█▌        | 5/32 [00:01<00:08,  3.17it/s]Inference:  19%|█▉        | 6/32 [00:01<00:08,  3.22it/s]Inference:  22%|██▏       | 7/32 [00:02<00:08,  2.89it/s]Inference:  25%|██▌       | 8/32 [00:02<00:07,  3.21it/s]Inference:  28%|██▊       | 9/32 [00:02<00:07,  3.21it/s]Inference:  31%|███▏      | 10/32 [00:03<00:07,  2.98it/s]Inference:  34%|███▍      | 11/32 [00:03<00:07,  2.98it/s]Inference:  38%|███▊      | 12/32 [00:03<00:06,  3.01it/s]Inference:  41%|████      | 13/32 [00:04<00:06,  2.78it/s]Inference:  44%|████▍     | 14/32 [00:04<00:06,  2.87it/s]Inference:  47%|████▋     | 15/32 [00:04<00:05,  3.02it/s]Inference:  50%|█████     | 16/32 [00:05<00:05,  2.85it/s]Inference:  53%|█████▎    | 17/32 [00:05<00:05,  2.97it/s]Inference:  56%|█████▋    | 18/32 [00:05<00:04,  3.05it/s]Inference:  59%|█████▉    | 19/32 [00:06<00:03,  3.40it/s]Inference:  62%|██████▎   | 20/32 [00:06<00:03,  3.72it/s]Inference:  66%|██████▌   | 21/32 [00:06<00:02,  4.00it/s]Inference:  69%|██████▉   | 22/32 [00:06<00:02,  4.24it/s]Inference:  72%|███████▏  | 23/32 [00:07<00:02,  3.97it/s]Inference:  75%|███████▌  | 24/32 [00:07<00:01,  4.19it/s]Inference:  78%|███████▊  | 25/32 [00:07<00:01,  4.35it/s]Inference:  81%|████████▏ | 26/32 [00:07<00:01,  4.01it/s]Inference:  84%|████████▍ | 27/32 [00:08<00:01,  3.73it/s]Inference:  88%|████████▊ | 28/32 [00:08<00:01,  3.54it/s]Inference:  91%|█████████ | 29/32 [00:08<00:00,  3.08it/s]Inference:  94%|█████████▍| 30/32 [00:09<00:00,  3.18it/s]Inference:  97%|█████████▋| 31/32 [00:09<00:00,  3.24it/s]Inference: 100%|██████████| 32/32 [00:09<00:00,  2.93it/s]Inference: 100%|██████████| 32/32 [00:09<00:00,  3.24it/s]
28487 MiB free out of 48676 MiB total
Saved layer 11 to ./models/meta-llama/Llama-2-7b-hf/2bpv/128/quantized/layer_11.pt
after cast to cpu
32489 MiB free out of 48676 MiB total
Done with layer 11 total_time elapsed: 6117 estimated time left: 10196
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:20,  1.49it/s]Inference:   6%|▋         | 2/32 [00:01<00:22,  1.35it/s]Inference:   9%|▉         | 3/32 [00:02<00:21,  1.35it/s]Inference:  12%|█▎        | 4/32 [00:03<00:21,  1.27it/s]Inference:  16%|█▌        | 5/32 [00:03<00:20,  1.34it/s]Inference:  19%|█▉        | 6/32 [00:04<00:18,  1.40it/s]Inference:  22%|██▏       | 7/32 [00:05<00:18,  1.37it/s]Inference:  25%|██▌       | 8/32 [00:06<00:18,  1.30it/s]Inference:  28%|██▊       | 9/32 [00:06<00:17,  1.35it/s]Inference:  31%|███▏      | 10/32 [00:07<00:15,  1.40it/s]Inference:  34%|███▍      | 11/32 [00:08<00:15,  1.38it/s]Inference:  38%|███▊      | 12/32 [00:08<00:14,  1.36it/s]Inference:  41%|████      | 13/32 [00:09<00:13,  1.40it/s]Inference:  44%|████▍     | 14/32 [00:10<00:12,  1.44it/s]Inference:  47%|████▋     | 15/32 [00:11<00:12,  1.35it/s]Inference:  50%|█████     | 16/32 [00:11<00:11,  1.40it/s]Inference:  53%|█████▎    | 17/32 [00:12<00:10,  1.43it/s]Inference:  56%|█████▋    | 18/32 [00:13<00:09,  1.40it/s]Inference:  59%|█████▉    | 19/32 [00:13<00:09,  1.38it/s]Inference:  62%|██████▎   | 20/32 [00:14<00:08,  1.37it/s]Inference:  66%|██████▌   | 21/32 [00:15<00:08,  1.35it/s]Inference:  69%|██████▉   | 22/32 [00:16<00:07,  1.35it/s]Inference:  72%|███████▏  | 23/32 [00:16<00:06,  1.39it/s]Inference:  75%|███████▌  | 24/32 [00:17<00:05,  1.37it/s]Inference:  78%|███████▊  | 25/32 [00:18<00:05,  1.34it/s]Inference:  81%|████████▏ | 26/32 [00:19<00:04,  1.34it/s]Inference:  84%|████████▍ | 27/32 [00:19<00:03,  1.44it/s]Inference:  88%|████████▊ | 28/32 [00:20<00:02,  1.51it/s]Inference:  91%|█████████ | 29/32 [00:20<00:02,  1.46it/s]Inference:  94%|█████████▍| 30/32 [00:21<00:01,  1.41it/s]Inference:  97%|█████████▋| 31/32 [00:22<00:00,  1.49it/s]Inference: 100%|██████████| 32/32 [00:22<00:00,  1.50it/s]Inference: 100%|██████████| 32/32 [00:22<00:00,  1.40it/s]
layer12: self_attn.q_proj
256
val_hessian None
iter 0, train loss 1178.0853271484375, val loss None, lr 0.001
iter 10, train loss 1265.3934326171875, val loss None, lr 0.001
iter 20, train loss 1201.97900390625, val loss None, lr 0.001
iter 30, train loss 1195.481201171875, val loss None, lr 0.001
iter 40, train loss 1195.7030029296875, val loss None, lr 0.001
iter 50, train loss 1193.2359619140625, val loss None, lr 0.001
iter 60, train loss 1191.0087890625, val loss None, lr 0.001
iter 70, train loss 1185.0511474609375, val loss None, lr 0.001
iter 80, train loss 1187.21630859375, val loss None, lr 0.001
iter 90, train loss 1187.8885498046875, val loss None, lr 0.001
best loss 1117.26220703125
layer12: self_attn.k_proj
256
val_hessian None
iter 0, train loss 1307.926513671875, val loss None, lr 0.001
iter 10, train loss 1398.76708984375, val loss None, lr 0.001
iter 20, train loss 1361.474853515625, val loss None, lr 0.001
iter 30, train loss 1319.612060546875, val loss None, lr 0.001
iter 40, train loss 1318.6795654296875, val loss None, lr 0.001
iter 50, train loss 1325.8125, val loss None, lr 0.001
iter 60, train loss 1328.841064453125, val loss None, lr 0.001
iter 70, train loss 1325.146484375, val loss None, lr 0.001
iter 80, train loss 1327.05615234375, val loss None, lr 0.001
iter 90, train loss 1321.9984130859375, val loss None, lr 0.001
best loss 1231.2322998046875
layer12: self_attn.v_proj
256
val_hessian None
iter 0, train loss 382.73626708984375, val loss None, lr 0.001
iter 10, train loss 383.6844482421875, val loss None, lr 0.001
iter 20, train loss 382.3923034667969, val loss None, lr 0.001
iter 30, train loss 381.223876953125, val loss None, lr 0.001
iter 40, train loss 380.6571960449219, val loss None, lr 0.001
iter 50, train loss 380.3330383300781, val loss None, lr 0.001
iter 60, train loss 380.2718811035156, val loss None, lr 0.001
iter 70, train loss 380.1025695800781, val loss None, lr 0.001
iter 80, train loss 379.66552734375, val loss None, lr 0.001
iter 90, train loss 379.1844787597656, val loss None, lr 0.001
best loss 378.95953369140625
layer12: self_attn.o_proj
256
val_hessian None
iter 0, train loss 33.86497497558594, val loss None, lr 0.001
iter 10, train loss 33.3436164855957, val loss None, lr 0.001
iter 20, train loss 33.13298034667969, val loss None, lr 0.001
iter 30, train loss 32.96528625488281, val loss None, lr 0.001
iter 40, train loss 32.82303237915039, val loss None, lr 0.001
iter 50, train loss 32.638790130615234, val loss None, lr 0.001
iter 60, train loss 32.610633850097656, val loss None, lr 0.001
iter 70, train loss 32.525203704833984, val loss None, lr 0.001
iter 80, train loss 32.45951461791992, val loss None, lr 0.001
iter 90, train loss 32.44763946533203, val loss None, lr 0.001
best loss 32.429534912109375
layer12: mlp.gate_proj
256
val_hessian None
iter 0, train loss 722.3134765625, val loss None, lr 0.001
iter 10, train loss 757.0696411132812, val loss None, lr 0.001
iter 20, train loss 740.0286865234375, val loss None, lr 0.001
iter 30, train loss 738.0985107421875, val loss None, lr 0.001
iter 40, train loss 736.8005981445312, val loss None, lr 0.001
iter 50, train loss 734.3021240234375, val loss None, lr 0.001
iter 60, train loss 731.6539306640625, val loss None, lr 0.001
iter 70, train loss 730.7039184570312, val loss None, lr 0.001
iter 80, train loss 730.1331787109375, val loss None, lr 0.001
iter 90, train loss 732.1581420898438, val loss None, lr 0.001
best loss 715.4113159179688
layer12: mlp.up_proj
256
val_hessian None
iter 0, train loss 645.2952880859375, val loss None, lr 0.001
iter 10, train loss 648.7979736328125, val loss None, lr 0.001
iter 20, train loss 650.7469482421875, val loss None, lr 0.001
iter 30, train loss 651.5762329101562, val loss None, lr 0.001
iter 40, train loss 652.9674072265625, val loss None, lr 0.001
iter 50, train loss 653.2752685546875, val loss None, lr 0.001
iter 60, train loss 653.6721801757812, val loss None, lr 0.001
iter 70, train loss 653.2899780273438, val loss None, lr 0.001
iter 80, train loss 653.5770263671875, val loss None, lr 0.001
iter 90, train loss 653.3042602539062, val loss None, lr 0.001
best loss 645.2612915039062
layer12: mlp.down_proj
256
val_hessian None
iter 0, train loss 15.386176109313965, val loss None, lr 0.001
iter 10, train loss 15.47313117980957, val loss None, lr 0.001
iter 20, train loss 15.464929580688477, val loss None, lr 0.001
iter 30, train loss 15.465312957763672, val loss None, lr 0.001
iter 40, train loss 15.506853103637695, val loss None, lr 0.001
iter 50, train loss 15.539403915405273, val loss None, lr 0.001
iter 60, train loss 15.570564270019531, val loss None, lr 0.001
iter 70, train loss 15.593616485595703, val loss None, lr 0.001
iter 80, train loss 15.65749740600586, val loss None, lr 0.001
iter 90, train loss 15.705753326416016, val loss None, lr 0.001
best loss 15.386176109313965
32489 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:11,  2.75it/s]Inference:   6%|▋         | 2/32 [00:00<00:10,  2.83it/s]Inference:   9%|▉         | 3/32 [00:01<00:11,  2.44it/s]Inference:  12%|█▎        | 4/32 [00:01<00:10,  2.73it/s]Inference:  16%|█▌        | 5/32 [00:01<00:09,  2.71it/s]Inference:  19%|█▉        | 6/32 [00:02<00:09,  2.66it/s]Inference:  22%|██▏       | 7/32 [00:02<00:08,  2.86it/s]Inference:  25%|██▌       | 8/32 [00:02<00:09,  2.62it/s]Inference:  28%|██▊       | 9/32 [00:03<00:08,  2.85it/s]Inference:  31%|███▏      | 10/32 [00:03<00:07,  2.79it/s]Inference:  34%|███▍      | 11/32 [00:04<00:07,  2.75it/s]Inference:  38%|███▊      | 12/32 [00:04<00:07,  2.77it/s]Inference:  41%|████      | 13/32 [00:04<00:06,  2.80it/s]Inference:  44%|████▍     | 14/32 [00:05<00:06,  2.99it/s]Inference:  47%|████▋     | 15/32 [00:05<00:05,  2.95it/s]Inference:  50%|█████     | 16/32 [00:05<00:05,  2.85it/s]Inference:  53%|█████▎    | 17/32 [00:06<00:05,  2.84it/s]Inference:  56%|█████▋    | 18/32 [00:06<00:05,  2.65it/s]Inference:  59%|█████▉    | 19/32 [00:07<00:05,  2.23it/s]Inference:  62%|██████▎   | 20/32 [00:07<00:05,  2.09it/s]Inference:  66%|██████▌   | 21/32 [00:08<00:04,  2.24it/s]Inference:  69%|██████▉   | 22/32 [00:08<00:04,  2.37it/s]Inference:  72%|███████▏  | 23/32 [00:08<00:04,  2.16it/s]Inference:  75%|███████▌  | 24/32 [00:09<00:03,  2.28it/s]Inference:  78%|███████▊  | 25/32 [00:09<00:02,  2.40it/s]Inference:  81%|████████▏ | 26/32 [00:10<00:02,  2.29it/s]Inference:  84%|████████▍ | 27/32 [00:10<00:02,  2.17it/s]Inference:  88%|████████▊ | 28/32 [00:11<00:01,  2.23it/s]Inference:  91%|█████████ | 29/32 [00:11<00:01,  2.12it/s]Inference:  94%|█████████▍| 30/32 [00:12<00:00,  2.31it/s]Inference:  97%|█████████▋| 31/32 [00:12<00:00,  2.37it/s]Inference: 100%|██████████| 32/32 [00:12<00:00,  2.54it/s]Inference: 100%|██████████| 32/32 [00:12<00:00,  2.51it/s]
27263 MiB free out of 48676 MiB total
Saved layer 12 to ./models/meta-llama/Llama-2-7b-hf/2bpv/128/quantized/layer_12.pt
after cast to cpu
31265 MiB free out of 48676 MiB total
Done with layer 12 total_time elapsed: 6645 estimated time left: 9712
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:29,  1.06it/s]Inference:   6%|▋         | 2/32 [00:01<00:27,  1.07it/s]Inference:   9%|▉         | 3/32 [00:02<00:25,  1.15it/s]Inference:  12%|█▎        | 4/32 [00:03<00:23,  1.17it/s]Inference:  16%|█▌        | 5/32 [00:04<00:22,  1.19it/s]Inference:  19%|█▉        | 6/32 [00:05<00:22,  1.15it/s]Inference:  22%|██▏       | 7/32 [00:06<00:21,  1.15it/s]Inference:  25%|██▌       | 8/32 [00:06<00:18,  1.30it/s]Inference:  28%|██▊       | 9/32 [00:07<00:16,  1.39it/s]Inference:  31%|███▏      | 10/32 [00:08<00:16,  1.37it/s]Inference:  34%|███▍      | 11/32 [00:08<00:14,  1.41it/s]Inference:  38%|███▊      | 12/32 [00:09<00:14,  1.39it/s]Inference:  41%|████      | 13/32 [00:10<00:14,  1.32it/s]Inference:  44%|████▍     | 14/32 [00:11<00:14,  1.21it/s]Inference:  47%|████▋     | 15/32 [00:12<00:14,  1.18it/s]Inference:  50%|█████     | 16/32 [00:12<00:13,  1.22it/s]Inference:  53%|█████▎    | 17/32 [00:13<00:11,  1.26it/s]Inference:  56%|█████▋    | 18/32 [00:14<00:11,  1.24it/s]Inference:  59%|█████▉    | 19/32 [00:15<00:10,  1.23it/s]Inference:  62%|██████▎   | 20/32 [00:16<00:09,  1.26it/s]Inference:  66%|██████▌   | 21/32 [00:16<00:08,  1.24it/s]Inference:  69%|██████▉   | 22/32 [00:17<00:08,  1.18it/s]Inference:  72%|███████▏  | 23/32 [00:18<00:07,  1.15it/s]Inference:  75%|███████▌  | 24/32 [00:19<00:07,  1.13it/s]Inference:  78%|███████▊  | 25/32 [00:20<00:05,  1.22it/s]Inference:  81%|████████▏ | 26/32 [00:21<00:05,  1.18it/s]Inference:  84%|████████▍ | 27/32 [00:22<00:04,  1.14it/s]Inference:  88%|████████▊ | 28/32 [00:23<00:03,  1.16it/s]Inference:  91%|█████████ | 29/32 [00:23<00:02,  1.21it/s]Inference:  94%|█████████▍| 30/32 [00:24<00:01,  1.18it/s]Inference:  97%|█████████▋| 31/32 [00:25<00:00,  1.15it/s]Inference: 100%|██████████| 32/32 [00:26<00:00,  1.16it/s]Inference: 100%|██████████| 32/32 [00:26<00:00,  1.21it/s]
layer13: self_attn.q_proj
256
val_hessian None
iter 0, train loss 1214.2198486328125, val loss None, lr 0.001
iter 10, train loss 1305.666259765625, val loss None, lr 0.001
iter 20, train loss 1237.148193359375, val loss None, lr 0.001
iter 30, train loss 1216.9853515625, val loss None, lr 0.001
iter 40, train loss 1225.18212890625, val loss None, lr 0.001
iter 50, train loss 1214.34228515625, val loss None, lr 0.001
iter 60, train loss 1216.8131103515625, val loss None, lr 0.001
iter 70, train loss 1220.646728515625, val loss None, lr 0.001
iter 80, train loss 1213.625244140625, val loss None, lr 0.001
iter 90, train loss 1224.193603515625, val loss None, lr 0.001
best loss 1141.4969482421875
layer13: self_attn.k_proj
256
val_hessian None
iter 0, train loss 1312.240966796875, val loss None, lr 0.001
iter 10, train loss 1409.21728515625, val loss None, lr 0.001
iter 20, train loss 1354.2467041015625, val loss None, lr 0.001
iter 30, train loss 1313.8438720703125, val loss None, lr 0.001
iter 40, train loss 1305.635009765625, val loss None, lr 0.001
iter 50, train loss 1303.7760009765625, val loss None, lr 0.001
iter 60, train loss 1303.06982421875, val loss None, lr 0.001
iter 70, train loss 1304.91943359375, val loss None, lr 0.001
iter 80, train loss 1298.503662109375, val loss None, lr 0.001
iter 90, train loss 1299.5648193359375, val loss None, lr 0.001
best loss 1221.7191162109375
layer13: self_attn.v_proj
256
val_hessian None
iter 0, train loss 423.7122497558594, val loss None, lr 0.001
iter 10, train loss 423.31243896484375, val loss None, lr 0.001
iter 20, train loss 421.3931884765625, val loss None, lr 0.001
iter 30, train loss 421.1741027832031, val loss None, lr 0.001
iter 40, train loss 419.9521484375, val loss None, lr 0.001
iter 50, train loss 419.9941101074219, val loss None, lr 0.001
iter 60, train loss 419.92120361328125, val loss None, lr 0.001
iter 70, train loss 419.6188659667969, val loss None, lr 0.001
iter 80, train loss 419.518798828125, val loss None, lr 0.001
iter 90, train loss 420.396484375, val loss None, lr 0.001
best loss 419.30511474609375
layer13: self_attn.o_proj
256
val_hessian None
iter 0, train loss 34.86094284057617, val loss None, lr 0.001
iter 10, train loss 33.63191604614258, val loss None, lr 0.001
iter 20, train loss 33.35566329956055, val loss None, lr 0.001
iter 30, train loss 32.97486114501953, val loss None, lr 0.001
iter 40, train loss 32.55855178833008, val loss None, lr 0.001
iter 50, train loss 32.56105041503906, val loss None, lr 0.001
iter 60, train loss 32.36024475097656, val loss None, lr 0.001
iter 70, train loss 32.346893310546875, val loss None, lr 0.001
iter 80, train loss 32.46900939941406, val loss None, lr 0.001
iter 90, train loss 32.515499114990234, val loss None, lr 0.001
best loss 32.313499450683594
layer13: mlp.gate_proj
256
val_hessian None
iter 0, train loss 768.955322265625, val loss None, lr 0.001
iter 10, train loss 804.7853393554688, val loss None, lr 0.001
iter 20, train loss 787.2089233398438, val loss None, lr 0.001
iter 30, train loss 786.1255493164062, val loss None, lr 0.001
iter 40, train loss 783.5430908203125, val loss None, lr 0.001
iter 50, train loss 780.7776489257812, val loss None, lr 0.001
iter 60, train loss 779.60791015625, val loss None, lr 0.001
iter 70, train loss 779.1168212890625, val loss None, lr 0.001
iter 80, train loss 778.1588134765625, val loss None, lr 0.001
iter 90, train loss 778.0782470703125, val loss None, lr 0.001
best loss 760.3880615234375
layer13: mlp.up_proj
256
val_hessian None
iter 0, train loss 706.6340942382812, val loss None, lr 0.001
iter 10, train loss 710.415283203125, val loss None, lr 0.001
iter 20, train loss 712.6468505859375, val loss None, lr 0.001
iter 30, train loss 714.5870361328125, val loss None, lr 0.001
iter 40, train loss 714.925048828125, val loss None, lr 0.001
iter 50, train loss 714.8243408203125, val loss None, lr 0.001
iter 60, train loss 714.4078369140625, val loss None, lr 0.001
iter 70, train loss 715.15673828125, val loss None, lr 0.001
iter 80, train loss 715.1063232421875, val loss None, lr 0.001
iter 90, train loss 715.1439208984375, val loss None, lr 0.001
best loss 706.6340942382812
layer13: mlp.down_proj
256
val_hessian None
iter 0, train loss 18.696714401245117, val loss None, lr 0.001
iter 10, train loss 18.68560791015625, val loss None, lr 0.001
iter 20, train loss 18.57888412475586, val loss None, lr 0.001
iter 30, train loss 18.51644515991211, val loss None, lr 0.001
iter 40, train loss 18.534042358398438, val loss None, lr 0.001
iter 50, train loss 18.540952682495117, val loss None, lr 0.001
iter 60, train loss 18.564687728881836, val loss None, lr 0.001
iter 70, train loss 18.575830459594727, val loss None, lr 0.001
iter 80, train loss 18.60297203063965, val loss None, lr 0.001
iter 90, train loss 18.64502716064453, val loss None, lr 0.001
best loss 18.513431549072266
31265 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:07,  4.32it/s]Inference:   6%|▋         | 2/32 [00:00<00:07,  4.07it/s]Inference:   9%|▉         | 3/32 [00:00<00:07,  3.86it/s]Inference:  12%|█▎        | 4/32 [00:00<00:06,  4.07it/s]Inference:  16%|█▌        | 5/32 [00:01<00:06,  4.05it/s]Inference:  19%|█▉        | 6/32 [00:01<00:06,  4.03it/s]Inference:  22%|██▏       | 7/32 [00:01<00:05,  4.22it/s]Inference:  25%|██▌       | 8/32 [00:01<00:05,  4.29it/s]Inference:  28%|██▊       | 9/32 [00:02<00:05,  4.19it/s]Inference:  31%|███▏      | 10/32 [00:02<00:05,  4.21it/s]Inference:  34%|███▍      | 11/32 [00:02<00:05,  4.15it/s]Inference:  38%|███▊      | 12/32 [00:02<00:04,  4.08it/s]Inference:  41%|████      | 13/32 [00:03<00:04,  4.18it/s]Inference:  44%|████▍     | 14/32 [00:03<00:04,  3.90it/s]Inference:  47%|████▋     | 15/32 [00:03<00:04,  3.91it/s]Inference:  50%|█████     | 16/32 [00:03<00:04,  3.97it/s]Inference:  53%|█████▎    | 17/32 [00:04<00:03,  4.02it/s]Inference:  56%|█████▋    | 18/32 [00:04<00:03,  3.81it/s]Inference:  59%|█████▉    | 19/32 [00:04<00:03,  3.87it/s]Inference:  62%|██████▎   | 20/32 [00:04<00:03,  3.93it/s]Inference:  66%|██████▌   | 21/32 [00:05<00:02,  3.77it/s]Inference:  69%|██████▉   | 22/32 [00:05<00:02,  3.81it/s]Inference:  72%|███████▏  | 23/32 [00:05<00:02,  3.86it/s]Inference:  75%|███████▌  | 24/32 [00:06<00:02,  3.92it/s]Inference:  78%|███████▊  | 25/32 [00:06<00:01,  3.62it/s]Inference:  81%|████████▏ | 26/32 [00:06<00:01,  3.54it/s]Inference:  84%|████████▍ | 27/32 [00:06<00:01,  3.53it/s]Inference:  88%|████████▊ | 28/32 [00:07<00:01,  3.55it/s]Inference:  91%|█████████ | 29/32 [00:07<00:00,  3.55it/s]Inference:  94%|█████████▍| 30/32 [00:07<00:00,  3.53it/s]Inference:  97%|█████████▋| 31/32 [00:08<00:00,  3.17it/s]Inference: 100%|██████████| 32/32 [00:08<00:00,  2.98it/s]Inference: 100%|██████████| 32/32 [00:08<00:00,  3.75it/s]
25975 MiB free out of 48676 MiB total
Saved layer 13 to ./models/meta-llama/Llama-2-7b-hf/2bpv/128/quantized/layer_13.pt
after cast to cpu
29977 MiB free out of 48676 MiB total
Done with layer 13 total_time elapsed: 7275 estimated time left: 9353
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:23,  1.35it/s]Inference:   6%|▋         | 2/32 [00:01<00:25,  1.17it/s]Inference:   9%|▉         | 3/32 [00:02<00:25,  1.13it/s]Inference:  12%|█▎        | 4/32 [00:03<00:24,  1.15it/s]Inference:  16%|█▌        | 5/32 [00:04<00:24,  1.12it/s]Inference:  19%|█▉        | 6/32 [00:05<00:23,  1.13it/s]Inference:  22%|██▏       | 7/32 [00:06<00:21,  1.15it/s]Inference:  25%|██▌       | 8/32 [00:06<00:21,  1.13it/s]Inference:  28%|██▊       | 9/32 [00:07<00:19,  1.19it/s]Inference:  31%|███▏      | 10/32 [00:08<00:18,  1.19it/s]Inference:  34%|███▍      | 11/32 [00:09<00:17,  1.23it/s]Inference:  38%|███▊      | 12/32 [00:10<00:17,  1.16it/s]Inference:  41%|████      | 13/32 [00:11<00:16,  1.13it/s]Inference:  44%|████▍     | 14/32 [00:12<00:15,  1.16it/s]Inference:  47%|████▋     | 15/32 [00:12<00:14,  1.21it/s]Inference:  50%|█████     | 16/32 [00:13<00:13,  1.21it/s]Inference:  53%|█████▎    | 17/32 [00:14<00:13,  1.13it/s]Inference:  56%|█████▋    | 18/32 [00:15<00:11,  1.19it/s]Inference:  59%|█████▉    | 19/32 [00:16<00:10,  1.22it/s]Inference:  62%|██████▎   | 20/32 [00:16<00:09,  1.22it/s]Inference:  66%|██████▌   | 21/32 [00:17<00:09,  1.21it/s]Inference:  69%|██████▉   | 22/32 [00:18<00:08,  1.22it/s]Inference:  72%|███████▏  | 23/32 [00:19<00:07,  1.22it/s]Inference:  75%|███████▌  | 24/32 [00:20<00:06,  1.21it/s]Inference:  78%|███████▊  | 25/32 [00:21<00:05,  1.20it/s]Inference:  81%|████████▏ | 26/32 [00:21<00:05,  1.20it/s]Inference:  84%|████████▍ | 27/32 [00:22<00:04,  1.20it/s]Inference:  88%|████████▊ | 28/32 [00:23<00:03,  1.26it/s]Inference:  91%|█████████ | 29/32 [00:24<00:02,  1.29it/s]Inference:  94%|█████████▍| 30/32 [00:24<00:01,  1.29it/s]Inference:  97%|█████████▋| 31/32 [00:25<00:00,  1.30it/s]Inference: 100%|██████████| 32/32 [00:26<00:00,  1.28it/s]Inference: 100%|██████████| 32/32 [00:26<00:00,  1.20it/s]
layer14: self_attn.q_proj
256
val_hessian None
iter 0, train loss 1281.336181640625, val loss None, lr 0.001
iter 10, train loss 1381.2403564453125, val loss None, lr 0.001
iter 20, train loss 1304.752197265625, val loss None, lr 0.001
iter 30, train loss 1277.7066650390625, val loss None, lr 0.001
iter 40, train loss 1284.24365234375, val loss None, lr 0.001
iter 50, train loss 1280.0643310546875, val loss None, lr 0.001
iter 60, train loss 1280.4576416015625, val loss None, lr 0.001
iter 70, train loss 1276.5155029296875, val loss None, lr 0.001
iter 80, train loss 1275.6727294921875, val loss None, lr 0.001
iter 90, train loss 1272.56884765625, val loss None, lr 0.001
best loss 1192.98876953125
layer14: self_attn.k_proj
256
val_hessian None
iter 0, train loss 1402.7938232421875, val loss None, lr 0.001
iter 10, train loss 1502.05615234375, val loss None, lr 0.001
iter 20, train loss 1451.4014892578125, val loss None, lr 0.001
iter 30, train loss 1415.4747314453125, val loss None, lr 0.001
iter 40, train loss 1398.1932373046875, val loss None, lr 0.001
iter 50, train loss 1397.43310546875, val loss None, lr 0.001
iter 60, train loss 1406.942138671875, val loss None, lr 0.001
iter 70, train loss 1403.782958984375, val loss None, lr 0.001
iter 80, train loss 1401.1353759765625, val loss None, lr 0.001
iter 90, train loss 1404.88623046875, val loss None, lr 0.001
best loss 1288.0997314453125
layer14: self_attn.v_proj
256
val_hessian None
iter 0, train loss 429.2867736816406, val loss None, lr 0.001
iter 10, train loss 427.63677978515625, val loss None, lr 0.001
iter 20, train loss 426.0151672363281, val loss None, lr 0.001
iter 30, train loss 425.44512939453125, val loss None, lr 0.001
iter 40, train loss 424.9134826660156, val loss None, lr 0.001
iter 50, train loss 424.89373779296875, val loss None, lr 0.001
iter 60, train loss 423.9192810058594, val loss None, lr 0.001
iter 70, train loss 423.72564697265625, val loss None, lr 0.001
iter 80, train loss 423.6544494628906, val loss None, lr 0.001
iter 90, train loss 423.17376708984375, val loss None, lr 0.001
best loss 423.17376708984375
layer14: self_attn.o_proj
256
val_hessian None
iter 0, train loss 47.61183166503906, val loss None, lr 0.001
iter 10, train loss 46.300697326660156, val loss None, lr 0.001
iter 20, train loss 44.98204803466797, val loss None, lr 0.001
iter 30, train loss 43.893924713134766, val loss None, lr 0.001
iter 40, train loss 43.12846374511719, val loss None, lr 0.001
iter 50, train loss 42.773223876953125, val loss None, lr 0.001
iter 60, train loss 42.6770133972168, val loss None, lr 0.001
iter 70, train loss 42.540313720703125, val loss None, lr 0.001
iter 80, train loss 42.41598892211914, val loss None, lr 0.001
iter 90, train loss 42.420719146728516, val loss None, lr 0.001
best loss 42.37641525268555
layer14: mlp.gate_proj
256
val_hessian None
iter 0, train loss 829.288330078125, val loss None, lr 0.001
iter 10, train loss 868.6484985351562, val loss None, lr 0.001
iter 20, train loss 851.455322265625, val loss None, lr 0.001
iter 30, train loss 850.3582763671875, val loss None, lr 0.001
iter 40, train loss 848.2007446289062, val loss None, lr 0.001
iter 50, train loss 846.1732788085938, val loss None, lr 0.001
iter 60, train loss 846.1226806640625, val loss None, lr 0.001
iter 70, train loss 847.0388793945312, val loss None, lr 0.001
iter 80, train loss 846.7382202148438, val loss None, lr 0.001
iter 90, train loss 846.5399780273438, val loss None, lr 0.001
best loss 820.3704833984375
layer14: mlp.up_proj
256
val_hessian None
iter 0, train loss 763.32861328125, val loss None, lr 0.001
iter 10, train loss 767.5987548828125, val loss None, lr 0.001
iter 20, train loss 771.3087768554688, val loss None, lr 0.001
iter 30, train loss 773.6532592773438, val loss None, lr 0.001
iter 40, train loss 774.6444702148438, val loss None, lr 0.001
iter 50, train loss 775.1076049804688, val loss None, lr 0.001
iter 60, train loss 776.23828125, val loss None, lr 0.001
iter 70, train loss 776.7943115234375, val loss None, lr 0.001
iter 80, train loss 776.8348388671875, val loss None, lr 0.001
iter 90, train loss 777.0283813476562, val loss None, lr 0.001
best loss 763.2933349609375
layer14: mlp.down_proj
256
val_hessian None
iter 0, train loss 21.445514678955078, val loss None, lr 0.001
iter 10, train loss 21.4666748046875, val loss None, lr 0.001
iter 20, train loss 21.410266876220703, val loss None, lr 0.001
iter 30, train loss 21.352205276489258, val loss None, lr 0.001
iter 40, train loss 21.34391212463379, val loss None, lr 0.001
iter 50, train loss 21.362869262695312, val loss None, lr 0.001
iter 60, train loss 21.382614135742188, val loss None, lr 0.001
iter 70, train loss 21.398265838623047, val loss None, lr 0.001
iter 80, train loss 21.415828704833984, val loss None, lr 0.001
iter 90, train loss 21.501544952392578, val loss None, lr 0.001
best loss 21.334259033203125
29977 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:19,  1.61it/s]Inference:   6%|▋         | 2/32 [00:01<00:15,  2.00it/s]Inference:   9%|▉         | 3/32 [00:01<00:15,  1.88it/s]Inference:  12%|█▎        | 4/32 [00:02<00:14,  1.90it/s]Inference:  16%|█▌        | 5/32 [00:02<00:14,  1.82it/s]Inference:  19%|█▉        | 6/32 [00:03<00:12,  2.01it/s]Inference:  22%|██▏       | 7/32 [00:03<00:11,  2.17it/s]Inference:  25%|██▌       | 8/32 [00:03<00:11,  2.12it/s]Inference:  28%|██▊       | 9/32 [00:04<00:10,  2.24it/s]Inference:  31%|███▏      | 10/32 [00:04<00:10,  2.17it/s]Inference:  34%|███▍      | 11/32 [00:05<00:09,  2.15it/s]Inference:  38%|███▊      | 12/32 [00:05<00:08,  2.30it/s]Inference:  41%|████      | 13/32 [00:06<00:08,  2.36it/s]Inference:  44%|████▍     | 14/32 [00:06<00:07,  2.27it/s]Inference:  47%|████▋     | 15/32 [00:07<00:08,  2.09it/s]Inference:  50%|█████     | 16/32 [00:07<00:08,  1.98it/s]Inference:  53%|█████▎    | 17/32 [00:08<00:06,  2.16it/s]Inference:  56%|█████▋    | 18/32 [00:08<00:06,  2.16it/s]Inference:  59%|█████▉    | 19/32 [00:09<00:06,  2.05it/s]Inference:  62%|██████▎   | 20/32 [00:09<00:05,  2.04it/s]Inference:  66%|██████▌   | 21/32 [00:10<00:05,  2.05it/s]Inference:  69%|██████▉   | 22/32 [00:10<00:04,  2.18it/s]Inference:  72%|███████▏  | 23/32 [00:10<00:03,  2.28it/s]Inference:  75%|███████▌  | 24/32 [00:11<00:03,  2.07it/s]Inference:  78%|███████▊  | 25/32 [00:11<00:03,  2.07it/s]Inference:  81%|████████▏ | 26/32 [00:12<00:03,  1.96it/s]Inference:  84%|████████▍ | 27/32 [00:12<00:02,  2.09it/s]Inference:  88%|████████▊ | 28/32 [00:13<00:01,  2.08it/s]Inference:  91%|█████████ | 29/32 [00:13<00:01,  2.00it/s]Inference:  94%|█████████▍| 30/32 [00:14<00:00,  2.22it/s]Inference:  97%|█████████▋| 31/32 [00:14<00:00,  2.41it/s]Inference: 100%|██████████| 32/32 [00:14<00:00,  2.56it/s]Inference: 100%|██████████| 32/32 [00:14<00:00,  2.14it/s]
24687 MiB free out of 48676 MiB total
Saved layer 14 to ./models/meta-llama/Llama-2-7b-hf/2bpv/128/quantized/layer_14.pt
after cast to cpu
28689 MiB free out of 48676 MiB total
Done with layer 14 total_time elapsed: 7954 estimated time left: 9015
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:01<00:31,  1.03s/it]Inference:   6%|▋         | 2/32 [00:01<00:27,  1.10it/s]Inference:   9%|▉         | 3/32 [00:02<00:25,  1.14it/s]Inference:  12%|█▎        | 4/32 [00:03<00:25,  1.10it/s]Inference:  16%|█▌        | 5/32 [00:04<00:26,  1.02it/s]Inference:  19%|█▉        | 6/32 [00:05<00:24,  1.04it/s]Inference:  22%|██▏       | 7/32 [00:06<00:23,  1.09it/s]Inference:  25%|██▌       | 8/32 [00:07<00:22,  1.05it/s]Inference:  28%|██▊       | 9/32 [00:08<00:21,  1.09it/s]Inference:  31%|███▏      | 10/32 [00:09<00:20,  1.09it/s]Inference:  34%|███▍      | 11/32 [00:10<00:19,  1.08it/s]Inference:  38%|███▊      | 12/32 [00:10<00:17,  1.15it/s]Inference:  41%|████      | 13/32 [00:11<00:16,  1.17it/s]Inference:  44%|████▍     | 14/32 [00:12<00:15,  1.13it/s]Inference:  47%|████▋     | 15/32 [00:13<00:14,  1.18it/s]Inference:  50%|█████     | 16/32 [00:14<00:13,  1.15it/s]Inference:  53%|█████▎    | 17/32 [00:15<00:13,  1.14it/s]Inference:  56%|█████▋    | 18/32 [00:16<00:12,  1.15it/s]Inference:  59%|█████▉    | 19/32 [00:16<00:10,  1.19it/s]Inference:  62%|██████▎   | 20/32 [00:17<00:09,  1.23it/s]Inference:  66%|██████▌   | 21/32 [00:18<00:09,  1.19it/s]Inference:  69%|██████▉   | 22/32 [00:19<00:08,  1.16it/s]Inference:  72%|███████▏  | 23/32 [00:20<00:07,  1.19it/s]Inference:  75%|███████▌  | 24/32 [00:21<00:06,  1.23it/s]Inference:  78%|███████▊  | 25/32 [00:21<00:05,  1.26it/s]Inference:  81%|████████▏ | 26/32 [00:22<00:04,  1.23it/s]Inference:  84%|████████▍ | 27/32 [00:23<00:04,  1.23it/s]Inference:  88%|████████▊ | 28/32 [00:24<00:03,  1.18it/s]Inference:  91%|█████████ | 29/32 [00:25<00:02,  1.19it/s]Inference:  94%|█████████▍| 30/32 [00:26<00:01,  1.20it/s]Inference:  97%|█████████▋| 31/32 [00:26<00:00,  1.20it/s]Inference: 100%|██████████| 32/32 [00:27<00:00,  1.24it/s]Inference: 100%|██████████| 32/32 [00:27<00:00,  1.16it/s]
layer15: self_attn.q_proj
256
val_hessian None
iter 0, train loss 1201.4542236328125, val loss None, lr 0.001
iter 10, train loss 1279.1163330078125, val loss None, lr 0.001
iter 20, train loss 1212.2593994140625, val loss None, lr 0.001
iter 30, train loss 1186.16650390625, val loss None, lr 0.001
iter 40, train loss 1189.245849609375, val loss None, lr 0.001
iter 50, train loss 1184.70458984375, val loss None, lr 0.001
iter 60, train loss 1180.9185791015625, val loss None, lr 0.001
iter 70, train loss 1186.1236572265625, val loss None, lr 0.001
iter 80, train loss 1193.35791015625, val loss None, lr 0.001
iter 90, train loss 1197.301025390625, val loss None, lr 0.001
best loss 1121.4503173828125
layer15: self_attn.k_proj
256
val_hessian None
iter 0, train loss 1343.4605712890625, val loss None, lr 0.001
iter 10, train loss 1450.65478515625, val loss None, lr 0.001
iter 20, train loss 1423.534423828125, val loss None, lr 0.001
iter 30, train loss 1376.144775390625, val loss None, lr 0.001
iter 40, train loss 1355.5506591796875, val loss None, lr 0.001
iter 50, train loss 1358.44189453125, val loss None, lr 0.001
iter 60, train loss 1357.19287109375, val loss None, lr 0.001
iter 70, train loss 1341.09130859375, val loss None, lr 0.001
iter 80, train loss 1360.1365966796875, val loss None, lr 0.001
iter 90, train loss 1356.25146484375, val loss None, lr 0.001
best loss 1234.8175048828125
layer15: self_attn.v_proj
256
val_hessian None
iter 0, train loss 447.88616943359375, val loss None, lr 0.001
iter 10, train loss 446.0383605957031, val loss None, lr 0.001
iter 20, train loss 444.95306396484375, val loss None, lr 0.001
iter 30, train loss 443.9195251464844, val loss None, lr 0.001
iter 40, train loss 441.8083190917969, val loss None, lr 0.001
iter 50, train loss 440.8533020019531, val loss None, lr 0.001
iter 60, train loss 440.6167297363281, val loss None, lr 0.001
iter 70, train loss 440.5479736328125, val loss None, lr 0.001
iter 80, train loss 440.38995361328125, val loss None, lr 0.001
iter 90, train loss 440.43975830078125, val loss None, lr 0.001
best loss 439.5998840332031
layer15: self_attn.o_proj
256
val_hessian None
iter 0, train loss 44.767845153808594, val loss None, lr 0.001
iter 10, train loss 44.053680419921875, val loss None, lr 0.001
iter 20, train loss 43.401634216308594, val loss None, lr 0.001
iter 30, train loss 43.41828918457031, val loss None, lr 0.001
iter 40, train loss 42.979408264160156, val loss None, lr 0.001
iter 50, train loss 43.0145263671875, val loss None, lr 0.001
iter 60, train loss 42.959930419921875, val loss None, lr 0.001
iter 70, train loss 42.92601013183594, val loss None, lr 0.001
iter 80, train loss 42.89318084716797, val loss None, lr 0.001
iter 90, train loss 42.93806457519531, val loss None, lr 0.001
best loss 42.82213592529297
layer15: mlp.gate_proj
256
val_hessian None
iter 0, train loss 911.2320556640625, val loss None, lr 0.001
iter 10, train loss 957.754638671875, val loss None, lr 0.001
iter 20, train loss 939.2101440429688, val loss None, lr 0.001
iter 30, train loss 935.778076171875, val loss None, lr 0.001
iter 40, train loss 931.9920043945312, val loss None, lr 0.001
iter 50, train loss 928.983642578125, val loss None, lr 0.001
iter 60, train loss 926.478515625, val loss None, lr 0.001
iter 70, train loss 925.8796997070312, val loss None, lr 0.001
iter 80, train loss 924.2239990234375, val loss None, lr 0.001
iter 90, train loss 923.515380859375, val loss None, lr 0.001
best loss 901.7652587890625
layer15: mlp.up_proj
256
val_hessian None
iter 0, train loss 840.187744140625, val loss None, lr 0.001
iter 10, train loss 846.14013671875, val loss None, lr 0.001
iter 20, train loss 850.1039428710938, val loss None, lr 0.001
iter 30, train loss 852.0345458984375, val loss None, lr 0.001
iter 40, train loss 852.7296142578125, val loss None, lr 0.001
iter 50, train loss 853.06494140625, val loss None, lr 0.001
iter 60, train loss 853.5814208984375, val loss None, lr 0.001
iter 70, train loss 854.1695556640625, val loss None, lr 0.001
iter 80, train loss 854.745849609375, val loss None, lr 0.001
iter 90, train loss 854.3712768554688, val loss None, lr 0.001
best loss 840.187744140625
layer15: mlp.down_proj
256
val_hessian None
iter 0, train loss 27.11186981201172, val loss None, lr 0.001
iter 10, train loss 27.10442352294922, val loss None, lr 0.001
iter 20, train loss 26.920934677124023, val loss None, lr 0.001
iter 30, train loss 26.795650482177734, val loss None, lr 0.001
iter 40, train loss 26.72986602783203, val loss None, lr 0.001
iter 50, train loss 26.71527099609375, val loss None, lr 0.001
iter 60, train loss 26.703651428222656, val loss None, lr 0.001
iter 70, train loss 26.732730865478516, val loss None, lr 0.001
iter 80, train loss 26.75787353515625, val loss None, lr 0.001
iter 90, train loss 26.793752670288086, val loss None, lr 0.001
best loss 26.698163986206055
28689 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:11,  2.68it/s]Inference:   6%|▋         | 2/32 [00:00<00:11,  2.63it/s]Inference:   9%|▉         | 3/32 [00:01<00:11,  2.62it/s]Inference:  12%|█▎        | 4/32 [00:01<00:10,  2.62it/s]Inference:  16%|█▌        | 5/32 [00:01<00:10,  2.62it/s]Inference:  19%|█▉        | 6/32 [00:02<00:10,  2.42it/s]Inference:  22%|██▏       | 7/32 [00:02<00:10,  2.49it/s]Inference:  25%|██▌       | 8/32 [00:03<00:09,  2.51it/s]Inference:  28%|██▊       | 9/32 [00:03<00:08,  2.56it/s]Inference:  31%|███▏      | 10/32 [00:03<00:08,  2.57it/s]Inference:  34%|███▍      | 11/32 [00:04<00:08,  2.43it/s]Inference:  38%|███▊      | 12/32 [00:04<00:08,  2.49it/s]Inference:  41%|████      | 13/32 [00:05<00:07,  2.70it/s]Inference:  44%|████▍     | 14/32 [00:05<00:07,  2.50it/s]Inference:  47%|████▋     | 15/32 [00:05<00:06,  2.55it/s]Inference:  50%|█████     | 16/32 [00:06<00:05,  2.77it/s]Inference:  53%|█████▎    | 17/32 [00:06<00:05,  2.51it/s]Inference:  56%|█████▋    | 18/32 [00:06<00:05,  2.73it/s]Inference:  59%|█████▉    | 19/32 [00:07<00:04,  2.94it/s]Inference:  62%|██████▎   | 20/32 [00:07<00:04,  2.83it/s]Inference:  66%|██████▌   | 21/32 [00:08<00:03,  2.76it/s]Inference:  69%|██████▉   | 22/32 [00:08<00:03,  2.72it/s]Inference:  72%|███████▏  | 23/32 [00:08<00:03,  2.46it/s]Inference:  75%|███████▌  | 24/32 [00:09<00:03,  2.33it/s]Inference:  78%|███████▊  | 25/32 [00:09<00:03,  2.26it/s]Inference:  81%|████████▏ | 26/32 [00:10<00:02,  2.18it/s]Inference:  84%|████████▍ | 27/32 [00:10<00:02,  2.28it/s]Inference:  88%|████████▊ | 28/32 [00:11<00:01,  2.36it/s]Inference:  91%|█████████ | 29/32 [00:11<00:01,  2.27it/s]Inference:  94%|█████████▍| 30/32 [00:11<00:00,  2.36it/s]Inference:  97%|█████████▋| 31/32 [00:12<00:00,  2.41it/s]Inference: 100%|██████████| 32/32 [00:12<00:00,  2.31it/s]Inference: 100%|██████████| 32/32 [00:12<00:00,  2.49it/s]
23463 MiB free out of 48676 MiB total
Saved layer 15 to ./models/meta-llama/Llama-2-7b-hf/2bpv/128/quantized/layer_15.pt
after cast to cpu
27465 MiB free out of 48676 MiB total
Done with layer 15 total_time elapsed: 8784 estimated time left: 8784
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:01<00:32,  1.05s/it]Inference:   6%|▋         | 2/32 [00:02<00:32,  1.10s/it]Inference:   9%|▉         | 3/32 [00:03<00:31,  1.08s/it]Inference:  12%|█▎        | 4/32 [00:04<00:31,  1.12s/it]Inference:  16%|█▌        | 5/32 [00:05<00:29,  1.09s/it]Inference:  19%|█▉        | 6/32 [00:06<00:26,  1.02s/it]Inference:  22%|██▏       | 7/32 [00:07<00:24,  1.01it/s]Inference:  25%|██▌       | 8/32 [00:08<00:21,  1.09it/s]Inference:  28%|██▊       | 9/32 [00:08<00:20,  1.15it/s]Inference:  31%|███▏      | 10/32 [00:09<00:20,  1.10it/s]Inference:  34%|███▍      | 11/32 [00:10<00:19,  1.10it/s]Inference:  38%|███▊      | 12/32 [00:11<00:18,  1.06it/s]Inference:  41%|████      | 13/32 [00:12<00:18,  1.03it/s]Inference:  44%|████▍     | 14/32 [00:13<00:17,  1.02it/s]Inference:  47%|████▋     | 15/32 [00:14<00:17,  1.02s/it]Inference:  50%|█████     | 16/32 [00:16<00:17,  1.12s/it]Inference:  53%|█████▎    | 17/32 [00:17<00:15,  1.06s/it]Inference:  56%|█████▋    | 18/32 [00:18<00:14,  1.07s/it]Inference:  59%|█████▉    | 19/32 [00:19<00:13,  1.03s/it]Inference:  62%|██████▎   | 20/32 [00:20<00:12,  1.02s/it]Inference:  66%|██████▌   | 21/32 [00:21<00:11,  1.02s/it]Inference:  69%|██████▉   | 22/32 [00:22<00:10,  1.02s/it]Inference:  72%|███████▏  | 23/32 [00:23<00:09,  1.02s/it]Inference:  75%|███████▌  | 24/32 [00:24<00:08,  1.01s/it]Inference:  78%|███████▊  | 25/32 [00:25<00:06,  1.02it/s]Inference:  81%|████████▏ | 26/32 [00:26<00:05,  1.01it/s]Inference:  84%|████████▍ | 27/32 [00:27<00:05,  1.05s/it]Inference:  88%|████████▊ | 28/32 [00:28<00:04,  1.03s/it]Inference:  91%|█████████ | 29/32 [00:29<00:03,  1.05s/it]Inference:  94%|█████████▍| 30/32 [00:30<00:02,  1.04s/it]Inference:  97%|█████████▋| 31/32 [00:31<00:01,  1.03s/it]Inference: 100%|██████████| 32/32 [00:32<00:00,  1.01it/s]Inference: 100%|██████████| 32/32 [00:32<00:00,  1.01s/it]
layer16: self_attn.q_proj
256
val_hessian None
iter 0, train loss 1263.5537109375, val loss None, lr 0.001
iter 10, train loss 1344.783935546875, val loss None, lr 0.001
iter 20, train loss 1286.4267578125, val loss None, lr 0.001
iter 30, train loss 1253.7646484375, val loss None, lr 0.001
iter 40, train loss 1259.341064453125, val loss None, lr 0.001
iter 50, train loss 1253.0794677734375, val loss None, lr 0.001
iter 60, train loss 1257.41552734375, val loss None, lr 0.001
iter 70, train loss 1255.94384765625, val loss None, lr 0.001
iter 80, train loss 1264.228271484375, val loss None, lr 0.001
iter 90, train loss 1271.9261474609375, val loss None, lr 0.001
best loss 1171.7864990234375
layer16: self_attn.k_proj
256
val_hessian None
iter 0, train loss 1427.95068359375, val loss None, lr 0.001
iter 10, train loss 1512.261962890625, val loss None, lr 0.001
iter 20, train loss 1493.40625, val loss None, lr 0.001
iter 30, train loss 1431.24951171875, val loss None, lr 0.001
iter 40, train loss 1414.4666748046875, val loss None, lr 0.001
iter 50, train loss 1418.56591796875, val loss None, lr 0.001
iter 60, train loss 1406.66064453125, val loss None, lr 0.001
iter 70, train loss 1406.6044921875, val loss None, lr 0.001
iter 80, train loss 1414.207275390625, val loss None, lr 0.001
iter 90, train loss 1415.5802001953125, val loss None, lr 0.001
best loss 1291.3199462890625
layer16: self_attn.v_proj
256
val_hessian None
iter 0, train loss 509.7169494628906, val loss None, lr 0.001
iter 10, train loss 506.93170166015625, val loss None, lr 0.001
iter 20, train loss 505.095703125, val loss None, lr 0.001
iter 30, train loss 503.9593505859375, val loss None, lr 0.001
iter 40, train loss 503.37127685546875, val loss None, lr 0.001
iter 50, train loss 502.8905944824219, val loss None, lr 0.001
iter 60, train loss 501.55755615234375, val loss None, lr 0.001
iter 70, train loss 501.9717712402344, val loss None, lr 0.001
iter 80, train loss 501.27764892578125, val loss None, lr 0.001
iter 90, train loss 501.5214538574219, val loss None, lr 0.001
best loss 501.27764892578125
layer16: self_attn.o_proj
256
val_hessian None
iter 0, train loss 53.51639938354492, val loss None, lr 0.001
iter 10, train loss 53.00440979003906, val loss None, lr 0.001
iter 20, train loss 52.71104049682617, val loss None, lr 0.001
iter 30, train loss 52.439231872558594, val loss None, lr 0.001
iter 40, train loss 52.263404846191406, val loss None, lr 0.001
iter 50, train loss 51.9388427734375, val loss None, lr 0.001
iter 60, train loss 51.819671630859375, val loss None, lr 0.001
iter 70, train loss 52.0445556640625, val loss None, lr 0.001
iter 80, train loss 52.19926452636719, val loss None, lr 0.001
iter 90, train loss 52.43815994262695, val loss None, lr 0.001
best loss 51.782962799072266
layer16: mlp.gate_proj
256
val_hessian None
iter 0, train loss 1064.3079833984375, val loss None, lr 0.001
iter 10, train loss 1123.2117919921875, val loss None, lr 0.001
iter 20, train loss 1100.8916015625, val loss None, lr 0.001
iter 30, train loss 1098.1348876953125, val loss None, lr 0.001
iter 40, train loss 1096.4671630859375, val loss None, lr 0.001
iter 50, train loss 1093.9287109375, val loss None, lr 0.001
iter 60, train loss 1094.964111328125, val loss None, lr 0.001
iter 70, train loss 1093.9019775390625, val loss None, lr 0.001
iter 80, train loss 1091.2109375, val loss None, lr 0.001
iter 90, train loss 1091.9923095703125, val loss None, lr 0.001
best loss 1049.2576904296875
layer16: mlp.up_proj
256
val_hessian None
iter 0, train loss 962.5509643554688, val loss None, lr 0.001
iter 10, train loss 966.7395629882812, val loss None, lr 0.001
iter 20, train loss 971.971435546875, val loss None, lr 0.001
iter 30, train loss 975.4039306640625, val loss None, lr 0.001
iter 40, train loss 977.5980224609375, val loss None, lr 0.001
iter 50, train loss 978.8548583984375, val loss None, lr 0.001
iter 60, train loss 980.2135009765625, val loss None, lr 0.001
iter 70, train loss 980.468017578125, val loss None, lr 0.001
iter 80, train loss 981.625244140625, val loss None, lr 0.001
iter 90, train loss 983.2698974609375, val loss None, lr 0.001
best loss 960.906982421875
layer16: mlp.down_proj
256
val_hessian None
iter 0, train loss 35.60798645019531, val loss None, lr 0.001
iter 10, train loss 35.73234939575195, val loss None, lr 0.001
iter 20, train loss 35.58920669555664, val loss None, lr 0.001
iter 30, train loss 35.551239013671875, val loss None, lr 0.001
iter 40, train loss 35.505287170410156, val loss None, lr 0.001
iter 50, train loss 35.524269104003906, val loss None, lr 0.001
iter 60, train loss 35.59214782714844, val loss None, lr 0.001
iter 70, train loss 35.62718200683594, val loss None, lr 0.001
iter 80, train loss 35.66412353515625, val loss None, lr 0.001
iter 90, train loss 35.705833435058594, val loss None, lr 0.001
best loss 35.49187088012695
27465 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:09,  3.18it/s]Inference:   6%|▋         | 2/32 [00:00<00:10,  2.86it/s]Inference:   9%|▉         | 3/32 [00:01<00:10,  2.80it/s]Inference:  12%|█▎        | 4/32 [00:01<00:11,  2.51it/s]Inference:  16%|█▌        | 5/32 [00:01<00:10,  2.51it/s]Inference:  19%|█▉        | 6/32 [00:02<00:11,  2.24it/s]Inference:  22%|██▏       | 7/32 [00:02<00:11,  2.11it/s]Inference:  25%|██▌       | 8/32 [00:03<00:11,  2.17it/s]Inference:  28%|██▊       | 9/32 [00:03<00:10,  2.21it/s]Inference:  31%|███▏      | 10/32 [00:04<00:10,  2.11it/s]Inference:  34%|███▍      | 11/32 [00:04<00:09,  2.19it/s]Inference:  38%|███▊      | 12/32 [00:05<00:09,  2.08it/s]Inference:  41%|████      | 13/32 [00:05<00:09,  2.04it/s]Inference:  44%|████▍     | 14/32 [00:06<00:08,  2.01it/s]Inference:  47%|████▋     | 15/32 [00:06<00:08,  2.11it/s]Inference:  50%|█████     | 16/32 [00:07<00:07,  2.04it/s]Inference:  53%|█████▎    | 17/32 [00:07<00:07,  1.90it/s]Inference:  56%|█████▋    | 18/32 [00:08<00:06,  2.01it/s]Inference:  59%|█████▉    | 19/32 [00:08<00:06,  1.98it/s]Inference:  62%|██████▎   | 20/32 [00:09<00:05,  2.07it/s]Inference:  66%|██████▌   | 21/32 [00:09<00:05,  2.15it/s]Inference:  69%|██████▉   | 22/32 [00:10<00:05,  1.96it/s]Inference:  72%|███████▏  | 23/32 [00:10<00:04,  2.06it/s]Inference:  75%|███████▌  | 24/32 [00:11<00:04,  1.87it/s]Inference:  78%|███████▊  | 25/32 [00:11<00:03,  1.81it/s]Inference:  81%|████████▏ | 26/32 [00:12<00:03,  1.94it/s]Inference:  84%|████████▍ | 27/32 [00:12<00:02,  1.94it/s]Inference:  88%|████████▊ | 28/32 [00:13<00:02,  1.93it/s]Inference:  91%|█████████ | 29/32 [00:13<00:01,  1.93it/s]Inference:  94%|█████████▍| 30/32 [00:14<00:01,  1.89it/s]Inference:  97%|█████████▋| 31/32 [00:15<00:00,  1.94it/s]Inference: 100%|██████████| 32/32 [00:15<00:00,  2.01it/s]Inference: 100%|██████████| 32/32 [00:15<00:00,  2.07it/s]
22175 MiB free out of 48676 MiB total
Saved layer 16 to ./models/meta-llama/Llama-2-7b-hf/2bpv/128/quantized/layer_16.pt
after cast to cpu
26177 MiB free out of 48676 MiB total
Done with layer 16 total_time elapsed: 9638 estimated time left: 8504
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:27,  1.11it/s]Inference:   6%|▋         | 2/32 [00:01<00:27,  1.11it/s]Inference:   9%|▉         | 3/32 [00:02<00:27,  1.05it/s]Inference:  12%|█▎        | 4/32 [00:03<00:25,  1.11it/s]Inference:  16%|█▌        | 5/32 [00:04<00:26,  1.01it/s]Inference:  19%|█▉        | 6/32 [00:05<00:25,  1.00it/s]Inference:  22%|██▏       | 7/32 [00:06<00:24,  1.03it/s]Inference:  25%|██▌       | 8/32 [00:07<00:23,  1.02it/s]Inference:  28%|██▊       | 9/32 [00:08<00:22,  1.02it/s]Inference:  31%|███▏      | 10/32 [00:09<00:21,  1.02it/s]Inference:  34%|███▍      | 11/32 [00:10<00:20,  1.03it/s]Inference:  38%|███▊      | 12/32 [00:11<00:18,  1.08it/s]Inference:  41%|████      | 13/32 [00:12<00:16,  1.14it/s]Inference:  44%|████▍     | 14/32 [00:13<00:15,  1.13it/s]Inference:  47%|████▋     | 15/32 [00:14<00:16,  1.06it/s]Inference:  50%|█████     | 16/32 [00:15<00:16,  1.01s/it]Inference:  53%|█████▎    | 17/32 [00:16<00:15,  1.03s/it]Inference:  56%|█████▋    | 18/32 [00:17<00:13,  1.00it/s]Inference:  59%|█████▉    | 19/32 [00:18<00:13,  1.00s/it]Inference:  62%|██████▎   | 20/32 [00:19<00:12,  1.05s/it]Inference:  66%|██████▌   | 21/32 [00:20<00:11,  1.01s/it]Inference:  69%|██████▉   | 22/32 [00:21<00:10,  1.01s/it]Inference:  72%|███████▏  | 23/32 [00:22<00:08,  1.02it/s]Inference:  75%|███████▌  | 24/32 [00:23<00:07,  1.04it/s]Inference:  78%|███████▊  | 25/32 [00:24<00:07,  1.02s/it]Inference:  81%|████████▏ | 26/32 [00:25<00:06,  1.00s/it]Inference:  84%|████████▍ | 27/32 [00:26<00:05,  1.05s/it]Inference:  88%|████████▊ | 28/32 [00:27<00:04,  1.06s/it]Inference:  91%|█████████ | 29/32 [00:28<00:03,  1.04s/it]Inference:  94%|█████████▍| 30/32 [00:29<00:02,  1.03s/it]Inference:  97%|█████████▋| 31/32 [00:30<00:01,  1.05s/it]Inference: 100%|██████████| 32/32 [00:31<00:00,  1.03s/it]Inference: 100%|██████████| 32/32 [00:31<00:00,  1.01it/s]
layer17: self_attn.q_proj
256
val_hessian None
iter 0, train loss 1319.76806640625, val loss None, lr 0.001
iter 10, train loss 1416.8077392578125, val loss None, lr 0.001
iter 20, train loss 1337.93212890625, val loss None, lr 0.001
iter 30, train loss 1319.75439453125, val loss None, lr 0.001
iter 40, train loss 1311.25732421875, val loss None, lr 0.001
iter 50, train loss 1318.0819091796875, val loss None, lr 0.001
iter 60, train loss 1303.6451416015625, val loss None, lr 0.001
iter 70, train loss 1304.42529296875, val loss None, lr 0.001
iter 80, train loss 1315.2679443359375, val loss None, lr 0.001
iter 90, train loss 1307.1322021484375, val loss None, lr 0.001
best loss 1217.712890625
layer17: self_attn.k_proj
256
val_hessian None
iter 0, train loss 1463.404052734375, val loss None, lr 0.001
iter 10, train loss 1574.33251953125, val loss None, lr 0.001
iter 20, train loss 1530.832763671875, val loss None, lr 0.001
iter 30, train loss 1473.297607421875, val loss None, lr 0.001
iter 40, train loss 1443.269775390625, val loss None, lr 0.001
iter 50, train loss 1439.755859375, val loss None, lr 0.001
iter 60, train loss 1436.47021484375, val loss None, lr 0.001
iter 70, train loss 1436.101806640625, val loss None, lr 0.001
iter 80, train loss 1430.6529541015625, val loss None, lr 0.001
iter 90, train loss 1438.3292236328125, val loss None, lr 0.001
best loss 1318.44775390625
layer17: self_attn.v_proj
256
val_hessian None
iter 0, train loss 538.8189697265625, val loss None, lr 0.001
iter 10, train loss 538.5706787109375, val loss None, lr 0.001
iter 20, train loss 536.8986206054688, val loss None, lr 0.001
iter 30, train loss 535.5167236328125, val loss None, lr 0.001
iter 40, train loss 534.5223999023438, val loss None, lr 0.001
iter 50, train loss 534.5552368164062, val loss None, lr 0.001
iter 60, train loss 534.5027465820312, val loss None, lr 0.001
iter 70, train loss 534.0950927734375, val loss None, lr 0.001
iter 80, train loss 534.84521484375, val loss None, lr 0.001
iter 90, train loss 535.5084838867188, val loss None, lr 0.001
best loss 533.5975341796875
layer17: self_attn.o_proj
256
val_hessian None
iter 0, train loss 35.175811767578125, val loss None, lr 0.001
iter 10, train loss 34.46520233154297, val loss None, lr 0.001
iter 20, train loss 33.72740173339844, val loss None, lr 0.001
iter 30, train loss 33.86281967163086, val loss None, lr 0.001
iter 40, train loss 33.90199661254883, val loss None, lr 0.001
iter 50, train loss 34.1463508605957, val loss None, lr 0.001
iter 60, train loss 34.01755142211914, val loss None, lr 0.001
iter 70, train loss 34.30402755737305, val loss None, lr 0.001
iter 80, train loss 34.32675552368164, val loss None, lr 0.001
iter 90, train loss 34.34697723388672, val loss None, lr 0.001
best loss 33.71125030517578
layer17: mlp.gate_proj
256
val_hessian None
iter 0, train loss 1201.10302734375, val loss None, lr 0.001
iter 10, train loss 1264.034912109375, val loss None, lr 0.001
iter 20, train loss 1252.7791748046875, val loss None, lr 0.001
iter 30, train loss 1247.22265625, val loss None, lr 0.001
iter 40, train loss 1244.28857421875, val loss None, lr 0.001
iter 50, train loss 1242.2083740234375, val loss None, lr 0.001
iter 60, train loss 1238.2703857421875, val loss None, lr 0.001
iter 70, train loss 1240.40283203125, val loss None, lr 0.001
iter 80, train loss 1241.5579833984375, val loss None, lr 0.001
iter 90, train loss 1242.013427734375, val loss None, lr 0.001
best loss 1186.255859375
layer17: mlp.up_proj
256
val_hessian None
iter 0, train loss 1055.8035888671875, val loss None, lr 0.001
iter 10, train loss 1059.84912109375, val loss None, lr 0.001
iter 20, train loss 1065.236572265625, val loss None, lr 0.001
iter 30, train loss 1071.32666015625, val loss None, lr 0.001
iter 40, train loss 1075.9739990234375, val loss None, lr 0.001
iter 50, train loss 1077.8203125, val loss None, lr 0.001
iter 60, train loss 1078.4849853515625, val loss None, lr 0.001
iter 70, train loss 1080.1749267578125, val loss None, lr 0.001
iter 80, train loss 1080.308837890625, val loss None, lr 0.001
iter 90, train loss 1081.95751953125, val loss None, lr 0.001
best loss 1055.8035888671875
layer17: mlp.down_proj
256
val_hessian None
iter 0, train loss 38.757694244384766, val loss None, lr 0.001
iter 10, train loss 38.71929931640625, val loss None, lr 0.001
iter 20, train loss 38.51679992675781, val loss None, lr 0.001
iter 30, train loss 38.30033874511719, val loss None, lr 0.001
iter 40, train loss 38.177650451660156, val loss None, lr 0.001
iter 50, train loss 38.10606002807617, val loss None, lr 0.001
iter 60, train loss 38.13035583496094, val loss None, lr 0.001
iter 70, train loss 38.10271453857422, val loss None, lr 0.001
iter 80, train loss 38.07549285888672, val loss None, lr 0.001
iter 90, train loss 38.075565338134766, val loss None, lr 0.001
best loss 38.056739807128906
26177 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:12,  2.57it/s]Inference:   6%|▋         | 2/32 [00:00<00:12,  2.31it/s]Inference:   9%|▉         | 3/32 [00:01<00:11,  2.43it/s]Inference:  12%|█▎        | 4/32 [00:01<00:11,  2.49it/s]Inference:  16%|█▌        | 5/32 [00:02<00:10,  2.51it/s]Inference:  19%|█▉        | 6/32 [00:02<00:10,  2.51it/s]Inference:  22%|██▏       | 7/32 [00:02<00:10,  2.33it/s]Inference:  25%|██▌       | 8/32 [00:03<00:10,  2.26it/s]Inference:  28%|██▊       | 9/32 [00:03<00:09,  2.34it/s]Inference:  31%|███▏      | 10/32 [00:04<00:09,  2.43it/s]Inference:  34%|███▍      | 11/32 [00:04<00:09,  2.33it/s]Inference:  38%|███▊      | 12/32 [00:05<00:08,  2.25it/s]Inference:  41%|████      | 13/32 [00:05<00:08,  2.21it/s]Inference:  44%|████▍     | 14/32 [00:05<00:07,  2.33it/s]Inference:  47%|████▋     | 15/32 [00:06<00:06,  2.57it/s]Inference:  50%|█████     | 16/32 [00:06<00:06,  2.59it/s]Inference:  53%|█████▎    | 17/32 [00:07<00:05,  2.59it/s]Inference:  56%|█████▋    | 18/32 [00:07<00:05,  2.48it/s]Inference:  59%|█████▉    | 19/32 [00:07<00:04,  2.65it/s]Inference:  62%|██████▎   | 20/32 [00:08<00:04,  2.75it/s]Inference:  66%|██████▌   | 21/32 [00:08<00:04,  2.48it/s]Inference:  69%|██████▉   | 22/32 [00:09<00:04,  2.21it/s]Inference:  72%|███████▏  | 23/32 [00:09<00:04,  2.03it/s]Inference:  75%|███████▌  | 24/32 [00:10<00:03,  2.18it/s]Inference:  78%|███████▊  | 25/32 [00:10<00:03,  2.30it/s]Inference:  81%|████████▏ | 26/32 [00:10<00:02,  2.38it/s]Inference:  84%|████████▍ | 27/32 [00:11<00:02,  2.43it/s]Inference:  88%|████████▊ | 28/32 [00:11<00:01,  2.46it/s]Inference:  91%|█████████ | 29/32 [00:12<00:01,  2.32it/s]Inference:  94%|█████████▍| 30/32 [00:12<00:00,  2.41it/s]Inference:  97%|█████████▋| 31/32 [00:13<00:00,  2.33it/s]Inference: 100%|██████████| 32/32 [00:13<00:00,  2.40it/s]Inference: 100%|██████████| 32/32 [00:13<00:00,  2.39it/s]
20887 MiB free out of 48676 MiB total
Saved layer 17 to ./models/meta-llama/Llama-2-7b-hf/2bpv/128/quantized/layer_17.pt
after cast to cpu
24889 MiB free out of 48676 MiB total
Done with layer 17 total_time elapsed: 10492 estimated time left: 8160
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:01<00:31,  1.00s/it]Inference:   6%|▋         | 2/32 [00:02<00:30,  1.03s/it]Inference:   9%|▉         | 3/32 [00:02<00:28,  1.01it/s]Inference:  12%|█▎        | 4/32 [00:04<00:28,  1.03s/it]Inference:  16%|█▌        | 5/32 [00:05<00:29,  1.10s/it]Inference:  19%|█▉        | 6/32 [00:06<00:28,  1.08s/it]Inference:  22%|██▏       | 7/32 [00:07<00:27,  1.08s/it]Inference:  25%|██▌       | 8/32 [00:08<00:25,  1.05s/it]Inference:  28%|██▊       | 9/32 [00:09<00:23,  1.04s/it]Inference:  31%|███▏      | 10/32 [00:10<00:23,  1.06s/it]Inference:  34%|███▍      | 11/32 [00:11<00:20,  1.00it/s]Inference:  38%|███▊      | 12/32 [00:12<00:19,  1.01it/s]Inference:  41%|████      | 13/32 [00:13<00:18,  1.01it/s]Inference:  44%|████▍     | 14/32 [00:14<00:18,  1.00s/it]Inference:  47%|████▋     | 15/32 [00:15<00:17,  1.00s/it]Inference:  50%|█████     | 16/32 [00:16<00:17,  1.08s/it]Inference:  53%|█████▎    | 17/32 [00:17<00:15,  1.06s/it]Inference:  56%|█████▋    | 18/32 [00:18<00:14,  1.07s/it]Inference:  59%|█████▉    | 19/32 [00:19<00:13,  1.03s/it]Inference:  62%|██████▎   | 20/32 [00:20<00:12,  1.03s/it]Inference:  66%|██████▌   | 21/32 [00:21<00:10,  1.01it/s]Inference:  69%|██████▉   | 22/32 [00:22<00:09,  1.00it/s]Inference:  72%|███████▏  | 23/32 [00:23<00:08,  1.00it/s]Inference:  75%|███████▌  | 24/32 [00:24<00:08,  1.03s/it]Inference:  78%|███████▊  | 25/32 [00:25<00:07,  1.03s/it]Inference:  81%|████████▏ | 26/32 [00:26<00:06,  1.00s/it]Inference:  84%|████████▍ | 27/32 [00:27<00:05,  1.00s/it]Inference:  88%|████████▊ | 28/32 [00:28<00:04,  1.00s/it]Inference:  91%|█████████ | 29/32 [00:29<00:03,  1.01s/it]Inference:  94%|█████████▍| 30/32 [00:30<00:02,  1.03s/it]Inference:  97%|█████████▋| 31/32 [00:31<00:01,  1.05s/it]Inference: 100%|██████████| 32/32 [00:32<00:00,  1.01it/s]Inference: 100%|██████████| 32/32 [00:32<00:00,  1.02s/it]
layer18: self_attn.q_proj
256
val_hessian None
iter 0, train loss 1432.972412109375, val loss None, lr 0.001
iter 10, train loss 1525.84228515625, val loss None, lr 0.001
iter 20, train loss 1429.242431640625, val loss None, lr 0.001
iter 30, train loss 1391.559814453125, val loss None, lr 0.001
iter 40, train loss 1383.4105224609375, val loss None, lr 0.001
iter 50, train loss 1385.166259765625, val loss None, lr 0.001
iter 60, train loss 1385.3968505859375, val loss None, lr 0.001
iter 70, train loss 1380.7633056640625, val loss None, lr 0.001
iter 80, train loss 1387.2933349609375, val loss None, lr 0.001
iter 90, train loss 1386.2437744140625, val loss None, lr 0.001
best loss 1313.61865234375
layer18: self_attn.k_proj
256
val_hessian None
iter 0, train loss 1543.4459228515625, val loss None, lr 0.001
iter 10, train loss 1657.5074462890625, val loss None, lr 0.001
iter 20, train loss 1600.415283203125, val loss None, lr 0.001
iter 30, train loss 1536.116455078125, val loss None, lr 0.001
iter 40, train loss 1514.6163330078125, val loss None, lr 0.001
iter 50, train loss 1501.8511962890625, val loss None, lr 0.001
iter 60, train loss 1484.338623046875, val loss None, lr 0.001
iter 70, train loss 1503.932373046875, val loss None, lr 0.001
iter 80, train loss 1509.5650634765625, val loss None, lr 0.001
iter 90, train loss 1507.775390625, val loss None, lr 0.001
best loss 1397.3995361328125
layer18: self_attn.v_proj
256
val_hessian None
iter 0, train loss 654.7402954101562, val loss None, lr 0.001
iter 10, train loss 652.39306640625, val loss None, lr 0.001
iter 20, train loss 650.6071166992188, val loss None, lr 0.001
iter 30, train loss 649.8565063476562, val loss None, lr 0.001
iter 40, train loss 649.7203979492188, val loss None, lr 0.001
iter 50, train loss 648.9452514648438, val loss None, lr 0.001
iter 60, train loss 648.3665161132812, val loss None, lr 0.001
iter 70, train loss 649.5048828125, val loss None, lr 0.001
iter 80, train loss 649.7838134765625, val loss None, lr 0.001
iter 90, train loss 649.3798217773438, val loss None, lr 0.001
best loss 648.3665161132812
layer18: self_attn.o_proj
256
val_hessian None
iter 0, train loss 36.09548568725586, val loss None, lr 0.001
iter 10, train loss 35.152164459228516, val loss None, lr 0.001
iter 20, train loss 34.453697204589844, val loss None, lr 0.001
iter 30, train loss 34.386817932128906, val loss None, lr 0.001
iter 40, train loss 33.97203063964844, val loss None, lr 0.001
iter 50, train loss 33.67530059814453, val loss None, lr 0.001
iter 60, train loss 33.590972900390625, val loss None, lr 0.001
iter 70, train loss 33.47100830078125, val loss None, lr 0.001
iter 80, train loss 33.45795440673828, val loss None, lr 0.001
iter 90, train loss 33.55487060546875, val loss None, lr 0.001
best loss 33.4156494140625
layer18: mlp.gate_proj
256
val_hessian None
iter 0, train loss 1364.2923583984375, val loss None, lr 0.001
iter 10, train loss 1423.7552490234375, val loss None, lr 0.001
iter 20, train loss 1420.135009765625, val loss None, lr 0.001
iter 30, train loss 1416.9697265625, val loss None, lr 0.001
iter 40, train loss 1421.4754638671875, val loss None, lr 0.001
iter 50, train loss 1417.4873046875, val loss None, lr 0.001
iter 60, train loss 1417.4000244140625, val loss None, lr 0.001
iter 70, train loss 1417.8812255859375, val loss None, lr 0.001
iter 80, train loss 1419.3953857421875, val loss None, lr 0.001
iter 90, train loss 1422.34228515625, val loss None, lr 0.001
best loss 1349.7489013671875
layer18: mlp.up_proj
256
val_hessian None
iter 0, train loss 1164.9400634765625, val loss None, lr 0.001
iter 10, train loss 1168.36962890625, val loss None, lr 0.001
iter 20, train loss 1173.7813720703125, val loss None, lr 0.001
iter 30, train loss 1179.7373046875, val loss None, lr 0.001
iter 40, train loss 1181.632080078125, val loss None, lr 0.001
iter 50, train loss 1184.4501953125, val loss None, lr 0.001
iter 60, train loss 1188.076416015625, val loss None, lr 0.001
iter 70, train loss 1190.5577392578125, val loss None, lr 0.001
iter 80, train loss 1193.6773681640625, val loss None, lr 0.001
iter 90, train loss 1195.402099609375, val loss None, lr 0.001
best loss 1164.9400634765625
layer18: mlp.down_proj
256
val_hessian None
iter 0, train loss 44.99671936035156, val loss None, lr 0.001
iter 10, train loss 45.039093017578125, val loss None, lr 0.001
iter 20, train loss 44.751068115234375, val loss None, lr 0.001
iter 30, train loss 44.529502868652344, val loss None, lr 0.001
iter 40, train loss 44.35676574707031, val loss None, lr 0.001
iter 50, train loss 44.22587203979492, val loss None, lr 0.001
iter 60, train loss 44.205379486083984, val loss None, lr 0.001
iter 70, train loss 44.12738037109375, val loss None, lr 0.001
iter 80, train loss 44.118629455566406, val loss None, lr 0.001
iter 90, train loss 44.10012435913086, val loss None, lr 0.001
best loss 44.09121322631836
24889 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:19,  1.58it/s]Inference:   6%|▋         | 2/32 [00:01<00:18,  1.60it/s]Inference:   9%|▉         | 3/32 [00:01<00:18,  1.58it/s]Inference:  12%|█▎        | 4/32 [00:02<00:15,  1.77it/s]Inference:  16%|█▌        | 5/32 [00:02<00:14,  1.80it/s]Inference:  19%|█▉        | 6/32 [00:03<00:12,  2.07it/s]Inference:  22%|██▏       | 7/32 [00:03<00:13,  1.88it/s]Inference:  25%|██▌       | 8/32 [00:04<00:12,  1.89it/s]Inference:  28%|██▊       | 9/32 [00:04<00:11,  2.00it/s]Inference:  31%|███▏      | 10/32 [00:05<00:10,  2.08it/s]Inference:  34%|███▍      | 11/32 [00:05<00:10,  2.01it/s]Inference:  38%|███▊      | 12/32 [00:06<00:09,  2.00it/s]Inference:  41%|████      | 13/32 [00:06<00:10,  1.90it/s]Inference:  44%|████▍     | 14/32 [00:07<00:10,  1.75it/s]Inference:  47%|████▋     | 15/32 [00:08<00:09,  1.74it/s]Inference:  50%|█████     | 16/32 [00:08<00:09,  1.65it/s]Inference:  53%|█████▎    | 17/32 [00:09<00:09,  1.60it/s]Inference:  56%|█████▋    | 18/32 [00:09<00:08,  1.72it/s]Inference:  59%|█████▉    | 19/32 [00:10<00:07,  1.64it/s]Inference:  62%|██████▎   | 20/32 [00:11<00:07,  1.65it/s]Inference:  66%|██████▌   | 21/32 [00:12<00:07,  1.51it/s]Inference:  69%|██████▉   | 22/32 [00:12<00:05,  1.67it/s]Inference:  72%|███████▏  | 23/32 [00:13<00:05,  1.71it/s]Inference:  75%|███████▌  | 24/32 [00:13<00:04,  1.83it/s]Inference:  78%|███████▊  | 25/32 [00:14<00:04,  1.74it/s]Inference:  81%|████████▏ | 26/32 [00:14<00:03,  1.75it/s]Inference:  84%|████████▍ | 27/32 [00:15<00:02,  1.77it/s]Inference:  88%|████████▊ | 28/32 [00:15<00:02,  1.78it/s]Inference:  91%|█████████ | 29/32 [00:16<00:01,  1.69it/s]Inference:  94%|█████████▍| 30/32 [00:17<00:01,  1.65it/s]Inference:  97%|█████████▋| 31/32 [00:17<00:00,  1.61it/s]Inference: 100%|██████████| 32/32 [00:18<00:00,  1.75it/s]Inference: 100%|██████████| 32/32 [00:18<00:00,  1.76it/s]
19663 MiB free out of 48676 MiB total
Saved layer 18 to ./models/meta-llama/Llama-2-7b-hf/2bpv/128/quantized/layer_18.pt
after cast to cpu
23665 MiB free out of 48676 MiB total
Done with layer 18 total_time elapsed: 11345 estimated time left: 7762
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:01<00:33,  1.09s/it]Inference:   6%|▋         | 2/32 [00:02<00:33,  1.13s/it]Inference:   9%|▉         | 3/32 [00:03<00:29,  1.01s/it]Inference:  12%|█▎        | 4/32 [00:03<00:26,  1.06it/s]Inference:  16%|█▌        | 5/32 [00:04<00:24,  1.09it/s]Inference:  19%|█▉        | 6/32 [00:05<00:25,  1.04it/s]Inference:  22%|██▏       | 7/32 [00:06<00:24,  1.04it/s]Inference:  25%|██▌       | 8/32 [00:07<00:22,  1.05it/s]Inference:  28%|██▊       | 9/32 [00:08<00:21,  1.07it/s]Inference:  31%|███▏      | 10/32 [00:09<00:21,  1.04it/s]Inference:  34%|███▍      | 11/32 [00:10<00:20,  1.03it/s]Inference:  38%|███▊      | 12/32 [00:11<00:19,  1.04it/s]Inference:  41%|████      | 13/32 [00:12<00:19,  1.04s/it]Inference:  44%|████▍     | 14/32 [00:13<00:18,  1.02s/it]Inference:  47%|████▋     | 15/32 [00:14<00:17,  1.04s/it]Inference:  50%|█████     | 16/32 [00:16<00:16,  1.06s/it]Inference:  53%|█████▎    | 17/32 [00:16<00:15,  1.03s/it]Inference:  56%|█████▋    | 18/32 [00:17<00:14,  1.01s/it]Inference:  59%|█████▉    | 19/32 [00:18<00:13,  1.01s/it]Inference:  62%|██████▎   | 20/32 [00:19<00:12,  1.01s/it]Inference:  66%|██████▌   | 21/32 [00:21<00:11,  1.06s/it]Inference:  69%|██████▉   | 22/32 [00:22<00:10,  1.04s/it]Inference:  72%|███████▏  | 23/32 [00:23<00:09,  1.03s/it]Inference:  75%|███████▌  | 24/32 [00:24<00:08,  1.06s/it]Inference:  78%|███████▊  | 25/32 [00:25<00:07,  1.08s/it]Inference:  81%|████████▏ | 26/32 [00:26<00:06,  1.06s/it]Inference:  84%|████████▍ | 27/32 [00:27<00:05,  1.10s/it]Inference:  88%|████████▊ | 28/32 [00:28<00:04,  1.05s/it]Inference:  91%|█████████ | 29/32 [00:29<00:03,  1.04s/it]Inference:  94%|█████████▍| 30/32 [00:30<00:02,  1.03s/it]Inference:  97%|█████████▋| 31/32 [00:31<00:01,  1.00s/it]Inference: 100%|██████████| 32/32 [00:32<00:00,  1.02it/s]Inference: 100%|██████████| 32/32 [00:32<00:00,  1.01s/it]
layer19: self_attn.q_proj
256
val_hessian None
iter 0, train loss 1353.4669189453125, val loss None, lr 0.001
iter 10, train loss 1452.9482421875, val loss None, lr 0.001
iter 20, train loss 1369.7083740234375, val loss None, lr 0.001
iter 30, train loss 1338.7216796875, val loss None, lr 0.001
iter 40, train loss 1334.73046875, val loss None, lr 0.001
iter 50, train loss 1331.1953125, val loss None, lr 0.001
iter 60, train loss 1327.8052978515625, val loss None, lr 0.001
iter 70, train loss 1333.0931396484375, val loss None, lr 0.001
iter 80, train loss 1327.706787109375, val loss None, lr 0.001
iter 90, train loss 1333.27001953125, val loss None, lr 0.001
best loss 1259.057373046875
layer19: self_attn.k_proj
256
val_hessian None
iter 0, train loss 1474.5107421875, val loss None, lr 0.001
iter 10, train loss 1569.6630859375, val loss None, lr 0.001
iter 20, train loss 1521.35595703125, val loss None, lr 0.001
iter 30, train loss 1472.3812255859375, val loss None, lr 0.001
iter 40, train loss 1460.814208984375, val loss None, lr 0.001
iter 50, train loss 1445.88232421875, val loss None, lr 0.001
iter 60, train loss 1440.38037109375, val loss None, lr 0.001
iter 70, train loss 1436.220703125, val loss None, lr 0.001
iter 80, train loss 1445.338134765625, val loss None, lr 0.001
iter 90, train loss 1451.86669921875, val loss None, lr 0.001
best loss 1342.990234375
layer19: self_attn.v_proj
256
val_hessian None
iter 0, train loss 660.4053955078125, val loss None, lr 0.001
iter 10, train loss 657.7130126953125, val loss None, lr 0.001
iter 20, train loss 655.8594970703125, val loss None, lr 0.001
iter 30, train loss 654.9457397460938, val loss None, lr 0.001
iter 40, train loss 655.1980590820312, val loss None, lr 0.001
iter 50, train loss 654.4713745117188, val loss None, lr 0.001
iter 60, train loss 655.245361328125, val loss None, lr 0.001
iter 70, train loss 654.3675537109375, val loss None, lr 0.001
iter 80, train loss 654.285888671875, val loss None, lr 0.001
iter 90, train loss 654.571533203125, val loss None, lr 0.001
best loss 653.91748046875
layer19: self_attn.o_proj
256
val_hessian None
iter 0, train loss 35.302879333496094, val loss None, lr 0.001
iter 10, train loss 34.47116470336914, val loss None, lr 0.001
iter 20, train loss 33.721866607666016, val loss None, lr 0.001
iter 30, train loss 33.78666687011719, val loss None, lr 0.001
iter 40, train loss 33.655189514160156, val loss None, lr 0.001
iter 50, train loss 33.55991744995117, val loss None, lr 0.001
iter 60, train loss 33.57651138305664, val loss None, lr 0.001
iter 70, train loss 33.70405578613281, val loss None, lr 0.001
iter 80, train loss 33.89503860473633, val loss None, lr 0.001
iter 90, train loss 33.896095275878906, val loss None, lr 0.001
best loss 33.473052978515625
layer19: mlp.gate_proj
256
val_hessian None
iter 0, train loss 1444.55029296875, val loss None, lr 0.001
iter 10, train loss 1501.1015625, val loss None, lr 0.001
iter 20, train loss 1503.0980224609375, val loss None, lr 0.001
iter 30, train loss 1498.26708984375, val loss None, lr 0.001
iter 40, train loss 1496.0596923828125, val loss None, lr 0.001
iter 50, train loss 1498.93408203125, val loss None, lr 0.001
iter 60, train loss 1501.9090576171875, val loss None, lr 0.001
iter 70, train loss 1502.25390625, val loss None, lr 0.001
iter 80, train loss 1507.666259765625, val loss None, lr 0.001
iter 90, train loss 1509.301513671875, val loss None, lr 0.001
best loss 1433.30712890625
layer19: mlp.up_proj
256
val_hessian None
iter 0, train loss 1241.3878173828125, val loss None, lr 0.001
iter 10, train loss 1244.028564453125, val loss None, lr 0.001
iter 20, train loss 1249.434814453125, val loss None, lr 0.001
iter 30, train loss 1257.5487060546875, val loss None, lr 0.001
iter 40, train loss 1263.25732421875, val loss None, lr 0.001
iter 50, train loss 1266.29931640625, val loss None, lr 0.001
iter 60, train loss 1269.3345947265625, val loss None, lr 0.001
iter 70, train loss 1271.1572265625, val loss None, lr 0.001
iter 80, train loss 1272.5689697265625, val loss None, lr 0.001
iter 90, train loss 1275.1346435546875, val loss None, lr 0.001
best loss 1241.3878173828125
layer19: mlp.down_proj
256
val_hessian None
iter 0, train loss 48.673404693603516, val loss None, lr 0.001
iter 10, train loss 48.6878776550293, val loss None, lr 0.001
iter 20, train loss 48.35115432739258, val loss None, lr 0.001
iter 30, train loss 48.11271667480469, val loss None, lr 0.001
iter 40, train loss 47.986907958984375, val loss None, lr 0.001
iter 50, train loss 47.890602111816406, val loss None, lr 0.001
iter 60, train loss 47.871124267578125, val loss None, lr 0.001
iter 70, train loss 47.863224029541016, val loss None, lr 0.001
iter 80, train loss 47.864036560058594, val loss None, lr 0.001
iter 90, train loss 47.889503479003906, val loss None, lr 0.001
best loss 47.82237243652344
23665 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:07,  3.96it/s]Inference:   6%|▋         | 2/32 [00:00<00:06,  4.48it/s]Inference:   9%|▉         | 3/32 [00:00<00:06,  4.44it/s]Inference:  12%|█▎        | 4/32 [00:00<00:06,  4.59it/s]Inference:  16%|█▌        | 5/32 [00:01<00:06,  4.42it/s]Inference:  19%|█▉        | 6/32 [00:01<00:05,  4.42it/s]Inference:  22%|██▏       | 7/32 [00:01<00:05,  4.42it/s]Inference:  25%|██▌       | 8/32 [00:01<00:05,  4.13it/s]Inference:  28%|██▊       | 9/32 [00:02<00:05,  4.18it/s]Inference:  31%|███▏      | 10/32 [00:02<00:05,  4.29it/s]Inference:  34%|███▍      | 11/32 [00:02<00:04,  4.36it/s]Inference:  38%|███▊      | 12/32 [00:02<00:04,  4.49it/s]Inference:  41%|████      | 13/32 [00:02<00:04,  4.49it/s]Inference:  44%|████▍     | 14/32 [00:03<00:04,  4.43it/s]Inference:  47%|████▋     | 15/32 [00:03<00:03,  4.43it/s]Inference:  50%|█████     | 16/32 [00:03<00:03,  4.41it/s]Inference:  53%|█████▎    | 17/32 [00:03<00:03,  4.39it/s]Inference:  56%|█████▋    | 18/32 [00:04<00:03,  4.42it/s]Inference:  59%|█████▉    | 19/32 [00:04<00:02,  4.50it/s]Inference:  62%|██████▎   | 20/32 [00:04<00:02,  4.48it/s]Inference:  66%|██████▌   | 21/32 [00:04<00:02,  4.48it/s]Inference:  69%|██████▉   | 22/32 [00:05<00:02,  4.40it/s]Inference:  72%|███████▏  | 23/32 [00:05<00:02,  4.41it/s]Inference:  75%|███████▌  | 24/32 [00:05<00:01,  4.42it/s]Inference:  78%|███████▊  | 25/32 [00:05<00:01,  4.38it/s]Inference:  81%|████████▏ | 26/32 [00:05<00:01,  4.42it/s]Inference:  84%|████████▍ | 27/32 [00:06<00:01,  4.05it/s]Inference:  88%|████████▊ | 28/32 [00:06<00:01,  3.85it/s]Inference:  91%|█████████ | 29/32 [00:06<00:00,  3.71it/s]Inference:  94%|█████████▍| 30/32 [00:07<00:00,  3.50it/s]Inference:  97%|█████████▋| 31/32 [00:07<00:00,  3.25it/s]Inference: 100%|██████████| 32/32 [00:07<00:00,  3.19it/s]Inference: 100%|██████████| 32/32 [00:07<00:00,  4.11it/s]
18375 MiB free out of 48676 MiB total
Saved layer 19 to ./models/meta-llama/Llama-2-7b-hf/2bpv/128/quantized/layer_19.pt
after cast to cpu
22377 MiB free out of 48676 MiB total
Done with layer 19 total_time elapsed: 12129 estimated time left: 7277
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:25,  1.23it/s]Inference:   6%|▋         | 2/32 [00:01<00:25,  1.16it/s]Inference:   9%|▉         | 3/32 [00:02<00:24,  1.18it/s]Inference:  12%|█▎        | 4/32 [00:03<00:24,  1.13it/s]Inference:  16%|█▌        | 5/32 [00:04<00:23,  1.15it/s]Inference:  19%|█▉        | 6/32 [00:05<00:22,  1.13it/s]Inference:  22%|██▏       | 7/32 [00:06<00:23,  1.08it/s]Inference:  25%|██▌       | 8/32 [00:07<00:21,  1.11it/s]Inference:  28%|██▊       | 9/32 [00:08<00:21,  1.09it/s]Inference:  31%|███▏      | 10/32 [00:08<00:19,  1.14it/s]Inference:  34%|███▍      | 11/32 [00:09<00:18,  1.14it/s]Inference:  38%|███▊      | 12/32 [00:10<00:17,  1.12it/s]Inference:  41%|████      | 13/32 [00:11<00:16,  1.17it/s]Inference:  44%|████▍     | 14/32 [00:12<00:14,  1.21it/s]Inference:  47%|████▋     | 15/32 [00:13<00:14,  1.18it/s]Inference:  50%|█████     | 16/32 [00:13<00:13,  1.18it/s]Inference:  53%|█████▎    | 17/32 [00:14<00:12,  1.19it/s]Inference:  56%|█████▋    | 18/32 [00:15<00:11,  1.19it/s]Inference:  59%|█████▉    | 19/32 [00:16<00:10,  1.19it/s]Inference:  62%|██████▎   | 20/32 [00:17<00:10,  1.16it/s]Inference:  66%|██████▌   | 21/32 [00:18<00:09,  1.14it/s]Inference:  69%|██████▉   | 22/32 [00:19<00:08,  1.12it/s]Inference:  72%|███████▏  | 23/32 [00:19<00:07,  1.15it/s]Inference:  75%|███████▌  | 24/32 [00:20<00:07,  1.14it/s]Inference:  78%|███████▊  | 25/32 [00:21<00:06,  1.13it/s]Inference:  81%|████████▏ | 26/32 [00:22<00:05,  1.15it/s]Inference:  84%|████████▍ | 27/32 [00:23<00:04,  1.20it/s]Inference:  88%|████████▊ | 28/32 [00:24<00:03,  1.20it/s]Inference:  91%|█████████ | 29/32 [00:25<00:02,  1.20it/s]Inference:  94%|█████████▍| 30/32 [00:25<00:01,  1.19it/s]Inference:  97%|█████████▋| 31/32 [00:26<00:00,  1.20it/s]Inference: 100%|██████████| 32/32 [00:27<00:00,  1.18it/s]Inference: 100%|██████████| 32/32 [00:27<00:00,  1.16it/s]
layer20: self_attn.q_proj
256
val_hessian None
iter 0, train loss 1401.3111572265625, val loss None, lr 0.001
iter 10, train loss 1498.766845703125, val loss None, lr 0.001
iter 20, train loss 1427.4478759765625, val loss None, lr 0.001
iter 30, train loss 1385.9517822265625, val loss None, lr 0.001
iter 40, train loss 1376.8309326171875, val loss None, lr 0.001
iter 50, train loss 1382.6907958984375, val loss None, lr 0.001
iter 60, train loss 1382.040771484375, val loss None, lr 0.001
iter 70, train loss 1371.05419921875, val loss None, lr 0.001
iter 80, train loss 1378.821533203125, val loss None, lr 0.001
iter 90, train loss 1382.88427734375, val loss None, lr 0.001
best loss 1299.760009765625
layer20: self_attn.k_proj
256
val_hessian None
iter 0, train loss 1510.7557373046875, val loss None, lr 0.001
iter 10, train loss 1612.4505615234375, val loss None, lr 0.001
iter 20, train loss 1559.585205078125, val loss None, lr 0.001
iter 30, train loss 1498.8973388671875, val loss None, lr 0.001
iter 40, train loss 1479.3798828125, val loss None, lr 0.001
iter 50, train loss 1491.962646484375, val loss None, lr 0.001
iter 60, train loss 1477.7900390625, val loss None, lr 0.001
iter 70, train loss 1490.0546875, val loss None, lr 0.001
iter 80, train loss 1497.7322998046875, val loss None, lr 0.001
iter 90, train loss 1492.3480224609375, val loss None, lr 0.001
best loss 1385.1689453125
layer20: self_attn.v_proj
256
val_hessian None
iter 0, train loss 682.1582641601562, val loss None, lr 0.001
iter 10, train loss 679.9088134765625, val loss None, lr 0.001
iter 20, train loss 677.5557861328125, val loss None, lr 0.001
iter 30, train loss 675.4544067382812, val loss None, lr 0.001
iter 40, train loss 674.7496337890625, val loss None, lr 0.001
iter 50, train loss 672.423828125, val loss None, lr 0.001
iter 60, train loss 672.2192993164062, val loss None, lr 0.001
iter 70, train loss 672.1161499023438, val loss None, lr 0.001
iter 80, train loss 672.8460693359375, val loss None, lr 0.001
iter 90, train loss 672.1649169921875, val loss None, lr 0.001
best loss 671.7245483398438
layer20: self_attn.o_proj
256
val_hessian None
iter 0, train loss 53.34571075439453, val loss None, lr 0.001
iter 10, train loss 49.328392028808594, val loss None, lr 0.001
iter 20, train loss 45.41415023803711, val loss None, lr 0.001
iter 30, train loss 43.994041442871094, val loss None, lr 0.001
iter 40, train loss 43.25973892211914, val loss None, lr 0.001
iter 50, train loss 42.695404052734375, val loss None, lr 0.001
iter 60, train loss 42.40904998779297, val loss None, lr 0.001
iter 70, train loss 42.15975570678711, val loss None, lr 0.001
iter 80, train loss 41.956382751464844, val loss None, lr 0.001
iter 90, train loss 41.815711975097656, val loss None, lr 0.001
best loss 41.76902389526367
layer20: mlp.gate_proj
256
val_hessian None
iter 0, train loss 1563.8851318359375, val loss None, lr 0.001
iter 10, train loss 1627.2452392578125, val loss None, lr 0.001
iter 20, train loss 1624.474365234375, val loss None, lr 0.001
iter 30, train loss 1624.5877685546875, val loss None, lr 0.001
iter 40, train loss 1627.60693359375, val loss None, lr 0.001
iter 50, train loss 1630.12841796875, val loss None, lr 0.001
iter 60, train loss 1627.216796875, val loss None, lr 0.001
iter 70, train loss 1636.9307861328125, val loss None, lr 0.001
iter 80, train loss 1633.67578125, val loss None, lr 0.001
iter 90, train loss 1638.204345703125, val loss None, lr 0.001
best loss 1548.6373291015625
layer20: mlp.up_proj
256
val_hessian None
iter 0, train loss 1332.9365234375, val loss None, lr 0.001
iter 10, train loss 1336.665283203125, val loss None, lr 0.001
iter 20, train loss 1346.505859375, val loss None, lr 0.001
iter 30, train loss 1355.3709716796875, val loss None, lr 0.001
iter 40, train loss 1363.77880859375, val loss None, lr 0.001
iter 50, train loss 1369.586669921875, val loss None, lr 0.001
iter 60, train loss 1373.1912841796875, val loss None, lr 0.001
iter 70, train loss 1375.9110107421875, val loss None, lr 0.001
iter 80, train loss 1377.873779296875, val loss None, lr 0.001
iter 90, train loss 1378.7900390625, val loss None, lr 0.001
best loss 1332.9365234375
layer20: mlp.down_proj
256
val_hessian None
iter 0, train loss 58.34999084472656, val loss None, lr 0.001
iter 10, train loss 58.30934524536133, val loss None, lr 0.001
iter 20, train loss 58.323890686035156, val loss None, lr 0.001
iter 30, train loss 58.1555061340332, val loss None, lr 0.001
iter 40, train loss 58.133827209472656, val loss None, lr 0.001
iter 50, train loss 58.235084533691406, val loss None, lr 0.001
iter 60, train loss 58.32744216918945, val loss None, lr 0.001
iter 70, train loss 58.47370147705078, val loss None, lr 0.001
iter 80, train loss 58.55809783935547, val loss None, lr 0.001
iter 90, train loss 58.70587158203125, val loss None, lr 0.001
best loss 58.133827209472656
22377 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:14,  2.21it/s]Inference:   6%|▋         | 2/32 [00:00<00:13,  2.30it/s]Inference:   9%|▉         | 3/32 [00:01<00:11,  2.60it/s]Inference:  12%|█▎        | 4/32 [00:01<00:11,  2.37it/s]Inference:  16%|█▌        | 5/32 [00:02<00:12,  2.24it/s]Inference:  19%|█▉        | 6/32 [00:02<00:11,  2.23it/s]Inference:  22%|██▏       | 7/32 [00:03<00:11,  2.12it/s]Inference:  25%|██▌       | 8/32 [00:03<00:11,  2.02it/s]Inference:  28%|██▊       | 9/32 [00:04<00:10,  2.16it/s]Inference:  31%|███▏      | 10/32 [00:04<00:10,  2.17it/s]Inference:  34%|███▍      | 11/32 [00:04<00:09,  2.19it/s]Inference:  38%|███▊      | 12/32 [00:05<00:09,  2.14it/s]Inference:  41%|████      | 13/32 [00:05<00:07,  2.39it/s]Inference:  44%|████▍     | 14/32 [00:06<00:07,  2.36it/s]Inference:  47%|████▋     | 15/32 [00:06<00:06,  2.57it/s]Inference:  50%|█████     | 16/32 [00:06<00:06,  2.53it/s]Inference:  53%|█████▎    | 17/32 [00:07<00:05,  2.68it/s]Inference:  56%|█████▋    | 18/32 [00:07<00:04,  2.81it/s]Inference:  59%|█████▉    | 19/32 [00:07<00:04,  2.66it/s]Inference:  62%|██████▎   | 20/32 [00:08<00:04,  2.74it/s]Inference:  66%|██████▌   | 21/32 [00:08<00:04,  2.64it/s]Inference:  69%|██████▉   | 22/32 [00:09<00:04,  2.37it/s]Inference:  72%|███████▏  | 23/32 [00:09<00:04,  2.21it/s]Inference:  75%|███████▌  | 24/32 [00:10<00:03,  2.23it/s]Inference:  78%|███████▊  | 25/32 [00:10<00:02,  2.42it/s]Inference:  81%|████████▏ | 26/32 [00:10<00:02,  2.37it/s]Inference:  84%|████████▍ | 27/32 [00:11<00:01,  2.52it/s]Inference:  88%|████████▊ | 28/32 [00:11<00:01,  2.61it/s]Inference:  91%|█████████ | 29/32 [00:12<00:01,  2.37it/s]Inference:  94%|█████████▍| 30/32 [00:12<00:00,  2.49it/s]Inference:  97%|█████████▋| 31/32 [00:12<00:00,  2.45it/s]Inference: 100%|██████████| 32/32 [00:13<00:00,  2.56it/s]Inference: 100%|██████████| 32/32 [00:13<00:00,  2.40it/s]
17087 MiB free out of 48676 MiB total
Saved layer 20 to ./models/meta-llama/Llama-2-7b-hf/2bpv/128/quantized/layer_20.pt
after cast to cpu
21089 MiB free out of 48676 MiB total
Done with layer 20 total_time elapsed: 12835 estimated time left: 6723
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:26,  1.17it/s]Inference:   6%|▋         | 2/32 [00:01<00:27,  1.11it/s]Inference:   9%|▉         | 3/32 [00:02<00:25,  1.15it/s]Inference:  12%|█▎        | 4/32 [00:03<00:25,  1.09it/s]Inference:  16%|█▌        | 5/32 [00:04<00:23,  1.17it/s]Inference:  19%|█▉        | 6/32 [00:05<00:21,  1.23it/s]Inference:  22%|██▏       | 7/32 [00:05<00:20,  1.23it/s]Inference:  25%|██▌       | 8/32 [00:06<00:20,  1.18it/s]Inference:  28%|██▊       | 9/32 [00:07<00:20,  1.12it/s]Inference:  31%|███▏      | 10/32 [00:08<00:19,  1.11it/s]Inference:  34%|███▍      | 11/32 [00:09<00:18,  1.14it/s]Inference:  38%|███▊      | 12/32 [00:10<00:17,  1.12it/s]Inference:  41%|████      | 13/32 [00:11<00:16,  1.14it/s]Inference:  44%|████▍     | 14/32 [00:12<00:15,  1.13it/s]Inference:  47%|████▋     | 15/32 [00:12<00:14,  1.18it/s]Inference:  50%|█████     | 16/32 [00:13<00:13,  1.15it/s]Inference:  53%|█████▎    | 17/32 [00:14<00:13,  1.13it/s]Inference:  56%|█████▋    | 18/32 [00:15<00:12,  1.13it/s]Inference:  59%|█████▉    | 19/32 [00:16<00:11,  1.12it/s]Inference:  62%|██████▎   | 20/32 [00:17<00:10,  1.12it/s]Inference:  66%|██████▌   | 21/32 [00:18<00:09,  1.11it/s]Inference:  69%|██████▉   | 22/32 [00:19<00:09,  1.10it/s]Inference:  72%|███████▏  | 23/32 [00:20<00:08,  1.11it/s]Inference:  75%|███████▌  | 24/32 [00:21<00:07,  1.14it/s]Inference:  78%|███████▊  | 25/32 [00:21<00:05,  1.19it/s]Inference:  81%|████████▏ | 26/32 [00:22<00:05,  1.12it/s]Inference:  84%|████████▍ | 27/32 [00:23<00:04,  1.11it/s]Inference:  88%|████████▊ | 28/32 [00:24<00:03,  1.10it/s]Inference:  91%|█████████ | 29/32 [00:25<00:02,  1.13it/s]Inference:  94%|█████████▍| 30/32 [00:26<00:01,  1.13it/s]Inference:  97%|█████████▋| 31/32 [00:27<00:00,  1.08it/s]Inference: 100%|██████████| 32/32 [00:28<00:00,  1.14it/s]Inference: 100%|██████████| 32/32 [00:28<00:00,  1.13it/s]
layer21: self_attn.q_proj
256
val_hessian None
iter 0, train loss 1447.6031494140625, val loss None, lr 0.001
iter 10, train loss 1550.420654296875, val loss None, lr 0.001
iter 20, train loss 1475.130859375, val loss None, lr 0.001
iter 30, train loss 1444.4886474609375, val loss None, lr 0.001
iter 40, train loss 1440.556640625, val loss None, lr 0.001
iter 50, train loss 1437.5264892578125, val loss None, lr 0.001
iter 60, train loss 1439.634033203125, val loss None, lr 0.001
iter 70, train loss 1433.9559326171875, val loss None, lr 0.001
iter 80, train loss 1438.75341796875, val loss None, lr 0.001
iter 90, train loss 1447.7001953125, val loss None, lr 0.001
best loss 1372.8822021484375
layer21: self_attn.k_proj
256
val_hessian None
iter 0, train loss 1534.768310546875, val loss None, lr 0.001
iter 10, train loss 1639.552978515625, val loss None, lr 0.001
iter 20, train loss 1543.426025390625, val loss None, lr 0.001
iter 30, train loss 1519.7825927734375, val loss None, lr 0.001
iter 40, train loss 1498.36328125, val loss None, lr 0.001
iter 50, train loss 1508.12890625, val loss None, lr 0.001
iter 60, train loss 1517.326904296875, val loss None, lr 0.001
iter 70, train loss 1521.87646484375, val loss None, lr 0.001
iter 80, train loss 1515.3037109375, val loss None, lr 0.001
iter 90, train loss 1515.107421875, val loss None, lr 0.001
best loss 1445.881591796875
layer21: self_attn.v_proj
256
val_hessian None
iter 0, train loss 805.4095458984375, val loss None, lr 0.001
iter 10, train loss 802.5357666015625, val loss None, lr 0.001
iter 20, train loss 801.9466552734375, val loss None, lr 0.001
iter 30, train loss 801.26513671875, val loss None, lr 0.001
iter 40, train loss 801.2974853515625, val loss None, lr 0.001
iter 50, train loss 801.4005126953125, val loss None, lr 0.001
iter 60, train loss 800.31103515625, val loss None, lr 0.001
iter 70, train loss 801.1536865234375, val loss None, lr 0.001
iter 80, train loss 800.4437255859375, val loss None, lr 0.001
iter 90, train loss 800.9200439453125, val loss None, lr 0.001
best loss 800.0704345703125
layer21: self_attn.o_proj
256
val_hessian None
iter 0, train loss 43.69205856323242, val loss None, lr 0.001
iter 10, train loss 40.441734313964844, val loss None, lr 0.001
iter 20, train loss 39.03632354736328, val loss None, lr 0.001
iter 30, train loss 38.73301315307617, val loss None, lr 0.001
iter 40, train loss 38.32118225097656, val loss None, lr 0.001
iter 50, train loss 37.9120979309082, val loss None, lr 0.001
iter 60, train loss 37.8332633972168, val loss None, lr 0.001
iter 70, train loss 37.74281311035156, val loss None, lr 0.001
iter 80, train loss 37.627445220947266, val loss None, lr 0.001
iter 90, train loss 37.5776252746582, val loss None, lr 0.001
best loss 37.550498962402344
layer21: mlp.gate_proj
256
val_hessian None
iter 0, train loss 1661.8697509765625, val loss None, lr 0.001
iter 10, train loss 1719.92236328125, val loss None, lr 0.001
iter 20, train loss 1724.03369140625, val loss None, lr 0.001
iter 30, train loss 1727.10986328125, val loss None, lr 0.001
iter 40, train loss 1733.4775390625, val loss None, lr 0.001
iter 50, train loss 1736.33740234375, val loss None, lr 0.001
iter 60, train loss 1741.6217041015625, val loss None, lr 0.001
iter 70, train loss 1748.89990234375, val loss None, lr 0.001
iter 80, train loss 1755.8472900390625, val loss None, lr 0.001
iter 90, train loss 1759.5152587890625, val loss None, lr 0.001
best loss 1649.748046875
layer21: mlp.up_proj
256
val_hessian None
iter 0, train loss 1396.474365234375, val loss None, lr 0.001
iter 10, train loss 1400.238037109375, val loss None, lr 0.001
iter 20, train loss 1412.0390625, val loss None, lr 0.001
iter 30, train loss 1425.486328125, val loss None, lr 0.001
iter 40, train loss 1434.414794921875, val loss None, lr 0.001
iter 50, train loss 1438.334716796875, val loss None, lr 0.001
iter 60, train loss 1441.7177734375, val loss None, lr 0.001
iter 70, train loss 1444.578125, val loss None, lr 0.001
iter 80, train loss 1447.908935546875, val loss None, lr 0.001
iter 90, train loss 1449.1611328125, val loss None, lr 0.001
best loss 1396.474365234375
layer21: mlp.down_proj
256
val_hessian None
iter 0, train loss 57.63091278076172, val loss None, lr 0.001
iter 10, train loss 57.69383239746094, val loss None, lr 0.001
iter 20, train loss 57.37519073486328, val loss None, lr 0.001
iter 30, train loss 57.19593811035156, val loss None, lr 0.001
iter 40, train loss 57.15756607055664, val loss None, lr 0.001
iter 50, train loss 57.184654235839844, val loss None, lr 0.001
iter 60, train loss 57.23493576049805, val loss None, lr 0.001
iter 70, train loss 57.32556915283203, val loss None, lr 0.001
iter 80, train loss 57.40607833862305, val loss None, lr 0.001
iter 90, train loss 57.50068664550781, val loss None, lr 0.001
best loss 57.151058197021484
21089 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:11,  2.61it/s]Inference:   6%|▋         | 2/32 [00:00<00:13,  2.28it/s]Inference:   9%|▉         | 3/32 [00:01<00:12,  2.41it/s]Inference:  12%|█▎        | 4/32 [00:01<00:11,  2.46it/s]Inference:  16%|█▌        | 5/32 [00:02<00:10,  2.52it/s]Inference:  19%|█▉        | 6/32 [00:02<00:09,  2.78it/s]Inference:  22%|██▏       | 7/32 [00:02<00:09,  2.67it/s]Inference:  25%|██▌       | 8/32 [00:03<00:09,  2.46it/s]Inference:  28%|██▊       | 9/32 [00:03<00:09,  2.49it/s]Inference:  31%|███▏      | 10/32 [00:04<00:09,  2.34it/s]Inference:  34%|███▍      | 11/32 [00:04<00:08,  2.40it/s]Inference:  38%|███▊      | 12/32 [00:04<00:08,  2.48it/s]Inference:  41%|████      | 13/32 [00:05<00:07,  2.49it/s]Inference:  44%|████▍     | 14/32 [00:05<00:07,  2.36it/s]Inference:  47%|████▋     | 15/32 [00:06<00:07,  2.29it/s]Inference:  50%|█████     | 16/32 [00:06<00:07,  2.24it/s]Inference:  53%|█████▎    | 17/32 [00:07<00:06,  2.34it/s]Inference:  56%|█████▋    | 18/32 [00:07<00:06,  2.27it/s]Inference:  59%|█████▉    | 19/32 [00:07<00:05,  2.36it/s]Inference:  62%|██████▎   | 20/32 [00:08<00:04,  2.43it/s]Inference:  66%|██████▌   | 21/32 [00:08<00:04,  2.33it/s]Inference:  69%|██████▉   | 22/32 [00:09<00:04,  2.42it/s]Inference:  72%|███████▏  | 23/32 [00:09<00:03,  2.36it/s]Inference:  75%|███████▌  | 24/32 [00:09<00:03,  2.37it/s]Inference:  78%|███████▊  | 25/32 [00:10<00:02,  2.46it/s]Inference:  81%|████████▏ | 26/32 [00:10<00:02,  2.51it/s]Inference:  84%|████████▍ | 27/32 [00:11<00:01,  2.54it/s]Inference:  88%|████████▊ | 28/32 [00:11<00:01,  2.43it/s]Inference:  91%|█████████ | 29/32 [00:11<00:01,  2.43it/s]Inference:  94%|█████████▍| 30/32 [00:12<00:00,  2.50it/s]Inference:  97%|█████████▋| 31/32 [00:12<00:00,  2.53it/s]Inference: 100%|██████████| 32/32 [00:13<00:00,  2.56it/s]Inference: 100%|██████████| 32/32 [00:13<00:00,  2.44it/s]
15863 MiB free out of 48676 MiB total
Saved layer 21 to ./models/meta-llama/Llama-2-7b-hf/2bpv/128/quantized/layer_21.pt
after cast to cpu
19865 MiB free out of 48676 MiB total
Done with layer 21 total_time elapsed: 13636 estimated time left: 6198
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:01<00:36,  1.19s/it]Inference:   6%|▋         | 2/32 [00:02<00:36,  1.23s/it]Inference:   9%|▉         | 3/32 [00:03<00:34,  1.18s/it]Inference:  12%|█▎        | 4/32 [00:04<00:30,  1.10s/it]Inference:  16%|█▌        | 5/32 [00:05<00:29,  1.08s/it]Inference:  19%|█▉        | 6/32 [00:06<00:26,  1.02s/it]Inference:  22%|██▏       | 7/32 [00:07<00:24,  1.02it/s]Inference:  25%|██▌       | 8/32 [00:08<00:24,  1.02s/it]Inference:  28%|██▊       | 9/32 [00:09<00:22,  1.02it/s]Inference:  31%|███▏      | 10/32 [00:10<00:21,  1.01it/s]Inference:  34%|███▍      | 11/32 [00:11<00:20,  1.01it/s]Inference:  38%|███▊      | 12/32 [00:12<00:20,  1.02s/it]Inference:  41%|████      | 13/32 [00:13<00:19,  1.01s/it]Inference:  44%|████▍     | 14/32 [00:14<00:17,  1.01it/s]Inference:  47%|████▋     | 15/32 [00:15<00:16,  1.04it/s]Inference:  50%|█████     | 16/32 [00:16<00:15,  1.02it/s]Inference:  53%|█████▎    | 17/32 [00:17<00:14,  1.05it/s]Inference:  56%|█████▋    | 18/32 [00:18<00:13,  1.04it/s]Inference:  59%|█████▉    | 19/32 [00:19<00:12,  1.05it/s]Inference:  62%|██████▎   | 20/32 [00:20<00:11,  1.04it/s]Inference:  66%|██████▌   | 21/32 [00:21<00:10,  1.02it/s]Inference:  69%|██████▉   | 22/32 [00:22<00:10,  1.03s/it]Inference:  72%|███████▏  | 23/32 [00:23<00:08,  1.01it/s]Inference:  75%|███████▌  | 24/32 [00:24<00:08,  1.02s/it]Inference:  78%|███████▊  | 25/32 [00:25<00:06,  1.01it/s]Inference:  81%|████████▏ | 26/32 [00:26<00:05,  1.00it/s]Inference:  84%|████████▍ | 27/32 [00:27<00:04,  1.01it/s]Inference:  88%|████████▊ | 28/32 [00:28<00:04,  1.04s/it]Inference:  91%|█████████ | 29/32 [00:29<00:03,  1.03s/it]Inference:  94%|█████████▍| 30/32 [00:30<00:01,  1.00it/s]Inference:  97%|█████████▋| 31/32 [00:31<00:00,  1.03it/s]Inference: 100%|██████████| 32/32 [00:32<00:00,  1.03it/s]Inference: 100%|██████████| 32/32 [00:32<00:00,  1.00s/it]
layer22: self_attn.q_proj
256
val_hessian None
iter 0, train loss 1513.729736328125, val loss None, lr 0.001
iter 10, train loss 1646.5135498046875, val loss None, lr 0.001
iter 20, train loss 1552.44189453125, val loss None, lr 0.001
iter 30, train loss 1507.7264404296875, val loss None, lr 0.001
iter 40, train loss 1513.9561767578125, val loss None, lr 0.001
iter 50, train loss 1516.4354248046875, val loss None, lr 0.001
iter 60, train loss 1502.381591796875, val loss None, lr 0.001
iter 70, train loss 1510.127685546875, val loss None, lr 0.001
iter 80, train loss 1515.2159423828125, val loss None, lr 0.001
iter 90, train loss 1514.0140380859375, val loss None, lr 0.001
best loss 1450.904052734375
layer22: self_attn.k_proj
256
val_hessian None
iter 0, train loss 1624.00341796875, val loss None, lr 0.001
iter 10, train loss 1713.6260986328125, val loss None, lr 0.001
iter 20, train loss 1643.026611328125, val loss None, lr 0.001
iter 30, train loss 1628.270751953125, val loss None, lr 0.001
iter 40, train loss 1615.220947265625, val loss None, lr 0.001
iter 50, train loss 1614.7596435546875, val loss None, lr 0.001
iter 60, train loss 1611.984130859375, val loss None, lr 0.001
iter 70, train loss 1619.536376953125, val loss None, lr 0.001
iter 80, train loss 1633.26513671875, val loss None, lr 0.001
iter 90, train loss 1624.5804443359375, val loss None, lr 0.001
best loss 1534.421875
layer22: self_attn.v_proj
256
val_hessian None
iter 0, train loss 824.9437255859375, val loss None, lr 0.001
iter 10, train loss 824.3525390625, val loss None, lr 0.001
iter 20, train loss 822.4642944335938, val loss None, lr 0.001
iter 30, train loss 821.0260009765625, val loss None, lr 0.001
iter 40, train loss 820.347900390625, val loss None, lr 0.001
iter 50, train loss 819.83251953125, val loss None, lr 0.001
iter 60, train loss 818.6565551757812, val loss None, lr 0.001
iter 70, train loss 819.1131591796875, val loss None, lr 0.001
iter 80, train loss 819.5213623046875, val loss None, lr 0.001
iter 90, train loss 819.0479736328125, val loss None, lr 0.001
best loss 818.2376098632812
layer22: self_attn.o_proj
256
val_hessian None
iter 0, train loss 229.91961669921875, val loss None, lr 0.001
iter 10, train loss 153.88461303710938, val loss None, lr 0.001
iter 20, train loss 118.55935668945312, val loss None, lr 0.001
iter 30, train loss 84.71540832519531, val loss None, lr 0.001
iter 40, train loss 67.32514190673828, val loss None, lr 0.001
iter 50, train loss 58.505760192871094, val loss None, lr 0.001
iter 60, train loss 56.79483413696289, val loss None, lr 0.001
iter 70, train loss 55.425201416015625, val loss None, lr 0.001
iter 80, train loss 54.57378387451172, val loss None, lr 0.001
iter 90, train loss 53.89591979980469, val loss None, lr 0.001
best loss 53.70305633544922
layer22: mlp.gate_proj
256
val_hessian None
iter 0, train loss 1685.92041015625, val loss None, lr 0.001
iter 10, train loss 1738.55322265625, val loss None, lr 0.001
iter 20, train loss 1746.0443115234375, val loss None, lr 0.001
iter 30, train loss 1746.6376953125, val loss None, lr 0.001
iter 40, train loss 1751.0601806640625, val loss None, lr 0.001
iter 50, train loss 1755.193359375, val loss None, lr 0.001
iter 60, train loss 1763.150390625, val loss None, lr 0.001
iter 70, train loss 1769.9569091796875, val loss None, lr 0.001
iter 80, train loss 1780.6376953125, val loss None, lr 0.001
iter 90, train loss 1787.7978515625, val loss None, lr 0.001
best loss 1673.3583984375
layer22: mlp.up_proj
256
val_hessian None
iter 0, train loss 1407.3883056640625, val loss None, lr 0.001
iter 10, train loss 1410.44580078125, val loss None, lr 0.001
iter 20, train loss 1422.13330078125, val loss None, lr 0.001
iter 30, train loss 1436.9796142578125, val loss None, lr 0.001
iter 40, train loss 1446.737548828125, val loss None, lr 0.001
iter 50, train loss 1454.01123046875, val loss None, lr 0.001
iter 60, train loss 1456.398193359375, val loss None, lr 0.001
iter 70, train loss 1459.797119140625, val loss None, lr 0.001
iter 80, train loss 1461.230712890625, val loss None, lr 0.001
iter 90, train loss 1464.4161376953125, val loss None, lr 0.001
best loss 1407.3883056640625
layer22: mlp.down_proj
256
val_hessian None
iter 0, train loss 60.20479965209961, val loss None, lr 0.001
iter 10, train loss 60.445579528808594, val loss None, lr 0.001
iter 20, train loss 60.266868591308594, val loss None, lr 0.001
iter 30, train loss 60.21845245361328, val loss None, lr 0.001
iter 40, train loss 60.24513244628906, val loss None, lr 0.001
iter 50, train loss 60.32271194458008, val loss None, lr 0.001
iter 60, train loss 60.379058837890625, val loss None, lr 0.001
iter 70, train loss 60.4974365234375, val loss None, lr 0.001
iter 80, train loss 60.561588287353516, val loss None, lr 0.001
iter 90, train loss 60.61333084106445, val loss None, lr 0.001
best loss 60.18590545654297
19865 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:12,  2.45it/s]Inference:   6%|▋         | 2/32 [00:00<00:11,  2.60it/s]Inference:   9%|▉         | 3/32 [00:01<00:12,  2.31it/s]Inference:  12%|█▎        | 4/32 [00:01<00:12,  2.31it/s]Inference:  16%|█▌        | 5/32 [00:02<00:12,  2.23it/s]Inference:  19%|█▉        | 6/32 [00:02<00:11,  2.26it/s]Inference:  22%|██▏       | 7/32 [00:03<00:12,  2.04it/s]Inference:  25%|██▌       | 8/32 [00:03<00:11,  2.05it/s]Inference:  28%|██▊       | 9/32 [00:04<00:11,  1.99it/s]Inference:  31%|███▏      | 10/32 [00:04<00:10,  2.09it/s]Inference:  34%|███▍      | 11/32 [00:05<00:09,  2.13it/s]Inference:  38%|███▊      | 12/32 [00:05<00:10,  1.91it/s]Inference:  41%|████      | 13/32 [00:06<00:11,  1.69it/s]Inference:  44%|████▍     | 14/32 [00:06<00:09,  1.87it/s]Inference:  47%|████▋     | 15/32 [00:07<00:09,  1.88it/s]Inference:  50%|█████     | 16/32 [00:07<00:08,  1.90it/s]Inference:  53%|█████▎    | 17/32 [00:08<00:07,  2.03it/s]Inference:  56%|█████▋    | 18/32 [00:08<00:06,  2.12it/s]Inference:  59%|█████▉    | 19/32 [00:09<00:06,  2.06it/s]Inference:  62%|██████▎   | 20/32 [00:09<00:05,  2.16it/s]Inference:  66%|██████▌   | 21/32 [00:10<00:05,  1.97it/s]Inference:  69%|██████▉   | 22/32 [00:10<00:04,  2.06it/s]Inference:  72%|███████▏  | 23/32 [00:11<00:04,  1.98it/s]Inference:  75%|███████▌  | 24/32 [00:11<00:04,  1.83it/s]Inference:  78%|███████▊  | 25/32 [00:12<00:03,  1.82it/s]Inference:  81%|████████▏ | 26/32 [00:13<00:03,  1.73it/s]Inference:  84%|████████▍ | 27/32 [00:13<00:02,  1.75it/s]Inference:  88%|████████▊ | 28/32 [00:14<00:02,  1.77it/s]Inference:  91%|█████████ | 29/32 [00:14<00:01,  1.91it/s]Inference:  94%|█████████▍| 30/32 [00:15<00:01,  1.91it/s]Inference:  97%|█████████▋| 31/32 [00:15<00:00,  2.02it/s]Inference: 100%|██████████| 32/32 [00:16<00:00,  1.86it/s]Inference: 100%|██████████| 32/32 [00:16<00:00,  1.97it/s]
14575 MiB free out of 48676 MiB total
Saved layer 22 to ./models/meta-llama/Llama-2-7b-hf/2bpv/128/quantized/layer_22.pt
after cast to cpu
18577 MiB free out of 48676 MiB total
Done with layer 22 total_time elapsed: 14475 estimated time left: 5664
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:01<00:31,  1.01s/it]Inference:   6%|▋         | 2/32 [00:02<00:31,  1.06s/it]Inference:   9%|▉         | 3/32 [00:03<00:30,  1.04s/it]Inference:  12%|█▎        | 4/32 [00:04<00:28,  1.02s/it]Inference:  16%|█▌        | 5/32 [00:05<00:26,  1.02it/s]Inference:  19%|█▉        | 6/32 [00:06<00:25,  1.02it/s]Inference:  22%|██▏       | 7/32 [00:06<00:24,  1.03it/s]Inference:  25%|██▌       | 8/32 [00:08<00:25,  1.07s/it]Inference:  28%|██▊       | 9/32 [00:09<00:24,  1.05s/it]Inference:  31%|███▏      | 10/32 [00:10<00:21,  1.03it/s]Inference:  34%|███▍      | 11/32 [00:11<00:21,  1.01s/it]Inference:  38%|███▊      | 12/32 [00:12<00:20,  1.01s/it]Inference:  41%|████      | 13/32 [00:13<00:19,  1.01s/it]Inference:  44%|████▍     | 14/32 [00:14<00:18,  1.04s/it]Inference:  47%|████▋     | 15/32 [00:15<00:17,  1.03s/it]Inference:  50%|█████     | 16/32 [00:16<00:16,  1.02s/it]Inference:  53%|█████▎    | 17/32 [00:17<00:15,  1.01s/it]Inference:  56%|█████▋    | 18/32 [00:18<00:14,  1.01s/it]Inference:  59%|█████▉    | 19/32 [00:19<00:13,  1.03s/it]Inference:  62%|██████▎   | 20/32 [00:20<00:12,  1.03s/it]Inference:  66%|██████▌   | 21/32 [00:21<00:11,  1.03s/it]Inference:  69%|██████▉   | 22/32 [00:22<00:09,  1.01it/s]Inference:  72%|███████▏  | 23/32 [00:23<00:08,  1.01it/s]Inference:  75%|███████▌  | 24/32 [00:24<00:07,  1.00it/s]Inference:  78%|███████▊  | 25/32 [00:25<00:07,  1.00s/it]Inference:  81%|████████▏ | 26/32 [00:26<00:06,  1.01s/it]Inference:  84%|████████▍ | 27/32 [00:27<00:05,  1.01s/it]Inference:  88%|████████▊ | 28/32 [00:28<00:03,  1.03it/s]Inference:  91%|█████████ | 29/32 [00:29<00:02,  1.05it/s]Inference:  94%|█████████▍| 30/32 [00:30<00:01,  1.02it/s]Inference:  97%|█████████▋| 31/32 [00:31<00:01,  1.00s/it]Inference: 100%|██████████| 32/32 [00:32<00:00,  1.01it/s]Inference: 100%|██████████| 32/32 [00:32<00:00,  1.01s/it]
layer23: self_attn.q_proj
256
val_hessian None
iter 0, train loss 1634.44677734375, val loss None, lr 0.001
iter 10, train loss 1704.3192138671875, val loss None, lr 0.001
iter 20, train loss 1619.811767578125, val loss None, lr 0.001
iter 30, train loss 1621.9940185546875, val loss None, lr 0.001
iter 40, train loss 1607.4857177734375, val loss None, lr 0.001
iter 50, train loss 1614.52880859375, val loss None, lr 0.001
iter 60, train loss 1615.700927734375, val loss None, lr 0.001
iter 70, train loss 1614.016357421875, val loss None, lr 0.001
iter 80, train loss 1615.42822265625, val loss None, lr 0.001
iter 90, train loss 1626.573974609375, val loss None, lr 0.001
best loss 1579.86279296875
layer23: self_attn.k_proj
256
val_hessian None
iter 0, train loss 1703.8802490234375, val loss None, lr 0.001
iter 10, train loss 1814.861083984375, val loss None, lr 0.001
iter 20, train loss 1710.3763427734375, val loss None, lr 0.001
iter 30, train loss 1685.3968505859375, val loss None, lr 0.001
iter 40, train loss 1682.9239501953125, val loss None, lr 0.001
iter 50, train loss 1685.6346435546875, val loss None, lr 0.001
iter 60, train loss 1697.1248779296875, val loss None, lr 0.001
iter 70, train loss 1691.67822265625, val loss None, lr 0.001
iter 80, train loss 1702.739013671875, val loss None, lr 0.001
iter 90, train loss 1710.3626708984375, val loss None, lr 0.001
best loss 1638.18115234375
layer23: self_attn.v_proj
256
val_hessian None
iter 0, train loss 1003.3319091796875, val loss None, lr 0.001
iter 10, train loss 1000.7421875, val loss None, lr 0.001
iter 20, train loss 997.9469604492188, val loss None, lr 0.001
iter 30, train loss 996.9298095703125, val loss None, lr 0.001
iter 40, train loss 996.9495849609375, val loss None, lr 0.001
iter 50, train loss 996.534912109375, val loss None, lr 0.001
iter 60, train loss 997.162109375, val loss None, lr 0.001
iter 70, train loss 998.8583984375, val loss None, lr 0.001
iter 80, train loss 998.269775390625, val loss None, lr 0.001
iter 90, train loss 998.18115234375, val loss None, lr 0.001
best loss 996.1813354492188
layer23: self_attn.o_proj
256
val_hessian None
iter 0, train loss 43.461456298828125, val loss None, lr 0.001
iter 10, train loss 43.09486389160156, val loss None, lr 0.001
iter 20, train loss 42.704254150390625, val loss None, lr 0.001
iter 30, train loss 42.52718734741211, val loss None, lr 0.001
iter 40, train loss 42.385093688964844, val loss None, lr 0.001
iter 50, train loss 42.34743881225586, val loss None, lr 0.001
iter 60, train loss 42.297828674316406, val loss None, lr 0.001
iter 70, train loss 42.08848571777344, val loss None, lr 0.001
iter 80, train loss 42.2296142578125, val loss None, lr 0.001
iter 90, train loss 42.19398498535156, val loss None, lr 0.001
best loss 42.08848571777344
layer23: mlp.gate_proj
256
val_hessian None
iter 0, train loss 1847.6357421875, val loss None, lr 0.001
iter 10, train loss 1891.860595703125, val loss None, lr 0.001
iter 20, train loss 1896.649169921875, val loss None, lr 0.001
iter 30, train loss 1909.4364013671875, val loss None, lr 0.001
iter 40, train loss 1914.02099609375, val loss None, lr 0.001
iter 50, train loss 1918.991455078125, val loss None, lr 0.001
iter 60, train loss 1925.9951171875, val loss None, lr 0.001
iter 70, train loss 1934.96484375, val loss None, lr 0.001
iter 80, train loss 1942.973388671875, val loss None, lr 0.001
iter 90, train loss 1951.594482421875, val loss None, lr 0.001
best loss 1836.31787109375
layer23: mlp.up_proj
256
val_hessian None
iter 0, train loss 1547.4521484375, val loss None, lr 0.001
iter 10, train loss 1551.6473388671875, val loss None, lr 0.001
iter 20, train loss 1559.7410888671875, val loss None, lr 0.001
iter 30, train loss 1568.8961181640625, val loss None, lr 0.001
iter 40, train loss 1575.6207275390625, val loss None, lr 0.001
iter 50, train loss 1580.83154296875, val loss None, lr 0.001
iter 60, train loss 1582.7420654296875, val loss None, lr 0.001
iter 70, train loss 1582.264404296875, val loss None, lr 0.001
iter 80, train loss 1582.849365234375, val loss None, lr 0.001
iter 90, train loss 1585.34716796875, val loss None, lr 0.001
best loss 1547.4521484375
layer23: mlp.down_proj
256
val_hessian None
iter 0, train loss 68.21192932128906, val loss None, lr 0.001
iter 10, train loss 68.2662582397461, val loss None, lr 0.001
iter 20, train loss 68.04974365234375, val loss None, lr 0.001
iter 30, train loss 67.8544692993164, val loss None, lr 0.001
iter 40, train loss 67.72447204589844, val loss None, lr 0.001
iter 50, train loss 67.68707275390625, val loss None, lr 0.001
iter 60, train loss 67.5770263671875, val loss None, lr 0.001
iter 70, train loss 67.52664947509766, val loss None, lr 0.001
iter 80, train loss 67.48963928222656, val loss None, lr 0.001
iter 90, train loss 67.4635009765625, val loss None, lr 0.001
best loss 67.44683837890625
18577 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:11,  2.63it/s]Inference:   6%|▋         | 2/32 [00:00<00:13,  2.29it/s]Inference:   9%|▉         | 3/32 [00:01<00:13,  2.18it/s]Inference:  12%|█▎        | 4/32 [00:01<00:11,  2.52it/s]Inference:  16%|█▌        | 5/32 [00:02<00:11,  2.36it/s]Inference:  19%|█▉        | 6/32 [00:02<00:10,  2.45it/s]Inference:  22%|██▏       | 7/32 [00:02<00:09,  2.52it/s]Inference:  25%|██▌       | 8/32 [00:03<00:09,  2.54it/s]Inference:  28%|██▊       | 9/32 [00:03<00:08,  2.57it/s]Inference:  31%|███▏      | 10/32 [00:04<00:09,  2.40it/s]Inference:  34%|███▍      | 11/32 [00:04<00:09,  2.26it/s]Inference:  38%|███▊      | 12/32 [00:05<00:08,  2.25it/s]Inference:  41%|████      | 13/32 [00:05<00:08,  2.34it/s]Inference:  44%|████▍     | 14/32 [00:05<00:07,  2.40it/s]Inference:  47%|████▋     | 15/32 [00:06<00:06,  2.46it/s]Inference:  50%|█████     | 16/32 [00:06<00:06,  2.53it/s]Inference:  53%|█████▎    | 17/32 [00:06<00:05,  2.55it/s]Inference:  56%|█████▋    | 18/32 [00:07<00:05,  2.56it/s]Inference:  59%|█████▉    | 19/32 [00:07<00:04,  2.62it/s]Inference:  62%|██████▎   | 20/32 [00:08<00:04,  2.62it/s]Inference:  66%|██████▌   | 21/32 [00:08<00:04,  2.45it/s]Inference:  69%|██████▉   | 22/32 [00:08<00:03,  2.69it/s]Inference:  72%|███████▏  | 23/32 [00:09<00:03,  2.47it/s]Inference:  75%|███████▌  | 24/32 [00:09<00:03,  2.33it/s]Inference:  78%|███████▊  | 25/32 [00:10<00:02,  2.55it/s]Inference:  81%|████████▏ | 26/32 [00:10<00:02,  2.47it/s]Inference:  84%|████████▍ | 27/32 [00:11<00:02,  2.39it/s]Inference:  88%|████████▊ | 28/32 [00:11<00:01,  2.44it/s]Inference:  91%|█████████ | 29/32 [00:11<00:01,  2.31it/s]Inference:  94%|█████████▍| 30/32 [00:12<00:00,  2.39it/s]Inference:  97%|█████████▋| 31/32 [00:12<00:00,  2.43it/s]Inference: 100%|██████████| 32/32 [00:13<00:00,  2.48it/s]Inference: 100%|██████████| 32/32 [00:13<00:00,  2.45it/s]
13287 MiB free out of 48676 MiB total
Saved layer 23 to ./models/meta-llama/Llama-2-7b-hf/2bpv/128/quantized/layer_23.pt
after cast to cpu
17289 MiB free out of 48676 MiB total
Done with layer 23 total_time elapsed: 15323 estimated time left: 5108
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:28,  1.10it/s]Inference:   6%|▋         | 2/32 [00:01<00:26,  1.11it/s]Inference:   9%|▉         | 3/32 [00:02<00:27,  1.06it/s]Inference:  12%|█▎        | 4/32 [00:03<00:26,  1.04it/s]Inference:  16%|█▌        | 5/32 [00:04<00:26,  1.02it/s]Inference:  19%|█▉        | 6/32 [00:05<00:25,  1.01it/s]Inference:  22%|██▏       | 7/32 [00:06<00:24,  1.04it/s]Inference:  25%|██▌       | 8/32 [00:07<00:22,  1.06it/s]Inference:  28%|██▊       | 9/32 [00:08<00:22,  1.00it/s]Inference:  31%|███▏      | 10/32 [00:09<00:21,  1.02it/s]Inference:  34%|███▍      | 11/32 [00:10<00:20,  1.02it/s]Inference:  38%|███▊      | 12/32 [00:11<00:18,  1.07it/s]Inference:  41%|████      | 13/32 [00:12<00:18,  1.05it/s]Inference:  44%|████▍     | 14/32 [00:13<00:17,  1.01it/s]Inference:  47%|████▋     | 15/32 [00:14<00:17,  1.00s/it]Inference:  50%|█████     | 16/32 [00:15<00:15,  1.02it/s]Inference:  53%|█████▎    | 17/32 [00:16<00:14,  1.05it/s]Inference:  56%|█████▋    | 18/32 [00:17<00:13,  1.06it/s]Inference:  59%|█████▉    | 19/32 [00:18<00:12,  1.04it/s]Inference:  62%|██████▎   | 20/32 [00:19<00:11,  1.03it/s]Inference:  66%|██████▌   | 21/32 [00:20<00:10,  1.04it/s]Inference:  69%|██████▉   | 22/32 [00:21<00:10,  1.02s/it]Inference:  72%|███████▏  | 23/32 [00:22<00:08,  1.01it/s]Inference:  75%|███████▌  | 24/32 [00:23<00:07,  1.01it/s]Inference:  78%|███████▊  | 25/32 [00:24<00:06,  1.00it/s]Inference:  81%|████████▏ | 26/32 [00:25<00:06,  1.03s/it]Inference:  84%|████████▍ | 27/32 [00:26<00:05,  1.02s/it]Inference:  88%|████████▊ | 28/32 [00:27<00:03,  1.02it/s]Inference:  91%|█████████ | 29/32 [00:28<00:02,  1.01it/s]Inference:  94%|█████████▍| 30/32 [00:29<00:01,  1.01it/s]Inference:  97%|█████████▋| 31/32 [00:30<00:00,  1.06it/s]Inference: 100%|██████████| 32/32 [00:31<00:00,  1.01it/s]Inference: 100%|██████████| 32/32 [00:31<00:00,  1.02it/s]
layer24: self_attn.q_proj
256
val_hessian None
iter 0, train loss 1487.201171875, val loss None, lr 0.001
iter 10, train loss 1550.5457763671875, val loss None, lr 0.001
iter 20, train loss 1503.590087890625, val loss None, lr 0.001
iter 30, train loss 1496.36328125, val loss None, lr 0.001
iter 40, train loss 1492.14892578125, val loss None, lr 0.001
iter 50, train loss 1486.347900390625, val loss None, lr 0.001
iter 60, train loss 1492.2353515625, val loss None, lr 0.001
iter 70, train loss 1494.71044921875, val loss None, lr 0.001
iter 80, train loss 1496.4462890625, val loss None, lr 0.001
iter 90, train loss 1497.3544921875, val loss None, lr 0.001
best loss 1433.091796875
layer24: self_attn.k_proj
256
val_hessian None
iter 0, train loss 1572.0230712890625, val loss None, lr 0.001
iter 10, train loss 1648.9974365234375, val loss None, lr 0.001
iter 20, train loss 1585.490234375, val loss None, lr 0.001
iter 30, train loss 1570.8955078125, val loss None, lr 0.001
iter 40, train loss 1553.5291748046875, val loss None, lr 0.001
iter 50, train loss 1559.694091796875, val loss None, lr 0.001
iter 60, train loss 1563.6788330078125, val loss None, lr 0.001
iter 70, train loss 1572.58349609375, val loss None, lr 0.001
iter 80, train loss 1576.64111328125, val loss None, lr 0.001
iter 90, train loss 1576.4344482421875, val loss None, lr 0.001
best loss 1501.6273193359375
layer24: self_attn.v_proj
256
val_hessian None
iter 0, train loss 948.820068359375, val loss None, lr 0.001
iter 10, train loss 947.208984375, val loss None, lr 0.001
iter 20, train loss 944.2244262695312, val loss None, lr 0.001
iter 30, train loss 943.5194702148438, val loss None, lr 0.001
iter 40, train loss 942.6920166015625, val loss None, lr 0.001
iter 50, train loss 942.6419677734375, val loss None, lr 0.001
iter 60, train loss 942.2813110351562, val loss None, lr 0.001
iter 70, train loss 943.3004150390625, val loss None, lr 0.001
iter 80, train loss 942.36572265625, val loss None, lr 0.001
iter 90, train loss 943.0466918945312, val loss None, lr 0.001
best loss 941.6083984375
layer24: self_attn.o_proj
256
val_hessian None
iter 0, train loss 116.55520629882812, val loss None, lr 0.001
iter 10, train loss 90.58357238769531, val loss None, lr 0.001
iter 20, train loss 75.02518463134766, val loss None, lr 0.001
iter 30, train loss 67.37071990966797, val loss None, lr 0.001
iter 40, train loss 64.807373046875, val loss None, lr 0.001
iter 50, train loss 64.04684448242188, val loss None, lr 0.001
iter 60, train loss 63.643272399902344, val loss None, lr 0.001
iter 70, train loss 63.18577575683594, val loss None, lr 0.001
iter 80, train loss 62.95430374145508, val loss None, lr 0.001
iter 90, train loss 62.8907356262207, val loss None, lr 0.001
best loss 62.55495071411133
layer24: mlp.gate_proj
256
val_hessian None
iter 0, train loss 1899.3880615234375, val loss None, lr 0.001
iter 10, train loss 1943.2091064453125, val loss None, lr 0.001
iter 20, train loss 1946.2694091796875, val loss None, lr 0.001
iter 30, train loss 1953.171630859375, val loss None, lr 0.001
iter 40, train loss 1959.713134765625, val loss None, lr 0.001
iter 50, train loss 1966.6368408203125, val loss None, lr 0.001
iter 60, train loss 1971.9027099609375, val loss None, lr 0.001
iter 70, train loss 1981.302001953125, val loss None, lr 0.001
iter 80, train loss 1990.8802490234375, val loss None, lr 0.001
iter 90, train loss 2002.4141845703125, val loss None, lr 0.001
best loss 1888.721435546875
layer24: mlp.up_proj
256
val_hessian None
iter 0, train loss 1599.4814453125, val loss None, lr 0.001
iter 10, train loss 1602.8443603515625, val loss None, lr 0.001
iter 20, train loss 1612.83544921875, val loss None, lr 0.001
iter 30, train loss 1625.101318359375, val loss None, lr 0.001
iter 40, train loss 1632.9608154296875, val loss None, lr 0.001
iter 50, train loss 1635.7279052734375, val loss None, lr 0.001
iter 60, train loss 1638.9014892578125, val loss None, lr 0.001
iter 70, train loss 1642.3961181640625, val loss None, lr 0.001
iter 80, train loss 1645.057861328125, val loss None, lr 0.001
iter 90, train loss 1646.723876953125, val loss None, lr 0.001
best loss 1599.4814453125
layer24: mlp.down_proj
256
val_hessian None
iter 0, train loss 69.85136413574219, val loss None, lr 0.001
iter 10, train loss 69.978759765625, val loss None, lr 0.001
iter 20, train loss 69.64701080322266, val loss None, lr 0.001
iter 30, train loss 69.49531555175781, val loss None, lr 0.001
iter 40, train loss 69.43649291992188, val loss None, lr 0.001
iter 50, train loss 69.41187286376953, val loss None, lr 0.001
iter 60, train loss 69.42399597167969, val loss None, lr 0.001
iter 70, train loss 69.46397399902344, val loss None, lr 0.001
iter 80, train loss 69.50276184082031, val loss None, lr 0.001
iter 90, train loss 69.54460144042969, val loss None, lr 0.001
best loss 69.38987731933594
17289 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:16,  1.91it/s]Inference:   6%|▋         | 2/32 [00:01<00:16,  1.84it/s]Inference:   9%|▉         | 3/32 [00:01<00:14,  1.99it/s]Inference:  12%|█▎        | 4/32 [00:02<00:13,  2.03it/s]Inference:  16%|█▌        | 5/32 [00:02<00:12,  2.24it/s]Inference:  19%|█▉        | 6/32 [00:02<00:10,  2.37it/s]Inference:  22%|██▏       | 7/32 [00:03<00:11,  2.13it/s]Inference:  25%|██▌       | 8/32 [00:03<00:10,  2.29it/s]Inference:  28%|██▊       | 9/32 [00:04<00:10,  2.27it/s]Inference:  31%|███▏      | 10/32 [00:04<00:09,  2.38it/s]Inference:  34%|███▍      | 11/32 [00:04<00:08,  2.34it/s]Inference:  38%|███▊      | 12/32 [00:05<00:08,  2.43it/s]Inference:  41%|████      | 13/32 [00:05<00:07,  2.51it/s]Inference:  44%|████▍     | 14/32 [00:06<00:07,  2.51it/s]Inference:  47%|████▋     | 15/32 [00:06<00:06,  2.57it/s]Inference:  50%|█████     | 16/32 [00:06<00:06,  2.44it/s]Inference:  53%|█████▎    | 17/32 [00:07<00:06,  2.37it/s]Inference:  56%|█████▋    | 18/32 [00:07<00:06,  2.31it/s]Inference:  59%|█████▉    | 19/32 [00:08<00:06,  2.14it/s]Inference:  62%|██████▎   | 20/32 [00:08<00:05,  2.16it/s]Inference:  66%|██████▌   | 21/32 [00:09<00:04,  2.30it/s]Inference:  69%|██████▉   | 22/32 [00:09<00:04,  2.22it/s]Inference:  72%|███████▏  | 23/32 [00:10<00:03,  2.39it/s]Inference:  75%|███████▌  | 24/32 [00:10<00:03,  2.38it/s]Inference:  78%|███████▊  | 25/32 [00:10<00:03,  2.33it/s]Inference:  81%|████████▏ | 26/32 [00:11<00:02,  2.48it/s]Inference:  84%|████████▍ | 27/32 [00:11<00:02,  2.43it/s]Inference:  88%|████████▊ | 28/32 [00:12<00:01,  2.40it/s]Inference:  91%|█████████ | 29/32 [00:12<00:01,  2.35it/s]Inference:  94%|█████████▍| 30/32 [00:12<00:00,  2.32it/s]Inference:  97%|█████████▋| 31/32 [00:13<00:00,  2.45it/s]Inference: 100%|██████████| 32/32 [00:13<00:00,  2.40it/s]Inference: 100%|██████████| 32/32 [00:13<00:00,  2.32it/s]
12063 MiB free out of 48676 MiB total
Saved layer 24 to ./models/meta-llama/Llama-2-7b-hf/2bpv/128/quantized/layer_24.pt
after cast to cpu
16065 MiB free out of 48676 MiB total
Done with layer 24 total_time elapsed: 16080 estimated time left: 4502
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:26,  1.19it/s]Inference:   6%|▋         | 2/32 [00:01<00:24,  1.20it/s]Inference:   9%|▉         | 3/32 [00:02<00:23,  1.21it/s]Inference:  12%|█▎        | 4/32 [00:03<00:23,  1.21it/s]Inference:  16%|█▌        | 5/32 [00:04<00:23,  1.17it/s]Inference:  19%|█▉        | 6/32 [00:05<00:23,  1.10it/s]Inference:  22%|██▏       | 7/32 [00:06<00:22,  1.10it/s]Inference:  25%|██▌       | 8/32 [00:07<00:21,  1.09it/s]Inference:  28%|██▊       | 9/32 [00:07<00:20,  1.13it/s]Inference:  31%|███▏      | 10/32 [00:08<00:19,  1.13it/s]Inference:  34%|███▍      | 11/32 [00:09<00:17,  1.17it/s]Inference:  38%|███▊      | 12/32 [00:10<00:16,  1.18it/s]Inference:  41%|████      | 13/32 [00:11<00:15,  1.19it/s]Inference:  44%|████▍     | 14/32 [00:12<00:15,  1.16it/s]Inference:  47%|████▋     | 15/32 [00:12<00:14,  1.21it/s]Inference:  50%|█████     | 16/32 [00:13<00:13,  1.21it/s]Inference:  53%|█████▎    | 17/32 [00:14<00:12,  1.21it/s]Inference:  56%|█████▋    | 18/32 [00:15<00:11,  1.22it/s]Inference:  59%|█████▉    | 19/32 [00:16<00:10,  1.25it/s]Inference:  62%|██████▎   | 20/32 [00:16<00:09,  1.24it/s]Inference:  66%|██████▌   | 21/32 [00:17<00:08,  1.23it/s]Inference:  69%|██████▉   | 22/32 [00:18<00:07,  1.26it/s]Inference:  72%|███████▏  | 23/32 [00:19<00:07,  1.28it/s]Inference:  75%|███████▌  | 24/32 [00:19<00:06,  1.30it/s]Inference:  78%|███████▊  | 25/32 [00:20<00:05,  1.20it/s]Inference:  81%|████████▏ | 26/32 [00:21<00:05,  1.20it/s]Inference:  84%|████████▍ | 27/32 [00:22<00:04,  1.24it/s]Inference:  88%|████████▊ | 28/32 [00:23<00:03,  1.24it/s]Inference:  91%|█████████ | 29/32 [00:24<00:02,  1.22it/s]Inference:  94%|█████████▍| 30/32 [00:24<00:01,  1.25it/s]Inference:  97%|█████████▋| 31/32 [00:25<00:00,  1.24it/s]Inference: 100%|██████████| 32/32 [00:26<00:00,  1.27it/s]Inference: 100%|██████████| 32/32 [00:26<00:00,  1.21it/s]
layer25: self_attn.q_proj
256
val_hessian None
iter 0, train loss 1695.8262939453125, val loss None, lr 0.001
iter 10, train loss 1770.828857421875, val loss None, lr 0.001
iter 20, train loss 1700.093994140625, val loss None, lr 0.001
iter 30, train loss 1693.56591796875, val loss None, lr 0.001
iter 40, train loss 1688.71240234375, val loss None, lr 0.001
iter 50, train loss 1696.41015625, val loss None, lr 0.001
iter 60, train loss 1689.36474609375, val loss None, lr 0.001
iter 70, train loss 1693.757568359375, val loss None, lr 0.001
iter 80, train loss 1707.9730224609375, val loss None, lr 0.001
iter 90, train loss 1701.5521240234375, val loss None, lr 0.001
best loss 1654.497314453125
layer25: self_attn.k_proj
256
val_hessian None
iter 0, train loss 1772.517822265625, val loss None, lr 0.001
iter 10, train loss 1881.40185546875, val loss None, lr 0.001
iter 20, train loss 1761.139404296875, val loss None, lr 0.001
iter 30, train loss 1756.0029296875, val loss None, lr 0.001
iter 40, train loss 1756.87890625, val loss None, lr 0.001
iter 50, train loss 1761.027099609375, val loss None, lr 0.001
iter 60, train loss 1770.6162109375, val loss None, lr 0.001
iter 70, train loss 1774.53759765625, val loss None, lr 0.001
iter 80, train loss 1781.4368896484375, val loss None, lr 0.001
iter 90, train loss 1781.77294921875, val loss None, lr 0.001
best loss 1709.7900390625
layer25: self_attn.v_proj
256
val_hessian None
iter 0, train loss 1162.2333984375, val loss None, lr 0.001
iter 10, train loss 1160.749755859375, val loss None, lr 0.001
iter 20, train loss 1158.040283203125, val loss None, lr 0.001
iter 30, train loss 1156.7738037109375, val loss None, lr 0.001
iter 40, train loss 1156.3486328125, val loss None, lr 0.001
iter 50, train loss 1156.3067626953125, val loss None, lr 0.001
iter 60, train loss 1155.310791015625, val loss None, lr 0.001
iter 70, train loss 1156.1937255859375, val loss None, lr 0.001
iter 80, train loss 1157.067626953125, val loss None, lr 0.001
iter 90, train loss 1158.4212646484375, val loss None, lr 0.001
best loss 1154.784423828125
layer25: self_attn.o_proj
256
val_hessian None
iter 0, train loss 41.28954315185547, val loss None, lr 0.001
iter 10, train loss 41.3145637512207, val loss None, lr 0.001
iter 20, train loss 41.477115631103516, val loss None, lr 0.001
iter 30, train loss 41.679054260253906, val loss None, lr 0.001
iter 40, train loss 41.53986740112305, val loss None, lr 0.001
iter 50, train loss 41.78334045410156, val loss None, lr 0.001
iter 60, train loss 41.683982849121094, val loss None, lr 0.001
iter 70, train loss 41.78871536254883, val loss None, lr 0.001
iter 80, train loss 41.864105224609375, val loss None, lr 0.001
iter 90, train loss 42.06433868408203, val loss None, lr 0.001
best loss 41.015342712402344
layer25: mlp.gate_proj
256
val_hessian None
iter 0, train loss 2063.3955078125, val loss None, lr 0.001
iter 10, train loss 2119.705078125, val loss None, lr 0.001
iter 20, train loss 2119.775634765625, val loss None, lr 0.001
iter 30, train loss 2122.83642578125, val loss None, lr 0.001
iter 40, train loss 2135.022705078125, val loss None, lr 0.001
iter 50, train loss 2144.181884765625, val loss None, lr 0.001
iter 60, train loss 2149.79638671875, val loss None, lr 0.001
iter 70, train loss 2151.322998046875, val loss None, lr 0.001
iter 80, train loss 2164.8466796875, val loss None, lr 0.001
iter 90, train loss 2176.623779296875, val loss None, lr 0.001
best loss 2050.322021484375
layer25: mlp.up_proj
256
val_hessian None
iter 0, train loss 1717.4400634765625, val loss None, lr 0.001
iter 10, train loss 1719.9417724609375, val loss None, lr 0.001
iter 20, train loss 1731.206298828125, val loss None, lr 0.001
iter 30, train loss 1746.59716796875, val loss None, lr 0.001
iter 40, train loss 1756.024169921875, val loss None, lr 0.001
iter 50, train loss 1763.1832275390625, val loss None, lr 0.001
iter 60, train loss 1766.6834716796875, val loss None, lr 0.001
iter 70, train loss 1769.1483154296875, val loss None, lr 0.001
iter 80, train loss 1770.6729736328125, val loss None, lr 0.001
iter 90, train loss 1771.98291015625, val loss None, lr 0.001
best loss 1717.4400634765625
layer25: mlp.down_proj
256
val_hessian None
iter 0, train loss 74.04803466796875, val loss None, lr 0.001
iter 10, train loss 74.10822296142578, val loss None, lr 0.001
iter 20, train loss 73.83937072753906, val loss None, lr 0.001
iter 30, train loss 73.69285583496094, val loss None, lr 0.001
iter 40, train loss 73.6974868774414, val loss None, lr 0.001
iter 50, train loss 73.66173553466797, val loss None, lr 0.001
iter 60, train loss 73.6800537109375, val loss None, lr 0.001
iter 70, train loss 73.66889190673828, val loss None, lr 0.001
iter 80, train loss 73.71475982666016, val loss None, lr 0.001
iter 90, train loss 73.73088073730469, val loss None, lr 0.001
best loss 73.65449523925781
16065 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:15,  2.06it/s]Inference:   6%|▋         | 2/32 [00:00<00:12,  2.35it/s]Inference:   9%|▉         | 3/32 [00:01<00:12,  2.33it/s]Inference:  12%|█▎        | 4/32 [00:01<00:11,  2.34it/s]Inference:  16%|█▌        | 5/32 [00:02<00:10,  2.67it/s]Inference:  19%|█▉        | 6/32 [00:02<00:09,  2.67it/s]Inference:  22%|██▏       | 7/32 [00:02<00:09,  2.62it/s]Inference:  25%|██▌       | 8/32 [00:03<00:09,  2.65it/s]Inference:  28%|██▊       | 9/32 [00:03<00:08,  2.64it/s]Inference:  31%|███▏      | 10/32 [00:03<00:08,  2.64it/s]Inference:  34%|███▍      | 11/32 [00:04<00:08,  2.61it/s]Inference:  38%|███▊      | 12/32 [00:04<00:08,  2.43it/s]Inference:  41%|████      | 13/32 [00:05<00:08,  2.33it/s]Inference:  44%|████▍     | 14/32 [00:05<00:08,  2.12it/s]Inference:  47%|████▋     | 15/32 [00:06<00:07,  2.37it/s]Inference:  50%|█████     | 16/32 [00:06<00:06,  2.46it/s]Inference:  53%|█████▎    | 17/32 [00:06<00:06,  2.50it/s]Inference:  56%|█████▋    | 18/32 [00:07<00:05,  2.53it/s]Inference:  59%|█████▉    | 19/32 [00:07<00:05,  2.55it/s]Inference:  62%|██████▎   | 20/32 [00:08<00:04,  2.57it/s]Inference:  66%|██████▌   | 21/32 [00:08<00:03,  2.79it/s]Inference:  69%|██████▉   | 22/32 [00:08<00:03,  2.58it/s]Inference:  72%|███████▏  | 23/32 [00:09<00:03,  2.60it/s]Inference:  75%|███████▌  | 24/32 [00:09<00:03,  2.59it/s]Inference:  78%|███████▊  | 25/32 [00:10<00:02,  2.42it/s]Inference:  81%|████████▏ | 26/32 [00:10<00:02,  2.29it/s]Inference:  84%|████████▍ | 27/32 [00:10<00:02,  2.38it/s]Inference:  88%|████████▊ | 28/32 [00:11<00:01,  2.42it/s]Inference:  91%|█████████ | 29/32 [00:11<00:01,  2.35it/s]Inference:  94%|█████████▍| 30/32 [00:12<00:00,  2.41it/s]Inference:  97%|█████████▋| 31/32 [00:12<00:00,  2.46it/s]Inference: 100%|██████████| 32/32 [00:12<00:00,  2.34it/s]Inference: 100%|██████████| 32/32 [00:12<00:00,  2.46it/s]
10775 MiB free out of 48676 MiB total
Saved layer 25 to ./models/meta-llama/Llama-2-7b-hf/2bpv/128/quantized/layer_25.pt
after cast to cpu
14777 MiB free out of 48676 MiB total
Done with layer 25 total_time elapsed: 16893 estimated time left: 3898
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:01<00:36,  1.17s/it]Inference:   6%|▋         | 2/32 [00:02<00:33,  1.12s/it]Inference:   9%|▉         | 3/32 [00:03<00:30,  1.06s/it]Inference:  12%|█▎        | 4/32 [00:04<00:29,  1.04s/it]Inference:  16%|█▌        | 5/32 [00:05<00:27,  1.02s/it]Inference:  19%|█▉        | 6/32 [00:06<00:26,  1.03s/it]Inference:  22%|██▏       | 7/32 [00:07<00:25,  1.03s/it]Inference:  25%|██▌       | 8/32 [00:08<00:25,  1.05s/it]Inference:  28%|██▊       | 9/32 [00:09<00:23,  1.03s/it]Inference:  31%|███▏      | 10/32 [00:10<00:22,  1.02s/it]Inference:  34%|███▍      | 11/32 [00:11<00:21,  1.04s/it]Inference:  38%|███▊      | 12/32 [00:12<00:20,  1.03s/it]Inference:  41%|████      | 13/32 [00:13<00:19,  1.02s/it]Inference:  44%|████▍     | 14/32 [00:14<00:17,  1.01it/s]Inference:  47%|████▋     | 15/32 [00:15<00:16,  1.01it/s]Inference:  50%|█████     | 16/32 [00:16<00:16,  1.02s/it]Inference:  53%|█████▎    | 17/32 [00:17<00:15,  1.01s/it]Inference:  56%|█████▋    | 18/32 [00:18<00:13,  1.02it/s]Inference:  59%|█████▉    | 19/32 [00:19<00:12,  1.01it/s]Inference:  62%|██████▎   | 20/32 [00:20<00:12,  1.02s/it]Inference:  66%|██████▌   | 21/32 [00:21<00:11,  1.04s/it]Inference:  69%|██████▉   | 22/32 [00:22<00:10,  1.06s/it]Inference:  72%|███████▏  | 23/32 [00:23<00:09,  1.09s/it]Inference:  75%|███████▌  | 24/32 [00:24<00:08,  1.07s/it]Inference:  78%|███████▊  | 25/32 [00:25<00:07,  1.05s/it]Inference:  81%|████████▏ | 26/32 [00:26<00:06,  1.04s/it]Inference:  84%|████████▍ | 27/32 [00:27<00:05,  1.03s/it]Inference:  88%|████████▊ | 28/32 [00:28<00:04,  1.04s/it]Inference:  91%|█████████ | 29/32 [00:29<00:03,  1.03s/it]Inference:  94%|█████████▍| 30/32 [00:30<00:01,  1.01it/s]Inference:  97%|█████████▋| 31/32 [00:31<00:00,  1.01it/s]Inference: 100%|██████████| 32/32 [00:32<00:00,  1.03it/s]Inference: 100%|██████████| 32/32 [00:32<00:00,  1.02s/it]
layer26: self_attn.q_proj
256
val_hessian None
iter 0, train loss 1629.708251953125, val loss None, lr 0.001
iter 10, train loss 1752.0108642578125, val loss None, lr 0.001
iter 20, train loss 1649.9052734375, val loss None, lr 0.001
iter 30, train loss 1634.0103759765625, val loss None, lr 0.001
iter 40, train loss 1615.294189453125, val loss None, lr 0.001
iter 50, train loss 1621.4400634765625, val loss None, lr 0.001
iter 60, train loss 1618.9600830078125, val loss None, lr 0.001
iter 70, train loss 1626.0032958984375, val loss None, lr 0.001
iter 80, train loss 1622.445068359375, val loss None, lr 0.001
iter 90, train loss 1621.9603271484375, val loss None, lr 0.001
best loss 1558.747802734375
layer26: self_attn.k_proj
256
val_hessian None
iter 0, train loss 1732.814208984375, val loss None, lr 0.001
iter 10, train loss 1838.99462890625, val loss None, lr 0.001
iter 20, train loss 1767.569091796875, val loss None, lr 0.001
iter 30, train loss 1734.607666015625, val loss None, lr 0.001
iter 40, train loss 1711.84423828125, val loss None, lr 0.001
iter 50, train loss 1713.0936279296875, val loss None, lr 0.001
iter 60, train loss 1700.9512939453125, val loss None, lr 0.001
iter 70, train loss 1717.2127685546875, val loss None, lr 0.001
iter 80, train loss 1713.067138671875, val loss None, lr 0.001
iter 90, train loss 1719.6251220703125, val loss None, lr 0.001
best loss 1626.0833740234375
layer26: self_attn.v_proj
256
val_hessian None
iter 0, train loss 1163.5653076171875, val loss None, lr 0.001
iter 10, train loss 1157.350830078125, val loss None, lr 0.001
iter 20, train loss 1153.845703125, val loss None, lr 0.001
iter 30, train loss 1151.86181640625, val loss None, lr 0.001
iter 40, train loss 1151.9200439453125, val loss None, lr 0.001
iter 50, train loss 1151.0789794921875, val loss None, lr 0.001
iter 60, train loss 1148.72509765625, val loss None, lr 0.001
iter 70, train loss 1149.927490234375, val loss None, lr 0.001
iter 80, train loss 1149.4755859375, val loss None, lr 0.001
iter 90, train loss 1149.7724609375, val loss None, lr 0.001
best loss 1148.3275146484375
layer26: self_attn.o_proj
256
val_hessian None
iter 0, train loss 207.06834411621094, val loss None, lr 0.001
iter 10, train loss 144.13955688476562, val loss None, lr 0.001
iter 20, train loss 109.22708129882812, val loss None, lr 0.001
iter 30, train loss 90.95842742919922, val loss None, lr 0.001
iter 40, train loss 83.30546569824219, val loss None, lr 0.001
iter 50, train loss 81.10033416748047, val loss None, lr 0.001
iter 60, train loss 80.38963317871094, val loss None, lr 0.001
iter 70, train loss 80.01726531982422, val loss None, lr 0.001
iter 80, train loss 79.54090881347656, val loss None, lr 0.001
iter 90, train loss 79.1561279296875, val loss None, lr 0.001
best loss 79.013427734375
layer26: mlp.gate_proj
256
val_hessian None
iter 0, train loss 2147.6416015625, val loss None, lr 0.001
iter 10, train loss 2231.5244140625, val loss None, lr 0.001
iter 20, train loss 2240.826171875, val loss None, lr 0.001
iter 30, train loss 2243.208984375, val loss None, lr 0.001
iter 40, train loss 2244.23046875, val loss None, lr 0.001
iter 50, train loss 2251.107421875, val loss None, lr 0.001
iter 60, train loss 2257.087158203125, val loss None, lr 0.001
iter 70, train loss 2265.820556640625, val loss None, lr 0.001
iter 80, train loss 2264.87158203125, val loss None, lr 0.001
iter 90, train loss 2277.327392578125, val loss None, lr 0.001
best loss 2122.760986328125
layer26: mlp.up_proj
256
val_hessian None
iter 0, train loss 1808.87255859375, val loss None, lr 0.001
iter 10, train loss 1809.970947265625, val loss None, lr 0.001
iter 20, train loss 1827.55322265625, val loss None, lr 0.001
iter 30, train loss 1847.1939697265625, val loss None, lr 0.001
iter 40, train loss 1855.340576171875, val loss None, lr 0.001
iter 50, train loss 1859.44384765625, val loss None, lr 0.001
iter 60, train loss 1858.2021484375, val loss None, lr 0.001
iter 70, train loss 1862.072998046875, val loss None, lr 0.001
iter 80, train loss 1866.9671630859375, val loss None, lr 0.001
iter 90, train loss 1867.769287109375, val loss None, lr 0.001
best loss 1808.87255859375
layer26: mlp.down_proj
256
val_hessian None
iter 0, train loss 81.41265869140625, val loss None, lr 0.001
iter 10, train loss 81.52119445800781, val loss None, lr 0.001
iter 20, train loss 81.322265625, val loss None, lr 0.001
iter 30, train loss 81.28465270996094, val loss None, lr 0.001
iter 40, train loss 81.38528442382812, val loss None, lr 0.001
iter 50, train loss 81.3907699584961, val loss None, lr 0.001
iter 60, train loss 81.45793151855469, val loss None, lr 0.001
iter 70, train loss 81.5064468383789, val loss None, lr 0.001
iter 80, train loss 81.61312866210938, val loss None, lr 0.001
iter 90, train loss 81.80612182617188, val loss None, lr 0.001
best loss 81.1755142211914
14777 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:13,  2.37it/s]Inference:   6%|▋         | 2/32 [00:00<00:14,  2.11it/s]Inference:   9%|▉         | 3/32 [00:01<00:13,  2.11it/s]Inference:  12%|█▎        | 4/32 [00:01<00:13,  2.09it/s]Inference:  16%|█▌        | 5/32 [00:02<00:13,  2.02it/s]Inference:  19%|█▉        | 6/32 [00:02<00:13,  1.91it/s]Inference:  22%|██▏       | 7/32 [00:03<00:14,  1.76it/s]Inference:  25%|██▌       | 8/32 [00:04<00:13,  1.78it/s]Inference:  28%|██▊       | 9/32 [00:04<00:11,  1.93it/s]Inference:  31%|███▏      | 10/32 [00:05<00:11,  1.95it/s]Inference:  34%|███▍      | 11/32 [00:05<00:10,  2.04it/s]Inference:  38%|███▊      | 12/32 [00:05<00:09,  2.11it/s]Inference:  41%|████      | 13/32 [00:06<00:09,  2.05it/s]Inference:  44%|████▍     | 14/32 [00:07<00:09,  1.89it/s]Inference:  47%|████▋     | 15/32 [00:07<00:09,  1.88it/s]Inference:  50%|█████     | 16/32 [00:08<00:08,  1.88it/s]Inference:  53%|█████▎    | 17/32 [00:08<00:07,  1.88it/s]Inference:  56%|█████▋    | 18/32 [00:09<00:07,  1.89it/s]Inference:  59%|█████▉    | 19/32 [00:09<00:06,  2.02it/s]Inference:  62%|██████▎   | 20/32 [00:10<00:05,  2.04it/s]Inference:  66%|██████▌   | 21/32 [00:10<00:05,  1.99it/s]Inference:  69%|██████▉   | 22/32 [00:11<00:05,  1.96it/s]Inference:  72%|███████▏  | 23/32 [00:11<00:04,  2.07it/s]Inference:  75%|███████▌  | 24/32 [00:12<00:03,  2.00it/s]Inference:  78%|███████▊  | 25/32 [00:12<00:03,  2.23it/s]Inference:  81%|████████▏ | 26/32 [00:13<00:02,  2.13it/s]Inference:  84%|████████▍ | 27/32 [00:13<00:02,  1.94it/s]Inference:  88%|████████▊ | 28/32 [00:14<00:01,  2.05it/s]Inference:  91%|█████████ | 29/32 [00:14<00:01,  2.00it/s]Inference:  94%|█████████▍| 30/32 [00:15<00:01,  1.96it/s]Inference:  97%|█████████▋| 31/32 [00:15<00:00,  1.93it/s]Inference: 100%|██████████| 32/32 [00:16<00:00,  2.02it/s]Inference: 100%|██████████| 32/32 [00:16<00:00,  1.98it/s]
9487 MiB free out of 48676 MiB total
Saved layer 26 to ./models/meta-llama/Llama-2-7b-hf/2bpv/128/quantized/layer_26.pt
after cast to cpu
13489 MiB free out of 48676 MiB total
Done with layer 26 total_time elapsed: 17708 estimated time left: 3279
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:01<00:34,  1.10s/it]Inference:   6%|▋         | 2/32 [00:02<00:29,  1.01it/s]Inference:   9%|▉         | 3/32 [00:03<00:28,  1.01it/s]Inference:  12%|█▎        | 4/32 [00:04<00:27,  1.01it/s]Inference:  16%|█▌        | 5/32 [00:05<00:26,  1.00it/s]Inference:  19%|█▉        | 6/32 [00:06<00:27,  1.06s/it]Inference:  22%|██▏       | 7/32 [00:07<00:26,  1.06s/it]Inference:  25%|██▌       | 8/32 [00:08<00:24,  1.04s/it]Inference:  28%|██▊       | 9/32 [00:09<00:24,  1.05s/it]Inference:  31%|███▏      | 10/32 [00:10<00:23,  1.06s/it]Inference:  34%|███▍      | 11/32 [00:11<00:21,  1.04s/it]Inference:  38%|███▊      | 12/32 [00:12<00:21,  1.06s/it]Inference:  41%|████      | 13/32 [00:13<00:19,  1.05s/it]Inference:  44%|████▍     | 14/32 [00:14<00:18,  1.05s/it]Inference:  47%|████▋     | 15/32 [00:15<00:18,  1.08s/it]Inference:  50%|█████     | 16/32 [00:16<00:17,  1.07s/it]Inference:  53%|█████▎    | 17/32 [00:17<00:16,  1.10s/it]Inference:  56%|█████▋    | 18/32 [00:18<00:15,  1.07s/it]Inference:  59%|█████▉    | 19/32 [00:20<00:14,  1.08s/it]Inference:  62%|██████▎   | 20/32 [00:21<00:12,  1.06s/it]Inference:  66%|██████▌   | 21/32 [00:22<00:11,  1.04s/it]Inference:  69%|██████▉   | 22/32 [00:23<00:10,  1.03s/it]Inference:  72%|███████▏  | 23/32 [00:24<00:09,  1.03s/it]Inference:  75%|███████▌  | 24/32 [00:25<00:08,  1.01s/it]Inference:  78%|███████▊  | 25/32 [00:26<00:07,  1.04s/it]Inference:  81%|████████▏ | 26/32 [00:27<00:06,  1.05s/it]Inference:  84%|████████▍ | 27/32 [00:28<00:05,  1.04s/it]Inference:  88%|████████▊ | 28/32 [00:29<00:04,  1.08s/it]Inference:  91%|█████████ | 29/32 [00:30<00:03,  1.06s/it]Inference:  94%|█████████▍| 30/32 [00:31<00:02,  1.03s/it]Inference:  97%|█████████▋| 31/32 [00:32<00:01,  1.05s/it]Inference: 100%|██████████| 32/32 [00:33<00:00,  1.04s/it]Inference: 100%|██████████| 32/32 [00:33<00:00,  1.05s/it]
layer27: self_attn.q_proj
256
val_hessian None
iter 0, train loss 1808.27978515625, val loss None, lr 0.001
iter 10, train loss 1925.868896484375, val loss None, lr 0.001
iter 20, train loss 1821.9793701171875, val loss None, lr 0.001
iter 30, train loss 1789.192138671875, val loss None, lr 0.001
iter 40, train loss 1775.04248046875, val loss None, lr 0.001
iter 50, train loss 1774.4443359375, val loss None, lr 0.001
iter 60, train loss 1765.79150390625, val loss None, lr 0.001
iter 70, train loss 1764.0863037109375, val loss None, lr 0.001
iter 80, train loss 1764.7569580078125, val loss None, lr 0.001
iter 90, train loss 1760.4093017578125, val loss None, lr 0.001
best loss 1699.2572021484375
layer27: self_attn.k_proj
256
val_hessian None
iter 0, train loss 1933.093505859375, val loss None, lr 0.001
iter 10, train loss 2037.8536376953125, val loss None, lr 0.001
iter 20, train loss 1931.546875, val loss None, lr 0.001
iter 30, train loss 1887.071044921875, val loss None, lr 0.001
iter 40, train loss 1856.3260498046875, val loss None, lr 0.001
iter 50, train loss 1835.2506103515625, val loss None, lr 0.001
iter 60, train loss 1840.6422119140625, val loss None, lr 0.001
iter 70, train loss 1852.92529296875, val loss None, lr 0.001
iter 80, train loss 1846.122314453125, val loss None, lr 0.001
iter 90, train loss 1860.483642578125, val loss None, lr 0.001
best loss 1757.0626220703125
layer27: self_attn.v_proj
256
val_hessian None
iter 0, train loss 1191.8868408203125, val loss None, lr 0.001
iter 10, train loss 1189.54443359375, val loss None, lr 0.001
iter 20, train loss 1186.7056884765625, val loss None, lr 0.001
iter 30, train loss 1185.66796875, val loss None, lr 0.001
iter 40, train loss 1184.66796875, val loss None, lr 0.001
iter 50, train loss 1185.7841796875, val loss None, lr 0.001
iter 60, train loss 1187.976806640625, val loss None, lr 0.001
iter 70, train loss 1185.8662109375, val loss None, lr 0.001
iter 80, train loss 1186.444091796875, val loss None, lr 0.001
iter 90, train loss 1187.1824951171875, val loss None, lr 0.001
best loss 1183.6588134765625
layer27: self_attn.o_proj
256
val_hessian None
iter 0, train loss 74.83287048339844, val loss None, lr 0.001
iter 10, train loss 67.60685729980469, val loss None, lr 0.001
iter 20, train loss 62.803932189941406, val loss None, lr 0.001
iter 30, train loss 59.87702941894531, val loss None, lr 0.001
iter 40, train loss 58.79890823364258, val loss None, lr 0.001
iter 50, train loss 58.106163024902344, val loss None, lr 0.001
iter 60, train loss 57.6499137878418, val loss None, lr 0.001
iter 70, train loss 57.228416442871094, val loss None, lr 0.001
iter 80, train loss 57.020877838134766, val loss None, lr 0.001
iter 90, train loss 56.77519989013672, val loss None, lr 0.001
best loss 56.652435302734375
layer27: mlp.gate_proj
256
val_hessian None
iter 0, train loss 2313.95361328125, val loss None, lr 0.001
iter 10, train loss 2420.31640625, val loss None, lr 0.001
iter 20, train loss 2409.37060546875, val loss None, lr 0.001
iter 30, train loss 2419.598388671875, val loss None, lr 0.001
iter 40, train loss 2420.1962890625, val loss None, lr 0.001
iter 50, train loss 2422.722412109375, val loss None, lr 0.001
iter 60, train loss 2443.301025390625, val loss None, lr 0.001
iter 70, train loss 2442.4443359375, val loss None, lr 0.001
iter 80, train loss 2451.759765625, val loss None, lr 0.001
iter 90, train loss 2459.60546875, val loss None, lr 0.001
best loss 2283.76318359375
layer27: mlp.up_proj
256
val_hessian None
iter 0, train loss 1947.5989990234375, val loss None, lr 0.001
iter 10, train loss 1953.5560302734375, val loss None, lr 0.001
iter 20, train loss 1969.095947265625, val loss None, lr 0.001
iter 30, train loss 1984.786376953125, val loss None, lr 0.001
iter 40, train loss 1992.998779296875, val loss None, lr 0.001
iter 50, train loss 1998.6429443359375, val loss None, lr 0.001
iter 60, train loss 1999.1114501953125, val loss None, lr 0.001
iter 70, train loss 2001.164306640625, val loss None, lr 0.001
iter 80, train loss 2003.2542724609375, val loss None, lr 0.001
iter 90, train loss 2004.6983642578125, val loss None, lr 0.001
best loss 1940.272705078125
layer27: mlp.down_proj
256
val_hessian None
iter 0, train loss 93.4062728881836, val loss None, lr 0.001
iter 10, train loss 93.62339782714844, val loss None, lr 0.001
iter 20, train loss 93.50425720214844, val loss None, lr 0.001
iter 30, train loss 93.44853973388672, val loss None, lr 0.001
iter 40, train loss 93.54431915283203, val loss None, lr 0.001
iter 50, train loss 93.52458190917969, val loss None, lr 0.001
iter 60, train loss 93.51885986328125, val loss None, lr 0.001
iter 70, train loss 93.71216583251953, val loss None, lr 0.001
iter 80, train loss 93.77648162841797, val loss None, lr 0.001
iter 90, train loss 93.68391418457031, val loss None, lr 0.001
best loss 93.4062728881836
13489 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:18,  1.69it/s]Inference:   6%|▋         | 2/32 [00:01<00:15,  1.91it/s]Inference:   9%|▉         | 3/32 [00:01<00:14,  2.00it/s]Inference:  12%|█▎        | 4/32 [00:02<00:13,  2.05it/s]Inference:  16%|█▌        | 5/32 [00:02<00:13,  2.05it/s]Inference:  19%|█▉        | 6/32 [00:02<00:12,  2.05it/s]Inference:  22%|██▏       | 7/32 [00:03<00:10,  2.35it/s]Inference:  25%|██▌       | 8/32 [00:03<00:10,  2.33it/s]Inference:  28%|██▊       | 9/32 [00:04<00:09,  2.32it/s]Inference:  31%|███▏      | 10/32 [00:04<00:09,  2.26it/s]Inference:  34%|███▍      | 11/32 [00:04<00:08,  2.50it/s]Inference:  38%|███▊      | 12/32 [00:05<00:07,  2.52it/s]Inference:  41%|████      | 13/32 [00:05<00:07,  2.54it/s]Inference:  44%|████▍     | 14/32 [00:06<00:07,  2.50it/s]Inference:  47%|████▋     | 15/32 [00:06<00:06,  2.52it/s]Inference:  50%|█████     | 16/32 [00:06<00:06,  2.35it/s]Inference:  53%|█████▎    | 17/32 [00:07<00:06,  2.42it/s]Inference:  56%|█████▋    | 18/32 [00:07<00:06,  2.31it/s]Inference:  59%|█████▉    | 19/32 [00:08<00:05,  2.39it/s]Inference:  62%|██████▎   | 20/32 [00:08<00:04,  2.44it/s]Inference:  66%|██████▌   | 21/32 [00:09<00:04,  2.35it/s]Inference:  69%|██████▉   | 22/32 [00:09<00:03,  2.60it/s]Inference:  72%|███████▏  | 23/32 [00:09<00:03,  2.62it/s]Inference:  75%|███████▌  | 24/32 [00:10<00:03,  2.60it/s]Inference:  78%|███████▊  | 25/32 [00:10<00:02,  2.60it/s]Inference:  81%|████████▏ | 26/32 [00:11<00:02,  2.41it/s]Inference:  84%|████████▍ | 27/32 [00:11<00:02,  2.32it/s]Inference:  88%|████████▊ | 28/32 [00:11<00:01,  2.23it/s]Inference:  91%|█████████ | 29/32 [00:12<00:01,  2.32it/s]Inference:  94%|█████████▍| 30/32 [00:12<00:00,  2.40it/s]Inference:  97%|█████████▋| 31/32 [00:13<00:00,  2.43it/s]Inference: 100%|██████████| 32/32 [00:13<00:00,  2.30it/s]Inference: 100%|██████████| 32/32 [00:13<00:00,  2.35it/s]
8263 MiB free out of 48676 MiB total
Saved layer 27 to ./models/meta-llama/Llama-2-7b-hf/2bpv/128/quantized/layer_27.pt
after cast to cpu
12265 MiB free out of 48676 MiB total
Done with layer 27 total_time elapsed: 18553 estimated time left: 2650
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:01<00:34,  1.10s/it]Inference:   6%|▋         | 2/32 [00:02<00:33,  1.10s/it]Inference:   9%|▉         | 3/32 [00:03<00:30,  1.06s/it]Inference:  12%|█▎        | 4/32 [00:04<00:29,  1.07s/it]Inference:  16%|█▌        | 5/32 [00:05<00:28,  1.04s/it]Inference:  19%|█▉        | 6/32 [00:06<00:27,  1.06s/it]Inference:  22%|██▏       | 7/32 [00:07<00:25,  1.02s/it]Inference:  25%|██▌       | 8/32 [00:08<00:24,  1.01s/it]Inference:  28%|██▊       | 9/32 [00:09<00:23,  1.01s/it]Inference:  31%|███▏      | 10/32 [00:10<00:22,  1.03s/it]Inference:  34%|███▍      | 11/32 [00:11<00:22,  1.06s/it]Inference:  38%|███▊      | 12/32 [00:12<00:20,  1.04s/it]Inference:  41%|████      | 13/32 [00:13<00:19,  1.03s/it]Inference:  44%|████▍     | 14/32 [00:14<00:19,  1.08s/it]Inference:  47%|████▋     | 15/32 [00:15<00:17,  1.05s/it]Inference:  50%|█████     | 16/32 [00:16<00:16,  1.04s/it]Inference:  53%|█████▎    | 17/32 [00:17<00:15,  1.03s/it]Inference:  56%|█████▋    | 18/32 [00:18<00:14,  1.07s/it]Inference:  59%|█████▉    | 19/32 [00:19<00:13,  1.04s/it]Inference:  62%|██████▎   | 20/32 [00:20<00:12,  1.00s/it]Inference:  66%|██████▌   | 21/32 [00:21<00:11,  1.03s/it]Inference:  69%|██████▉   | 22/32 [00:22<00:10,  1.02s/it]Inference:  72%|███████▏  | 23/32 [00:23<00:09,  1.02s/it]Inference:  75%|███████▌  | 24/32 [00:24<00:08,  1.01s/it]Inference:  78%|███████▊  | 25/32 [00:25<00:06,  1.02it/s]Inference:  81%|████████▏ | 26/32 [00:26<00:05,  1.01it/s]Inference:  84%|████████▍ | 27/32 [00:27<00:04,  1.01it/s]Inference:  88%|████████▊ | 28/32 [00:28<00:04,  1.02s/it]Inference:  91%|█████████ | 29/32 [00:30<00:03,  1.06s/it]Inference:  94%|█████████▍| 30/32 [00:31<00:02,  1.07s/it]Inference:  97%|█████████▋| 31/32 [00:32<00:01,  1.05s/it]Inference: 100%|██████████| 32/32 [00:33<00:00,  1.03s/it]Inference: 100%|██████████| 32/32 [00:33<00:00,  1.03s/it]
layer28: self_attn.q_proj
256
val_hessian None
iter 0, train loss 1799.4736328125, val loss None, lr 0.001
iter 10, train loss 1942.6519775390625, val loss None, lr 0.001
iter 20, train loss 1815.955322265625, val loss None, lr 0.001
iter 30, train loss 1785.37841796875, val loss None, lr 0.001
iter 40, train loss 1767.085205078125, val loss None, lr 0.001
iter 50, train loss 1757.2037353515625, val loss None, lr 0.001
iter 60, train loss 1767.73486328125, val loss None, lr 0.001
iter 70, train loss 1759.2957763671875, val loss None, lr 0.001
iter 80, train loss 1746.731201171875, val loss None, lr 0.001
iter 90, train loss 1760.4722900390625, val loss None, lr 0.001
best loss 1676.6129150390625
layer28: self_attn.k_proj
256
val_hessian None
iter 0, train loss 1931.76123046875, val loss None, lr 0.001
iter 10, train loss 2078.249755859375, val loss None, lr 0.001
iter 20, train loss 1960.2432861328125, val loss None, lr 0.001
iter 30, train loss 1901.808837890625, val loss None, lr 0.001
iter 40, train loss 1884.826171875, val loss None, lr 0.001
iter 50, train loss 1858.8037109375, val loss None, lr 0.001
iter 60, train loss 1850.45751953125, val loss None, lr 0.001
iter 70, train loss 1849.280517578125, val loss None, lr 0.001
iter 80, train loss 1844.2099609375, val loss None, lr 0.001
iter 90, train loss 1857.861572265625, val loss None, lr 0.001
best loss 1743.5333251953125
layer28: self_attn.v_proj
256
val_hessian None
iter 0, train loss 1316.723388671875, val loss None, lr 0.001
iter 10, train loss 1315.56640625, val loss None, lr 0.001
iter 20, train loss 1313.68798828125, val loss None, lr 0.001
iter 30, train loss 1316.1971435546875, val loss None, lr 0.001
iter 40, train loss 1317.896484375, val loss None, lr 0.001
iter 50, train loss 1314.49169921875, val loss None, lr 0.001
iter 60, train loss 1316.8603515625, val loss None, lr 0.001
iter 70, train loss 1318.28759765625, val loss None, lr 0.001
iter 80, train loss 1317.0634765625, val loss None, lr 0.001
iter 90, train loss 1318.5546875, val loss None, lr 0.001
best loss 1312.522705078125
layer28: self_attn.o_proj
256
val_hessian None
iter 0, train loss 95.91522216796875, val loss None, lr 0.001
iter 10, train loss 91.59027862548828, val loss None, lr 0.001
iter 20, train loss 90.17173767089844, val loss None, lr 0.001
iter 30, train loss 90.27327728271484, val loss None, lr 0.001
iter 40, train loss 90.62779998779297, val loss None, lr 0.001
iter 50, train loss 89.91263580322266, val loss None, lr 0.001
iter 60, train loss 89.61383056640625, val loss None, lr 0.001
iter 70, train loss 89.91242218017578, val loss None, lr 0.001
iter 80, train loss 89.44021606445312, val loss None, lr 0.001
iter 90, train loss 89.37957000732422, val loss None, lr 0.001
best loss 89.01618957519531
layer28: mlp.gate_proj
256
val_hessian None
iter 0, train loss 2434.689453125, val loss None, lr 0.001
iter 10, train loss 2569.54833984375, val loss None, lr 0.001
iter 20, train loss 2531.81103515625, val loss None, lr 0.001
iter 30, train loss 2547.727783203125, val loss None, lr 0.001
iter 40, train loss 2562.078125, val loss None, lr 0.001
iter 50, train loss 2559.83349609375, val loss None, lr 0.001
iter 60, train loss 2562.505859375, val loss None, lr 0.001
iter 70, train loss 2568.998046875, val loss None, lr 0.001
iter 80, train loss 2571.8427734375, val loss None, lr 0.001
iter 90, train loss 2570.378662109375, val loss None, lr 0.001
best loss 2395.8896484375
layer28: mlp.up_proj
256
val_hessian None
iter 0, train loss 2150.783447265625, val loss None, lr 0.001
iter 10, train loss 2198.46630859375, val loss None, lr 0.001
iter 20, train loss 2166.45166015625, val loss None, lr 0.001
iter 30, train loss 2176.1220703125, val loss None, lr 0.001
iter 40, train loss 2183.8134765625, val loss None, lr 0.001
iter 50, train loss 2190.590087890625, val loss None, lr 0.001
iter 60, train loss 2192.075927734375, val loss None, lr 0.001
iter 70, train loss 2194.07568359375, val loss None, lr 0.001
iter 80, train loss 2197.1298828125, val loss None, lr 0.001
iter 90, train loss 2199.938232421875, val loss None, lr 0.001
best loss 2132.474609375
layer28: mlp.down_proj
256
val_hessian None
iter 0, train loss 120.61528015136719, val loss None, lr 0.001
iter 10, train loss 120.21574401855469, val loss None, lr 0.001
iter 20, train loss 119.88506317138672, val loss None, lr 0.001
iter 30, train loss 119.33753967285156, val loss None, lr 0.001
iter 40, train loss 119.22418975830078, val loss None, lr 0.001
iter 50, train loss 118.94719696044922, val loss None, lr 0.001
iter 60, train loss 118.75564575195312, val loss None, lr 0.001
iter 70, train loss 118.74786376953125, val loss None, lr 0.001
iter 80, train loss 118.64667510986328, val loss None, lr 0.001
iter 90, train loss 118.46612548828125, val loss None, lr 0.001
best loss 118.34786987304688
12265 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:10,  3.08it/s]Inference:   6%|▋         | 2/32 [00:00<00:09,  3.20it/s]Inference:   9%|▉         | 3/32 [00:00<00:09,  3.10it/s]Inference:  12%|█▎        | 4/32 [00:01<00:09,  3.07it/s]Inference:  16%|█▌        | 5/32 [00:01<00:08,  3.06it/s]Inference:  19%|█▉        | 6/32 [00:01<00:08,  3.07it/s]Inference:  22%|██▏       | 7/32 [00:02<00:07,  3.13it/s]Inference:  25%|██▌       | 8/32 [00:02<00:07,  3.20it/s]Inference:  28%|██▊       | 9/32 [00:02<00:07,  3.17it/s]Inference:  31%|███▏      | 10/32 [00:03<00:06,  3.27it/s]Inference:  34%|███▍      | 11/32 [00:03<00:06,  3.08it/s]Inference:  38%|███▊      | 12/32 [00:03<00:06,  3.23it/s]Inference:  41%|████      | 13/32 [00:04<00:05,  3.19it/s]Inference:  44%|████▍     | 14/32 [00:04<00:05,  3.16it/s]Inference:  47%|████▋     | 15/32 [00:04<00:05,  3.16it/s]Inference:  50%|█████     | 16/32 [00:05<00:05,  3.14it/s]Inference:  53%|█████▎    | 17/32 [00:05<00:04,  3.30it/s]Inference:  56%|█████▋    | 18/32 [00:05<00:04,  3.24it/s]Inference:  59%|█████▉    | 19/32 [00:05<00:04,  3.19it/s]Inference:  62%|██████▎   | 20/32 [00:06<00:03,  3.17it/s]Inference:  66%|██████▌   | 21/32 [00:06<00:03,  3.20it/s]Inference:  69%|██████▉   | 22/32 [00:06<00:03,  3.29it/s]Inference:  72%|███████▏  | 23/32 [00:07<00:02,  3.25it/s]Inference:  75%|███████▌  | 24/32 [00:07<00:02,  3.23it/s]Inference:  78%|███████▊  | 25/32 [00:07<00:02,  3.23it/s]Inference:  81%|████████▏ | 26/32 [00:08<00:01,  3.23it/s]Inference:  84%|████████▍ | 27/32 [00:08<00:01,  3.22it/s]Inference:  88%|████████▊ | 28/32 [00:08<00:01,  3.33it/s]Inference:  91%|█████████ | 29/32 [00:09<00:00,  3.29it/s]Inference:  94%|█████████▍| 30/32 [00:09<00:00,  3.30it/s]Inference:  97%|█████████▋| 31/32 [00:09<00:00,  3.42it/s]Inference: 100%|██████████| 32/32 [00:09<00:00,  3.35it/s]Inference: 100%|██████████| 32/32 [00:09<00:00,  3.22it/s]
6975 MiB free out of 48676 MiB total
Saved layer 28 to ./models/meta-llama/Llama-2-7b-hf/2bpv/128/quantized/layer_28.pt
after cast to cpu
10977 MiB free out of 48676 MiB total
Done with layer 28 total_time elapsed: 19184 estimated time left: 1985
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:17,  1.81it/s]Inference:   6%|▋         | 2/32 [00:01<00:16,  1.82it/s]Inference:   9%|▉         | 3/32 [00:01<00:15,  1.85it/s]Inference:  12%|█▎        | 4/32 [00:02<00:14,  1.88it/s]Inference:  16%|█▌        | 5/32 [00:02<00:14,  1.91it/s]Inference:  19%|█▉        | 6/32 [00:03<00:13,  1.89it/s]Inference:  22%|██▏       | 7/32 [00:03<00:13,  1.89it/s]Inference:  25%|██▌       | 8/32 [00:04<00:12,  1.90it/s]Inference:  28%|██▊       | 9/32 [00:04<00:11,  1.93it/s]Inference:  31%|███▏      | 10/32 [00:05<00:11,  1.92it/s]Inference:  34%|███▍      | 11/32 [00:05<00:11,  1.90it/s]Inference:  38%|███▊      | 12/32 [00:06<00:10,  1.88it/s]Inference:  41%|████      | 13/32 [00:06<00:10,  1.86it/s]Inference:  44%|████▍     | 14/32 [00:07<00:09,  1.86it/s]Inference:  47%|████▋     | 15/32 [00:07<00:09,  1.85it/s]Inference:  50%|█████     | 16/32 [00:08<00:08,  1.86it/s]Inference:  53%|█████▎    | 17/32 [00:09<00:07,  1.90it/s]Inference:  56%|█████▋    | 18/32 [00:09<00:07,  1.92it/s]Inference:  59%|█████▉    | 19/32 [00:10<00:06,  1.92it/s]Inference:  62%|██████▎   | 20/32 [00:10<00:06,  1.93it/s]Inference:  66%|██████▌   | 21/32 [00:11<00:05,  1.94it/s]Inference:  69%|██████▉   | 22/32 [00:11<00:05,  1.94it/s]Inference:  72%|███████▏  | 23/32 [00:12<00:04,  1.96it/s]Inference:  75%|███████▌  | 24/32 [00:12<00:04,  1.97it/s]Inference:  78%|███████▊  | 25/32 [00:13<00:03,  1.97it/s]Inference:  81%|████████▏ | 26/32 [00:13<00:03,  1.98it/s]Inference:  84%|████████▍ | 27/32 [00:14<00:02,  1.95it/s]Inference:  88%|████████▊ | 28/32 [00:14<00:02,  1.91it/s]Inference:  91%|█████████ | 29/32 [00:15<00:01,  1.89it/s]Inference:  94%|█████████▍| 30/32 [00:15<00:01,  1.87it/s]Inference:  97%|█████████▋| 31/32 [00:16<00:00,  1.87it/s]Inference: 100%|██████████| 32/32 [00:16<00:00,  1.91it/s]Inference: 100%|██████████| 32/32 [00:16<00:00,  1.91it/s]
layer29: self_attn.q_proj
256
val_hessian None
iter 0, train loss 1640.554931640625, val loss None, lr 0.001
iter 10, train loss 1758.490478515625, val loss None, lr 0.001
iter 20, train loss 1673.3536376953125, val loss None, lr 0.001
iter 30, train loss 1610.0731201171875, val loss None, lr 0.001
iter 40, train loss 1585.7843017578125, val loss None, lr 0.001
iter 50, train loss 1580.869384765625, val loss None, lr 0.001
iter 60, train loss 1586.5191650390625, val loss None, lr 0.001
iter 70, train loss 1586.4061279296875, val loss None, lr 0.001
iter 80, train loss 1583.546875, val loss None, lr 0.001
iter 90, train loss 1588.0496826171875, val loss None, lr 0.001
best loss 1494.81494140625
layer29: self_attn.k_proj
256
val_hessian None
iter 0, train loss 1790.343994140625, val loss None, lr 0.001
iter 10, train loss 1898.989013671875, val loss None, lr 0.001
iter 20, train loss 1824.1817626953125, val loss None, lr 0.001
iter 30, train loss 1758.3271484375, val loss None, lr 0.001
iter 40, train loss 1712.06298828125, val loss None, lr 0.001
iter 50, train loss 1704.0390625, val loss None, lr 0.001
iter 60, train loss 1689.059326171875, val loss None, lr 0.001
iter 70, train loss 1684.6978759765625, val loss None, lr 0.001
iter 80, train loss 1698.409423828125, val loss None, lr 0.001
iter 90, train loss 1691.1951904296875, val loss None, lr 0.001
best loss 1569.107421875
layer29: self_attn.v_proj
256
val_hessian None
iter 0, train loss 1238.16015625, val loss None, lr 0.001
iter 10, train loss 1233.3983154296875, val loss None, lr 0.001
iter 20, train loss 1230.581298828125, val loss None, lr 0.001
iter 30, train loss 1230.1324462890625, val loss None, lr 0.001
iter 40, train loss 1227.5872802734375, val loss None, lr 0.001
iter 50, train loss 1229.5595703125, val loss None, lr 0.001
iter 60, train loss 1227.5645751953125, val loss None, lr 0.001
iter 70, train loss 1227.954833984375, val loss None, lr 0.001
iter 80, train loss 1228.779052734375, val loss None, lr 0.001
iter 90, train loss 1231.4842529296875, val loss None, lr 0.001
best loss 1226.8621826171875
layer29: self_attn.o_proj
256
val_hessian None
iter 0, train loss 116.11064910888672, val loss None, lr 0.001
iter 10, train loss 92.32908630371094, val loss None, lr 0.001
iter 20, train loss 84.1695327758789, val loss None, lr 0.001
iter 30, train loss 82.441650390625, val loss None, lr 0.001
iter 40, train loss 80.65834045410156, val loss None, lr 0.001
iter 50, train loss 79.42161560058594, val loss None, lr 0.001
iter 60, train loss 78.55548095703125, val loss None, lr 0.001
iter 70, train loss 78.09758758544922, val loss None, lr 0.001
iter 80, train loss 78.04103088378906, val loss None, lr 0.001
iter 90, train loss 78.28015899658203, val loss None, lr 0.001
best loss 77.6856918334961
layer29: mlp.gate_proj
256
val_hessian None
iter 0, train loss 2555.7548828125, val loss None, lr 0.001
iter 10, train loss 2719.636474609375, val loss None, lr 0.001
iter 20, train loss 2649.507568359375, val loss None, lr 0.001
iter 30, train loss 2660.557861328125, val loss None, lr 0.001
iter 40, train loss 2681.52099609375, val loss None, lr 0.001
iter 50, train loss 2682.51220703125, val loss None, lr 0.001
iter 60, train loss 2691.83984375, val loss None, lr 0.001
iter 70, train loss 2680.357421875, val loss None, lr 0.001
iter 80, train loss 2686.345947265625, val loss None, lr 0.001
iter 90, train loss 2682.13037109375, val loss None, lr 0.001
best loss 2484.3330078125
layer29: mlp.up_proj
256
val_hessian None
iter 0, train loss 2308.482421875, val loss None, lr 0.001
iter 10, train loss 2403.726806640625, val loss None, lr 0.001
iter 20, train loss 2285.48779296875, val loss None, lr 0.001
iter 30, train loss 2282.478271484375, val loss None, lr 0.001
iter 40, train loss 2288.619140625, val loss None, lr 0.001
iter 50, train loss 2297.11865234375, val loss None, lr 0.001
iter 60, train loss 2296.611083984375, val loss None, lr 0.001
iter 70, train loss 2292.64599609375, val loss None, lr 0.001
iter 80, train loss 2295.27001953125, val loss None, lr 0.001
iter 90, train loss 2293.29736328125, val loss None, lr 0.001
best loss 2251.227783203125
layer29: mlp.down_proj
256
val_hessian None
iter 0, train loss 144.4258270263672, val loss None, lr 0.001
iter 10, train loss 144.8665008544922, val loss None, lr 0.001
iter 20, train loss 145.054931640625, val loss None, lr 0.001
iter 30, train loss 144.88369750976562, val loss None, lr 0.001
iter 40, train loss 144.84486389160156, val loss None, lr 0.001
iter 50, train loss 144.70359802246094, val loss None, lr 0.001
iter 60, train loss 144.2332763671875, val loss None, lr 0.001
iter 70, train loss 143.92095947265625, val loss None, lr 0.001
iter 80, train loss 143.85377502441406, val loss None, lr 0.001
iter 90, train loss 144.01766967773438, val loss None, lr 0.001
best loss 143.50787353515625
10977 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:18,  1.70it/s]Inference:   6%|▋         | 2/32 [00:00<00:14,  2.11it/s]Inference:   9%|▉         | 3/32 [00:01<00:12,  2.31it/s]Inference:  12%|█▎        | 4/32 [00:01<00:12,  2.23it/s]Inference:  16%|█▌        | 5/32 [00:02<00:10,  2.54it/s]Inference:  19%|█▉        | 6/32 [00:02<00:10,  2.38it/s]Inference:  22%|██▏       | 7/32 [00:02<00:10,  2.45it/s]Inference:  25%|██▌       | 8/32 [00:03<00:09,  2.50it/s]Inference:  28%|██▊       | 9/32 [00:03<00:09,  2.51it/s]Inference:  31%|███▏      | 10/32 [00:04<00:09,  2.35it/s]Inference:  34%|███▍      | 11/32 [00:04<00:08,  2.41it/s]Inference:  38%|███▊      | 12/32 [00:05<00:08,  2.27it/s]Inference:  41%|████      | 13/32 [00:05<00:08,  2.37it/s]Inference:  44%|████▍     | 14/32 [00:06<00:07,  2.26it/s]Inference:  47%|████▋     | 15/32 [00:06<00:07,  2.36it/s]Inference:  50%|█████     | 16/32 [00:06<00:06,  2.41it/s]Inference:  53%|█████▎    | 17/32 [00:07<00:06,  2.29it/s]Inference:  56%|█████▋    | 18/32 [00:07<00:05,  2.36it/s]Inference:  59%|█████▉    | 19/32 [00:08<00:05,  2.43it/s]Inference:  62%|██████▎   | 20/32 [00:08<00:04,  2.47it/s]Inference:  66%|██████▌   | 21/32 [00:08<00:04,  2.50it/s]Inference:  69%|██████▉   | 22/32 [00:09<00:03,  2.56it/s]Inference:  72%|███████▏  | 23/32 [00:09<00:03,  2.37it/s]Inference:  75%|███████▌  | 24/32 [00:10<00:03,  2.29it/s]Inference:  78%|███████▊  | 25/32 [00:10<00:02,  2.39it/s]Inference:  81%|████████▏ | 26/32 [00:10<00:02,  2.44it/s]Inference:  84%|████████▍ | 27/32 [00:11<00:02,  2.32it/s]Inference:  88%|████████▊ | 28/32 [00:11<00:01,  2.40it/s]Inference:  91%|█████████ | 29/32 [00:12<00:01,  2.46it/s]Inference:  94%|█████████▍| 30/32 [00:12<00:00,  2.35it/s]Inference:  97%|█████████▋| 31/32 [00:13<00:00,  2.40it/s]Inference: 100%|██████████| 32/32 [00:13<00:00,  2.30it/s]Inference: 100%|██████████| 32/32 [00:13<00:00,  2.37it/s]
5687 MiB free out of 48676 MiB total
Saved layer 29 to ./models/meta-llama/Llama-2-7b-hf/2bpv/128/quantized/layer_29.pt
after cast to cpu
9689 MiB free out of 48676 MiB total
Done with layer 29 total_time elapsed: 19926 estimated time left: 1328
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:01<00:33,  1.07s/it]Inference:   6%|▋         | 2/32 [00:01<00:29,  1.03it/s]Inference:   9%|▉         | 3/32 [00:02<00:27,  1.06it/s]Inference:  12%|█▎        | 4/32 [00:03<00:27,  1.03it/s]Inference:  16%|█▌        | 5/32 [00:04<00:27,  1.01s/it]Inference:  19%|█▉        | 6/32 [00:05<00:25,  1.02it/s]Inference:  22%|██▏       | 7/32 [00:06<00:24,  1.02it/s]Inference:  25%|██▌       | 8/32 [00:07<00:23,  1.01it/s]Inference:  28%|██▊       | 9/32 [00:08<00:22,  1.01it/s]Inference:  31%|███▏      | 10/32 [00:09<00:21,  1.01it/s]Inference:  34%|███▍      | 11/32 [00:10<00:20,  1.01it/s]Inference:  38%|███▊      | 12/32 [00:11<00:19,  1.00it/s]Inference:  41%|████      | 13/32 [00:12<00:18,  1.03it/s]Inference:  44%|████▍     | 14/32 [00:13<00:18,  1.01s/it]Inference:  47%|████▋     | 15/32 [00:14<00:16,  1.06it/s]Inference:  50%|█████     | 16/32 [00:15<00:16,  1.00s/it]Inference:  53%|█████▎    | 17/32 [00:16<00:15,  1.00s/it]Inference:  56%|█████▋    | 18/32 [00:17<00:14,  1.00s/it]Inference:  59%|█████▉    | 19/32 [00:18<00:13,  1.03s/it]Inference:  62%|██████▎   | 20/32 [00:19<00:12,  1.03s/it]Inference:  66%|██████▌   | 21/32 [00:20<00:11,  1.02s/it]Inference:  69%|██████▉   | 22/32 [00:21<00:09,  1.01it/s]Inference:  72%|███████▏  | 23/32 [00:22<00:09,  1.02s/it]Inference:  75%|███████▌  | 24/32 [00:24<00:08,  1.05s/it]Inference:  78%|███████▊  | 25/32 [00:25<00:07,  1.03s/it]Inference:  81%|████████▏ | 26/32 [00:26<00:06,  1.03s/it]Inference:  84%|████████▍ | 27/32 [00:27<00:05,  1.05s/it]Inference:  88%|████████▊ | 28/32 [00:28<00:04,  1.03s/it]Inference:  91%|█████████ | 29/32 [00:29<00:03,  1.05s/it]Inference:  94%|█████████▍| 30/32 [00:30<00:02,  1.03s/it]Inference:  97%|█████████▋| 31/32 [00:31<00:01,  1.03s/it]Inference: 100%|██████████| 32/32 [00:32<00:00,  1.01it/s]Inference: 100%|██████████| 32/32 [00:32<00:00,  1.01s/it]
layer30: self_attn.q_proj
256
val_hessian None
iter 0, train loss 1795.7734375, val loss None, lr 0.001
iter 10, train loss 1943.9698486328125, val loss None, lr 0.001
iter 20, train loss 1813.2652587890625, val loss None, lr 0.001
iter 30, train loss 1759.620849609375, val loss None, lr 0.001
iter 40, train loss 1742.3380126953125, val loss None, lr 0.001
iter 50, train loss 1730.6046142578125, val loss None, lr 0.001
iter 60, train loss 1707.541259765625, val loss None, lr 0.001
iter 70, train loss 1724.7044677734375, val loss None, lr 0.001
iter 80, train loss 1725.9114990234375, val loss None, lr 0.001
iter 90, train loss 1726.2781982421875, val loss None, lr 0.001
best loss 1621.9246826171875
layer30: self_attn.k_proj
256
val_hessian None
iter 0, train loss 1941.5350341796875, val loss None, lr 0.001
iter 10, train loss 2075.81787109375, val loss None, lr 0.001
iter 20, train loss 1983.9520263671875, val loss None, lr 0.001
iter 30, train loss 1860.77294921875, val loss None, lr 0.001
iter 40, train loss 1831.161376953125, val loss None, lr 0.001
iter 50, train loss 1799.14892578125, val loss None, lr 0.001
iter 60, train loss 1811.48388671875, val loss None, lr 0.001
iter 70, train loss 1817.251220703125, val loss None, lr 0.001
iter 80, train loss 1796.9832763671875, val loss None, lr 0.001
iter 90, train loss 1800.05712890625, val loss None, lr 0.001
best loss 1685.365478515625
layer30: self_attn.v_proj
256
val_hessian None
iter 0, train loss 1408.668701171875, val loss None, lr 0.001
iter 10, train loss 1411.468017578125, val loss None, lr 0.001
iter 20, train loss 1412.5206298828125, val loss None, lr 0.001
iter 30, train loss 1415.6767578125, val loss None, lr 0.001
iter 40, train loss 1414.927001953125, val loss None, lr 0.001
iter 50, train loss 1415.590087890625, val loss None, lr 0.001
iter 60, train loss 1419.54736328125, val loss None, lr 0.001
iter 70, train loss 1421.086181640625, val loss None, lr 0.001
iter 80, train loss 1420.1597900390625, val loss None, lr 0.001
iter 90, train loss 1422.80224609375, val loss None, lr 0.001
best loss 1408.4356689453125
layer30: self_attn.o_proj
256
val_hessian None
iter 0, train loss 151.29800415039062, val loss None, lr 0.001
iter 10, train loss 123.47203826904297, val loss None, lr 0.001
iter 20, train loss 110.86190795898438, val loss None, lr 0.001
iter 30, train loss 107.07054138183594, val loss None, lr 0.001
iter 40, train loss 104.91458129882812, val loss None, lr 0.001
iter 50, train loss 103.7982177734375, val loss None, lr 0.001
iter 60, train loss 103.33262634277344, val loss None, lr 0.001
iter 70, train loss 102.65023803710938, val loss None, lr 0.001
iter 80, train loss 101.95516967773438, val loss None, lr 0.001
iter 90, train loss 101.74410247802734, val loss None, lr 0.001
best loss 101.2130355834961
layer30: mlp.gate_proj
256
val_hessian None
iter 0, train loss 2939.882080078125, val loss None, lr 0.001
iter 10, train loss 3116.42041015625, val loss None, lr 0.001
iter 20, train loss 2894.636962890625, val loss None, lr 0.001
iter 30, train loss 2904.82861328125, val loss None, lr 0.001
iter 40, train loss 2911.947265625, val loss None, lr 0.001
iter 50, train loss 2912.747314453125, val loss None, lr 0.001
iter 60, train loss 2893.006591796875, val loss None, lr 0.001
iter 70, train loss 2902.14013671875, val loss None, lr 0.001
iter 80, train loss 2871.5263671875, val loss None, lr 0.001
iter 90, train loss 2882.069091796875, val loss None, lr 0.001
best loss 2690.00390625
layer30: mlp.up_proj
256
val_hessian None
iter 0, train loss 2571.18212890625, val loss None, lr 0.001
iter 10, train loss 2723.73046875, val loss None, lr 0.001
iter 20, train loss 2548.701416015625, val loss None, lr 0.001
iter 30, train loss 2463.774658203125, val loss None, lr 0.001
iter 40, train loss 2448.8583984375, val loss None, lr 0.001
iter 50, train loss 2449.78076171875, val loss None, lr 0.001
iter 60, train loss 2434.36669921875, val loss None, lr 0.001
iter 70, train loss 2453.551025390625, val loss None, lr 0.001
iter 80, train loss 2442.0498046875, val loss None, lr 0.001
iter 90, train loss 2450.15380859375, val loss None, lr 0.001
best loss 2365.86279296875
layer30: mlp.down_proj
256
val_hessian None
iter 0, train loss 213.60006713867188, val loss None, lr 0.001
iter 10, train loss 217.4375457763672, val loss None, lr 0.001
iter 20, train loss 214.40365600585938, val loss None, lr 0.001
iter 30, train loss 214.28216552734375, val loss None, lr 0.001
iter 40, train loss 213.14599609375, val loss None, lr 0.001
iter 50, train loss 211.75473022460938, val loss None, lr 0.001
iter 60, train loss 212.0275115966797, val loss None, lr 0.001
iter 70, train loss 212.01156616210938, val loss None, lr 0.001
iter 80, train loss 211.17208862304688, val loss None, lr 0.001
iter 90, train loss 210.00448608398438, val loss None, lr 0.001
best loss 210.00448608398438
9689 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:17,  1.79it/s]Inference:   6%|▋         | 2/32 [00:01<00:16,  1.82it/s]Inference:   9%|▉         | 3/32 [00:01<00:12,  2.25it/s]Inference:  12%|█▎        | 4/32 [00:01<00:12,  2.28it/s]Inference:  16%|█▌        | 5/32 [00:02<00:12,  2.12it/s]Inference:  19%|█▉        | 6/32 [00:02<00:12,  2.04it/s]Inference:  22%|██▏       | 7/32 [00:03<00:12,  1.99it/s]Inference:  25%|██▌       | 8/32 [00:04<00:13,  1.82it/s]Inference:  28%|██▊       | 9/32 [00:04<00:13,  1.74it/s]Inference:  31%|███▏      | 10/32 [00:05<00:11,  1.88it/s]Inference:  34%|███▍      | 11/32 [00:05<00:11,  1.88it/s]Inference:  38%|███▊      | 12/32 [00:06<00:10,  2.00it/s]Inference:  41%|████      | 13/32 [00:06<00:09,  2.07it/s]Inference:  44%|████▍     | 14/32 [00:06<00:08,  2.15it/s]Inference:  47%|████▋     | 15/32 [00:07<00:07,  2.20it/s]Inference:  50%|█████     | 16/32 [00:08<00:08,  1.98it/s]Inference:  53%|█████▎    | 17/32 [00:08<00:07,  2.07it/s]Inference:  56%|█████▋    | 18/32 [00:08<00:06,  2.01it/s]Inference:  59%|█████▉    | 19/32 [00:09<00:05,  2.22it/s]Inference:  62%|██████▎   | 20/32 [00:09<00:06,  1.99it/s]Inference:  66%|██████▌   | 21/32 [00:10<00:05,  1.95it/s]Inference:  69%|██████▉   | 22/32 [00:11<00:05,  1.81it/s]Inference:  72%|███████▏  | 23/32 [00:11<00:04,  1.87it/s]Inference:  75%|███████▌  | 24/32 [00:12<00:04,  1.97it/s]Inference:  78%|███████▊  | 25/32 [00:12<00:03,  1.93it/s]Inference:  81%|████████▏ | 26/32 [00:13<00:03,  1.90it/s]Inference:  84%|████████▍ | 27/32 [00:13<00:02,  1.86it/s]Inference:  88%|████████▊ | 28/32 [00:14<00:02,  1.76it/s]Inference:  91%|█████████ | 29/32 [00:15<00:01,  1.63it/s]Inference:  94%|█████████▍| 30/32 [00:15<00:01,  1.69it/s]Inference:  97%|█████████▋| 31/32 [00:16<00:00,  1.84it/s]Inference: 100%|██████████| 32/32 [00:16<00:00,  1.95it/s]Inference: 100%|██████████| 32/32 [00:16<00:00,  1.94it/s]
4463 MiB free out of 48676 MiB total
Saved layer 30 to ./models/meta-llama/Llama-2-7b-hf/2bpv/128/quantized/layer_30.pt
after cast to cpu
8465 MiB free out of 48676 MiB total
Done with layer 30 total_time elapsed: 20796 estimated time left: 671
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:28,  1.10it/s]Inference:   6%|▋         | 2/32 [00:01<00:28,  1.04it/s]Inference:   9%|▉         | 3/32 [00:02<00:26,  1.11it/s]Inference:  12%|█▎        | 4/32 [00:03<00:27,  1.03it/s]Inference:  16%|█▌        | 5/32 [00:04<00:27,  1.02s/it]Inference:  19%|█▉        | 6/32 [00:05<00:26,  1.01s/it]Inference:  22%|██▏       | 7/32 [00:07<00:27,  1.09s/it]Inference:  25%|██▌       | 8/32 [00:08<00:26,  1.09s/it]Inference:  28%|██▊       | 9/32 [00:09<00:24,  1.09s/it]Inference:  31%|███▏      | 10/32 [00:10<00:22,  1.03s/it]Inference:  34%|███▍      | 11/32 [00:11<00:21,  1.02s/it]Inference:  38%|███▊      | 12/32 [00:12<00:20,  1.01s/it]Inference:  41%|████      | 13/32 [00:13<00:19,  1.04s/it]Inference:  44%|████▍     | 14/32 [00:14<00:18,  1.03s/it]Inference:  47%|████▋     | 15/32 [00:15<00:17,  1.03s/it]Inference:  50%|█████     | 16/32 [00:16<00:16,  1.01s/it]Inference:  53%|█████▎    | 17/32 [00:17<00:15,  1.01s/it]Inference:  56%|█████▋    | 18/32 [00:18<00:14,  1.06s/it]Inference:  59%|█████▉    | 19/32 [00:19<00:13,  1.04s/it]Inference:  62%|██████▎   | 20/32 [00:20<00:12,  1.05s/it]Inference:  66%|██████▌   | 21/32 [00:21<00:11,  1.03s/it]Inference:  69%|██████▉   | 22/32 [00:22<00:10,  1.02s/it]Inference:  72%|███████▏  | 23/32 [00:23<00:09,  1.02s/it]Inference:  75%|███████▌  | 24/32 [00:24<00:07,  1.01it/s]Inference:  78%|███████▊  | 25/32 [00:25<00:06,  1.01it/s]Inference:  81%|████████▏ | 26/32 [00:26<00:05,  1.01it/s]Inference:  84%|████████▍ | 27/32 [00:27<00:04,  1.00it/s]Inference:  88%|████████▊ | 28/32 [00:28<00:03,  1.03it/s]Inference:  91%|█████████ | 29/32 [00:29<00:02,  1.02it/s]Inference:  94%|█████████▍| 30/32 [00:30<00:02,  1.03s/it]Inference:  97%|█████████▋| 31/32 [00:31<00:01,  1.03s/it]Inference: 100%|██████████| 32/32 [00:32<00:00,  1.02s/it]Inference: 100%|██████████| 32/32 [00:32<00:00,  1.02s/it]
layer31: self_attn.q_proj
256
val_hessian None
iter 0, train loss 1366.045654296875, val loss None, lr 0.001
iter 10, train loss 1441.885986328125, val loss None, lr 0.001
iter 20, train loss 1375.546630859375, val loss None, lr 0.001
iter 30, train loss 1302.0313720703125, val loss None, lr 0.001
iter 40, train loss 1284.65869140625, val loss None, lr 0.001
iter 50, train loss 1284.858642578125, val loss None, lr 0.001
iter 60, train loss 1287.73388671875, val loss None, lr 0.001
iter 70, train loss 1283.734619140625, val loss None, lr 0.001
iter 80, train loss 1288.4625244140625, val loss None, lr 0.001
iter 90, train loss 1282.168701171875, val loss None, lr 0.001
best loss 1158.3466796875
layer31: self_attn.k_proj
256
val_hessian None
iter 0, train loss 1619.322509765625, val loss None, lr 0.001
iter 10, train loss 1630.668701171875, val loss None, lr 0.001
iter 20, train loss 1610.656005859375, val loss None, lr 0.001
iter 30, train loss 1499.363037109375, val loss None, lr 0.001
iter 40, train loss 1455.080810546875, val loss None, lr 0.001
iter 50, train loss 1444.0081787109375, val loss None, lr 0.001
iter 60, train loss 1432.77001953125, val loss None, lr 0.001
iter 70, train loss 1444.511962890625, val loss None, lr 0.001
iter 80, train loss 1443.7613525390625, val loss None, lr 0.001
iter 90, train loss 1429.5933837890625, val loss None, lr 0.001
best loss 1279.337890625
layer31: self_attn.v_proj
256
val_hessian None
iter 0, train loss 807.1309204101562, val loss None, lr 0.001
iter 10, train loss 809.2550659179688, val loss None, lr 0.001
iter 20, train loss 809.7783203125, val loss None, lr 0.001
iter 30, train loss 810.6766967773438, val loss None, lr 0.001
iter 40, train loss 809.3601684570312, val loss None, lr 0.001
iter 50, train loss 808.67138671875, val loss None, lr 0.001
iter 60, train loss 808.7754516601562, val loss None, lr 0.001
iter 70, train loss 808.3493041992188, val loss None, lr 0.001
iter 80, train loss 807.6036376953125, val loss None, lr 0.001
iter 90, train loss 809.4918212890625, val loss None, lr 0.001
best loss 806.66650390625
layer31: self_attn.o_proj
256
val_hessian None
iter 0, train loss 1038.6007080078125, val loss None, lr 0.001
iter 10, train loss 787.4344482421875, val loss None, lr 0.001
iter 20, train loss 647.671630859375, val loss None, lr 0.001
iter 30, train loss 527.4567260742188, val loss None, lr 0.001
iter 40, train loss 435.4571228027344, val loss None, lr 0.001
iter 50, train loss 373.8155212402344, val loss None, lr 0.001
iter 60, train loss 342.9274597167969, val loss None, lr 0.001
iter 70, train loss 332.88055419921875, val loss None, lr 0.001
iter 80, train loss 328.525390625, val loss None, lr 0.001
iter 90, train loss 326.2239074707031, val loss None, lr 0.001
best loss 322.4019775390625
layer31: mlp.gate_proj
256
val_hessian None
iter 0, train loss 2737.572265625, val loss None, lr 0.001
iter 10, train loss 2773.35107421875, val loss None, lr 0.001
iter 20, train loss 2611.67041015625, val loss None, lr 0.001
iter 30, train loss 2561.6142578125, val loss None, lr 0.001
iter 40, train loss 2572.359375, val loss None, lr 0.001
iter 50, train loss 2596.06982421875, val loss None, lr 0.001
iter 60, train loss 2571.900390625, val loss None, lr 0.001
iter 70, train loss 2577.156982421875, val loss None, lr 0.001
iter 80, train loss 2567.509765625, val loss None, lr 0.001
iter 90, train loss 2561.18505859375, val loss None, lr 0.001
best loss 2325.64013671875
layer31: mlp.up_proj
256
val_hessian None
iter 0, train loss 2640.37060546875, val loss None, lr 0.001
iter 10, train loss 2639.080810546875, val loss None, lr 0.001
iter 20, train loss 2514.264892578125, val loss None, lr 0.001
iter 30, train loss 2338.36474609375, val loss None, lr 0.001
iter 40, train loss 2298.145263671875, val loss None, lr 0.001
iter 50, train loss 2291.702880859375, val loss None, lr 0.001
iter 60, train loss 2299.113525390625, val loss None, lr 0.001
iter 70, train loss 2298.638671875, val loss None, lr 0.001
iter 80, train loss 2295.59375, val loss None, lr 0.001
iter 90, train loss 2307.35302734375, val loss None, lr 0.001
best loss 2142.1376953125
layer31: mlp.down_proj
256
val_hessian None
iter 0, train loss 500.8779602050781, val loss None, lr 0.001
iter 10, train loss 506.2034606933594, val loss None, lr 0.001
iter 20, train loss 468.532470703125, val loss None, lr 0.001
iter 30, train loss 436.9273681640625, val loss None, lr 0.001
iter 40, train loss 419.8392028808594, val loss None, lr 0.001
iter 50, train loss 403.3320007324219, val loss None, lr 0.001
iter 60, train loss 396.88836669921875, val loss None, lr 0.001
iter 70, train loss 392.97210693359375, val loss None, lr 0.001
iter 80, train loss 386.208251953125, val loss None, lr 0.001
iter 90, train loss 385.4064636230469, val loss None, lr 0.001
best loss 384.4259948730469
8465 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:07,  4.07it/s]Inference:   6%|▋         | 2/32 [00:00<00:07,  3.91it/s]Inference:   9%|▉         | 3/32 [00:00<00:07,  3.98it/s]Inference:  12%|█▎        | 4/32 [00:01<00:06,  4.00it/s]Inference:  16%|█▌        | 5/32 [00:01<00:06,  3.96it/s]Inference:  19%|█▉        | 6/32 [00:01<00:06,  3.93it/s]Inference:  22%|██▏       | 7/32 [00:01<00:06,  3.97it/s]Inference:  25%|██▌       | 8/32 [00:02<00:06,  3.97it/s]Inference:  28%|██▊       | 9/32 [00:02<00:05,  3.98it/s]Inference:  31%|███▏      | 10/32 [00:02<00:05,  3.97it/s]Inference:  34%|███▍      | 11/32 [00:02<00:05,  4.07it/s]Inference:  38%|███▊      | 12/32 [00:02<00:04,  4.05it/s]Inference:  41%|████      | 13/32 [00:03<00:04,  4.03it/s]Inference:  44%|████▍     | 14/32 [00:03<00:04,  4.05it/s]Inference:  47%|████▋     | 15/32 [00:03<00:04,  4.10it/s]Inference:  50%|█████     | 16/32 [00:03<00:03,  4.07it/s]Inference:  53%|█████▎    | 17/32 [00:04<00:03,  4.04it/s]Inference:  56%|█████▋    | 18/32 [00:04<00:03,  4.04it/s]Inference:  59%|█████▉    | 19/32 [00:04<00:03,  4.03it/s]Inference:  62%|██████▎   | 20/32 [00:04<00:02,  4.03it/s]Inference:  66%|██████▌   | 21/32 [00:05<00:02,  3.97it/s]Inference:  69%|██████▉   | 22/32 [00:05<00:02,  4.03it/s]Inference:  72%|███████▏  | 23/32 [00:05<00:02,  4.02it/s]Inference:  75%|███████▌  | 24/32 [00:05<00:01,  4.05it/s]Inference:  78%|███████▊  | 25/32 [00:06<00:01,  4.04it/s]Inference:  81%|████████▏ | 26/32 [00:06<00:01,  4.00it/s]Inference:  84%|████████▍ | 27/32 [00:06<00:01,  4.15it/s]Inference:  88%|████████▊ | 28/32 [00:06<00:00,  4.12it/s]Inference:  91%|█████████ | 29/32 [00:07<00:00,  4.08it/s]Inference:  94%|█████████▍| 30/32 [00:07<00:00,  4.07it/s]Inference:  97%|█████████▋| 31/32 [00:07<00:00,  4.03it/s]Inference: 100%|██████████| 32/32 [00:07<00:00,  4.05it/s]Inference: 100%|██████████| 32/32 [00:07<00:00,  4.03it/s]
3175 MiB free out of 48676 MiB total
Saved layer 31 to ./models/meta-llama/Llama-2-7b-hf/2bpv/128/quantized/layer_31.pt
after cast to cpu
7177 MiB free out of 48676 MiB total
Done with layer 31 total_time elapsed: 21257 estimated time left: 0
Total bits: 12995657728.0 Total params: 6476005376
average bits per value: 2.0067397992227978
total time taken: 21257.373911857605
2024-12-03 17:11:27.374960: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-12-03 17:11:27.391303: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-12-03 17:11:27.396585: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-12-03 17:11:27.408825: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-12-03 17:11:28.794730: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  3.81it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  3.95it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  3.93it/s]
Model loaded. LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
/data/lliu/huffman/perplexity_eval.py:84: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  layer.load_state_dict(torch.load(os.path.join(checkpoint_path, f"layer_{i}.pt")))
Evaluating ...
nsamples 83
testenc.numel() 341469
  0%|          | 0/32 [00:00<?, ?it/s]torch.Size([83, 4096, 4096]) batch size 8
  3%|▎         | 1/32 [00:02<01:10,  2.27s/it]torch.Size([83, 4096, 4096]) batch size 8
  6%|▋         | 2/32 [00:04<01:06,  2.20s/it]torch.Size([83, 4096, 4096]) batch size 8
  9%|▉         | 3/32 [00:06<01:03,  2.18s/it]torch.Size([83, 4096, 4096]) batch size 8
 12%|█▎        | 4/32 [00:08<01:00,  2.17s/it]torch.Size([83, 4096, 4096]) batch size 8
 16%|█▌        | 5/32 [00:10<00:58,  2.17s/it]torch.Size([83, 4096, 4096]) batch size 8
 19%|█▉        | 6/32 [00:13<00:56,  2.16s/it]torch.Size([83, 4096, 4096]) batch size 8
 22%|██▏       | 7/32 [00:15<00:54,  2.17s/it]torch.Size([83, 4096, 4096]) batch size 8
 25%|██▌       | 8/32 [00:17<00:51,  2.16s/it]torch.Size([83, 4096, 4096]) batch size 8
 28%|██▊       | 9/32 [00:19<00:49,  2.16s/it]torch.Size([83, 4096, 4096]) batch size 8
 31%|███▏      | 10/32 [00:21<00:47,  2.16s/it]torch.Size([83, 4096, 4096]) batch size 8
 34%|███▍      | 11/32 [00:23<00:45,  2.16s/it]torch.Size([83, 4096, 4096]) batch size 8
 38%|███▊      | 12/32 [00:26<00:43,  2.16s/it]torch.Size([83, 4096, 4096]) batch size 8
 41%|████      | 13/32 [00:28<00:41,  2.16s/it]torch.Size([83, 4096, 4096]) batch size 8
 44%|████▍     | 14/32 [00:30<00:38,  2.16s/it]torch.Size([83, 4096, 4096]) batch size 8
 47%|████▋     | 15/32 [00:32<00:36,  2.16s/it]torch.Size([83, 4096, 4096]) batch size 8
 50%|█████     | 16/32 [00:34<00:34,  2.16s/it]torch.Size([83, 4096, 4096]) batch size 8
 53%|█████▎    | 17/32 [00:36<00:32,  2.16s/it]torch.Size([83, 4096, 4096]) batch size 8
 56%|█████▋    | 18/32 [00:39<00:30,  2.16s/it]torch.Size([83, 4096, 4096]) batch size 8
 59%|█████▉    | 19/32 [00:41<00:28,  2.17s/it]torch.Size([83, 4096, 4096]) batch size 8
 62%|██████▎   | 20/32 [00:43<00:25,  2.17s/it]torch.Size([83, 4096, 4096]) batch size 8
 66%|██████▌   | 21/32 [00:45<00:23,  2.16s/it]torch.Size([83, 4096, 4096]) batch size 8
 69%|██████▉   | 22/32 [00:47<00:21,  2.16s/it]torch.Size([83, 4096, 4096]) batch size 8
 72%|███████▏  | 23/32 [00:49<00:19,  2.16s/it]torch.Size([83, 4096, 4096]) batch size 8
 75%|███████▌  | 24/32 [00:52<00:18,  2.28s/it]torch.Size([83, 4096, 4096]) batch size 8
 78%|███████▊  | 25/32 [00:54<00:15,  2.24s/it]torch.Size([83, 4096, 4096]) batch size 8
 81%|████████▏ | 26/32 [00:56<00:13,  2.22s/it]torch.Size([83, 4096, 4096]) batch size 8
 84%|████████▍ | 27/32 [00:58<00:11,  2.20s/it]torch.Size([83, 4096, 4096]) batch size 8
 88%|████████▊ | 28/32 [01:01<00:08,  2.19s/it]torch.Size([83, 4096, 4096]) batch size 8
 91%|█████████ | 29/32 [01:03<00:06,  2.19s/it]torch.Size([83, 4096, 4096]) batch size 8
 94%|█████████▍| 30/32 [01:05<00:04,  2.18s/it]torch.Size([83, 4096, 4096]) batch size 8
 97%|█████████▋| 31/32 [01:07<00:02,  2.18s/it]torch.Size([83, 4096, 4096]) batch size 8
100%|██████████| 32/32 [01:09<00:00,  2.18s/it]100%|██████████| 32/32 [01:09<00:00,  2.18s/it]
  0%|          | 0/83 [00:00<?, ?it/s] 95%|█████████▌| 79/83 [00:00<00:00, 774.47it/s]100%|██████████| 83/83 [00:00<00:00, 560.91it/s]
Perplexity: 7.628365
Traceback (most recent call last):
  File "/data/lliu/huffman/perplexity_eval.py", line 232, in <module>
    llama_eval(model, testloader, args.device, dataset, args.log_wandb,
  File "/home/lliu/anaconda3/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/data/lliu/huffman/perplexity_eval.py", line 189, in llama_eval
    wandb.log({f"{dataset}/perplexity": ppl.item()})
  File "/home/lliu/anaconda3/lib/python3.11/site-packages/wandb/sdk/lib/preinit.py", line 36, in preinit_wrapper
    raise wandb.Error(f"You must call wandb.init() before {name}()")
wandb.errors.Error: You must call wandb.init() before wandb.log()
