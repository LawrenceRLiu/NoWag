2024-12-06 02:13:00.405521: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-12-06 02:13:00.421911: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-12-06 02:13:00.426798: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-12-06 02:13:00.439104: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-12-06 02:13:01.571337: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  3.87it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  4.12it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  4.08it/s]
Model loaded. LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
Starting...
getting inputs:   0%|          | 0/128 [00:00<?, ?it/s]getting inputs:   1%|          | 1/128 [00:00<01:28,  1.44it/s]getting inputs:   4%|▍         | 5/128 [00:00<00:16,  7.69it/s]getting inputs:   7%|▋         | 9/128 [00:00<00:08, 13.23it/s]getting inputs:  10%|█         | 13/128 [00:01<00:06, 17.98it/s]getting inputs:  13%|█▎        | 17/128 [00:01<00:05, 21.82it/s]getting inputs:  16%|█▋        | 21/128 [00:01<00:04, 24.39it/s]getting inputs:  20%|█▉        | 25/128 [00:01<00:03, 26.11it/s]getting inputs:  22%|██▏       | 28/128 [00:01<00:03, 26.77it/s]getting inputs:  25%|██▌       | 32/128 [00:01<00:03, 28.60it/s]getting inputs:  28%|██▊       | 36/128 [00:01<00:03, 30.43it/s]getting inputs:  31%|███▏      | 40/128 [00:01<00:02, 31.14it/s]getting inputs:  34%|███▍      | 44/128 [00:02<00:02, 32.36it/s]getting inputs:  38%|███▊      | 48/128 [00:02<00:02, 33.53it/s]getting inputs:  41%|████      | 52/128 [00:02<00:02, 34.17it/s]getting inputs:  44%|████▍     | 56/128 [00:02<00:02, 34.37it/s]getting inputs:  47%|████▋     | 60/128 [00:02<00:01, 34.62it/s]getting inputs:  50%|█████     | 64/128 [00:02<00:01, 34.12it/s]getting inputs:  53%|█████▎    | 68/128 [00:02<00:01, 34.20it/s]getting inputs:  56%|█████▋    | 72/128 [00:02<00:01, 34.50it/s]getting inputs:  59%|█████▉    | 76/128 [00:02<00:01, 34.31it/s]getting inputs:  62%|██████▎   | 80/128 [00:03<00:01, 34.50it/s]getting inputs:  66%|██████▌   | 84/128 [00:03<00:01, 34.67it/s]getting inputs:  69%|██████▉   | 88/128 [00:03<00:01, 34.76it/s]getting inputs:  72%|███████▏  | 92/128 [00:03<00:01, 34.73it/s]getting inputs:  75%|███████▌  | 96/128 [00:03<00:00, 34.76it/s]getting inputs:  78%|███████▊  | 100/128 [00:03<00:00, 34.82it/s]getting inputs:  81%|████████▏ | 104/128 [00:03<00:00, 34.86it/s]getting inputs:  84%|████████▍ | 108/128 [00:03<00:00, 34.89it/s]getting inputs:  88%|████████▊ | 112/128 [00:03<00:00, 34.90it/s]getting inputs:  91%|█████████ | 116/128 [00:04<00:00, 34.88it/s]getting inputs:  94%|█████████▍| 120/128 [00:04<00:00, 34.91it/s]getting inputs:  97%|█████████▋| 124/128 [00:04<00:00, 34.89it/s]getting inputs: 100%|██████████| 128/128 [00:04<00:00, 34.81it/s]getting inputs: 100%|██████████| 128/128 [00:04<00:00, 28.90it/s]
48323 MiB free out of 48676 MiB total
Ready.
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:22,  1.41it/s]Inference:   6%|▋         | 2/32 [00:01<00:17,  1.71it/s]Inference:   9%|▉         | 3/32 [00:01<00:15,  1.82it/s]Inference:  12%|█▎        | 4/32 [00:02<00:14,  1.88it/s]Inference:  16%|█▌        | 5/32 [00:02<00:14,  1.92it/s]Inference:  19%|█▉        | 6/32 [00:03<00:13,  1.94it/s]Inference:  22%|██▏       | 7/32 [00:03<00:12,  1.95it/s]Inference:  25%|██▌       | 8/32 [00:04<00:12,  1.96it/s]Inference:  28%|██▊       | 9/32 [00:04<00:11,  1.98it/s]Inference:  31%|███▏      | 10/32 [00:05<00:11,  1.98it/s]Inference:  34%|███▍      | 11/32 [00:05<00:10,  1.97it/s]Inference:  38%|███▊      | 12/32 [00:06<00:10,  2.00it/s]Inference:  41%|████      | 13/32 [00:06<00:09,  2.00it/s]Inference:  44%|████▍     | 14/32 [00:07<00:08,  2.00it/s]Inference:  47%|████▋     | 15/32 [00:07<00:08,  2.02it/s]Inference:  50%|█████     | 16/32 [00:08<00:07,  2.03it/s]Inference:  53%|█████▎    | 17/32 [00:08<00:07,  2.03it/s]Inference:  56%|█████▋    | 18/32 [00:09<00:06,  2.04it/s]Inference:  59%|█████▉    | 19/32 [00:09<00:06,  2.03it/s]Inference:  62%|██████▎   | 20/32 [00:10<00:05,  2.04it/s]Inference:  66%|██████▌   | 21/32 [00:10<00:05,  2.04it/s]Inference:  69%|██████▉   | 22/32 [00:11<00:04,  2.04it/s]Inference:  72%|███████▏  | 23/32 [00:11<00:04,  2.04it/s]Inference:  75%|███████▌  | 24/32 [00:12<00:03,  2.04it/s]Inference:  78%|███████▊  | 25/32 [00:13<00:04,  1.64it/s]Inference:  81%|████████▏ | 26/32 [00:13<00:04,  1.43it/s]Inference:  84%|████████▍ | 27/32 [00:14<00:03,  1.31it/s]Inference:  88%|████████▊ | 28/32 [00:15<00:03,  1.25it/s]Inference:  91%|█████████ | 29/32 [00:16<00:02,  1.24it/s]Inference:  94%|█████████▍| 30/32 [00:17<00:01,  1.24it/s]Inference:  97%|█████████▋| 31/32 [00:18<00:00,  1.22it/s]Inference: 100%|██████████| 32/32 [00:19<00:00,  1.17it/s]Inference: 100%|██████████| 32/32 [00:19<00:00,  1.67it/s]
layer0: self_attn.q_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 21644.2734375, val loss None, lr 0.01
iter 250, train loss 23.77821922302246, val loss None, lr 0.01
iter 500, train loss 25.41039276123047, val loss None, lr 0.01
iter 750, train loss 11.216647148132324, val loss None, lr 0.003333
iter 1000, train loss 8.693724632263184, val loss None, lr 0.001111
iter 1250, train loss 8.165831565856934, val loss None, lr 0.001111
iter 1500, train loss 8.00347900390625, val loss None, lr 0.001111
iter 1750, train loss 7.40529727935791, val loss None, lr 0.00037
iter 2000, train loss 7.229047775268555, val loss None, lr 0.00037
iter 2250, train loss 7.06007194519043, val loss None, lr 0.00037
best loss 6.89670991897583
running bpv: 0.515625
layer0: self_attn.k_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 13562.05078125, val loss None, lr 0.01
iter 250, train loss 22.86334991455078, val loss None, lr 0.01
iter 500, train loss 22.05643081665039, val loss None, lr 0.01
iter 750, train loss 13.626896858215332, val loss None, lr 0.003333
iter 1000, train loss 12.860172271728516, val loss None, lr 0.001111
iter 1250, train loss 12.383703231811523, val loss None, lr 0.001111
iter 1500, train loss 11.939817428588867, val loss None, lr 0.00037
iter 1750, train loss 11.653436660766602, val loss None, lr 0.00037
iter 2000, train loss 11.46377182006836, val loss None, lr 0.00037
iter 2250, train loss 11.283818244934082, val loss None, lr 0.00037
best loss 11.125028610229492
running bpv: 0.515625
layer0: self_attn.v_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 33.00853729248047, val loss None, lr 0.01
iter 250, train loss 1.7910383939743042, val loss None, lr 0.01
iter 500, train loss 1.740170955657959, val loss None, lr 0.01
iter 750, train loss 1.6761829853057861, val loss None, lr 0.01
iter 1000, train loss 1.6467905044555664, val loss None, lr 0.003333
iter 1250, train loss 1.6311734914779663, val loss None, lr 0.003333
iter 1500, train loss 1.6237239837646484, val loss None, lr 0.003333
iter 1750, train loss 1.6149641275405884, val loss None, lr 0.003333
iter 2000, train loss 1.6085695028305054, val loss None, lr 0.001111
iter 2250, train loss 1.6040034294128418, val loss None, lr 0.00037
best loss 1.6012847423553467
running bpv: 0.515625
layer0: self_attn.o_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 7.725079536437988, val loss None, lr 0.01
iter 250, train loss 0.15386085212230682, val loss None, lr 0.01
iter 500, train loss 0.13794028759002686, val loss None, lr 0.01
iter 750, train loss 0.1280180662870407, val loss None, lr 0.003333
iter 1000, train loss 0.12439598888158798, val loss None, lr 0.003333
iter 1250, train loss 0.1199575737118721, val loss None, lr 0.001111
iter 1500, train loss 0.11859890818595886, val loss None, lr 0.001111
iter 1750, train loss 0.11740361899137497, val loss None, lr 0.001111
iter 2000, train loss 0.11638546735048294, val loss None, lr 0.001111
iter 2250, train loss 0.1159767359495163, val loss None, lr 0.001111
best loss 0.11497912555932999
running bpv: 0.515625
layer0: mlp.gate_proj
weight shape torch.Size([11008, 4096])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([22, 8, 8, 8]), -3, -2
iter 0, train loss 502.8291320800781, val loss None, lr 0.01
iter 250, train loss 204.804443359375, val loss None, lr 0.01
iter 500, train loss 201.0331573486328, val loss None, lr 0.01
iter 750, train loss 199.0667266845703, val loss None, lr 0.01
iter 1000, train loss 198.7809600830078, val loss None, lr 0.01
iter 1250, train loss 196.23361206054688, val loss None, lr 0.003333
iter 1500, train loss 195.19021606445312, val loss None, lr 0.003333
iter 1750, train loss 195.11111450195312, val loss None, lr 0.003333
iter 2000, train loss 194.42367553710938, val loss None, lr 0.001111
iter 2250, train loss 194.60633850097656, val loss None, lr 0.001111
best loss 193.92300415039062
running bpv: 0.38850759345794394
layer0: mlp.up_proj
weight shape torch.Size([11008, 4096])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([22, 8, 8, 8]), -3, -2
iter 0, train loss 341.3420104980469, val loss None, lr 0.01
iter 250, train loss 183.49847412109375, val loss None, lr 0.01
iter 500, train loss 181.62301635742188, val loss None, lr 0.01
iter 750, train loss 178.75735473632812, val loss None, lr 0.01
iter 1000, train loss 178.8837432861328, val loss None, lr 0.01
iter 1250, train loss 177.02432250976562, val loss None, lr 0.003333
iter 1500, train loss 176.47360229492188, val loss None, lr 0.003333
iter 1750, train loss 176.1378173828125, val loss None, lr 0.001111
iter 2000, train loss 175.9182891845703, val loss None, lr 0.001111
iter 2250, train loss 175.80001831054688, val loss None, lr 0.001111
best loss 175.65475463867188
running bpv: 0.33427083333333335
layer0: mlp.down_proj
weight shape torch.Size([4096, 11008])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 22, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 1.918962001800537, val loss None, lr 0.01
iter 250, train loss 0.5801646709442139, val loss None, lr 0.01
iter 500, train loss 0.590535044670105, val loss None, lr 0.01
iter 750, train loss 0.563019871711731, val loss None, lr 0.003333
iter 1000, train loss 0.5610699653625488, val loss None, lr 0.003333
iter 1250, train loss 0.5603090524673462, val loss None, lr 0.003333
iter 1500, train loss 0.5585416555404663, val loss None, lr 0.003333
iter 1750, train loss 0.558560848236084, val loss None, lr 0.003333
iter 2000, train loss 0.5587483644485474, val loss None, lr 0.003333
iter 2250, train loss 0.5569475889205933, val loss None, lr 0.003333
best loss 0.5564546585083008
running bpv: 0.3393579987046632
48323 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:11,  2.71it/s]Inference:   6%|▋         | 2/32 [00:00<00:11,  2.64it/s]Inference:   9%|▉         | 3/32 [00:01<00:11,  2.42it/s]Inference:  12%|█▎        | 4/32 [00:01<00:11,  2.48it/s]Inference:  16%|█▌        | 5/32 [00:01<00:10,  2.50it/s]Inference:  19%|█▉        | 6/32 [00:02<00:10,  2.52it/s]Inference:  22%|██▏       | 7/32 [00:02<00:09,  2.50it/s]Inference:  25%|██▌       | 8/32 [00:03<00:09,  2.54it/s]Inference:  28%|██▊       | 9/32 [00:03<00:09,  2.55it/s]Inference:  31%|███▏      | 10/32 [00:03<00:08,  2.57it/s]Inference:  34%|███▍      | 11/32 [00:04<00:08,  2.56it/s]Inference:  38%|███▊      | 12/32 [00:04<00:07,  2.55it/s]Inference:  41%|████      | 13/32 [00:05<00:07,  2.55it/s]Inference:  44%|████▍     | 14/32 [00:05<00:07,  2.55it/s]Inference:  47%|████▋     | 15/32 [00:05<00:06,  2.53it/s]Inference:  50%|█████     | 16/32 [00:06<00:06,  2.53it/s]Inference:  53%|█████▎    | 17/32 [00:06<00:05,  2.54it/s]Inference:  56%|█████▋    | 18/32 [00:07<00:05,  2.51it/s]Inference:  59%|█████▉    | 19/32 [00:07<00:05,  2.50it/s]Inference:  62%|██████▎   | 20/32 [00:07<00:04,  2.52it/s]Inference:  66%|██████▌   | 21/32 [00:08<00:04,  2.53it/s]Inference:  69%|██████▉   | 22/32 [00:08<00:03,  2.53it/s]Inference:  72%|███████▏  | 23/32 [00:09<00:03,  2.53it/s]Inference:  75%|███████▌  | 24/32 [00:09<00:03,  2.53it/s]Inference:  78%|███████▊  | 25/32 [00:09<00:02,  2.55it/s]Inference:  81%|████████▏ | 26/32 [00:10<00:02,  2.54it/s]Inference:  84%|████████▍ | 27/32 [00:10<00:01,  2.51it/s]Inference:  88%|████████▊ | 28/32 [00:11<00:01,  2.53it/s]Inference:  91%|█████████ | 29/32 [00:11<00:01,  2.54it/s]Inference:  94%|█████████▍| 30/32 [00:11<00:00,  2.56it/s]Inference:  97%|█████████▋| 31/32 [00:12<00:00,  2.55it/s]Inference: 100%|██████████| 32/32 [00:12<00:00,  2.56it/s]Inference: 100%|██████████| 32/32 [00:12<00:00,  2.54it/s]
34944 MiB free out of 48676 MiB total
Saved layer 0 to ./models/meta-llama/Llama-2-7b-hf/tensorized/128/64/no_finetune/layer_0.pt
after cast to cpu
38952 MiB free out of 48676 MiB total
running bpv: 0.3393579987046632
Done with layer 0 total_time elapsed: 1883 estimated time left: 58372
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:01<00:31,  1.00s/it]Inference:   6%|▋         | 2/32 [00:01<00:29,  1.01it/s]Inference:   9%|▉         | 3/32 [00:02<00:28,  1.01it/s]Inference:  12%|█▎        | 4/32 [00:03<00:27,  1.02it/s]Inference:  16%|█▌        | 5/32 [00:04<00:26,  1.01it/s]Inference:  19%|█▉        | 6/32 [00:05<00:25,  1.01it/s]Inference:  22%|██▏       | 7/32 [00:06<00:24,  1.01it/s]Inference:  25%|██▌       | 8/32 [00:07<00:23,  1.01it/s]Inference:  28%|██▊       | 9/32 [00:08<00:22,  1.00it/s]Inference:  31%|███▏      | 10/32 [00:09<00:22,  1.00s/it]Inference:  34%|███▍      | 11/32 [00:10<00:21,  1.00s/it]Inference:  38%|███▊      | 12/32 [00:11<00:20,  1.00s/it]Inference:  41%|████      | 13/32 [00:12<00:18,  1.00it/s]Inference:  44%|████▍     | 14/32 [00:13<00:17,  1.00it/s]Inference:  47%|████▋     | 15/32 [00:14<00:16,  1.00it/s]Inference:  50%|█████     | 16/32 [00:15<00:15,  1.00it/s]Inference:  53%|█████▎    | 17/32 [00:16<00:14,  1.00it/s]Inference:  56%|█████▋    | 18/32 [00:17<00:13,  1.00it/s]Inference:  59%|█████▉    | 19/32 [00:18<00:12,  1.00it/s]Inference:  62%|██████▎   | 20/32 [00:19<00:11,  1.00it/s]Inference:  66%|██████▌   | 21/32 [00:20<00:10,  1.02it/s]Inference:  69%|██████▉   | 22/32 [00:21<00:09,  1.01it/s]Inference:  72%|███████▏  | 23/32 [00:22<00:08,  1.01it/s]Inference:  75%|███████▌  | 24/32 [00:23<00:07,  1.01it/s]Inference:  78%|███████▊  | 25/32 [00:24<00:06,  1.00it/s]Inference:  81%|████████▏ | 26/32 [00:25<00:05,  1.01it/s]Inference:  84%|████████▍ | 27/32 [00:26<00:04,  1.01it/s]Inference:  88%|████████▊ | 28/32 [00:27<00:03,  1.02it/s]Inference:  91%|█████████ | 29/32 [00:28<00:02,  1.02it/s]Inference:  94%|█████████▍| 30/32 [00:29<00:01,  1.02it/s]Inference:  97%|█████████▋| 31/32 [00:30<00:00,  1.02it/s]Inference: 100%|██████████| 32/32 [00:31<00:00,  1.02it/s]Inference: 100%|██████████| 32/32 [00:31<00:00,  1.01it/s]
layer1: self_attn.q_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 46457.41796875, val loss None, lr 0.01
iter 250, train loss 463.1588134765625, val loss None, lr 0.01
iter 500, train loss 378.38543701171875, val loss None, lr 0.01
iter 750, train loss 330.9483337402344, val loss None, lr 0.003333
iter 1000, train loss 308.121826171875, val loss None, lr 0.001111
iter 1250, train loss 299.8196105957031, val loss None, lr 0.001111
iter 1500, train loss 295.1351013183594, val loss None, lr 0.00037
iter 1750, train loss 288.0008239746094, val loss None, lr 0.00037
iter 2000, train loss 284.81646728515625, val loss None, lr 0.00037
iter 2250, train loss 282.0716552734375, val loss None, lr 0.00037
best loss 279.9914245605469
running bpv: 0.3528521232057416
layer1: self_attn.k_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 48313.6328125, val loss None, lr 0.01
iter 250, train loss 443.01287841796875, val loss None, lr 0.01
iter 500, train loss 371.14404296875, val loss None, lr 0.003333
iter 750, train loss 356.2000732421875, val loss None, lr 0.003333
iter 1000, train loss 329.6889953613281, val loss None, lr 0.001111
iter 1250, train loss 322.5359802246094, val loss None, lr 0.001111
iter 1500, train loss 316.2183837890625, val loss None, lr 0.001111
iter 1750, train loss 310.66064453125, val loss None, lr 0.00037
iter 2000, train loss 306.82330322265625, val loss None, lr 0.00037
iter 2250, train loss 304.412109375, val loss None, lr 0.00037
best loss 302.0631103515625
running bpv: 0.36442708333333335
layer1: self_attn.v_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 119.49608612060547, val loss None, lr 0.01
iter 250, train loss 20.02281951904297, val loss None, lr 0.01
iter 500, train loss 19.456439971923828, val loss None, lr 0.01
iter 750, train loss 19.261871337890625, val loss None, lr 0.003333
iter 1000, train loss 19.204193115234375, val loss None, lr 0.003333
iter 1250, train loss 19.122875213623047, val loss None, lr 0.003333
iter 1500, train loss 19.136821746826172, val loss None, lr 0.001111
iter 1750, train loss 18.96994400024414, val loss None, lr 0.001111
iter 2000, train loss 18.944931030273438, val loss None, lr 0.001111
iter 2250, train loss 18.926652908325195, val loss None, lr 0.001111
best loss 18.90454864501953
running bpv: 0.3744651192946058
layer1: self_attn.o_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 11.928495407104492, val loss None, lr 0.01
iter 250, train loss 2.0976877212524414, val loss None, lr 0.01
iter 500, train loss 2.0744247436523438, val loss None, lr 0.01
iter 750, train loss 2.061204433441162, val loss None, lr 0.01
iter 1000, train loss 2.0665621757507324, val loss None, lr 0.01
iter 1250, train loss 2.01784610748291, val loss None, lr 0.01
iter 1500, train loss 2.0101284980773926, val loss None, lr 0.003333
iter 1750, train loss 2.0048606395721436, val loss None, lr 0.003333
iter 2000, train loss 2.0042638778686523, val loss None, lr 0.003333
iter 2250, train loss 1.998970866203308, val loss None, lr 0.001111
best loss 1.9955153465270996
running bpv: 0.38325328307393
layer1: mlp.gate_proj
weight shape torch.Size([11008, 4096])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([22, 8, 8, 8]), -3, -2
iter 0, train loss 1915.0023193359375, val loss None, lr 0.01
iter 250, train loss 853.1759643554688, val loss None, lr 0.01
iter 500, train loss 832.8071899414062, val loss None, lr 0.01
iter 750, train loss 825.9048461914062, val loss None, lr 0.01
iter 1000, train loss 813.889404296875, val loss None, lr 0.003333
iter 1250, train loss 813.3591918945312, val loss None, lr 0.003333
iter 1500, train loss 811.5040283203125, val loss None, lr 0.003333
iter 1750, train loss 807.2551879882812, val loss None, lr 0.003333
iter 2000, train loss 805.9574584960938, val loss None, lr 0.003333
iter 2250, train loss 805.5975952148438, val loss None, lr 0.003333
best loss 803.7489013671875
running bpv: 0.3568880208333333
layer1: mlp.up_proj
weight shape torch.Size([11008, 4096])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([22, 8, 8, 8]), -3, -2
iter 0, train loss 849.7416381835938, val loss None, lr 0.01
iter 250, train loss 580.0995483398438, val loss None, lr 0.01
iter 500, train loss 572.192626953125, val loss None, lr 0.01
iter 750, train loss 571.4140014648438, val loss None, lr 0.01
iter 1000, train loss 568.72607421875, val loss None, lr 0.003333
iter 1250, train loss 566.60791015625, val loss None, lr 0.003333
iter 1500, train loss 566.1990966796875, val loss None, lr 0.003333
iter 1750, train loss 565.7581176757812, val loss None, lr 0.001111
iter 2000, train loss 565.2756958007812, val loss None, lr 0.001111
iter 2250, train loss 565.1143798828125, val loss None, lr 0.001111
best loss 564.9056396484375
running bpv: 0.33713329081632654
layer1: mlp.down_proj
weight shape torch.Size([4096, 11008])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 22, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 414.0840759277344, val loss None, lr 0.01
iter 250, train loss 2.6994011402130127, val loss None, lr 0.01
iter 500, train loss 2.5246827602386475, val loss None, lr 0.003333
iter 750, train loss 2.4528968334198, val loss None, lr 0.001111
iter 1000, train loss 2.4342846870422363, val loss None, lr 0.001111
iter 1250, train loss 2.423746109008789, val loss None, lr 0.001111
iter 1500, train loss 2.414851665496826, val loss None, lr 0.001111
iter 1750, train loss 2.41042423248291, val loss None, lr 0.001111
iter 2000, train loss 2.405745029449463, val loss None, lr 0.001111
iter 2250, train loss 2.428581714630127, val loss None, lr 0.001111
best loss 2.3994786739349365
running bpv: 0.3393579987046632
38952 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:11,  2.80it/s]Inference:   6%|▋         | 2/32 [00:00<00:11,  2.67it/s]Inference:   9%|▉         | 3/32 [00:01<00:10,  2.67it/s]Inference:  12%|█▎        | 4/32 [00:01<00:10,  2.65it/s]Inference:  16%|█▌        | 5/32 [00:01<00:10,  2.63it/s]Inference:  19%|█▉        | 6/32 [00:02<00:09,  2.71it/s]Inference:  22%|██▏       | 7/32 [00:02<00:09,  2.65it/s]Inference:  25%|██▌       | 8/32 [00:03<00:09,  2.63it/s]Inference:  28%|██▊       | 9/32 [00:03<00:08,  2.61it/s]Inference:  31%|███▏      | 10/32 [00:03<00:08,  2.59it/s]Inference:  34%|███▍      | 11/32 [00:04<00:08,  2.57it/s]Inference:  38%|███▊      | 12/32 [00:04<00:07,  2.57it/s]Inference:  41%|████      | 13/32 [00:04<00:07,  2.55it/s]Inference:  44%|████▍     | 14/32 [00:05<00:07,  2.57it/s]Inference:  47%|████▋     | 15/32 [00:05<00:06,  2.56it/s]Inference:  50%|█████     | 16/32 [00:06<00:06,  2.65it/s]Inference:  53%|█████▎    | 17/32 [00:06<00:05,  2.62it/s]Inference:  56%|█████▋    | 18/32 [00:06<00:05,  2.61it/s]Inference:  59%|█████▉    | 19/32 [00:07<00:05,  2.60it/s]Inference:  62%|██████▎   | 20/32 [00:07<00:04,  2.60it/s]Inference:  66%|██████▌   | 21/32 [00:08<00:04,  2.59it/s]Inference:  69%|██████▉   | 22/32 [00:08<00:03,  2.59it/s]Inference:  72%|███████▏  | 23/32 [00:08<00:03,  2.56it/s]Inference:  75%|███████▌  | 24/32 [00:09<00:03,  2.55it/s]Inference:  78%|███████▊  | 25/32 [00:09<00:02,  2.55it/s]Inference:  81%|████████▏ | 26/32 [00:10<00:02,  2.56it/s]Inference:  84%|████████▍ | 27/32 [00:10<00:01,  2.54it/s]Inference:  88%|████████▊ | 28/32 [00:10<00:01,  2.52it/s]Inference:  91%|█████████ | 29/32 [00:11<00:01,  2.54it/s]Inference:  94%|█████████▍| 30/32 [00:11<00:00,  2.54it/s]Inference:  97%|█████████▋| 31/32 [00:11<00:00,  2.56it/s]Inference: 100%|██████████| 32/32 [00:12<00:00,  2.58it/s]Inference: 100%|██████████| 32/32 [00:12<00:00,  2.59it/s]
32344 MiB free out of 48676 MiB total
Saved layer 1 to ./models/meta-llama/Llama-2-7b-hf/tensorized/128/64/no_finetune/layer_1.pt
after cast to cpu
36374 MiB free out of 48676 MiB total
running bpv: 0.3393579987046632
Done with layer 1 total_time elapsed: 3778 estimated time left: 56677
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:01<00:31,  1.00s/it]Inference:   6%|▋         | 2/32 [00:01<00:29,  1.00it/s]Inference:   9%|▉         | 3/32 [00:02<00:28,  1.00it/s]Inference:  12%|█▎        | 4/32 [00:03<00:27,  1.00it/s]Inference:  16%|█▌        | 5/32 [00:04<00:26,  1.00it/s]Inference:  19%|█▉        | 6/32 [00:05<00:25,  1.00it/s]Inference:  22%|██▏       | 7/32 [00:06<00:24,  1.00it/s]Inference:  25%|██▌       | 8/32 [00:07<00:23,  1.01it/s]Inference:  28%|██▊       | 9/32 [00:08<00:22,  1.00it/s]Inference:  31%|███▏      | 10/32 [00:09<00:21,  1.00it/s]Inference:  34%|███▍      | 11/32 [00:10<00:21,  1.00s/it]Inference:  38%|███▊      | 12/32 [00:11<00:20,  1.00s/it]Inference:  41%|████      | 13/32 [00:12<00:18,  1.00it/s]Inference:  44%|████▍     | 14/32 [00:13<00:17,  1.00it/s]Inference:  47%|████▋     | 15/32 [00:14<00:16,  1.01it/s]Inference:  50%|█████     | 16/32 [00:15<00:15,  1.01it/s]Inference:  53%|█████▎    | 17/32 [00:16<00:14,  1.01it/s]Inference:  56%|█████▋    | 18/32 [00:17<00:13,  1.01it/s]Inference:  59%|█████▉    | 19/32 [00:18<00:12,  1.01it/s]Inference:  62%|██████▎   | 20/32 [00:19<00:11,  1.01it/s]Inference:  66%|██████▌   | 21/32 [00:20<00:10,  1.01it/s]Inference:  69%|██████▉   | 22/32 [00:21<00:09,  1.01it/s]Inference:  72%|███████▏  | 23/32 [00:22<00:08,  1.01it/s]Inference:  75%|███████▌  | 24/32 [00:23<00:07,  1.01it/s]Inference:  78%|███████▊  | 25/32 [00:24<00:06,  1.01it/s]Inference:  81%|████████▏ | 26/32 [00:25<00:05,  1.01it/s]Inference:  84%|████████▍ | 27/32 [00:26<00:04,  1.01it/s]Inference:  88%|████████▊ | 28/32 [00:27<00:03,  1.01it/s]Inference:  91%|█████████ | 29/32 [00:28<00:02,  1.01it/s]Inference:  94%|█████████▍| 30/32 [00:29<00:01,  1.01it/s]Inference:  97%|█████████▋| 31/32 [00:30<00:00,  1.01it/s]Inference: 100%|██████████| 32/32 [00:31<00:00,  1.02it/s]Inference: 100%|██████████| 32/32 [00:31<00:00,  1.01it/s]
layer2: self_attn.q_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 104392.328125, val loss None, lr 0.01
iter 250, train loss 1299.5750732421875, val loss None, lr 0.01
iter 500, train loss 1158.272216796875, val loss None, lr 0.01
iter 750, train loss 1125.881103515625, val loss None, lr 0.01
iter 1000, train loss 1094.2896728515625, val loss None, lr 0.01
iter 1250, train loss 1022.0087890625, val loss None, lr 0.003333
iter 1500, train loss 1015.615966796875, val loss None, lr 0.003333
iter 1750, train loss 1005.4887084960938, val loss None, lr 0.003333
iter 2000, train loss 1003.319580078125, val loss None, lr 0.003333
iter 2250, train loss 1001.00341796875, val loss None, lr 0.003333
best loss 985.2177734375
running bpv: 0.34637360074626866
layer2: self_attn.k_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 137761.6875, val loss None, lr 0.01
iter 250, train loss 1673.823974609375, val loss None, lr 0.01
iter 500, train loss 1318.904052734375, val loss None, lr 0.01
iter 750, train loss 1177.2822265625, val loss None, lr 0.003333
iter 1000, train loss 1132.69482421875, val loss None, lr 0.001111
iter 1250, train loss 1112.931396484375, val loss None, lr 0.001111
iter 1500, train loss 1103.196533203125, val loss None, lr 0.001111
iter 1750, train loss 1095.1566162109375, val loss None, lr 0.001111
iter 2000, train loss 1086.441162109375, val loss None, lr 0.00037
iter 2250, train loss 1081.8555908203125, val loss None, lr 0.00037
best loss 1077.369140625
running bpv: 0.3528521232057416
layer2: self_attn.v_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 754.0284423828125, val loss None, lr 0.01
iter 250, train loss 221.73097229003906, val loss None, lr 0.01
iter 500, train loss 216.45486450195312, val loss None, lr 0.01
iter 750, train loss 215.57321166992188, val loss None, lr 0.01
iter 1000, train loss 213.6573486328125, val loss None, lr 0.01
iter 1250, train loss 213.13352966308594, val loss None, lr 0.01
iter 1500, train loss 212.1469268798828, val loss None, lr 0.003333
iter 1750, train loss 211.5323486328125, val loss None, lr 0.001111
iter 2000, train loss 211.35264587402344, val loss None, lr 0.001111
iter 2250, train loss 211.1255340576172, val loss None, lr 0.00037
best loss 211.0183563232422
running bpv: 0.35885296658986177
layer2: self_attn.o_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 14.987159729003906, val loss None, lr 0.01
iter 250, train loss 3.5849218368530273, val loss None, lr 0.01
iter 500, train loss 3.4926419258117676, val loss None, lr 0.01
iter 750, train loss 3.448679208755493, val loss None, lr 0.01
iter 1000, train loss 3.4292635917663574, val loss None, lr 0.01
iter 1250, train loss 3.404503345489502, val loss None, lr 0.003333
iter 1500, train loss 3.390838146209717, val loss None, lr 0.001111
iter 1750, train loss 3.3855180740356445, val loss None, lr 0.001111
iter 2000, train loss 3.3830692768096924, val loss None, lr 0.001111
iter 2250, train loss 3.379399299621582, val loss None, lr 0.001111
best loss 3.378013849258423
running bpv: 0.36442708333333335
layer2: mlp.gate_proj
weight shape torch.Size([11008, 4096])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([22, 8, 8, 8]), -3, -2
iter 0, train loss 1720.610595703125, val loss None, lr 0.01
iter 250, train loss 998.6843872070312, val loss None, lr 0.01
iter 500, train loss 984.0086669921875, val loss None, lr 0.01
iter 750, train loss 980.8828735351562, val loss None, lr 0.01
iter 1000, train loss 973.901123046875, val loss None, lr 0.01
iter 1250, train loss 971.251953125, val loss None, lr 0.01
iter 1500, train loss 972.033447265625, val loss None, lr 0.01
iter 1750, train loss 972.4540405273438, val loss None, lr 0.01
iter 2000, train loss 965.2431640625, val loss None, lr 0.003333
iter 2250, train loss 964.1199951171875, val loss None, lr 0.003333
best loss 962.9404296875
running bpv: 0.35002535496957404
layer2: mlp.up_proj
weight shape torch.Size([11008, 4096])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([22, 8, 8, 8]), -3, -2
iter 0, train loss 1102.428955078125, val loss None, lr 0.01
iter 250, train loss 740.7998046875, val loss None, lr 0.01
iter 500, train loss 731.8275146484375, val loss None, lr 0.01
iter 750, train loss 728.3082885742188, val loss None, lr 0.01
iter 1000, train loss 726.0828247070312, val loss None, lr 0.01
iter 1250, train loss 725.1483764648438, val loss None, lr 0.01
iter 1500, train loss 723.2088623046875, val loss None, lr 0.003333
iter 1750, train loss 722.223876953125, val loss None, lr 0.003333
iter 2000, train loss 722.5348510742188, val loss None, lr 0.003333
iter 2250, train loss 721.2554321289062, val loss None, lr 0.001111
best loss 721.0095825195312
running bpv: 0.33793435167910446
layer2: mlp.down_proj
weight shape torch.Size([4096, 11008])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 22, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 4.559598922729492, val loss None, lr 0.01
iter 250, train loss 2.1019809246063232, val loss None, lr 0.01
iter 500, train loss 2.0838727951049805, val loss None, lr 0.01
iter 750, train loss 2.081678867340088, val loss None, lr 0.01
iter 1000, train loss 2.0718259811401367, val loss None, lr 0.01
iter 1250, train loss 2.063408851623535, val loss None, lr 0.003333
iter 1500, train loss 2.0633387565612793, val loss None, lr 0.003333
iter 1750, train loss 2.0607099533081055, val loss None, lr 0.003333
iter 2000, train loss 2.0591514110565186, val loss None, lr 0.001111
iter 2250, train loss 2.058563709259033, val loss None, lr 0.001111
best loss 2.0579886436462402
running bpv: 0.3393579987046632
36374 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:11,  2.80it/s]Inference:   6%|▋         | 2/32 [00:00<00:11,  2.67it/s]Inference:   9%|▉         | 3/32 [00:01<00:11,  2.63it/s]Inference:  12%|█▎        | 4/32 [00:01<00:10,  2.61it/s]Inference:  16%|█▌        | 5/32 [00:01<00:10,  2.60it/s]Inference:  19%|█▉        | 6/32 [00:02<00:09,  2.60it/s]Inference:  22%|██▏       | 7/32 [00:02<00:09,  2.60it/s]Inference:  25%|██▌       | 8/32 [00:03<00:09,  2.60it/s]Inference:  28%|██▊       | 9/32 [00:03<00:08,  2.60it/s]Inference:  31%|███▏      | 10/32 [00:03<00:08,  2.60it/s]Inference:  34%|███▍      | 11/32 [00:04<00:08,  2.61it/s]Inference:  38%|███▊      | 12/32 [00:04<00:07,  2.61it/s]Inference:  41%|████      | 13/32 [00:04<00:07,  2.61it/s]Inference:  44%|████▍     | 14/32 [00:05<00:06,  2.60it/s]Inference:  47%|████▋     | 15/32 [00:05<00:06,  2.59it/s]Inference:  50%|█████     | 16/32 [00:06<00:06,  2.58it/s]Inference:  53%|█████▎    | 17/32 [00:06<00:05,  2.59it/s]Inference:  56%|█████▋    | 18/32 [00:06<00:05,  2.59it/s]Inference:  59%|█████▉    | 19/32 [00:07<00:05,  2.58it/s]Inference:  62%|██████▎   | 20/32 [00:07<00:04,  2.58it/s]Inference:  66%|██████▌   | 21/32 [00:08<00:04,  2.58it/s]Inference:  69%|██████▉   | 22/32 [00:08<00:03,  2.58it/s]Inference:  72%|███████▏  | 23/32 [00:08<00:03,  2.60it/s]Inference:  75%|███████▌  | 24/32 [00:09<00:03,  2.59it/s]Inference:  78%|███████▊  | 25/32 [00:09<00:02,  2.60it/s]Inference:  81%|████████▏ | 26/32 [00:10<00:02,  2.59it/s]Inference:  84%|████████▍ | 27/32 [00:10<00:01,  2.60it/s]Inference:  88%|████████▊ | 28/32 [00:10<00:01,  2.61it/s]Inference:  91%|█████████ | 29/32 [00:11<00:01,  2.63it/s]Inference:  94%|█████████▍| 30/32 [00:11<00:00,  2.62it/s]Inference:  97%|█████████▋| 31/32 [00:11<00:00,  2.63it/s]Inference: 100%|██████████| 32/32 [00:12<00:00,  2.62it/s]Inference: 100%|██████████| 32/32 [00:12<00:00,  2.61it/s]
29766 MiB free out of 48676 MiB total
Saved layer 2 to ./models/meta-llama/Llama-2-7b-hf/tensorized/128/64/no_finetune/layer_2.pt
after cast to cpu
33796 MiB free out of 48676 MiB total
running bpv: 0.3393579987046632
Done with layer 2 total_time elapsed: 5668 estimated time left: 54795
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:29,  1.04it/s]Inference:   6%|▋         | 2/32 [00:01<00:29,  1.03it/s]Inference:   9%|▉         | 3/32 [00:02<00:28,  1.03it/s]Inference:  12%|█▎        | 4/32 [00:03<00:27,  1.03it/s]Inference:  16%|█▌        | 5/32 [00:04<00:26,  1.03it/s]Inference:  19%|█▉        | 6/32 [00:05<00:25,  1.03it/s]Inference:  22%|██▏       | 7/32 [00:06<00:24,  1.03it/s]Inference:  25%|██▌       | 8/32 [00:07<00:23,  1.03it/s]Inference:  28%|██▊       | 9/32 [00:08<00:22,  1.03it/s]Inference:  31%|███▏      | 10/32 [00:09<00:21,  1.03it/s]Inference:  34%|███▍      | 11/32 [00:10<00:20,  1.03it/s]Inference:  38%|███▊      | 12/32 [00:11<00:19,  1.03it/s]Inference:  41%|████      | 13/32 [00:12<00:18,  1.03it/s]Inference:  44%|████▍     | 14/32 [00:13<00:17,  1.02it/s]Inference:  47%|████▋     | 15/32 [00:14<00:16,  1.02it/s]Inference:  50%|█████     | 16/32 [00:15<00:15,  1.03it/s]Inference:  53%|█████▎    | 17/32 [00:16<00:14,  1.02it/s]Inference:  56%|█████▋    | 18/32 [00:17<00:13,  1.03it/s]Inference:  59%|█████▉    | 19/32 [00:18<00:12,  1.02it/s]Inference:  62%|██████▎   | 20/32 [00:19<00:11,  1.03it/s]Inference:  66%|██████▌   | 21/32 [00:20<00:10,  1.02it/s]Inference:  69%|██████▉   | 22/32 [00:21<00:09,  1.02it/s]Inference:  72%|███████▏  | 23/32 [00:22<00:08,  1.02it/s]Inference:  75%|███████▌  | 24/32 [00:23<00:07,  1.02it/s]Inference:  78%|███████▊  | 25/32 [00:24<00:06,  1.02it/s]Inference:  81%|████████▏ | 26/32 [00:25<00:05,  1.03it/s]Inference:  84%|████████▍ | 27/32 [00:26<00:04,  1.03it/s]Inference:  88%|████████▊ | 28/32 [00:27<00:03,  1.03it/s]Inference:  91%|█████████ | 29/32 [00:28<00:02,  1.03it/s]Inference:  94%|█████████▍| 30/32 [00:29<00:01,  1.03it/s]Inference:  97%|█████████▋| 31/32 [00:30<00:00,  1.03it/s]Inference: 100%|██████████| 32/32 [00:31<00:00,  1.03it/s]Inference: 100%|██████████| 32/32 [00:31<00:00,  1.03it/s]
layer3: self_attn.q_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 100089.21875, val loss None, lr 0.01
iter 250, train loss 2502.757080078125, val loss None, lr 0.01
iter 500, train loss 2322.83251953125, val loss None, lr 0.01
iter 750, train loss 2212.80078125, val loss None, lr 0.01
iter 1000, train loss 2175.74560546875, val loss None, lr 0.01
iter 1250, train loss 2122.397216796875, val loss None, lr 0.003333
iter 1500, train loss 2112.63330078125, val loss None, lr 0.001111
iter 1750, train loss 2092.37158203125, val loss None, lr 0.001111
iter 2000, train loss 2084.962890625, val loss None, lr 0.001111
iter 2250, train loss 2081.529541015625, val loss None, lr 0.001111
best loss 2073.503173828125
running bpv: 0.34409795168067225
layer3: self_attn.k_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 133284.5, val loss None, lr 0.01
iter 250, train loss 2625.85009765625, val loss None, lr 0.01
iter 500, train loss 2452.442138671875, val loss None, lr 0.01
iter 750, train loss 2511.17236328125, val loss None, lr 0.01
iter 1000, train loss 2270.72119140625, val loss None, lr 0.003333
iter 1250, train loss 2223.0361328125, val loss None, lr 0.001111
iter 1500, train loss 2209.1376953125, val loss None, lr 0.001111
iter 1750, train loss 2209.513671875, val loss None, lr 0.001111
iter 2000, train loss 2186.399658203125, val loss None, lr 0.00037
iter 2250, train loss 2180.054931640625, val loss None, lr 0.00037
best loss 2175.28271484375
running bpv: 0.3485896583469722
layer3: self_attn.v_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 1739.834716796875, val loss None, lr 0.01
iter 250, train loss 496.6455383300781, val loss None, lr 0.01
iter 500, train loss 485.9672546386719, val loss None, lr 0.01
iter 750, train loss 482.2061767578125, val loss None, lr 0.01
iter 1000, train loss 479.20721435546875, val loss None, lr 0.01
iter 1250, train loss 476.24493408203125, val loss None, lr 0.003333
iter 1500, train loss 474.654052734375, val loss None, lr 0.001111
iter 1750, train loss 474.00732421875, val loss None, lr 0.001111
iter 2000, train loss 473.5072021484375, val loss None, lr 0.001111
iter 2250, train loss 473.17425537109375, val loss None, lr 0.001111
best loss 472.793701171875
running bpv: 0.3528521232057416
layer3: self_attn.o_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 37.48145294189453, val loss None, lr 0.01
iter 250, train loss 3.648592948913574, val loss None, lr 0.01
iter 500, train loss 3.3126840591430664, val loss None, lr 0.01
iter 750, train loss 3.1024608612060547, val loss None, lr 0.003333
iter 1000, train loss 3.1509804725646973, val loss None, lr 0.003333
iter 1250, train loss 3.039316177368164, val loss None, lr 0.003333
iter 1500, train loss 3.011612892150879, val loss None, lr 0.001111
iter 1750, train loss 3.0048556327819824, val loss None, lr 0.001111
iter 2000, train loss 3.034604072570801, val loss None, lr 0.001111
iter 2250, train loss 2.9857254028320312, val loss None, lr 0.00037
best loss 2.976714611053467
running bpv: 0.3569024591757387
layer3: mlp.gate_proj
weight shape torch.Size([11008, 4096])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([22, 8, 8, 8]), -3, -2
iter 0, train loss 2320.114501953125, val loss None, lr 0.01
iter 250, train loss 1228.1339111328125, val loss None, lr 0.01
iter 500, train loss 1206.987060546875, val loss None, lr 0.01
iter 750, train loss 1198.9388427734375, val loss None, lr 0.01
iter 1000, train loss 1202.3861083984375, val loss None, lr 0.01
iter 1250, train loss 1181.8668212890625, val loss None, lr 0.003333
iter 1500, train loss 1181.451171875, val loss None, lr 0.003333
iter 1750, train loss 1178.4320068359375, val loss None, lr 0.001111
iter 2000, train loss 1178.4327392578125, val loss None, lr 0.001111
iter 2250, train loss 1176.268310546875, val loss None, lr 0.001111
best loss 1175.5030517578125
running bpv: 0.3470241891399417
layer3: mlp.up_proj
weight shape torch.Size([11008, 4096])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([22, 8, 8, 8]), -3, -2
iter 0, train loss 1662.01513671875, val loss None, lr 0.01
iter 250, train loss 960.2323608398438, val loss None, lr 0.01
iter 500, train loss 948.7425537109375, val loss None, lr 0.01
iter 750, train loss 943.3010864257812, val loss None, lr 0.01
iter 1000, train loss 938.4127197265625, val loss None, lr 0.01
iter 1250, train loss 938.9598388671875, val loss None, lr 0.01
iter 1500, train loss 932.5208129882812, val loss None, lr 0.003333
iter 1750, train loss 931.8209228515625, val loss None, lr 0.003333
iter 2000, train loss 930.194091796875, val loss None, lr 0.003333
iter 2250, train loss 929.1898193359375, val loss None, lr 0.001111
best loss 928.6151123046875
running bpv: 0.33831125685871055
layer3: mlp.down_proj
weight shape torch.Size([4096, 11008])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 22, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 10.211517333984375, val loss None, lr 0.01
iter 250, train loss 3.7287163734436035, val loss None, lr 0.01
iter 500, train loss 3.6822397708892822, val loss None, lr 0.01
iter 750, train loss 3.685046434402466, val loss None, lr 0.01
iter 1000, train loss 3.66251277923584, val loss None, lr 0.01
iter 1250, train loss 3.660038471221924, val loss None, lr 0.01
iter 1500, train loss 3.6231794357299805, val loss None, lr 0.003333
iter 1750, train loss 3.6262032985687256, val loss None, lr 0.003333
iter 2000, train loss 3.613128662109375, val loss None, lr 0.001111
iter 2250, train loss 3.6113474369049072, val loss None, lr 0.00037
best loss 3.6103365421295166
running bpv: 0.3393579987046632
33796 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:17,  1.76it/s]Inference:   6%|▋         | 2/32 [00:01<00:16,  1.81it/s]Inference:   9%|▉         | 3/32 [00:01<00:16,  1.77it/s]Inference:  12%|█▎        | 4/32 [00:02<00:15,  1.80it/s]Inference:  16%|█▌        | 5/32 [00:02<00:15,  1.79it/s]Inference:  19%|█▉        | 6/32 [00:03<00:14,  1.81it/s]Inference:  22%|██▏       | 7/32 [00:03<00:13,  1.80it/s]Inference:  25%|██▌       | 8/32 [00:04<00:13,  1.82it/s]Inference:  28%|██▊       | 9/32 [00:04<00:12,  1.82it/s]Inference:  31%|███▏      | 10/32 [00:05<00:12,  1.82it/s]Inference:  34%|███▍      | 11/32 [00:06<00:11,  1.80it/s]Inference:  38%|███▊      | 12/32 [00:06<00:10,  1.85it/s]Inference:  41%|████      | 13/32 [00:07<00:10,  1.81it/s]Inference:  44%|████▍     | 14/32 [00:07<00:09,  1.81it/s]Inference:  47%|████▋     | 15/32 [00:08<00:09,  1.79it/s]Inference:  50%|█████     | 16/32 [00:08<00:08,  1.80it/s]Inference:  53%|█████▎    | 17/32 [00:09<00:08,  1.81it/s]Inference:  56%|█████▋    | 18/32 [00:09<00:07,  1.80it/s]Inference:  59%|█████▉    | 19/32 [00:10<00:07,  1.78it/s]Inference:  62%|██████▎   | 20/32 [00:11<00:06,  1.81it/s]Inference:  66%|██████▌   | 21/32 [00:11<00:06,  1.82it/s]Inference:  69%|██████▉   | 22/32 [00:12<00:05,  1.80it/s]Inference:  72%|███████▏  | 23/32 [00:12<00:04,  1.84it/s]Inference:  75%|███████▌  | 24/32 [00:13<00:04,  1.84it/s]Inference:  78%|███████▊  | 25/32 [00:13<00:03,  1.84it/s]Inference:  81%|████████▏ | 26/32 [00:14<00:03,  1.85it/s]Inference:  84%|████████▍ | 27/32 [00:14<00:02,  1.86it/s]Inference:  88%|████████▊ | 28/32 [00:15<00:02,  1.86it/s]Inference:  91%|█████████ | 29/32 [00:15<00:01,  1.86it/s]Inference:  94%|█████████▍| 30/32 [00:16<00:01,  1.86it/s]Inference:  97%|█████████▋| 31/32 [00:17<00:00,  1.86it/s]Inference: 100%|██████████| 32/32 [00:17<00:00,  1.85it/s]Inference: 100%|██████████| 32/32 [00:17<00:00,  1.82it/s]
27316 MiB free out of 48676 MiB total
Saved layer 3 to ./models/meta-llama/Llama-2-7b-hf/tensorized/128/64/no_finetune/layer_3.pt
after cast to cpu
31346 MiB free out of 48676 MiB total
running bpv: 0.3393579987046632
Done with layer 3 total_time elapsed: 7582 estimated time left: 53074
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:30,  1.01it/s]Inference:   6%|▋         | 2/32 [00:01<00:29,  1.01it/s]Inference:   9%|▉         | 3/32 [00:02<00:28,  1.01it/s]Inference:  12%|█▎        | 4/32 [00:03<00:27,  1.00it/s]Inference:  16%|█▌        | 5/32 [00:04<00:27,  1.00s/it]Inference:  19%|█▉        | 6/32 [00:05<00:25,  1.02it/s]Inference:  22%|██▏       | 7/32 [00:06<00:24,  1.01it/s]Inference:  25%|██▌       | 8/32 [00:07<00:23,  1.01it/s]Inference:  28%|██▊       | 9/32 [00:08<00:22,  1.01it/s]Inference:  31%|███▏      | 10/32 [00:09<00:21,  1.01it/s]Inference:  34%|███▍      | 11/32 [00:10<00:20,  1.00it/s]Inference:  38%|███▊      | 12/32 [00:11<00:19,  1.01it/s]Inference:  41%|████      | 13/32 [00:12<00:18,  1.00it/s]Inference:  44%|████▍     | 14/32 [00:13<00:17,  1.00it/s]Inference:  47%|████▋     | 15/32 [00:14<00:16,  1.00it/s]Inference:  50%|█████     | 16/32 [00:15<00:15,  1.00it/s]Inference:  53%|█████▎    | 17/32 [00:16<00:15,  1.00s/it]Inference:  56%|█████▋    | 18/32 [00:17<00:13,  1.00it/s]Inference:  59%|█████▉    | 19/32 [00:18<00:12,  1.00it/s]Inference:  62%|██████▎   | 20/32 [00:19<00:11,  1.00it/s]Inference:  66%|██████▌   | 21/32 [00:20<00:11,  1.00s/it]Inference:  69%|██████▉   | 22/32 [00:21<00:10,  1.00s/it]Inference:  72%|███████▏  | 23/32 [00:22<00:08,  1.00it/s]Inference:  75%|███████▌  | 24/32 [00:23<00:07,  1.00it/s]Inference:  78%|███████▊  | 25/32 [00:24<00:06,  1.02it/s]Inference:  81%|████████▏ | 26/32 [00:25<00:05,  1.01it/s]Inference:  84%|████████▍ | 27/32 [00:26<00:04,  1.01it/s]Inference:  88%|████████▊ | 28/32 [00:27<00:03,  1.00it/s]Inference:  91%|█████████ | 29/32 [00:28<00:02,  1.01it/s]Inference:  94%|█████████▍| 30/32 [00:29<00:01,  1.04it/s]Inference:  97%|█████████▋| 31/32 [00:30<00:00,  1.04it/s]Inference: 100%|██████████| 32/32 [00:31<00:00,  1.05it/s]Inference: 100%|██████████| 32/32 [00:31<00:00,  1.01it/s]
layer4: self_attn.q_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 104601.703125, val loss None, lr 0.01
iter 250, train loss 2144.736572265625, val loss None, lr 0.01
iter 500, train loss 1943.8052978515625, val loss None, lr 0.01
iter 750, train loss 1923.54931640625, val loss None, lr 0.01
iter 1000, train loss 1813.540771484375, val loss None, lr 0.003333
iter 1250, train loss 1876.8485107421875, val loss None, lr 0.003333
iter 1500, train loss 1763.4727783203125, val loss None, lr 0.001111
iter 1750, train loss 1754.8741455078125, val loss None, lr 0.001111
iter 2000, train loss 1744.6732177734375, val loss None, lr 0.00037
iter 2250, train loss 1739.2442626953125, val loss None, lr 0.00037
best loss 1734.08837890625
running bpv: 0.34293702411167515
layer4: self_attn.k_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 147190.4375, val loss None, lr 0.01
iter 250, train loss 2264.574951171875, val loss None, lr 0.01
iter 500, train loss 1989.04248046875, val loss None, lr 0.01
iter 750, train loss 1954.0606689453125, val loss None, lr 0.003333
iter 1000, train loss 1847.620361328125, val loss None, lr 0.003333
iter 1250, train loss 1793.2392578125, val loss None, lr 0.001111
iter 1500, train loss 1778.05078125, val loss None, lr 0.001111
iter 1750, train loss 1767.901611328125, val loss None, lr 0.001111
iter 2000, train loss 1761.8377685546875, val loss None, lr 0.001111
iter 2250, train loss 1751.0164794921875, val loss None, lr 0.00037
best loss 1744.7100830078125
running bpv: 0.34637360074626866
layer4: self_attn.v_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 2151.90478515625, val loss None, lr 0.01
iter 250, train loss 418.7813720703125, val loss None, lr 0.01
iter 500, train loss 410.3626403808594, val loss None, lr 0.01
iter 750, train loss 403.03240966796875, val loss None, lr 0.01
iter 1000, train loss 399.3375549316406, val loss None, lr 0.01
iter 1250, train loss 399.038330078125, val loss None, lr 0.01
iter 1500, train loss 395.5685729980469, val loss None, lr 0.01
iter 1750, train loss 396.2760009765625, val loss None, lr 0.01
iter 2000, train loss 392.9176940917969, val loss None, lr 0.003333
iter 2250, train loss 391.64117431640625, val loss None, lr 0.001111
best loss 391.2095947265625
running bpv: 0.3496760670731707
layer4: self_attn.o_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 84.60958862304688, val loss None, lr 0.01
iter 250, train loss 5.498964786529541, val loss None, lr 0.01
iter 500, train loss 4.716858386993408, val loss None, lr 0.003333
iter 750, train loss 4.42552375793457, val loss None, lr 0.003333
iter 1000, train loss 4.367725372314453, val loss None, lr 0.003333
iter 1250, train loss 4.362482070922852, val loss None, lr 0.003333
iter 1500, train loss 4.289227485656738, val loss None, lr 0.001111
iter 1750, train loss 4.277318477630615, val loss None, lr 0.001111
iter 2000, train loss 4.255372047424316, val loss None, lr 0.001111
iter 2250, train loss 4.240267276763916, val loss None, lr 0.001111
best loss 4.22686767578125
running bpv: 0.3528521232057416
layer4: mlp.gate_proj
weight shape torch.Size([11008, 4096])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([22, 8, 8, 8]), -3, -2
iter 0, train loss 5152.9267578125, val loss None, lr 0.01
iter 250, train loss 1948.248291015625, val loss None, lr 0.01
iter 500, train loss 1913.1302490234375, val loss None, lr 0.01
iter 750, train loss 1894.068603515625, val loss None, lr 0.01
iter 1000, train loss 1865.4107666015625, val loss None, lr 0.003333
iter 1250, train loss 1855.92822265625, val loss None, lr 0.003333
iter 1500, train loss 1850.257568359375, val loss None, lr 0.003333
iter 1750, train loss 1847.7110595703125, val loss None, lr 0.003333
iter 2000, train loss 1843.239501953125, val loss None, lr 0.003333
iter 2250, train loss 1841.005859375, val loss None, lr 0.003333
best loss 1835.551025390625
running bpv: 0.345340941410694
layer4: mlp.up_proj
weight shape torch.Size([11008, 4096])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([22, 8, 8, 8]), -3, -2
iter 0, train loss 3099.09765625, val loss None, lr 0.01
iter 250, train loss 1400.22265625, val loss None, lr 0.01
iter 500, train loss 1386.281494140625, val loss None, lr 0.01
iter 750, train loss 1367.373046875, val loss None, lr 0.01
iter 1000, train loss 1350.690185546875, val loss None, lr 0.003333
iter 1250, train loss 1345.9542236328125, val loss None, lr 0.003333
iter 1500, train loss 1343.4971923828125, val loss None, lr 0.003333
iter 1750, train loss 1341.0611572265625, val loss None, lr 0.003333
iter 2000, train loss 1340.1036376953125, val loss None, lr 0.003333
iter 2250, train loss 1335.9970703125, val loss None, lr 0.001111
best loss 1334.67333984375
running bpv: 0.33853036876355747
layer4: mlp.down_proj
weight shape torch.Size([4096, 11008])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 22, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 46.942466735839844, val loss None, lr 0.01
iter 250, train loss 10.420639991760254, val loss None, lr 0.01
iter 500, train loss 10.102272987365723, val loss None, lr 0.01
iter 750, train loss 10.005048751831055, val loss None, lr 0.01
iter 1000, train loss 9.935835838317871, val loss None, lr 0.01
iter 1250, train loss 9.814435005187988, val loss None, lr 0.003333
iter 1500, train loss 9.872371673583984, val loss None, lr 0.003333
iter 1750, train loss 9.773258209228516, val loss None, lr 0.001111
iter 2000, train loss 9.759425163269043, val loss None, lr 0.001111
iter 2250, train loss 9.748445510864258, val loss None, lr 0.00037
best loss 9.740899085998535
running bpv: 0.3393579987046632
31346 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:11,  2.74it/s]Inference:   6%|▋         | 2/32 [00:00<00:11,  2.56it/s]Inference:   9%|▉         | 3/32 [00:01<00:11,  2.49it/s]Inference:  12%|█▎        | 4/32 [00:01<00:11,  2.46it/s]Inference:  16%|█▌        | 5/32 [00:02<00:11,  2.43it/s]Inference:  19%|█▉        | 6/32 [00:02<00:10,  2.41it/s]Inference:  22%|██▏       | 7/32 [00:02<00:10,  2.41it/s]Inference:  25%|██▌       | 8/32 [00:03<00:10,  2.39it/s]Inference:  28%|██▊       | 9/32 [00:03<00:09,  2.39it/s]Inference:  31%|███▏      | 10/32 [00:04<00:09,  2.37it/s]Inference:  34%|███▍      | 11/32 [00:04<00:08,  2.39it/s]Inference:  38%|███▊      | 12/32 [00:04<00:08,  2.40it/s]Inference:  41%|████      | 13/32 [00:05<00:07,  2.40it/s]Inference:  44%|████▍     | 14/32 [00:05<00:07,  2.41it/s]Inference:  47%|████▋     | 15/32 [00:06<00:07,  2.40it/s]Inference:  50%|█████     | 16/32 [00:06<00:06,  2.41it/s]Inference:  53%|█████▎    | 17/32 [00:07<00:06,  2.41it/s]Inference:  56%|█████▋    | 18/32 [00:07<00:05,  2.42it/s]Inference:  59%|█████▉    | 19/32 [00:07<00:05,  2.42it/s]Inference:  62%|██████▎   | 20/32 [00:08<00:04,  2.55it/s]Inference:  66%|██████▌   | 21/32 [00:08<00:04,  2.50it/s]Inference:  69%|██████▉   | 22/32 [00:09<00:04,  2.47it/s]Inference:  72%|███████▏  | 23/32 [00:09<00:03,  2.47it/s]Inference:  75%|███████▌  | 24/32 [00:09<00:03,  2.45it/s]Inference:  78%|███████▊  | 25/32 [00:10<00:02,  2.44it/s]Inference:  81%|████████▏ | 26/32 [00:10<00:02,  2.43it/s]Inference:  84%|████████▍ | 27/32 [00:11<00:02,  2.43it/s]Inference:  88%|████████▊ | 28/32 [00:11<00:01,  2.43it/s]Inference:  91%|█████████ | 29/32 [00:11<00:01,  2.43it/s]Inference:  94%|█████████▍| 30/32 [00:12<00:00,  2.42it/s]Inference:  97%|█████████▋| 31/32 [00:12<00:00,  2.42it/s]Inference: 100%|██████████| 32/32 [00:13<00:00,  2.39it/s]Inference: 100%|██████████| 32/32 [00:13<00:00,  2.43it/s]
24738 MiB free out of 48676 MiB total
Saved layer 4 to ./models/meta-llama/Llama-2-7b-hf/tensorized/128/64/no_finetune/layer_4.pt
after cast to cpu
28768 MiB free out of 48676 MiB total
running bpv: 0.3393579987046632
Done with layer 4 total_time elapsed: 9476 estimated time left: 51172
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:01<00:33,  1.09s/it]Inference:   6%|▋         | 2/32 [00:02<00:32,  1.08s/it]Inference:   9%|▉         | 3/32 [00:03<00:31,  1.09s/it]Inference:  12%|█▎        | 4/32 [00:04<00:30,  1.09s/it]Inference:  16%|█▌        | 5/32 [00:05<00:29,  1.08s/it]Inference:  19%|█▉        | 6/32 [00:06<00:28,  1.08s/it]Inference:  22%|██▏       | 7/32 [00:07<00:27,  1.08s/it]Inference:  25%|██▌       | 8/32 [00:08<00:25,  1.08s/it]Inference:  28%|██▊       | 9/32 [00:09<00:24,  1.08s/it]Inference:  31%|███▏      | 10/32 [00:10<00:23,  1.08s/it]Inference:  34%|███▍      | 11/32 [00:11<00:22,  1.08s/it]Inference:  38%|███▊      | 12/32 [00:12<00:21,  1.08s/it]Inference:  41%|████      | 13/32 [00:14<00:20,  1.08s/it]Inference:  44%|████▍     | 14/32 [00:15<00:19,  1.08s/it]Inference:  47%|████▋     | 15/32 [00:16<00:18,  1.08s/it]Inference:  50%|█████     | 16/32 [00:17<00:17,  1.07s/it]Inference:  53%|█████▎    | 17/32 [00:18<00:16,  1.07s/it]Inference:  56%|█████▋    | 18/32 [00:19<00:14,  1.07s/it]Inference:  59%|█████▉    | 19/32 [00:20<00:13,  1.07s/it]Inference:  62%|██████▎   | 20/32 [00:21<00:12,  1.06s/it]Inference:  66%|██████▌   | 21/32 [00:22<00:11,  1.04s/it]Inference:  69%|██████▉   | 22/32 [00:23<00:10,  1.03s/it]Inference:  72%|███████▏  | 23/32 [00:24<00:09,  1.02s/it]Inference:  75%|███████▌  | 24/32 [00:25<00:08,  1.02s/it]Inference:  78%|███████▊  | 25/32 [00:26<00:07,  1.02s/it]Inference:  81%|████████▏ | 26/32 [00:27<00:06,  1.01s/it]Inference:  84%|████████▍ | 27/32 [00:28<00:05,  1.01s/it]Inference:  88%|████████▊ | 28/32 [00:29<00:04,  1.00s/it]Inference:  91%|█████████ | 29/32 [00:30<00:03,  1.00s/it]Inference:  94%|█████████▍| 30/32 [00:31<00:02,  1.00s/it]Inference:  97%|█████████▋| 31/32 [00:32<00:01,  1.00s/it]Inference: 100%|██████████| 32/32 [00:33<00:00,  1.00it/s]Inference: 100%|██████████| 32/32 [00:33<00:00,  1.05s/it]
layer5: self_attn.q_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 87800.578125, val loss None, lr 0.01
iter 250, train loss 1665.756591796875, val loss None, lr 0.01
iter 500, train loss 1440.3275146484375, val loss None, lr 0.01
iter 750, train loss 1428.118408203125, val loss None, lr 0.01
iter 1000, train loss 1386.085205078125, val loss None, lr 0.01
iter 1250, train loss 1289.6494140625, val loss None, lr 0.003333
iter 1500, train loss 1297.5330810546875, val loss None, lr 0.003333
iter 1750, train loss 1259.84765625, val loss None, lr 0.001111
iter 2000, train loss 1252.1431884765625, val loss None, lr 0.001111
iter 2250, train loss 1254.0087890625, val loss None, lr 0.001111
best loss 1241.483642578125
running bpv: 0.34223289373088683
layer5: self_attn.k_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 126649.53125, val loss None, lr 0.01
iter 250, train loss 1758.3956298828125, val loss None, lr 0.01
iter 500, train loss 1567.6612548828125, val loss None, lr 0.003333
iter 750, train loss 1473.608642578125, val loss None, lr 0.003333
iter 1000, train loss 1424.847900390625, val loss None, lr 0.003333
iter 1250, train loss 1375.966796875, val loss None, lr 0.001111
iter 1500, train loss 1357.10546875, val loss None, lr 0.001111
iter 1750, train loss 1346.666259765625, val loss None, lr 0.001111
iter 2000, train loss 1337.6077880859375, val loss None, lr 0.001111
iter 2250, train loss 1335.0921630859375, val loss None, lr 0.001111
best loss 1318.406982421875
running bpv: 0.34501551529588764
layer5: self_attn.v_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 2722.037109375, val loss None, lr 0.01
iter 250, train loss 312.0350341796875, val loss None, lr 0.01
iter 500, train loss 298.8586120605469, val loss None, lr 0.01
iter 750, train loss 293.1961669921875, val loss None, lr 0.01
iter 1000, train loss 290.0313415527344, val loss None, lr 0.01
iter 1250, train loss 290.54302978515625, val loss None, lr 0.01
iter 1500, train loss 286.55548095703125, val loss None, lr 0.01
iter 1750, train loss 283.50067138671875, val loss None, lr 0.003333
iter 2000, train loss 283.1203308105469, val loss None, lr 0.003333
iter 2250, train loss 281.59344482421875, val loss None, lr 0.001111
best loss 281.275390625
running bpv: 0.34771023568608095
layer5: self_attn.o_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 240.02749633789062, val loss None, lr 0.01
iter 250, train loss 11.816883087158203, val loss None, lr 0.01
iter 500, train loss 13.519876480102539, val loss None, lr 0.01
iter 750, train loss 10.432072639465332, val loss None, lr 0.003333
iter 1000, train loss 10.230701446533203, val loss None, lr 0.003333
iter 1250, train loss 10.042085647583008, val loss None, lr 0.001111
iter 1500, train loss 9.99273681640625, val loss None, lr 0.001111
iter 1750, train loss 9.972044944763184, val loss None, lr 0.001111
iter 2000, train loss 9.947689056396484, val loss None, lr 0.001111
iter 2250, train loss 9.887500762939453, val loss None, lr 0.00037
best loss 9.866447448730469
running bpv: 0.35032115524781343
layer5: mlp.gate_proj
weight shape torch.Size([11008, 4096])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([22, 8, 8, 8]), -3, -2
iter 0, train loss 7276.44482421875, val loss None, lr 0.01
iter 250, train loss 1842.538818359375, val loss None, lr 0.01
iter 500, train loss 1786.741455078125, val loss None, lr 0.01
iter 750, train loss 1731.7916259765625, val loss None, lr 0.003333
iter 1000, train loss 1719.783935546875, val loss None, lr 0.003333
iter 1250, train loss 1711.8966064453125, val loss None, lr 0.003333
iter 1500, train loss 1705.271240234375, val loss None, lr 0.003333
iter 1750, train loss 1731.845947265625, val loss None, lr 0.001111
iter 2000, train loss 1688.6190185546875, val loss None, lr 0.001111
iter 2250, train loss 1684.7646484375, val loss None, lr 0.00037
best loss 1681.1669921875
running bpv: 0.34426378847947764
layer5: mlp.up_proj
weight shape torch.Size([11008, 4096])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([22, 8, 8, 8]), -3, -2
iter 0, train loss 4057.107421875, val loss None, lr 0.01
iter 250, train loss 1245.765869140625, val loss None, lr 0.01
iter 500, train loss 1213.45654296875, val loss None, lr 0.01
iter 750, train loss 1199.3372802734375, val loss None, lr 0.01
iter 1000, train loss 1199.528076171875, val loss None, lr 0.01
iter 1250, train loss 1173.46044921875, val loss None, lr 0.003333
iter 1500, train loss 1167.0677490234375, val loss None, lr 0.003333
iter 1750, train loss 1162.860595703125, val loss None, lr 0.001111
iter 2000, train loss 1160.51806640625, val loss None, lr 0.001111
iter 2250, train loss 1158.5498046875, val loss None, lr 0.00037
best loss 1156.8511962890625
running bpv: 0.3386736266816143
layer5: mlp.down_proj
weight shape torch.Size([4096, 11008])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 22, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 134.86538696289062, val loss None, lr 0.01
iter 250, train loss 10.121099472045898, val loss None, lr 0.01
iter 500, train loss 9.857879638671875, val loss None, lr 0.01
iter 750, train loss 9.360137939453125, val loss None, lr 0.01
iter 1000, train loss 9.179976463317871, val loss None, lr 0.003333
iter 1250, train loss 9.101771354675293, val loss None, lr 0.003333
iter 1500, train loss 9.010727882385254, val loss None, lr 0.001111
iter 1750, train loss 8.993559837341309, val loss None, lr 0.001111
iter 2000, train loss 8.974435806274414, val loss None, lr 0.001111
iter 2250, train loss 8.951969146728516, val loss None, lr 0.00037
best loss 8.932598114013672
running bpv: 0.3393579987046632
28768 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:11,  2.65it/s]Inference:   6%|▋         | 2/32 [00:00<00:11,  2.59it/s]Inference:   9%|▉         | 3/32 [00:01<00:10,  2.66it/s]Inference:  12%|█▎        | 4/32 [00:01<00:10,  2.61it/s]Inference:  16%|█▌        | 5/32 [00:01<00:10,  2.59it/s]Inference:  19%|█▉        | 6/32 [00:02<00:09,  2.61it/s]Inference:  22%|██▏       | 7/32 [00:02<00:09,  2.62it/s]Inference:  25%|██▌       | 8/32 [00:03<00:09,  2.59it/s]Inference:  28%|██▊       | 9/32 [00:03<00:08,  2.66it/s]Inference:  31%|███▏      | 10/32 [00:03<00:08,  2.67it/s]Inference:  34%|███▍      | 11/32 [00:04<00:07,  2.63it/s]Inference:  38%|███▊      | 12/32 [00:04<00:07,  2.60it/s]Inference:  41%|████      | 13/32 [00:04<00:07,  2.58it/s]Inference:  44%|████▍     | 14/32 [00:05<00:07,  2.56it/s]Inference:  47%|████▋     | 15/32 [00:05<00:06,  2.54it/s]Inference:  50%|█████     | 16/32 [00:06<00:06,  2.60it/s]Inference:  53%|█████▎    | 17/32 [00:06<00:05,  2.59it/s]Inference:  56%|█████▋    | 18/32 [00:06<00:05,  2.57it/s]Inference:  59%|█████▉    | 19/32 [00:07<00:04,  2.64it/s]Inference:  62%|██████▎   | 20/32 [00:07<00:04,  2.72it/s]Inference:  66%|██████▌   | 21/32 [00:08<00:04,  2.64it/s]Inference:  69%|██████▉   | 22/32 [00:08<00:03,  2.52it/s]Inference:  72%|███████▏  | 23/32 [00:08<00:03,  2.44it/s]Inference:  75%|███████▌  | 24/32 [00:09<00:03,  2.42it/s]Inference:  78%|███████▊  | 25/32 [00:09<00:02,  2.38it/s]Inference:  81%|████████▏ | 26/32 [00:10<00:02,  2.35it/s]Inference:  84%|████████▍ | 27/32 [00:10<00:02,  2.34it/s]Inference:  88%|████████▊ | 28/32 [00:11<00:01,  2.32it/s]Inference:  91%|█████████ | 29/32 [00:11<00:01,  2.36it/s]Inference:  94%|█████████▍| 30/32 [00:11<00:00,  2.34it/s]Inference:  97%|█████████▋| 31/32 [00:12<00:00,  2.32it/s]Inference: 100%|██████████| 32/32 [00:12<00:00,  2.29it/s]Inference: 100%|██████████| 32/32 [00:12<00:00,  2.50it/s]
22162 MiB free out of 48676 MiB total
Saved layer 5 to ./models/meta-llama/Llama-2-7b-hf/tensorized/128/64/no_finetune/layer_5.pt
after cast to cpu
26192 MiB free out of 48676 MiB total
running bpv: 0.3393579987046632
Done with layer 5 total_time elapsed: 11384 estimated time left: 49330
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:01<00:31,  1.01s/it]Inference:   6%|▋         | 2/32 [00:02<00:30,  1.00s/it]Inference:   9%|▉         | 3/32 [00:03<00:29,  1.01s/it]Inference:  12%|█▎        | 4/32 [00:04<00:28,  1.02s/it]Inference:  16%|█▌        | 5/32 [00:05<00:27,  1.01s/it]Inference:  19%|█▉        | 6/32 [00:06<00:26,  1.02s/it]Inference:  22%|██▏       | 7/32 [00:07<00:25,  1.02s/it]Inference:  25%|██▌       | 8/32 [00:08<00:24,  1.02s/it]Inference:  28%|██▊       | 9/32 [00:09<00:23,  1.02s/it]Inference:  31%|███▏      | 10/32 [00:10<00:22,  1.02s/it]Inference:  34%|███▍      | 11/32 [00:11<00:21,  1.02s/it]Inference:  38%|███▊      | 12/32 [00:12<00:20,  1.01s/it]Inference:  41%|████      | 13/32 [00:13<00:19,  1.01s/it]Inference:  44%|████▍     | 14/32 [00:14<00:18,  1.01s/it]Inference:  47%|████▋     | 15/32 [00:15<00:17,  1.01s/it]Inference:  50%|█████     | 16/32 [00:16<00:16,  1.01s/it]Inference:  53%|█████▎    | 17/32 [00:17<00:15,  1.02s/it]Inference:  56%|█████▋    | 18/32 [00:18<00:14,  1.02s/it]Inference:  59%|█████▉    | 19/32 [00:19<00:13,  1.01s/it]Inference:  62%|██████▎   | 20/32 [00:20<00:12,  1.01s/it]Inference:  66%|██████▌   | 21/32 [00:21<00:11,  1.01s/it]Inference:  69%|██████▉   | 22/32 [00:22<00:10,  1.01s/it]Inference:  72%|███████▏  | 23/32 [00:23<00:09,  1.02s/it]Inference:  75%|███████▌  | 24/32 [00:24<00:08,  1.01s/it]Inference:  78%|███████▊  | 25/32 [00:25<00:07,  1.01s/it]Inference:  81%|████████▏ | 26/32 [00:26<00:06,  1.01s/it]Inference:  84%|████████▍ | 27/32 [00:27<00:05,  1.01s/it]Inference:  88%|████████▊ | 28/32 [00:28<00:04,  1.01s/it]Inference:  91%|█████████ | 29/32 [00:29<00:03,  1.01s/it]Inference:  94%|█████████▍| 30/32 [00:30<00:02,  1.01s/it]Inference:  97%|█████████▋| 31/32 [00:31<00:01,  1.01s/it]Inference: 100%|██████████| 32/32 [00:32<00:00,  1.01s/it]Inference: 100%|██████████| 32/32 [00:32<00:00,  1.01s/it]
layer6: self_attn.q_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 74845.65625, val loss None, lr 0.01
iter 250, train loss 1673.56005859375, val loss None, lr 0.01
iter 500, train loss 1463.3536376953125, val loss None, lr 0.01
iter 750, train loss 1301.147705078125, val loss None, lr 0.003333
iter 1000, train loss 1247.610107421875, val loss None, lr 0.001111
iter 1250, train loss 1227.141845703125, val loss None, lr 0.001111
iter 1500, train loss 1215.341552734375, val loss None, lr 0.001111
iter 1750, train loss 1208.2786865234375, val loss None, lr 0.001111
iter 2000, train loss 1212.83447265625, val loss None, lr 0.001111
iter 2250, train loss 1193.051513671875, val loss None, lr 0.001111
best loss 1184.4423828125
running bpv: 0.3417602747018739
layer6: self_attn.k_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 102835.125, val loss None, lr 0.01
iter 250, train loss 1637.780029296875, val loss None, lr 0.01
iter 500, train loss 1369.0201416015625, val loss None, lr 0.01
iter 750, train loss 1459.076416015625, val loss None, lr 0.01
iter 1000, train loss 1207.9849853515625, val loss None, lr 0.003333
iter 1250, train loss 1182.341552734375, val loss None, lr 0.003333
iter 1500, train loss 1145.686279296875, val loss None, lr 0.001111
iter 1750, train loss 1135.27685546875, val loss None, lr 0.001111
iter 2000, train loss 1128.89990234375, val loss None, lr 0.001111
iter 2250, train loss 1125.6680908203125, val loss None, lr 0.001111
best loss 1112.5048828125
running bpv: 0.34409795168067225
layer6: self_attn.v_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 3515.918701171875, val loss None, lr 0.01
iter 250, train loss 295.4045715332031, val loss None, lr 0.01
iter 500, train loss 277.6851501464844, val loss None, lr 0.01
iter 750, train loss 270.6315002441406, val loss None, lr 0.01
iter 1000, train loss 265.7995300292969, val loss None, lr 0.01
iter 1250, train loss 264.1600036621094, val loss None, lr 0.01
iter 1500, train loss 261.8897399902344, val loss None, lr 0.01
iter 1750, train loss 257.6891174316406, val loss None, lr 0.003333
iter 2000, train loss 255.73922729492188, val loss None, lr 0.001111
iter 2250, train loss 255.11419677734375, val loss None, lr 0.001111
best loss 254.4854278564453
running bpv: 0.34637360074626866
layer6: self_attn.o_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 393.85345458984375, val loss None, lr 0.01
iter 250, train loss 16.639623641967773, val loss None, lr 0.01
iter 500, train loss 11.65762710571289, val loss None, lr 0.01
iter 750, train loss 11.39950942993164, val loss None, lr 0.01
iter 1000, train loss 11.113840103149414, val loss None, lr 0.01
iter 1250, train loss 10.177974700927734, val loss None, lr 0.003333
iter 1500, train loss 10.065633773803711, val loss None, lr 0.001111
iter 1750, train loss 10.001367568969727, val loss None, lr 0.001111
iter 2000, train loss 9.967718124389648, val loss None, lr 0.001111
iter 2250, train loss 9.96054744720459, val loss None, lr 0.001111
best loss 9.877679824829102
running bpv: 0.3485896583469722
layer6: mlp.gate_proj
weight shape torch.Size([11008, 4096])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([22, 8, 8, 8]), -3, -2
iter 0, train loss 10065.00390625, val loss None, lr 0.01
iter 250, train loss 1904.589599609375, val loss None, lr 0.01
iter 500, train loss 1835.50146484375, val loss None, lr 0.01
iter 750, train loss 1798.8956298828125, val loss None, lr 0.01
iter 1000, train loss 1763.722412109375, val loss None, lr 0.01
iter 1250, train loss 1725.90380859375, val loss None, lr 0.003333
iter 1500, train loss 1732.7176513671875, val loss None, lr 0.003333
iter 1750, train loss 1712.623046875, val loss None, lr 0.001111
iter 2000, train loss 1707.6107177734375, val loss None, lr 0.001111
iter 2250, train loss 1705.798828125, val loss None, lr 0.001111
best loss 1700.986572265625
running bpv: 0.3435153162055336
layer6: mlp.up_proj
weight shape torch.Size([11008, 4096])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([22, 8, 8, 8]), -3, -2
iter 0, train loss 4495.42431640625, val loss None, lr 0.01
iter 250, train loss 1250.259033203125, val loss None, lr 0.01
iter 500, train loss 1220.094970703125, val loss None, lr 0.01
iter 750, train loss 1190.56298828125, val loss None, lr 0.003333
iter 1000, train loss 1181.043701171875, val loss None, lr 0.003333
iter 1250, train loss 1174.7734375, val loss None, lr 0.001111
iter 1500, train loss 1170.8902587890625, val loss None, lr 0.001111
iter 1750, train loss 1168.71240234375, val loss None, lr 0.001111
iter 2000, train loss 1166.14501953125, val loss None, lr 0.00037
iter 2250, train loss 1164.56982421875, val loss None, lr 0.00037
best loss 1163.0152587890625
running bpv: 0.33877460818042815
layer6: mlp.down_proj
weight shape torch.Size([4096, 11008])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 22, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 81.06498718261719, val loss None, lr 0.01
iter 250, train loss 9.71299934387207, val loss None, lr 0.01
iter 500, train loss 8.310806274414062, val loss None, lr 0.01
iter 750, train loss 8.280237197875977, val loss None, lr 0.01
iter 1000, train loss 8.089129447937012, val loss None, lr 0.01
iter 1250, train loss 7.937234401702881, val loss None, lr 0.003333
iter 1500, train loss 7.8836517333984375, val loss None, lr 0.001111
iter 1750, train loss 7.843329429626465, val loss None, lr 0.001111
iter 2000, train loss 7.845868110656738, val loss None, lr 0.001111
iter 2250, train loss 7.810776710510254, val loss None, lr 0.001111
best loss 7.793174743652344
running bpv: 0.3393579987046632
26192 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:15,  1.98it/s]Inference:   6%|▋         | 2/32 [00:00<00:14,  2.12it/s]Inference:   9%|▉         | 3/32 [00:01<00:13,  2.18it/s]Inference:  12%|█▎        | 4/32 [00:01<00:12,  2.20it/s]Inference:  16%|█▌        | 5/32 [00:02<00:12,  2.21it/s]Inference:  19%|█▉        | 6/32 [00:02<00:11,  2.22it/s]Inference:  22%|██▏       | 7/32 [00:03<00:11,  2.23it/s]Inference:  25%|██▌       | 8/32 [00:03<00:10,  2.23it/s]Inference:  28%|██▊       | 9/32 [00:04<00:10,  2.23it/s]Inference:  31%|███▏      | 10/32 [00:04<00:09,  2.27it/s]Inference:  34%|███▍      | 11/32 [00:04<00:09,  2.26it/s]Inference:  38%|███▊      | 12/32 [00:05<00:08,  2.25it/s]Inference:  41%|████      | 13/32 [00:05<00:08,  2.25it/s]Inference:  44%|████▍     | 14/32 [00:06<00:08,  2.24it/s]Inference:  47%|████▋     | 15/32 [00:06<00:07,  2.23it/s]Inference:  50%|█████     | 16/32 [00:07<00:07,  2.24it/s]Inference:  53%|█████▎    | 17/32 [00:07<00:06,  2.24it/s]Inference:  56%|█████▋    | 18/32 [00:08<00:06,  2.23it/s]Inference:  59%|█████▉    | 19/32 [00:08<00:05,  2.24it/s]Inference:  62%|██████▎   | 20/32 [00:08<00:05,  2.22it/s]Inference:  66%|██████▌   | 21/32 [00:09<00:04,  2.23it/s]Inference:  69%|██████▉   | 22/32 [00:09<00:04,  2.24it/s]Inference:  72%|███████▏  | 23/32 [00:10<00:04,  2.24it/s]Inference:  75%|███████▌  | 24/32 [00:10<00:03,  2.35it/s]Inference:  78%|███████▊  | 25/32 [00:11<00:02,  2.37it/s]Inference:  81%|████████▏ | 26/32 [00:11<00:02,  2.38it/s]Inference:  84%|████████▍ | 27/32 [00:11<00:02,  2.40it/s]Inference:  88%|████████▊ | 28/32 [00:12<00:01,  2.40it/s]Inference:  91%|█████████ | 29/32 [00:12<00:01,  2.41it/s]Inference:  94%|█████████▍| 30/32 [00:13<00:00,  2.43it/s]Inference:  97%|█████████▋| 31/32 [00:13<00:00,  2.44it/s]Inference: 100%|██████████| 32/32 [00:13<00:00,  2.54it/s]Inference: 100%|██████████| 32/32 [00:13<00:00,  2.30it/s]
19714 MiB free out of 48676 MiB total
Saved layer 6 to ./models/meta-llama/Llama-2-7b-hf/tensorized/128/64/no_finetune/layer_6.pt
after cast to cpu
23744 MiB free out of 48676 MiB total
running bpv: 0.3393579987046632
Done with layer 6 total_time elapsed: 13298 estimated time left: 47495
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:30,  1.02it/s]Inference:   6%|▋         | 2/32 [00:01<00:29,  1.02it/s]Inference:   9%|▉         | 3/32 [00:02<00:27,  1.04it/s]Inference:  12%|█▎        | 4/32 [00:03<00:27,  1.03it/s]Inference:  16%|█▌        | 5/32 [00:04<00:26,  1.02it/s]Inference:  19%|█▉        | 6/32 [00:05<00:25,  1.02it/s]Inference:  22%|██▏       | 7/32 [00:06<00:24,  1.03it/s]Inference:  25%|██▌       | 8/32 [00:07<00:23,  1.04it/s]Inference:  28%|██▊       | 9/32 [00:08<00:22,  1.03it/s]Inference:  31%|███▏      | 10/32 [00:09<00:21,  1.02it/s]Inference:  34%|███▍      | 11/32 [00:10<00:20,  1.01it/s]Inference:  38%|███▊      | 12/32 [00:11<00:19,  1.01it/s]Inference:  41%|████      | 13/32 [00:12<00:18,  1.02it/s]Inference:  44%|████▍     | 14/32 [00:13<00:17,  1.02it/s]Inference:  47%|████▋     | 15/32 [00:14<00:16,  1.01it/s]Inference:  50%|█████     | 16/32 [00:15<00:15,  1.01it/s]Inference:  53%|█████▎    | 17/32 [00:16<00:14,  1.01it/s]Inference:  56%|█████▋    | 18/32 [00:17<00:13,  1.01it/s]Inference:  59%|█████▉    | 19/32 [00:18<00:12,  1.01it/s]Inference:  62%|██████▎   | 20/32 [00:19<00:11,  1.01it/s]Inference:  66%|██████▌   | 21/32 [00:20<00:10,  1.01it/s]Inference:  69%|██████▉   | 22/32 [00:21<00:09,  1.01it/s]Inference:  72%|███████▏  | 23/32 [00:22<00:08,  1.00it/s]Inference:  75%|███████▌  | 24/32 [00:23<00:07,  1.00it/s]Inference:  78%|███████▊  | 25/32 [00:24<00:06,  1.00it/s]Inference:  81%|████████▏ | 26/32 [00:25<00:05,  1.00it/s]Inference:  84%|████████▍ | 27/32 [00:26<00:04,  1.00it/s]Inference:  88%|████████▊ | 28/32 [00:27<00:03,  1.00it/s]Inference:  91%|█████████ | 29/32 [00:28<00:02,  1.00it/s]Inference:  94%|█████████▍| 30/32 [00:29<00:01,  1.00it/s]Inference:  97%|█████████▋| 31/32 [00:30<00:00,  1.02it/s]Inference: 100%|██████████| 32/32 [00:31<00:00,  1.02it/s]Inference: 100%|██████████| 32/32 [00:31<00:00,  1.01it/s]
layer7: self_attn.q_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 75897.53125, val loss None, lr 0.01
iter 250, train loss 1934.79638671875, val loss None, lr 0.01
iter 500, train loss 1736.0458984375, val loss None, lr 0.01
iter 750, train loss 1664.982177734375, val loss None, lr 0.01
iter 1000, train loss 1565.5780029296875, val loss None, lr 0.003333
iter 1250, train loss 1556.7637939453125, val loss None, lr 0.003333
iter 1500, train loss 1510.55712890625, val loss None, lr 0.001111
iter 1750, train loss 1499.2816162109375, val loss None, lr 0.001111
iter 2000, train loss 1490.874755859375, val loss None, lr 0.001111
iter 2250, train loss 1482.0428466796875, val loss None, lr 0.001111
best loss 1477.0877685546875
running bpv: 0.34142110918068763
layer7: self_attn.k_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 90215.625, val loss None, lr 0.01
iter 250, train loss 1803.4063720703125, val loss None, lr 0.01
iter 500, train loss 1663.140869140625, val loss None, lr 0.01
iter 750, train loss 1525.9993896484375, val loss None, lr 0.01
iter 1000, train loss 1515.179931640625, val loss None, lr 0.01
iter 1250, train loss 1416.5640869140625, val loss None, lr 0.003333
iter 1500, train loss 1384.30615234375, val loss None, lr 0.001111
iter 1750, train loss 1373.7015380859375, val loss None, lr 0.001111
iter 2000, train loss 1383.021240234375, val loss None, lr 0.001111
iter 2250, train loss 1355.809326171875, val loss None, lr 0.00037
best loss 1350.429931640625
running bpv: 0.34343648318872017
layer7: self_attn.v_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 3472.12109375, val loss None, lr 0.01
iter 250, train loss 389.8248291015625, val loss None, lr 0.01
iter 500, train loss 371.4900817871094, val loss None, lr 0.01
iter 750, train loss 358.9255676269531, val loss None, lr 0.01
iter 1000, train loss 354.99554443359375, val loss None, lr 0.01
iter 1250, train loss 348.9259338378906, val loss None, lr 0.003333
iter 1500, train loss 346.57354736328125, val loss None, lr 0.001111
iter 1750, train loss 345.323486328125, val loss None, lr 0.001111
iter 2000, train loss 344.62237548828125, val loss None, lr 0.001111
iter 2250, train loss 344.44097900390625, val loss None, lr 0.001111
best loss 343.13201904296875
running bpv: 0.3454057585775554
layer7: self_attn.o_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 288.4392395019531, val loss None, lr 0.01
iter 250, train loss 19.478151321411133, val loss None, lr 0.01
iter 500, train loss 16.667585372924805, val loss None, lr 0.01
iter 750, train loss 15.859291076660156, val loss None, lr 0.01
iter 1000, train loss 16.20954704284668, val loss None, lr 0.003333
iter 1250, train loss 15.138869285583496, val loss None, lr 0.001111
iter 1500, train loss 15.061328887939453, val loss None, lr 0.001111
iter 1750, train loss 15.017586708068848, val loss None, lr 0.001111
iter 2000, train loss 14.97011947631836, val loss None, lr 0.001111
iter 2250, train loss 14.94028091430664, val loss None, lr 0.00037
best loss 14.880487442016602
running bpv: 0.3473304991166078
layer7: mlp.gate_proj
weight shape torch.Size([11008, 4096])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([22, 8, 8, 8]), -3, -2
iter 0, train loss 11984.857421875, val loss None, lr 0.01
iter 250, train loss 2480.667236328125, val loss None, lr 0.01
iter 500, train loss 2416.950439453125, val loss None, lr 0.01
iter 750, train loss 2381.4365234375, val loss None, lr 0.01
iter 1000, train loss 2356.73876953125, val loss None, lr 0.01
iter 1250, train loss 2332.9931640625, val loss None, lr 0.01
iter 1500, train loss 2288.0517578125, val loss None, lr 0.003333
iter 1750, train loss 2272.704833984375, val loss None, lr 0.003333
iter 2000, train loss 2265.04052734375, val loss None, lr 0.003333
iter 2250, train loss 2259.356201171875, val loss None, lr 0.003333
best loss 2252.115966796875
running bpv: 0.34296499914266115
layer7: mlp.up_proj
weight shape torch.Size([11008, 4096])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([22, 8, 8, 8]), -3, -2
iter 0, train loss 5448.42138671875, val loss None, lr 0.01
iter 250, train loss 1723.6793212890625, val loss None, lr 0.01
iter 500, train loss 1687.3642578125, val loss None, lr 0.01
iter 750, train loss 1662.6708984375, val loss None, lr 0.01
iter 1000, train loss 1631.1917724609375, val loss None, lr 0.003333
iter 1250, train loss 1619.400634765625, val loss None, lr 0.001111
iter 1500, train loss 1615.8173828125, val loss None, lr 0.001111
iter 1750, train loss 1611.8876953125, val loss None, lr 0.001111
iter 2000, train loss 1608.8082275390625, val loss None, lr 0.00037
iter 2250, train loss 1606.7847900390625, val loss None, lr 0.00037
best loss 1604.787109375
running bpv: 0.3388496210859427
layer7: mlp.down_proj
weight shape torch.Size([4096, 11008])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 22, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 96.0789566040039, val loss None, lr 0.01
iter 250, train loss 15.188684463500977, val loss None, lr 0.01
iter 500, train loss 14.577329635620117, val loss None, lr 0.01
iter 750, train loss 14.50849723815918, val loss None, lr 0.01
iter 1000, train loss 13.953594207763672, val loss None, lr 0.003333
iter 1250, train loss 13.903578758239746, val loss None, lr 0.003333
iter 1500, train loss 13.976465225219727, val loss None, lr 0.003333
iter 1750, train loss 13.777677536010742, val loss None, lr 0.003333
iter 2000, train loss 13.751140594482422, val loss None, lr 0.001111
iter 2250, train loss 13.721927642822266, val loss None, lr 0.001111
best loss 13.695899963378906
running bpv: 0.3393579987046632
23744 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:11,  2.77it/s]Inference:   6%|▋         | 2/32 [00:00<00:11,  2.61it/s]Inference:   9%|▉         | 3/32 [00:01<00:11,  2.58it/s]Inference:  12%|█▎        | 4/32 [00:01<00:10,  2.57it/s]Inference:  16%|█▌        | 5/32 [00:01<00:10,  2.55it/s]Inference:  19%|█▉        | 6/32 [00:02<00:09,  2.66it/s]Inference:  22%|██▏       | 7/32 [00:02<00:09,  2.63it/s]Inference:  25%|██▌       | 8/32 [00:03<00:08,  2.70it/s]Inference:  28%|██▊       | 9/32 [00:03<00:08,  2.66it/s]Inference:  31%|███▏      | 10/32 [00:03<00:08,  2.62it/s]Inference:  34%|███▍      | 11/32 [00:04<00:08,  2.60it/s]Inference:  38%|███▊      | 12/32 [00:04<00:07,  2.60it/s]Inference:  41%|████      | 13/32 [00:04<00:07,  2.59it/s]Inference:  44%|████▍     | 14/32 [00:05<00:06,  2.59it/s]Inference:  47%|████▋     | 15/32 [00:05<00:06,  2.57it/s]Inference:  50%|█████     | 16/32 [00:06<00:06,  2.58it/s]Inference:  53%|█████▎    | 17/32 [00:06<00:05,  2.57it/s]Inference:  56%|█████▋    | 18/32 [00:06<00:05,  2.57it/s]Inference:  59%|█████▉    | 19/32 [00:07<00:05,  2.56it/s]Inference:  62%|██████▎   | 20/32 [00:07<00:04,  2.57it/s]Inference:  66%|██████▌   | 21/32 [00:08<00:04,  2.56it/s]Inference:  69%|██████▉   | 22/32 [00:08<00:03,  2.57it/s]Inference:  72%|███████▏  | 23/32 [00:08<00:03,  2.56it/s]Inference:  75%|███████▌  | 24/32 [00:09<00:03,  2.57it/s]Inference:  78%|███████▊  | 25/32 [00:09<00:02,  2.56it/s]Inference:  81%|████████▏ | 26/32 [00:10<00:02,  2.55it/s]Inference:  84%|████████▍ | 27/32 [00:10<00:01,  2.72it/s]Inference:  88%|████████▊ | 28/32 [00:10<00:01,  2.73it/s]Inference:  91%|█████████ | 29/32 [00:11<00:01,  2.75it/s]Inference:  94%|█████████▍| 30/32 [00:11<00:00,  2.76it/s]Inference:  97%|█████████▋| 31/32 [00:11<00:00,  2.89it/s]Inference: 100%|██████████| 32/32 [00:12<00:00,  2.87it/s]Inference: 100%|██████████| 32/32 [00:12<00:00,  2.64it/s]
17136 MiB free out of 48676 MiB total
Saved layer 7 to ./models/meta-llama/Llama-2-7b-hf/tensorized/128/64/no_finetune/layer_7.pt
after cast to cpu
21166 MiB free out of 48676 MiB total
running bpv: 0.3393579987046632
Done with layer 7 total_time elapsed: 15212 estimated time left: 45635
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:30,  1.02it/s]Inference:   6%|▋         | 2/32 [00:02<00:30,  1.01s/it]Inference:   9%|▉         | 3/32 [00:02<00:28,  1.00it/s]Inference:  12%|█▎        | 4/32 [00:03<00:27,  1.01it/s]Inference:  16%|█▌        | 5/32 [00:04<00:26,  1.00it/s]Inference:  19%|█▉        | 6/32 [00:05<00:25,  1.00it/s]Inference:  22%|██▏       | 7/32 [00:07<00:25,  1.01s/it]Inference:  25%|██▌       | 8/32 [00:07<00:23,  1.00it/s]Inference:  28%|██▊       | 9/32 [00:09<00:23,  1.01s/it]Inference:  31%|███▏      | 10/32 [00:10<00:22,  1.01s/it]Inference:  34%|███▍      | 11/32 [00:11<00:21,  1.01s/it]Inference:  38%|███▊      | 12/32 [00:12<00:20,  1.01s/it]Inference:  41%|████      | 13/32 [00:13<00:19,  1.01s/it]Inference:  44%|████▍     | 14/32 [00:14<00:18,  1.02s/it]Inference:  47%|████▋     | 15/32 [00:15<00:17,  1.02s/it]Inference:  50%|█████     | 16/32 [00:16<00:16,  1.02s/it]Inference:  53%|█████▎    | 17/32 [00:17<00:15,  1.03s/it]Inference:  56%|█████▋    | 18/32 [00:18<00:14,  1.03s/it]Inference:  59%|█████▉    | 19/32 [00:19<00:13,  1.03s/it]Inference:  62%|██████▎   | 20/32 [00:20<00:12,  1.03s/it]Inference:  66%|██████▌   | 21/32 [00:21<00:11,  1.03s/it]Inference:  69%|██████▉   | 22/32 [00:22<00:10,  1.02s/it]Inference:  72%|███████▏  | 23/32 [00:23<00:09,  1.02s/it]Inference:  75%|███████▌  | 24/32 [00:24<00:08,  1.03s/it]Inference:  78%|███████▊  | 25/32 [00:25<00:07,  1.03s/it]Inference:  81%|████████▏ | 26/32 [00:26<00:06,  1.03s/it]Inference:  84%|████████▍ | 27/32 [00:27<00:05,  1.03s/it]Inference:  88%|████████▊ | 28/32 [00:28<00:04,  1.02s/it]Inference:  91%|█████████ | 29/32 [00:29<00:03,  1.02s/it]Inference:  94%|█████████▍| 30/32 [00:30<00:02,  1.02s/it]Inference:  97%|█████████▋| 31/32 [00:31<00:01,  1.03s/it]Inference: 100%|██████████| 32/32 [00:32<00:00,  1.03s/it]Inference: 100%|██████████| 32/32 [00:32<00:00,  1.02s/it]
layer8: self_attn.q_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 73695.984375, val loss None, lr 0.01
iter 250, train loss 2173.128662109375, val loss None, lr 0.01
iter 500, train loss 1918.040283203125, val loss None, lr 0.01
iter 750, train loss 1771.7501220703125, val loss None, lr 0.003333
iter 1000, train loss 1727.897216796875, val loss None, lr 0.003333
iter 1250, train loss 1694.166259765625, val loss None, lr 0.001111
iter 1500, train loss 1677.22216796875, val loss None, lr 0.001111
iter 1750, train loss 1670.70654296875, val loss None, lr 0.001111
iter 2000, train loss 1656.53759765625, val loss None, lr 0.001111
iter 2250, train loss 1654.9561767578125, val loss None, lr 0.001111
best loss 1643.4713134765625
running bpv: 0.3411658653846154
layer8: self_attn.k_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 96662.515625, val loss None, lr 0.01
iter 250, train loss 1930.484619140625, val loss None, lr 0.01
iter 500, train loss 1812.9718017578125, val loss None, lr 0.01
iter 750, train loss 1640.418212890625, val loss None, lr 0.003333
iter 1000, train loss 1626.148681640625, val loss None, lr 0.003333
iter 1250, train loss 1555.653564453125, val loss None, lr 0.001111
iter 1500, train loss 1540.955322265625, val loss None, lr 0.001111
iter 1750, train loss 1529.2374267578125, val loss None, lr 0.001111
iter 2000, train loss 1520.2193603515625, val loss None, lr 0.001111
iter 2250, train loss 1510.16162109375, val loss None, lr 0.00037
best loss 1503.513916015625
running bpv: 0.34293702411167515
layer8: self_attn.v_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 3825.323974609375, val loss None, lr 0.01
iter 250, train loss 470.4088134765625, val loss None, lr 0.01
iter 500, train loss 449.15216064453125, val loss None, lr 0.01
iter 750, train loss 442.44390869140625, val loss None, lr 0.01
iter 1000, train loss 437.2073974609375, val loss None, lr 0.01
iter 1250, train loss 433.4903564453125, val loss None, lr 0.01
iter 1500, train loss 430.65521240234375, val loss None, lr 0.01
iter 1750, train loss 425.0668640136719, val loss None, lr 0.003333
iter 2000, train loss 422.90875244140625, val loss None, lr 0.001111
iter 2250, train loss 422.1689453125, val loss None, lr 0.001111
best loss 421.4626770019531
running bpv: 0.34467258165829145
layer8: self_attn.o_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 362.8060607910156, val loss None, lr 0.01
iter 250, train loss 25.248071670532227, val loss None, lr 0.01
iter 500, train loss 23.86574935913086, val loss None, lr 0.01
iter 750, train loss 22.250247955322266, val loss None, lr 0.003333
iter 1000, train loss 22.730993270874023, val loss None, lr 0.003333
iter 1250, train loss 21.664472579956055, val loss None, lr 0.001111
iter 1500, train loss 21.558204650878906, val loss None, lr 0.001111
iter 1750, train loss 21.459484100341797, val loss None, lr 0.001111
iter 2000, train loss 21.423503875732422, val loss None, lr 0.001111
iter 2250, train loss 21.395305633544922, val loss None, lr 0.001111
best loss 21.313804626464844
running bpv: 0.34637360074626866
layer8: mlp.gate_proj
weight shape torch.Size([11008, 4096])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([22, 8, 8, 8]), -3, -2
iter 0, train loss 12054.75390625, val loss None, lr 0.01
iter 250, train loss 2620.6533203125, val loss None, lr 0.01
iter 500, train loss 2549.771240234375, val loss None, lr 0.01
iter 750, train loss 2495.742919921875, val loss None, lr 0.01
iter 1000, train loss 2482.079345703125, val loss None, lr 0.01
iter 1250, train loss 2471.560546875, val loss None, lr 0.01
iter 1500, train loss 2462.784423828125, val loss None, lr 0.01
iter 1750, train loss 2414.239013671875, val loss None, lr 0.003333
iter 2000, train loss 2405.802001953125, val loss None, lr 0.003333
iter 2250, train loss 2400.790771484375, val loss None, lr 0.001111
best loss 2391.931640625
running bpv: 0.34254334494245914
layer8: mlp.up_proj
weight shape torch.Size([11008, 4096])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([22, 8, 8, 8]), -3, -2
iter 0, train loss 6304.42626953125, val loss None, lr 0.01
iter 250, train loss 1964.631591796875, val loss None, lr 0.01
iter 500, train loss 1950.6846923828125, val loss None, lr 0.01
iter 750, train loss 1907.52490234375, val loss None, lr 0.01
iter 1000, train loss 1871.03271484375, val loss None, lr 0.003333
iter 1250, train loss 1863.565185546875, val loss None, lr 0.003333
iter 1500, train loss 1861.672607421875, val loss None, lr 0.003333
iter 1750, train loss 1852.127685546875, val loss None, lr 0.001111
iter 2000, train loss 1848.214111328125, val loss None, lr 0.00037
iter 2250, train loss 1845.5621337890625, val loss None, lr 0.00037
best loss 1843.239501953125
running bpv: 0.33890754132231404
layer8: mlp.down_proj
weight shape torch.Size([4096, 11008])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 22, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 111.20762634277344, val loss None, lr 0.01
iter 250, train loss 19.25782012939453, val loss None, lr 0.01
iter 500, train loss 18.697364807128906, val loss None, lr 0.01
iter 750, train loss 18.36695098876953, val loss None, lr 0.01
iter 1000, train loss 17.70332908630371, val loss None, lr 0.003333
iter 1250, train loss 17.55129623413086, val loss None, lr 0.001111
iter 1500, train loss 17.52849578857422, val loss None, lr 0.001111
iter 1750, train loss 17.514713287353516, val loss None, lr 0.001111
iter 2000, train loss 17.435514450073242, val loss None, lr 0.00037
iter 2250, train loss 17.415218353271484, val loss None, lr 0.00037
best loss 17.39484405517578
running bpv: 0.3393579987046632
21166 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:10,  2.85it/s]Inference:   6%|▋         | 2/32 [00:00<00:11,  2.63it/s]Inference:   9%|▉         | 3/32 [00:01<00:11,  2.58it/s]Inference:  12%|█▎        | 4/32 [00:01<00:10,  2.57it/s]Inference:  16%|█▌        | 5/32 [00:01<00:10,  2.54it/s]Inference:  19%|█▉        | 6/32 [00:02<00:10,  2.55it/s]Inference:  22%|██▏       | 7/32 [00:02<00:09,  2.65it/s]Inference:  25%|██▌       | 8/32 [00:03<00:09,  2.60it/s]Inference:  28%|██▊       | 9/32 [00:03<00:08,  2.58it/s]Inference:  31%|███▏      | 10/32 [00:03<00:08,  2.56it/s]Inference:  34%|███▍      | 11/32 [00:04<00:07,  2.65it/s]Inference:  38%|███▊      | 12/32 [00:04<00:07,  2.61it/s]Inference:  41%|████      | 13/32 [00:04<00:07,  2.68it/s]Inference:  44%|████▍     | 14/32 [00:05<00:06,  2.63it/s]Inference:  47%|████▋     | 15/32 [00:05<00:06,  2.60it/s]Inference:  50%|█████     | 16/32 [00:06<00:06,  2.58it/s]Inference:  53%|█████▎    | 17/32 [00:06<00:05,  2.58it/s]Inference:  56%|█████▋    | 18/32 [00:06<00:05,  2.57it/s]Inference:  59%|█████▉    | 19/32 [00:07<00:05,  2.56it/s]Inference:  62%|██████▎   | 20/32 [00:07<00:04,  2.56it/s]Inference:  66%|██████▌   | 21/32 [00:08<00:04,  2.55it/s]Inference:  69%|██████▉   | 22/32 [00:08<00:03,  2.55it/s]Inference:  72%|███████▏  | 23/32 [00:08<00:03,  2.57it/s]Inference:  75%|███████▌  | 24/32 [00:09<00:03,  2.56it/s]Inference:  78%|███████▊  | 25/32 [00:09<00:02,  2.55it/s]Inference:  81%|████████▏ | 26/32 [00:10<00:02,  2.54it/s]Inference:  84%|████████▍ | 27/32 [00:10<00:01,  2.54it/s]Inference:  88%|████████▊ | 28/32 [00:10<00:01,  2.55it/s]Inference:  91%|█████████ | 29/32 [00:11<00:01,  2.58it/s]Inference:  94%|█████████▍| 30/32 [00:11<00:00,  2.57it/s]Inference:  97%|█████████▋| 31/32 [00:11<00:00,  2.63it/s]Inference: 100%|██████████| 32/32 [00:12<00:00,  2.60it/s]Inference: 100%|██████████| 32/32 [00:12<00:00,  2.59it/s]
14560 MiB free out of 48676 MiB total
Saved layer 8 to ./models/meta-llama/Llama-2-7b-hf/tensorized/128/64/no_finetune/layer_8.pt
after cast to cpu
18590 MiB free out of 48676 MiB total
running bpv: 0.3393579987046632
Done with layer 8 total_time elapsed: 17122 estimated time left: 43755
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:30,  1.01it/s]Inference:   6%|▋         | 2/32 [00:01<00:29,  1.02it/s]Inference:   9%|▉         | 3/32 [00:02<00:28,  1.01it/s]Inference:  12%|█▎        | 4/32 [00:03<00:28,  1.00s/it]Inference:  16%|█▌        | 5/32 [00:05<00:27,  1.01s/it]Inference:  19%|█▉        | 6/32 [00:06<00:26,  1.01s/it]Inference:  22%|██▏       | 7/32 [00:07<00:25,  1.02s/it]Inference:  25%|██▌       | 8/32 [00:08<00:24,  1.02s/it]Inference:  28%|██▊       | 9/32 [00:09<00:23,  1.00s/it]Inference:  31%|███▏      | 10/32 [00:10<00:22,  1.01s/it]Inference:  34%|███▍      | 11/32 [00:11<00:21,  1.02s/it]Inference:  38%|███▊      | 12/32 [00:12<00:20,  1.02s/it]Inference:  41%|████      | 13/32 [00:13<00:19,  1.00s/it]Inference:  44%|████▍     | 14/32 [00:14<00:18,  1.01s/it]Inference:  47%|████▋     | 15/32 [00:15<00:17,  1.00s/it]Inference:  50%|█████     | 16/32 [00:16<00:16,  1.01s/it]Inference:  53%|█████▎    | 17/32 [00:17<00:14,  1.00it/s]Inference:  56%|█████▋    | 18/32 [00:18<00:14,  1.01s/it]Inference:  59%|█████▉    | 19/32 [00:19<00:13,  1.01s/it]Inference:  62%|██████▎   | 20/32 [00:20<00:12,  1.01s/it]Inference:  66%|██████▌   | 21/32 [00:21<00:11,  1.02s/it]Inference:  69%|██████▉   | 22/32 [00:22<00:10,  1.02s/it]Inference:  72%|███████▏  | 23/32 [00:23<00:09,  1.02s/it]Inference:  75%|███████▌  | 24/32 [00:24<00:08,  1.02s/it]Inference:  78%|███████▊  | 25/32 [00:25<00:07,  1.02s/it]Inference:  81%|████████▏ | 26/32 [00:26<00:06,  1.02s/it]Inference:  84%|████████▍ | 27/32 [00:27<00:05,  1.02s/it]Inference:  88%|████████▊ | 28/32 [00:28<00:04,  1.02s/it]Inference:  91%|█████████ | 29/32 [00:29<00:03,  1.02s/it]Inference:  94%|█████████▍| 30/32 [00:30<00:02,  1.02s/it]Inference:  97%|█████████▋| 31/32 [00:31<00:01,  1.02s/it]Inference: 100%|██████████| 32/32 [00:32<00:00,  1.02s/it]Inference: 100%|██████████| 32/32 [00:32<00:00,  1.01s/it]
layer9: self_attn.q_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 72629.3046875, val loss None, lr 0.01
iter 250, train loss 2170.157958984375, val loss None, lr 0.01
iter 500, train loss 2041.83447265625, val loss None, lr 0.01
iter 750, train loss 2010.46142578125, val loss None, lr 0.01
iter 1000, train loss 1948.277099609375, val loss None, lr 0.01
iter 1250, train loss 1815.49609375, val loss None, lr 0.003333
iter 1500, train loss 1794.60302734375, val loss None, lr 0.003333
iter 1750, train loss 1770.42041015625, val loss None, lr 0.001111
iter 2000, train loss 1761.10546875, val loss None, lr 0.001111
iter 2250, train loss 1754.5401611328125, val loss None, lr 0.001111
best loss 1747.087890625
running bpv: 0.34096682472903594
layer9: self_attn.k_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 105658.453125, val loss None, lr 0.01
iter 250, train loss 2076.674560546875, val loss None, lr 0.01
iter 500, train loss 2274.98291015625, val loss None, lr 0.01
iter 750, train loss 1750.2586669921875, val loss None, lr 0.003333
iter 1000, train loss 1700.829833984375, val loss None, lr 0.001111
iter 1250, train loss 1676.500732421875, val loss None, lr 0.001111
iter 1500, train loss 1661.19287109375, val loss None, lr 0.001111
iter 1750, train loss 1651.173095703125, val loss None, lr 0.001111
iter 2000, train loss 1641.67822265625, val loss None, lr 0.001111
iter 2250, train loss 1632.2454833984375, val loss None, lr 0.001111
best loss 1622.5765380859375
running bpv: 0.3425465481910684
layer9: self_attn.v_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 3679.3642578125, val loss None, lr 0.01
iter 250, train loss 501.2601623535156, val loss None, lr 0.01
iter 500, train loss 482.5408935546875, val loss None, lr 0.01
iter 750, train loss 472.6666259765625, val loss None, lr 0.01
iter 1000, train loss 472.5054931640625, val loss None, lr 0.01
iter 1250, train loss 463.24609375, val loss None, lr 0.003333
iter 1500, train loss 461.25860595703125, val loss None, lr 0.003333
iter 1750, train loss 458.73114013671875, val loss None, lr 0.001111
iter 2000, train loss 457.7834167480469, val loss None, lr 0.001111
iter 2250, train loss 457.07562255859375, val loss None, lr 0.001111
best loss 456.2369079589844
running bpv: 0.34409795168067225
layer9: self_attn.o_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 371.754150390625, val loss None, lr 0.01
iter 250, train loss 27.98155975341797, val loss None, lr 0.01
iter 500, train loss 23.685497283935547, val loss None, lr 0.01
iter 750, train loss 23.298599243164062, val loss None, lr 0.01
iter 1000, train loss 22.9921875, val loss None, lr 0.01
iter 1250, train loss 22.033794403076172, val loss None, lr 0.003333
iter 1500, train loss 21.80950164794922, val loss None, lr 0.001111
iter 1750, train loss 21.69274139404297, val loss None, lr 0.001111
iter 2000, train loss 21.63614273071289, val loss None, lr 0.001111
iter 2250, train loss 21.577884674072266, val loss None, lr 0.001111
best loss 21.530948638916016
running bpv: 0.34562178997779014
layer9: mlp.gate_proj
weight shape torch.Size([11008, 4096])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([22, 8, 8, 8]), -3, -2
iter 0, train loss 13051.3994140625, val loss None, lr 0.01
iter 250, train loss 2805.818115234375, val loss None, lr 0.01
iter 500, train loss 2709.32470703125, val loss None, lr 0.01
iter 750, train loss 2662.350830078125, val loss None, lr 0.01
iter 1000, train loss 2670.9609375, val loss None, lr 0.01
iter 1250, train loss 2573.275634765625, val loss None, lr 0.003333
iter 1500, train loss 2553.55224609375, val loss None, lr 0.001111
iter 1750, train loss 2547.6171875, val loss None, lr 0.00037
iter 2000, train loss 2541.38232421875, val loss None, lr 0.00037
iter 2250, train loss 2537.02392578125, val loss None, lr 0.00037
best loss 2533.985595703125
running bpv: 0.3422099545824295
layer9: mlp.up_proj
weight shape torch.Size([11008, 4096])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([22, 8, 8, 8]), -3, -2
iter 0, train loss 6844.3916015625, val loss None, lr 0.01
iter 250, train loss 2186.344482421875, val loss None, lr 0.01
iter 500, train loss 2170.315673828125, val loss None, lr 0.01
iter 750, train loss 2094.69287109375, val loss None, lr 0.003333
iter 1000, train loss 2074.703369140625, val loss None, lr 0.003333
iter 1250, train loss 2064.107177734375, val loss None, lr 0.001111
iter 1500, train loss 2059.5107421875, val loss None, lr 0.001111
iter 1750, train loss 2055.069091796875, val loss None, lr 0.001111
iter 2000, train loss 2052.288818359375, val loss None, lr 0.00037
iter 2250, train loss 2048.64404296875, val loss None, lr 0.00037
best loss 2046.446044921875
running bpv: 0.3389536135400106
layer9: mlp.down_proj
weight shape torch.Size([4096, 11008])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 22, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 146.58445739746094, val loss None, lr 0.01
iter 250, train loss 23.86123275756836, val loss None, lr 0.01
iter 500, train loss 23.47439956665039, val loss None, lr 0.01
iter 750, train loss 22.109882354736328, val loss None, lr 0.003333
iter 1000, train loss 21.921844482421875, val loss None, lr 0.003333
iter 1250, train loss 21.77919578552246, val loss None, lr 0.003333
iter 1500, train loss 21.62933349609375, val loss None, lr 0.001111
iter 1750, train loss 21.564918518066406, val loss None, lr 0.001111
iter 2000, train loss 21.603124618530273, val loss None, lr 0.001111
iter 2250, train loss 21.537458419799805, val loss None, lr 0.001111
best loss 21.475908279418945
running bpv: 0.3393579987046632
18590 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:11,  2.82it/s]Inference:   6%|▋         | 2/32 [00:00<00:11,  2.63it/s]Inference:   9%|▉         | 3/32 [00:01<00:11,  2.61it/s]Inference:  12%|█▎        | 4/32 [00:01<00:10,  2.57it/s]Inference:  16%|█▌        | 5/32 [00:01<00:10,  2.54it/s]Inference:  19%|█▉        | 6/32 [00:02<00:10,  2.54it/s]Inference:  22%|██▏       | 7/32 [00:02<00:09,  2.53it/s]Inference:  25%|██▌       | 8/32 [00:03<00:09,  2.52it/s]Inference:  28%|██▊       | 9/32 [00:03<00:09,  2.52it/s]Inference:  31%|███▏      | 10/32 [00:03<00:08,  2.52it/s]Inference:  34%|███▍      | 11/32 [00:04<00:08,  2.53it/s]Inference:  38%|███▊      | 12/32 [00:04<00:07,  2.53it/s]Inference:  41%|████      | 13/32 [00:05<00:07,  2.65it/s]Inference:  44%|████▍     | 14/32 [00:05<00:06,  2.61it/s]Inference:  47%|████▋     | 15/32 [00:05<00:06,  2.58it/s]Inference:  50%|█████     | 16/32 [00:06<00:06,  2.66it/s]Inference:  53%|█████▎    | 17/32 [00:06<00:05,  2.61it/s]Inference:  56%|█████▋    | 18/32 [00:06<00:05,  2.68it/s]Inference:  59%|█████▉    | 19/32 [00:07<00:04,  2.63it/s]Inference:  62%|██████▎   | 20/32 [00:07<00:04,  2.63it/s]Inference:  66%|██████▌   | 21/32 [00:08<00:04,  2.60it/s]Inference:  69%|██████▉   | 22/32 [00:08<00:03,  2.58it/s]Inference:  72%|███████▏  | 23/32 [00:08<00:03,  2.57it/s]Inference:  75%|███████▌  | 24/32 [00:09<00:03,  2.56it/s]Inference:  78%|███████▊  | 25/32 [00:09<00:02,  2.55it/s]Inference:  81%|████████▏ | 26/32 [00:10<00:02,  2.56it/s]Inference:  84%|████████▍ | 27/32 [00:10<00:01,  2.55it/s]Inference:  88%|████████▊ | 28/32 [00:10<00:01,  2.56it/s]Inference:  91%|█████████ | 29/32 [00:11<00:01,  2.64it/s]Inference:  94%|█████████▍| 30/32 [00:11<00:00,  2.61it/s]Inference:  97%|█████████▋| 31/32 [00:11<00:00,  2.59it/s]Inference: 100%|██████████| 32/32 [00:12<00:00,  2.68it/s]Inference: 100%|██████████| 32/32 [00:12<00:00,  2.59it/s]
12112 MiB free out of 48676 MiB total
Saved layer 9 to ./models/meta-llama/Llama-2-7b-hf/tensorized/128/64/no_finetune/layer_9.pt
after cast to cpu
16142 MiB free out of 48676 MiB total
running bpv: 0.3393579987046632
Done with layer 9 total_time elapsed: 19041 estimated time left: 41891
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:30,  1.01it/s]Inference:   6%|▋         | 2/32 [00:02<00:30,  1.01s/it]Inference:   9%|▉         | 3/32 [00:03<00:29,  1.01s/it]Inference:  12%|█▎        | 4/32 [00:04<00:28,  1.02s/it]Inference:  16%|█▌        | 5/32 [00:05<00:27,  1.02s/it]Inference:  19%|█▉        | 6/32 [00:06<00:26,  1.01s/it]Inference:  22%|██▏       | 7/32 [00:07<00:24,  1.00it/s]Inference:  25%|██▌       | 8/32 [00:08<00:24,  1.00s/it]Inference:  28%|██▊       | 9/32 [00:09<00:23,  1.01s/it]Inference:  31%|███▏      | 10/32 [00:10<00:22,  1.01s/it]Inference:  34%|███▍      | 11/32 [00:11<00:20,  1.00it/s]Inference:  38%|███▊      | 12/32 [00:12<00:19,  1.00it/s]Inference:  41%|████      | 13/32 [00:13<00:19,  1.00s/it]Inference:  44%|████▍     | 14/32 [00:14<00:18,  1.01s/it]Inference:  47%|████▋     | 15/32 [00:15<00:17,  1.01s/it]Inference:  50%|█████     | 16/32 [00:16<00:16,  1.01s/it]Inference:  53%|█████▎    | 17/32 [00:17<00:15,  1.02s/it]Inference:  56%|█████▋    | 18/32 [00:18<00:14,  1.02s/it]Inference:  59%|█████▉    | 19/32 [00:19<00:13,  1.02s/it]Inference:  62%|██████▎   | 20/32 [00:20<00:12,  1.00s/it]Inference:  66%|██████▌   | 21/32 [00:21<00:10,  1.00it/s]Inference:  69%|██████▉   | 22/32 [00:22<00:10,  1.01s/it]Inference:  72%|███████▏  | 23/32 [00:23<00:09,  1.01s/it]Inference:  75%|███████▌  | 24/32 [00:24<00:08,  1.01s/it]Inference:  78%|███████▊  | 25/32 [00:25<00:07,  1.02s/it]Inference:  81%|████████▏ | 26/32 [00:26<00:06,  1.02s/it]Inference:  84%|████████▍ | 27/32 [00:27<00:05,  1.02s/it]Inference:  88%|████████▊ | 28/32 [00:28<00:04,  1.02s/it]Inference:  91%|█████████ | 29/32 [00:29<00:03,  1.02s/it]Inference:  94%|█████████▍| 30/32 [00:30<00:02,  1.02s/it]Inference:  97%|█████████▋| 31/32 [00:31<00:01,  1.02s/it]Inference: 100%|██████████| 32/32 [00:32<00:00,  1.02s/it]Inference: 100%|██████████| 32/32 [00:32<00:00,  1.01s/it]
layer10: self_attn.q_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 75202.15625, val loss None, lr 0.01
iter 250, train loss 2494.623046875, val loss None, lr 0.01
iter 500, train loss 2266.235595703125, val loss None, lr 0.01
iter 750, train loss 2275.48095703125, val loss None, lr 0.01
iter 1000, train loss 2049.004150390625, val loss None, lr 0.003333
iter 1250, train loss 1997.685791015625, val loss None, lr 0.001111
iter 1500, train loss 1983.2191162109375, val loss None, lr 0.001111
iter 1750, train loss 1971.969970703125, val loss None, lr 0.001111
iter 2000, train loss 1965.0906982421875, val loss None, lr 0.001111
iter 2250, train loss 1954.0596923828125, val loss None, lr 0.00037
best loss 1947.295654296875
running bpv: 0.34080726490236385
layer10: self_attn.k_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 101737.8046875, val loss None, lr 0.01
iter 250, train loss 2271.89501953125, val loss None, lr 0.01
iter 500, train loss 2098.974853515625, val loss None, lr 0.01
iter 750, train loss 1971.4376220703125, val loss None, lr 0.01
iter 1000, train loss 1995.114501953125, val loss None, lr 0.01
iter 1250, train loss 1854.7200927734375, val loss None, lr 0.003333
iter 1500, train loss 1819.8397216796875, val loss None, lr 0.003333
iter 1750, train loss 1796.1236572265625, val loss None, lr 0.001111
iter 2000, train loss 1790.30419921875, val loss None, lr 0.001111
iter 2250, train loss 1782.45263671875, val loss None, lr 0.001111
best loss 1775.089599609375
running bpv: 0.34223289373088683
layer10: self_attn.v_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 3706.96044921875, val loss None, lr 0.01
iter 250, train loss 565.003173828125, val loss None, lr 0.01
iter 500, train loss 541.7965698242188, val loss None, lr 0.01
iter 750, train loss 535.491943359375, val loss None, lr 0.01
iter 1000, train loss 523.31201171875, val loss None, lr 0.003333
iter 1250, train loss 519.4564819335938, val loss None, lr 0.001111
iter 1500, train loss 517.8175048828125, val loss None, lr 0.001111
iter 1750, train loss 516.122802734375, val loss None, lr 0.001111
iter 2000, train loss 515.29052734375, val loss None, lr 0.001111
iter 2250, train loss 514.24462890625, val loss None, lr 0.001111
best loss 513.4407958984375
running bpv: 0.3436354587967644
layer10: self_attn.o_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 482.3009033203125, val loss None, lr 0.01
iter 250, train loss 53.131282806396484, val loss None, lr 0.01
iter 500, train loss 51.317649841308594, val loss None, lr 0.01
iter 750, train loss 48.258460998535156, val loss None, lr 0.01
iter 1000, train loss 47.47386932373047, val loss None, lr 0.003333
iter 1250, train loss 47.10343933105469, val loss None, lr 0.003333
iter 1500, train loss 47.80975341796875, val loss None, lr 0.003333
iter 1750, train loss 46.591453552246094, val loss None, lr 0.001111
iter 2000, train loss 46.48175048828125, val loss None, lr 0.001111
iter 2250, train loss 46.44866943359375, val loss None, lr 0.001111
best loss 46.30234909057617
running bpv: 0.34501551529588764
layer10: mlp.gate_proj
weight shape torch.Size([11008, 4096])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([22, 8, 8, 8]), -3, -2
iter 0, train loss 12204.3828125, val loss None, lr 0.01
iter 250, train loss 2910.190185546875, val loss None, lr 0.01
iter 500, train loss 2803.0380859375, val loss None, lr 0.01
iter 750, train loss 2802.2509765625, val loss None, lr 0.01
iter 1000, train loss 2708.01806640625, val loss None, lr 0.003333
iter 1250, train loss 2688.700927734375, val loss None, lr 0.003333
iter 1500, train loss 2694.67578125, val loss None, lr 0.003333
iter 1750, train loss 2668.4755859375, val loss None, lr 0.001111
iter 2000, train loss 2662.61474609375, val loss None, lr 0.001111
iter 2250, train loss 2655.609375, val loss None, lr 0.00037
best loss 2652.209716796875
running bpv: 0.34193973981345116
layer10: mlp.up_proj
weight shape torch.Size([11008, 4096])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([22, 8, 8, 8]), -3, -2
iter 0, train loss 7407.140625, val loss None, lr 0.01
iter 250, train loss 2386.02099609375, val loss None, lr 0.01
iter 500, train loss 2356.2138671875, val loss None, lr 0.01
iter 750, train loss 2318.926025390625, val loss None, lr 0.01
iter 1000, train loss 2311.691650390625, val loss None, lr 0.01
iter 1250, train loss 2300.06103515625, val loss None, lr 0.01
iter 1500, train loss 2258.28076171875, val loss None, lr 0.003333
iter 1750, train loss 2249.0166015625, val loss None, lr 0.003333
iter 2000, train loss 2241.09326171875, val loss None, lr 0.001111
iter 2250, train loss 2237.32470703125, val loss None, lr 0.001111
best loss 2234.1005859375
running bpv: 0.33899113581730766
layer10: mlp.down_proj
weight shape torch.Size([4096, 11008])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 22, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 142.64019775390625, val loss None, lr 0.01
iter 250, train loss 27.817134857177734, val loss None, lr 0.01
iter 500, train loss 26.772171020507812, val loss None, lr 0.01
iter 750, train loss 26.195552825927734, val loss None, lr 0.01
iter 1000, train loss 26.676889419555664, val loss None, lr 0.01
iter 1250, train loss 25.620880126953125, val loss None, lr 0.003333
iter 1500, train loss 25.519580841064453, val loss None, lr 0.003333
iter 1750, train loss 25.603229522705078, val loss None, lr 0.003333
iter 2000, train loss 25.589540481567383, val loss None, lr 0.001111
iter 2250, train loss 25.342876434326172, val loss None, lr 0.001111
best loss 25.300655364990234
running bpv: 0.3393579987046632
16142 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:09,  3.24it/s]Inference:   6%|▋         | 2/32 [00:00<00:09,  3.06it/s]Inference:   9%|▉         | 3/32 [00:00<00:09,  3.02it/s]Inference:  12%|█▎        | 4/32 [00:01<00:09,  2.99it/s]Inference:  16%|█▌        | 5/32 [00:01<00:09,  2.97it/s]Inference:  19%|█▉        | 6/32 [00:02<00:08,  2.95it/s]Inference:  22%|██▏       | 7/32 [00:02<00:08,  2.95it/s]Inference:  25%|██▌       | 8/32 [00:02<00:08,  2.95it/s]Inference:  28%|██▊       | 9/32 [00:03<00:07,  2.94it/s]Inference:  31%|███▏      | 10/32 [00:03<00:07,  2.93it/s]Inference:  34%|███▍      | 11/32 [00:03<00:07,  2.94it/s]Inference:  38%|███▊      | 12/32 [00:04<00:06,  2.94it/s]Inference:  41%|████      | 13/32 [00:04<00:06,  2.94it/s]Inference:  44%|████▍     | 14/32 [00:04<00:05,  3.04it/s]Inference:  47%|████▋     | 15/32 [00:05<00:05,  3.02it/s]Inference:  50%|█████     | 16/32 [00:05<00:05,  2.98it/s]Inference:  53%|█████▎    | 17/32 [00:05<00:05,  2.97it/s]Inference:  56%|█████▋    | 18/32 [00:06<00:04,  2.95it/s]Inference:  59%|█████▉    | 19/32 [00:06<00:04,  2.95it/s]Inference:  62%|██████▎   | 20/32 [00:06<00:04,  2.94it/s]Inference:  66%|██████▌   | 21/32 [00:07<00:03,  2.95it/s]Inference:  69%|██████▉   | 22/32 [00:07<00:03,  2.94it/s]Inference:  72%|███████▏  | 23/32 [00:07<00:03,  2.95it/s]Inference:  75%|███████▌  | 24/32 [00:08<00:02,  2.94it/s]Inference:  78%|███████▊  | 25/32 [00:08<00:02,  2.95it/s]Inference:  81%|████████▏ | 26/32 [00:08<00:02,  2.95it/s]Inference:  84%|████████▍ | 27/32 [00:09<00:01,  2.93it/s]Inference:  88%|████████▊ | 28/32 [00:09<00:01,  2.95it/s]Inference:  91%|█████████ | 29/32 [00:09<00:01,  2.94it/s]Inference:  94%|█████████▍| 30/32 [00:10<00:00,  2.95it/s]Inference:  97%|█████████▋| 31/32 [00:10<00:00,  2.94it/s]Inference: 100%|██████████| 32/32 [00:10<00:00,  2.94it/s]Inference: 100%|██████████| 32/32 [00:10<00:00,  2.96it/s]
9536 MiB free out of 48676 MiB total
Saved layer 10 to ./models/meta-llama/Llama-2-7b-hf/tensorized/128/64/no_finetune/layer_10.pt
after cast to cpu
13566 MiB free out of 48676 MiB total
running bpv: 0.3393579987046632
Done with layer 10 total_time elapsed: 20941 estimated time left: 39979
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:30,  1.03it/s]Inference:   6%|▋         | 2/32 [00:01<00:29,  1.03it/s]Inference:   9%|▉         | 3/32 [00:02<00:28,  1.03it/s]Inference:  12%|█▎        | 4/32 [00:03<00:27,  1.02it/s]Inference:  16%|█▌        | 5/32 [00:04<00:26,  1.02it/s]Inference:  19%|█▉        | 6/32 [00:05<00:25,  1.02it/s]Inference:  22%|██▏       | 7/32 [00:06<00:24,  1.02it/s]Inference:  25%|██▌       | 8/32 [00:07<00:23,  1.02it/s]Inference:  28%|██▊       | 9/32 [00:08<00:22,  1.02it/s]Inference:  31%|███▏      | 10/32 [00:09<00:21,  1.02it/s]Inference:  34%|███▍      | 11/32 [00:10<00:20,  1.02it/s]Inference:  38%|███▊      | 12/32 [00:11<00:19,  1.02it/s]Inference:  41%|████      | 13/32 [00:12<00:18,  1.02it/s]Inference:  44%|████▍     | 14/32 [00:13<00:17,  1.02it/s]Inference:  47%|████▋     | 15/32 [00:14<00:16,  1.02it/s]Inference:  50%|█████     | 16/32 [00:15<00:15,  1.02it/s]Inference:  53%|█████▎    | 17/32 [00:16<00:14,  1.02it/s]Inference:  56%|█████▋    | 18/32 [00:17<00:13,  1.02it/s]Inference:  59%|█████▉    | 19/32 [00:18<00:12,  1.02it/s]Inference:  62%|██████▎   | 20/32 [00:19<00:11,  1.02it/s]Inference:  66%|██████▌   | 21/32 [00:20<00:10,  1.02it/s]Inference:  69%|██████▉   | 22/32 [00:21<00:09,  1.02it/s]Inference:  72%|███████▏  | 23/32 [00:22<00:08,  1.02it/s]Inference:  75%|███████▌  | 24/32 [00:23<00:07,  1.02it/s]Inference:  78%|███████▊  | 25/32 [00:24<00:06,  1.02it/s]Inference:  81%|████████▏ | 26/32 [00:25<00:05,  1.02it/s]Inference:  84%|████████▍ | 27/32 [00:26<00:04,  1.02it/s]Inference:  88%|████████▊ | 28/32 [00:27<00:03,  1.02it/s]Inference:  91%|█████████ | 29/32 [00:28<00:02,  1.02it/s]Inference:  94%|█████████▍| 30/32 [00:29<00:01,  1.02it/s]Inference:  97%|█████████▋| 31/32 [00:30<00:00,  1.02it/s]Inference: 100%|██████████| 32/32 [00:31<00:00,  1.02it/s]Inference: 100%|██████████| 32/32 [00:31<00:00,  1.02it/s]
layer11: self_attn.q_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 79969.28125, val loss None, lr 0.01
iter 250, train loss 2906.97119140625, val loss None, lr 0.01
iter 500, train loss 2595.816650390625, val loss None, lr 0.01
iter 750, train loss 2504.740966796875, val loss None, lr 0.01
iter 1000, train loss 2366.94482421875, val loss None, lr 0.003333
iter 1250, train loss 2357.365234375, val loss None, lr 0.003333
iter 1500, train loss 2309.1552734375, val loss None, lr 0.001111
iter 1750, train loss 2283.59814453125, val loss None, lr 0.001111
iter 2000, train loss 2270.80517578125, val loss None, lr 0.001111
iter 2250, train loss 2257.511962890625, val loss None, lr 0.00037
best loss 2250.44091796875
running bpv: 0.3406764989481066
layer11: self_attn.k_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 98519.21875, val loss None, lr 0.01
iter 250, train loss 2475.84033203125, val loss None, lr 0.01
iter 500, train loss 2266.404296875, val loss None, lr 0.01
iter 750, train loss 2129.385498046875, val loss None, lr 0.01
iter 1000, train loss 1996.389892578125, val loss None, lr 0.003333
iter 1250, train loss 1958.890869140625, val loss None, lr 0.001111
iter 1500, train loss 1942.4625244140625, val loss None, lr 0.001111
iter 1750, train loss 1931.4078369140625, val loss None, lr 0.001111
iter 2000, train loss 1922.43115234375, val loss None, lr 0.001111
iter 2250, train loss 1914.079833984375, val loss None, lr 0.001111
best loss 1905.037109375
running bpv: 0.3419754205336427
layer11: self_attn.v_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 4762.11474609375, val loss None, lr 0.01
iter 250, train loss 753.6795654296875, val loss None, lr 0.01
iter 500, train loss 723.1807250976562, val loss None, lr 0.01
iter 750, train loss 707.4459228515625, val loss None, lr 0.01
iter 1000, train loss 701.7538452148438, val loss None, lr 0.01
iter 1250, train loss 695.4002685546875, val loss None, lr 0.003333
iter 1500, train loss 691.734619140625, val loss None, lr 0.003333
iter 1750, train loss 688.7830200195312, val loss None, lr 0.001111
iter 2000, train loss 686.9693603515625, val loss None, lr 0.001111
iter 2250, train loss 685.8338623046875, val loss None, lr 0.001111
best loss 684.6489868164062
running bpv: 0.34325519633809304
layer11: self_attn.o_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 636.4693603515625, val loss None, lr 0.01
iter 250, train loss 53.20521926879883, val loss None, lr 0.01
iter 500, train loss 51.482139587402344, val loss None, lr 0.01
iter 750, train loss 48.935855865478516, val loss None, lr 0.01
iter 1000, train loss 51.39145278930664, val loss None, lr 0.01
iter 1250, train loss 45.619781494140625, val loss None, lr 0.003333
iter 1500, train loss 45.34397888183594, val loss None, lr 0.003333
iter 1750, train loss 45.470645904541016, val loss None, lr 0.003333
iter 2000, train loss 44.85239028930664, val loss None, lr 0.001111
iter 2250, train loss 44.753684997558594, val loss None, lr 0.001111
best loss 44.66499710083008
running bpv: 0.3445162465706447
layer11: mlp.gate_proj
weight shape torch.Size([11008, 4096])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([22, 8, 8, 8]), -3, -2
iter 0, train loss 12616.6064453125, val loss None, lr 0.01
iter 250, train loss 3051.1953125, val loss None, lr 0.01
iter 500, train loss 2978.8623046875, val loss None, lr 0.01
iter 750, train loss 2909.023193359375, val loss None, lr 0.01
iter 1000, train loss 2881.706787109375, val loss None, lr 0.01
iter 1250, train loss 2823.98974609375, val loss None, lr 0.003333
iter 1500, train loss 2808.1689453125, val loss None, lr 0.001111
iter 1750, train loss 2801.116455078125, val loss None, lr 0.001111
iter 2000, train loss 2794.5537109375, val loss None, lr 0.001111
iter 2250, train loss 2790.044189453125, val loss None, lr 0.001111
best loss 2785.835205078125
running bpv: 0.3417162976457399
layer11: mlp.up_proj
weight shape torch.Size([11008, 4096])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([22, 8, 8, 8]), -3, -2
iter 0, train loss 7641.76904296875, val loss None, lr 0.01
iter 250, train loss 2578.58154296875, val loss None, lr 0.01
iter 500, train loss 2524.7041015625, val loss None, lr 0.01
iter 750, train loss 2531.297607421875, val loss None, lr 0.01
iter 1000, train loss 2490.8271484375, val loss None, lr 0.01
iter 1250, train loss 2455.05126953125, val loss None, lr 0.003333
iter 1500, train loss 2446.92236328125, val loss None, lr 0.003333
iter 1750, train loss 2443.78759765625, val loss None, lr 0.003333
iter 2000, train loss 2439.581787109375, val loss None, lr 0.003333
iter 2250, train loss 2436.3193359375, val loss None, lr 0.003333
best loss 2429.427734375
running bpv: 0.3390222860756709
layer11: mlp.down_proj
weight shape torch.Size([4096, 11008])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 22, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 141.64654541015625, val loss None, lr 0.01
iter 250, train loss 29.324539184570312, val loss None, lr 0.01
iter 500, train loss 27.74347686767578, val loss None, lr 0.01
iter 750, train loss 27.056499481201172, val loss None, lr 0.01
iter 1000, train loss 26.88568115234375, val loss None, lr 0.003333
iter 1250, train loss 26.721879959106445, val loss None, lr 0.003333
iter 1500, train loss 26.588912963867188, val loss None, lr 0.001111
iter 1750, train loss 26.519638061523438, val loss None, lr 0.001111
iter 2000, train loss 26.531625747680664, val loss None, lr 0.001111
iter 2250, train loss 26.470687866210938, val loss None, lr 0.001111
best loss 26.420835494995117
running bpv: 0.3393579987046632
13566 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:09,  3.12it/s]Inference:   6%|▋         | 2/32 [00:00<00:10,  2.93it/s]Inference:   9%|▉         | 3/32 [00:01<00:10,  2.87it/s]Inference:  12%|█▎        | 4/32 [00:01<00:09,  2.85it/s]Inference:  16%|█▌        | 5/32 [00:01<00:09,  2.83it/s]Inference:  19%|█▉        | 6/32 [00:02<00:09,  2.85it/s]Inference:  22%|██▏       | 7/32 [00:02<00:08,  2.84it/s]Inference:  25%|██▌       | 8/32 [00:02<00:08,  2.84it/s]Inference:  28%|██▊       | 9/32 [00:03<00:08,  2.83it/s]Inference:  31%|███▏      | 10/32 [00:03<00:07,  2.82it/s]Inference:  34%|███▍      | 11/32 [00:03<00:07,  2.81it/s]Inference:  38%|███▊      | 12/32 [00:04<00:07,  2.80it/s]Inference:  41%|████      | 13/32 [00:04<00:06,  2.81it/s]Inference:  44%|████▍     | 14/32 [00:04<00:06,  2.81it/s]Inference:  47%|████▋     | 15/32 [00:05<00:06,  2.82it/s]Inference:  50%|█████     | 16/32 [00:05<00:05,  2.82it/s]Inference:  53%|█████▎    | 17/32 [00:05<00:05,  2.83it/s]Inference:  56%|█████▋    | 18/32 [00:06<00:04,  2.82it/s]Inference:  59%|█████▉    | 19/32 [00:06<00:04,  2.81it/s]Inference:  62%|██████▎   | 20/32 [00:07<00:04,  2.80it/s]Inference:  66%|██████▌   | 21/32 [00:07<00:03,  2.81it/s]Inference:  69%|██████▉   | 22/32 [00:07<00:03,  2.82it/s]Inference:  72%|███████▏  | 23/32 [00:08<00:03,  2.82it/s]Inference:  75%|███████▌  | 24/32 [00:08<00:02,  2.83it/s]Inference:  78%|███████▊  | 25/32 [00:08<00:02,  2.83it/s]Inference:  81%|████████▏ | 26/32 [00:09<00:02,  2.81it/s]Inference:  84%|████████▍ | 27/32 [00:09<00:01,  2.79it/s]Inference:  88%|████████▊ | 28/32 [00:09<00:01,  2.79it/s]Inference:  91%|█████████ | 29/32 [00:10<00:01,  2.78it/s]Inference:  94%|█████████▍| 30/32 [00:10<00:00,  2.78it/s]Inference:  97%|█████████▋| 31/32 [00:11<00:00,  2.77it/s]Inference: 100%|██████████| 32/32 [00:11<00:00,  2.79it/s]Inference: 100%|██████████| 32/32 [00:11<00:00,  2.82it/s]
6960 MiB free out of 48676 MiB total
Saved layer 11 to ./models/meta-llama/Llama-2-7b-hf/tensorized/128/64/no_finetune/layer_11.pt
after cast to cpu
10990 MiB free out of 48676 MiB total
running bpv: 0.3393579987046632
Done with layer 11 total_time elapsed: 22840 estimated time left: 38067
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:30,  1.01it/s]Inference:   6%|▋         | 2/32 [00:01<00:29,  1.01it/s]Inference:   9%|▉         | 3/32 [00:02<00:28,  1.01it/s]Inference:  12%|█▎        | 4/32 [00:03<00:27,  1.01it/s]Inference:  16%|█▌        | 5/32 [00:04<00:26,  1.01it/s]Inference:  19%|█▉        | 6/32 [00:05<00:25,  1.00it/s]Inference:  22%|██▏       | 7/32 [00:06<00:24,  1.00it/s]Inference:  25%|██▌       | 8/32 [00:07<00:23,  1.01it/s]Inference:  28%|██▊       | 9/32 [00:08<00:22,  1.01it/s]Inference:  31%|███▏      | 10/32 [00:09<00:21,  1.01it/s]Inference:  34%|███▍      | 11/32 [00:10<00:20,  1.01it/s]Inference:  38%|███▊      | 12/32 [00:11<00:19,  1.01it/s]Inference:  41%|████      | 13/32 [00:12<00:18,  1.01it/s]Inference:  44%|████▍     | 14/32 [00:13<00:17,  1.01it/s]Inference:  47%|████▋     | 15/32 [00:14<00:16,  1.01it/s]Inference:  50%|█████     | 16/32 [00:15<00:15,  1.01it/s]Inference:  53%|█████▎    | 17/32 [00:16<00:14,  1.01it/s]Inference:  56%|█████▋    | 18/32 [00:17<00:13,  1.01it/s]Inference:  59%|█████▉    | 19/32 [00:18<00:12,  1.01it/s]Inference:  62%|██████▎   | 20/32 [00:19<00:11,  1.01it/s]Inference:  66%|██████▌   | 21/32 [00:20<00:10,  1.01it/s]Inference:  69%|██████▉   | 22/32 [00:21<00:09,  1.01it/s]Inference:  72%|███████▏  | 23/32 [00:22<00:08,  1.01it/s]Inference:  75%|███████▌  | 24/32 [00:23<00:07,  1.01it/s]Inference:  78%|███████▊  | 25/32 [00:24<00:06,  1.01it/s]Inference:  81%|████████▏ | 26/32 [00:25<00:05,  1.01it/s]Inference:  84%|████████▍ | 27/32 [00:26<00:04,  1.01it/s]Inference:  88%|████████▊ | 28/32 [00:27<00:03,  1.01it/s]Inference:  91%|█████████ | 29/32 [00:28<00:02,  1.01it/s]Inference:  94%|█████████▍| 30/32 [00:29<00:01,  1.01it/s]Inference:  97%|█████████▋| 31/32 [00:30<00:00,  1.01it/s]Inference: 100%|██████████| 32/32 [00:31<00:00,  1.01it/s]Inference: 100%|██████████| 32/32 [00:31<00:00,  1.01it/s]
layer12: self_attn.q_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 76853.375, val loss None, lr 0.01
iter 250, train loss 2756.537353515625, val loss None, lr 0.01
iter 500, train loss 2611.12451171875, val loss None, lr 0.01
iter 750, train loss 2387.345703125, val loss None, lr 0.01
iter 1000, train loss 2375.78173828125, val loss None, lr 0.01
iter 1250, train loss 2343.319580078125, val loss None, lr 0.01
iter 1500, train loss 2247.465087890625, val loss None, lr 0.003333
iter 1750, train loss 2245.548828125, val loss None, lr 0.003333
iter 2000, train loss 2209.756103515625, val loss None, lr 0.001111
iter 2250, train loss 2202.07958984375, val loss None, lr 0.001111
best loss 2194.557373046875
running bpv: 0.34056737778730706
layer12: self_attn.k_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 97684.109375, val loss None, lr 0.01
iter 250, train loss 2640.320068359375, val loss None, lr 0.01
iter 500, train loss 2375.041259765625, val loss None, lr 0.01
iter 750, train loss 2220.297607421875, val loss None, lr 0.003333
iter 1000, train loss 2165.466796875, val loss None, lr 0.003333
iter 1250, train loss 2166.8701171875, val loss None, lr 0.003333
iter 1500, train loss 2137.36083984375, val loss None, lr 0.001111
iter 1750, train loss 2108.63330078125, val loss None, lr 0.001111
iter 2000, train loss 2101.466064453125, val loss None, lr 0.001111
iter 2250, train loss 2087.9638671875, val loss None, lr 0.00037
best loss 2082.227294921875
running bpv: 0.3417602747018739
layer12: self_attn.v_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 4542.58154296875, val loss None, lr 0.01
iter 250, train loss 731.5640869140625, val loss None, lr 0.01
iter 500, train loss 700.8226318359375, val loss None, lr 0.01
iter 750, train loss 692.3250122070312, val loss None, lr 0.01
iter 1000, train loss 681.2125854492188, val loss None, lr 0.01
iter 1250, train loss 678.7239990234375, val loss None, lr 0.01
iter 1500, train loss 671.32861328125, val loss None, lr 0.003333
iter 1750, train loss 670.5580444335938, val loss None, lr 0.003333
iter 2000, train loss 666.573974609375, val loss None, lr 0.001111
iter 2250, train loss 665.537353515625, val loss None, lr 0.001111
best loss 664.41357421875
running bpv: 0.34293702411167515
layer12: self_attn.o_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 680.3773803710938, val loss None, lr 0.01
iter 250, train loss 45.70119094848633, val loss None, lr 0.01
iter 500, train loss 40.80284881591797, val loss None, lr 0.003333
iter 750, train loss 37.68205261230469, val loss None, lr 0.003333
iter 1000, train loss 36.717994689941406, val loss None, lr 0.003333
iter 1250, train loss 36.4556999206543, val loss None, lr 0.003333
iter 1500, train loss 35.90053939819336, val loss None, lr 0.001111
iter 1750, train loss 35.595760345458984, val loss None, lr 0.001111
iter 2000, train loss 35.474830627441406, val loss None, lr 0.001111
iter 2250, train loss 35.498897552490234, val loss None, lr 0.001111
best loss 35.23093795776367
running bpv: 0.34409795168067225
layer12: mlp.gate_proj
weight shape torch.Size([11008, 4096])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([22, 8, 8, 8]), -3, -2
iter 0, train loss 13282.51171875, val loss None, lr 0.01
iter 250, train loss 2993.3271484375, val loss None, lr 0.01
iter 500, train loss 2839.0400390625, val loss None, lr 0.01
iter 750, train loss 2814.6865234375, val loss None, lr 0.01
iter 1000, train loss 2762.81640625, val loss None, lr 0.003333
iter 1250, train loss 2746.986328125, val loss None, lr 0.003333
iter 1500, train loss 2728.691650390625, val loss None, lr 0.001111
iter 1750, train loss 2720.664306640625, val loss None, lr 0.001111
iter 2000, train loss 2716.198486328125, val loss None, lr 0.00037
iter 2250, train loss 2711.86328125, val loss None, lr 0.00037
best loss 2709.34228515625
running bpv: 0.3415284513000413
layer12: mlp.up_proj
weight shape torch.Size([11008, 4096])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([22, 8, 8, 8]), -3, -2
iter 0, train loss 7982.5634765625, val loss None, lr 0.01
iter 250, train loss 2633.9775390625, val loss None, lr 0.01
iter 500, train loss 2574.60302734375, val loss None, lr 0.01
iter 750, train loss 2557.10693359375, val loss None, lr 0.01
iter 1000, train loss 2548.788330078125, val loss None, lr 0.01
iter 1250, train loss 2502.438232421875, val loss None, lr 0.003333
iter 1500, train loss 2492.0029296875, val loss None, lr 0.001111
iter 1750, train loss 2487.559326171875, val loss None, lr 0.001111
iter 2000, train loss 2484.05517578125, val loss None, lr 0.001111
iter 2250, train loss 2480.220458984375, val loss None, lr 0.00037
best loss 2477.858154296875
running bpv: 0.3390485604217356
layer12: mlp.down_proj
weight shape torch.Size([4096, 11008])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 22, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 153.20285034179688, val loss None, lr 0.01
iter 250, train loss 30.59750747680664, val loss None, lr 0.01
iter 500, train loss 28.3737850189209, val loss None, lr 0.01
iter 750, train loss 27.349742889404297, val loss None, lr 0.003333
iter 1000, train loss 27.223812103271484, val loss None, lr 0.003333
iter 1250, train loss 27.00446319580078, val loss None, lr 0.001111
iter 1500, train loss 26.94478988647461, val loss None, lr 0.001111
iter 1750, train loss 26.898624420166016, val loss None, lr 0.001111
iter 2000, train loss 26.843578338623047, val loss None, lr 0.001111
iter 2250, train loss 26.7836856842041, val loss None, lr 0.001111
best loss 26.752262115478516
running bpv: 0.3393579987046632
10990 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:09,  3.24it/s]Inference:   6%|▋         | 2/32 [00:00<00:09,  3.00it/s]Inference:   9%|▉         | 3/32 [00:01<00:09,  2.93it/s]Inference:  12%|█▎        | 4/32 [00:01<00:09,  2.88it/s]Inference:  16%|█▌        | 5/32 [00:01<00:09,  2.86it/s]Inference:  19%|█▉        | 6/32 [00:02<00:09,  2.84it/s]Inference:  22%|██▏       | 7/32 [00:02<00:08,  2.84it/s]Inference:  25%|██▌       | 8/32 [00:02<00:08,  2.83it/s]Inference:  28%|██▊       | 9/32 [00:03<00:08,  2.84it/s]Inference:  31%|███▏      | 10/32 [00:03<00:07,  2.83it/s]Inference:  34%|███▍      | 11/32 [00:03<00:07,  2.85it/s]Inference:  38%|███▊      | 12/32 [00:04<00:07,  2.85it/s]Inference:  41%|████      | 13/32 [00:04<00:06,  2.84it/s]Inference:  44%|████▍     | 14/32 [00:04<00:06,  2.84it/s]Inference:  47%|████▋     | 15/32 [00:05<00:05,  2.83it/s]Inference:  50%|█████     | 16/32 [00:05<00:05,  2.82it/s]Inference:  53%|█████▎    | 17/32 [00:05<00:05,  2.82it/s]Inference:  56%|█████▋    | 18/32 [00:06<00:04,  2.81it/s]Inference:  59%|█████▉    | 19/32 [00:06<00:04,  2.81it/s]Inference:  62%|██████▎   | 20/32 [00:07<00:04,  2.80it/s]Inference:  66%|██████▌   | 21/32 [00:07<00:03,  2.80it/s]Inference:  69%|██████▉   | 22/32 [00:07<00:03,  2.79it/s]Inference:  72%|███████▏  | 23/32 [00:08<00:03,  2.78it/s]Inference:  75%|███████▌  | 24/32 [00:08<00:02,  2.77it/s]Inference:  78%|███████▊  | 25/32 [00:08<00:02,  2.78it/s]Inference:  81%|████████▏ | 26/32 [00:09<00:02,  2.77it/s]Inference:  84%|████████▍ | 27/32 [00:09<00:01,  2.77it/s]Inference:  88%|████████▊ | 28/32 [00:09<00:01,  2.76it/s]Inference:  91%|█████████ | 29/32 [00:10<00:01,  2.77it/s]Inference:  94%|█████████▍| 30/32 [00:10<00:00,  2.77it/s]Inference:  97%|█████████▋| 31/32 [00:11<00:00,  2.76it/s]Inference: 100%|██████████| 32/32 [00:11<00:00,  2.75it/s]Inference: 100%|██████████| 32/32 [00:11<00:00,  2.81it/s]
4512 MiB free out of 48676 MiB total
Saved layer 12 to ./models/meta-llama/Llama-2-7b-hf/tensorized/128/64/no_finetune/layer_12.pt
after cast to cpu
8542 MiB free out of 48676 MiB total
running bpv: 0.3393579987046632
Done with layer 12 total_time elapsed: 24740 estimated time left: 36159
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:30,  1.01it/s]Inference:   6%|▋         | 2/32 [00:01<00:29,  1.01it/s]Inference:   9%|▉         | 3/32 [00:02<00:28,  1.00it/s]Inference:  12%|█▎        | 4/32 [00:04<00:28,  1.00s/it]Inference:  16%|█▌        | 5/32 [00:04<00:27,  1.00s/it]Inference:  19%|█▉        | 6/32 [00:06<00:26,  1.00s/it]Inference:  22%|██▏       | 7/32 [00:07<00:25,  1.00s/it]Inference:  25%|██▌       | 8/32 [00:08<00:24,  1.00s/it]Inference:  28%|██▊       | 9/32 [00:09<00:23,  1.00s/it]Inference:  31%|███▏      | 10/32 [00:10<00:21,  1.00it/s]Inference:  34%|███▍      | 11/32 [00:11<00:21,  1.00s/it]Inference:  38%|███▊      | 12/32 [00:12<00:20,  1.00s/it]Inference:  41%|████      | 13/32 [00:13<00:19,  1.00s/it]Inference:  44%|████▍     | 14/32 [00:14<00:17,  1.00it/s]Inference:  47%|████▋     | 15/32 [00:15<00:17,  1.00s/it]Inference:  50%|█████     | 16/32 [00:16<00:16,  1.00s/it]Inference:  53%|█████▎    | 17/32 [00:17<00:14,  1.00it/s]Inference:  56%|█████▋    | 18/32 [00:17<00:13,  1.00it/s]Inference:  59%|█████▉    | 19/32 [00:18<00:12,  1.00it/s]Inference:  62%|██████▎   | 20/32 [00:19<00:11,  1.00it/s]Inference:  66%|██████▌   | 21/32 [00:20<00:10,  1.00it/s]Inference:  69%|██████▉   | 22/32 [00:21<00:10,  1.00s/it]Inference:  72%|███████▏  | 23/32 [00:22<00:08,  1.00it/s]Inference:  75%|███████▌  | 24/32 [00:23<00:08,  1.00s/it]Inference:  78%|███████▊  | 25/32 [00:24<00:07,  1.00s/it]Inference:  81%|████████▏ | 26/32 [00:26<00:06,  1.00s/it]Inference:  84%|████████▍ | 27/32 [00:26<00:04,  1.00it/s]Inference:  88%|████████▊ | 28/32 [00:27<00:03,  1.00it/s]Inference:  91%|█████████ | 29/32 [00:28<00:02,  1.00it/s]Inference:  94%|█████████▍| 30/32 [00:29<00:01,  1.00it/s]Inference:  97%|█████████▋| 31/32 [00:30<00:00,  1.00it/s]Inference: 100%|██████████| 32/32 [00:31<00:00,  1.00it/s]Inference: 100%|██████████| 32/32 [00:31<00:00,  1.00it/s]
layer13: self_attn.q_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 77847.171875, val loss None, lr 0.01
iter 250, train loss 2723.330810546875, val loss None, lr 0.01
iter 500, train loss 2544.543701171875, val loss None, lr 0.01
iter 750, train loss 2391.707763671875, val loss None, lr 0.003333
iter 1000, train loss 2342.6064453125, val loss None, lr 0.003333
iter 1250, train loss 2309.130859375, val loss None, lr 0.001111
iter 1500, train loss 2290.240234375, val loss None, lr 0.001111
iter 1750, train loss 2277.384033203125, val loss None, lr 0.001111
iter 2000, train loss 2268.2431640625, val loss None, lr 0.001111
iter 2250, train loss 2258.892578125, val loss None, lr 0.001111
best loss 2249.765869140625
running bpv: 0.34047493811881185
layer13: self_attn.k_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 103660.328125, val loss None, lr 0.01
iter 250, train loss 2525.010009765625, val loss None, lr 0.01
iter 500, train loss 2437.528076171875, val loss None, lr 0.01
iter 750, train loss 2307.272705078125, val loss None, lr 0.01
iter 1000, train loss 2162.2197265625, val loss None, lr 0.003333
iter 1250, train loss 2130.11474609375, val loss None, lr 0.003333
iter 1500, train loss 2099.4013671875, val loss None, lr 0.001111
iter 1750, train loss 2078.9951171875, val loss None, lr 0.001111
iter 2000, train loss 2068.819580078125, val loss None, lr 0.001111
iter 2250, train loss 2063.65576171875, val loss None, lr 0.001111
best loss 2050.4892578125
running bpv: 0.3415778113931523
layer13: self_attn.v_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 4707.673828125, val loss None, lr 0.01
iter 250, train loss 816.1761474609375, val loss None, lr 0.01
iter 500, train loss 782.9893188476562, val loss None, lr 0.01
iter 750, train loss 774.3544921875, val loss None, lr 0.01
iter 1000, train loss 765.1332397460938, val loss None, lr 0.01
iter 1250, train loss 754.88916015625, val loss None, lr 0.003333
iter 1500, train loss 753.8203125, val loss None, lr 0.003333
iter 1750, train loss 749.7830810546875, val loss None, lr 0.001111
iter 2000, train loss 748.3546142578125, val loss None, lr 0.001111
iter 2250, train loss 747.4891357421875, val loss None, lr 0.001111
best loss 746.479248046875
running bpv: 0.34266688257723893
layer13: self_attn.o_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 493.75048828125, val loss None, lr 0.01
iter 250, train loss 37.380821228027344, val loss None, lr 0.01
iter 500, train loss 36.075443267822266, val loss None, lr 0.01
iter 750, train loss 36.80299758911133, val loss None, lr 0.003333
iter 1000, train loss 32.04168701171875, val loss None, lr 0.003333
iter 1250, train loss 31.78158187866211, val loss None, lr 0.003333
iter 1500, train loss 31.5689697265625, val loss None, lr 0.001111
iter 1750, train loss 31.40059471130371, val loss None, lr 0.001111
iter 2000, train loss 31.36629867553711, val loss None, lr 0.001111
iter 2250, train loss 31.13852882385254, val loss None, lr 0.001111
best loss 31.076074600219727
running bpv: 0.34374240915274
layer13: mlp.gate_proj
weight shape torch.Size([11008, 4096])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([22, 8, 8, 8]), -3, -2
iter 0, train loss 13817.150390625, val loss None, lr 0.01
iter 250, train loss 3024.4814453125, val loss None, lr 0.01
iter 500, train loss 2924.75634765625, val loss None, lr 0.01
iter 750, train loss 2926.2822265625, val loss None, lr 0.01
iter 1000, train loss 2819.7685546875, val loss None, lr 0.003333
iter 1250, train loss 2800.680908203125, val loss None, lr 0.003333
iter 1500, train loss 2786.2373046875, val loss None, lr 0.003333
iter 1750, train loss 2780.44580078125, val loss None, lr 0.003333
iter 2000, train loss 2776.310791015625, val loss None, lr 0.003333
iter 2250, train loss 2766.33349609375, val loss None, lr 0.001111
best loss 2761.32666015625
running bpv: 0.3413683223432722
layer13: mlp.up_proj
weight shape torch.Size([11008, 4096])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([22, 8, 8, 8]), -3, -2
iter 0, train loss 8351.8388671875, val loss None, lr 0.01
iter 250, train loss 2773.52001953125, val loss None, lr 0.01
iter 500, train loss 2722.2802734375, val loss None, lr 0.01
iter 750, train loss 2691.6044921875, val loss None, lr 0.01
iter 1000, train loss 2649.843994140625, val loss None, lr 0.003333
iter 1250, train loss 2634.856689453125, val loss None, lr 0.001111
iter 1500, train loss 2628.821044921875, val loss None, lr 0.001111
iter 1750, train loss 2624.201171875, val loss None, lr 0.001111
iter 2000, train loss 2619.211669921875, val loss None, lr 0.001111
iter 2250, train loss 2615.730224609375, val loss None, lr 0.00037
best loss 2613.294189453125
running bpv: 0.3390710205904475
layer13: mlp.down_proj
weight shape torch.Size([4096, 11008])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 22, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 177.2791748046875, val loss None, lr 0.01
iter 250, train loss 33.598785400390625, val loss None, lr 0.01
iter 500, train loss 32.68927001953125, val loss None, lr 0.01
iter 750, train loss 31.07461929321289, val loss None, lr 0.01
iter 1000, train loss 30.729156494140625, val loss None, lr 0.003333
iter 1250, train loss 30.521930694580078, val loss None, lr 0.001111
iter 1500, train loss 30.435855865478516, val loss None, lr 0.001111
iter 1750, train loss 30.378646850585938, val loss None, lr 0.001111
iter 2000, train loss 30.369935989379883, val loss None, lr 0.001111
iter 2250, train loss 30.27109146118164, val loss None, lr 0.001111
best loss 30.23590660095215
running bpv: 0.3393579987046632
8542 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:09,  3.12it/s]Inference:   6%|▋         | 2/32 [00:00<00:10,  2.87it/s]Inference:   9%|▉         | 3/32 [00:01<00:10,  2.79it/s]Inference:  12%|█▎        | 4/32 [00:01<00:10,  2.77it/s]Inference:  16%|█▌        | 5/32 [00:01<00:09,  2.76it/s]Inference:  19%|█▉        | 6/32 [00:02<00:09,  2.76it/s]Inference:  22%|██▏       | 7/32 [00:02<00:09,  2.74it/s]Inference:  25%|██▌       | 8/32 [00:02<00:08,  2.75it/s]Inference:  28%|██▊       | 9/32 [00:03<00:08,  2.73it/s]Inference:  31%|███▏      | 10/32 [00:03<00:08,  2.73it/s]Inference:  34%|███▍      | 11/32 [00:03<00:07,  2.73it/s]Inference:  38%|███▊      | 12/32 [00:04<00:07,  2.72it/s]Inference:  41%|████      | 13/32 [00:04<00:06,  2.73it/s]Inference:  44%|████▍     | 14/32 [00:05<00:06,  2.73it/s]Inference:  47%|████▋     | 15/32 [00:05<00:06,  2.73it/s]Inference:  50%|█████     | 16/32 [00:05<00:05,  2.73it/s]Inference:  53%|█████▎    | 17/32 [00:06<00:05,  2.73it/s]Inference:  56%|█████▋    | 18/32 [00:06<00:05,  2.73it/s]Inference:  59%|█████▉    | 19/32 [00:06<00:04,  2.73it/s]Inference:  62%|██████▎   | 20/32 [00:07<00:04,  2.73it/s]Inference:  66%|██████▌   | 21/32 [00:07<00:04,  2.73it/s]Inference:  69%|██████▉   | 22/32 [00:08<00:03,  2.73it/s]Inference:  72%|███████▏  | 23/32 [00:08<00:03,  2.72it/s]Inference:  75%|███████▌  | 24/32 [00:08<00:02,  2.72it/s]Inference:  78%|███████▊  | 25/32 [00:09<00:02,  2.72it/s]Inference:  81%|████████▏ | 26/32 [00:09<00:02,  2.73it/s]Inference:  84%|████████▍ | 27/32 [00:09<00:01,  2.72it/s]Inference:  88%|████████▊ | 28/32 [00:10<00:01,  2.72it/s]Inference:  91%|█████████ | 29/32 [00:10<00:01,  2.72it/s]Inference:  94%|█████████▍| 30/32 [00:10<00:00,  2.73it/s]Inference:  97%|█████████▋| 31/32 [00:11<00:00,  2.73it/s]Inference: 100%|██████████| 32/32 [00:11<00:00,  2.73it/s]Inference: 100%|██████████| 32/32 [00:11<00:00,  2.74it/s]
1936 MiB free out of 48676 MiB total
Saved layer 13 to ./models/meta-llama/Llama-2-7b-hf/tensorized/128/64/no_finetune/layer_13.pt
after cast to cpu
5966 MiB free out of 48676 MiB total
running bpv: 0.3393579987046632
Done with layer 13 total_time elapsed: 26644 estimated time left: 34256
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:30,  1.01it/s]Inference:   6%|▋         | 2/32 [00:01<00:29,  1.00it/s]Inference:   9%|▉         | 3/32 [00:02<00:28,  1.01it/s]Inference:  12%|█▎        | 4/32 [00:03<00:27,  1.00it/s]Inference:  16%|█▌        | 5/32 [00:04<00:26,  1.00it/s]Inference:  19%|█▉        | 6/32 [00:05<00:25,  1.00it/s]Inference:  22%|██▏       | 7/32 [00:06<00:24,  1.00it/s]Inference:  25%|██▌       | 8/32 [00:07<00:23,  1.00it/s]Inference:  28%|██▊       | 9/32 [00:08<00:22,  1.00it/s]Inference:  31%|███▏      | 10/32 [00:09<00:21,  1.00it/s]Inference:  34%|███▍      | 11/32 [00:10<00:21,  1.00s/it]Inference:  38%|███▊      | 12/32 [00:11<00:19,  1.00it/s]Inference:  41%|████      | 13/32 [00:12<00:18,  1.00it/s]Inference:  44%|████▍     | 14/32 [00:13<00:17,  1.00it/s]Inference:  47%|████▋     | 15/32 [00:14<00:16,  1.00it/s]Inference:  50%|█████     | 16/32 [00:15<00:15,  1.00it/s]Inference:  53%|█████▎    | 17/32 [00:16<00:14,  1.00it/s]Inference:  56%|█████▋    | 18/32 [00:17<00:13,  1.00it/s]Inference:  59%|█████▉    | 19/32 [00:18<00:12,  1.00it/s]Inference:  62%|██████▎   | 20/32 [00:19<00:11,  1.00it/s]Inference:  66%|██████▌   | 21/32 [00:20<00:10,  1.00it/s]Inference:  69%|██████▉   | 22/32 [00:21<00:09,  1.00it/s]Inference:  72%|███████▏  | 23/32 [00:22<00:09,  1.00s/it]Inference:  75%|███████▌  | 24/32 [00:23<00:07,  1.00it/s]Inference:  78%|███████▊  | 25/32 [00:24<00:06,  1.00it/s]Inference:  81%|████████▏ | 26/32 [00:25<00:06,  1.00s/it]Inference:  84%|████████▍ | 27/32 [00:26<00:04,  1.00it/s]Inference:  88%|████████▊ | 28/32 [00:27<00:03,  1.00it/s]Inference:  91%|█████████ | 29/32 [00:28<00:02,  1.00it/s]Inference:  94%|█████████▍| 30/32 [00:29<00:01,  1.00it/s]Inference:  97%|█████████▋| 31/32 [00:30<00:00,  1.02it/s]Inference: 100%|██████████| 32/32 [00:31<00:00,  1.02it/s]Inference: 100%|██████████| 32/32 [00:31<00:00,  1.00it/s]
layer14: self_attn.q_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 72139.1015625, val loss None, lr 0.01
iter 250, train loss 2834.60693359375, val loss None, lr 0.01
iter 500, train loss 2575.485595703125, val loss None, lr 0.01
iter 750, train loss 2441.048095703125, val loss None, lr 0.01
iter 1000, train loss 2441.086669921875, val loss None, lr 0.01
iter 1250, train loss 2294.70947265625, val loss None, lr 0.003333
iter 1500, train loss 2266.588134765625, val loss None, lr 0.001111
iter 1750, train loss 2254.291748046875, val loss None, lr 0.001111
iter 2000, train loss 2243.969482421875, val loss None, lr 0.001111
iter 2250, train loss 2236.691162109375, val loss None, lr 0.001111
best loss 2229.685546875
running bpv: 0.34039562637969095
layer14: self_attn.k_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 98796.375, val loss None, lr 0.01
iter 250, train loss 2515.095703125, val loss None, lr 0.01
iter 500, train loss 2302.084716796875, val loss None, lr 0.01
iter 750, train loss 2334.999267578125, val loss None, lr 0.01
iter 1000, train loss 2262.90576171875, val loss None, lr 0.01
iter 1250, train loss 2219.253662109375, val loss None, lr 0.01
iter 1500, train loss 2108.126953125, val loss None, lr 0.003333
iter 1750, train loss 2063.8671875, val loss None, lr 0.001111
iter 2000, train loss 2048.89404296875, val loss None, lr 0.001111
iter 2250, train loss 2038.180908203125, val loss None, lr 0.001111
best loss 2034.3182373046875
running bpv: 0.34142110918068763
layer14: self_attn.v_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 4255.625, val loss None, lr 0.01
iter 250, train loss 708.073486328125, val loss None, lr 0.01
iter 500, train loss 683.466064453125, val loss None, lr 0.01
iter 750, train loss 674.1732788085938, val loss None, lr 0.01
iter 1000, train loss 662.5075073242188, val loss None, lr 0.003333
iter 1250, train loss 661.99560546875, val loss None, lr 0.003333
iter 1500, train loss 655.9633178710938, val loss None, lr 0.001111
iter 1750, train loss 654.6111450195312, val loss None, lr 0.001111
iter 2000, train loss 653.26318359375, val loss None, lr 0.001111
iter 2250, train loss 652.397705078125, val loss None, lr 0.001111
best loss 651.3912353515625
running bpv: 0.3424346590909091
layer14: self_attn.o_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 555.4293212890625, val loss None, lr 0.01
iter 250, train loss 39.509132385253906, val loss None, lr 0.01
iter 500, train loss 34.3302116394043, val loss None, lr 0.01
iter 750, train loss 33.2103271484375, val loss None, lr 0.01
iter 1000, train loss 32.59950256347656, val loss None, lr 0.003333
iter 1250, train loss 33.08479309082031, val loss None, lr 0.003333
iter 1500, train loss 31.926280975341797, val loss None, lr 0.001111
iter 1750, train loss 31.79906463623047, val loss None, lr 0.001111
iter 2000, train loss 31.71331214904785, val loss None, lr 0.001111
iter 2250, train loss 31.65976905822754, val loss None, lr 0.001111
best loss 31.554929733276367
running bpv: 0.34343648318872017
layer14: mlp.gate_proj
weight shape torch.Size([11008, 4096])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([22, 8, 8, 8]), -3, -2
iter 0, train loss 13684.80859375, val loss None, lr 0.01
iter 250, train loss 3278.943115234375, val loss None, lr 0.01
iter 500, train loss 3191.26171875, val loss None, lr 0.01
iter 750, train loss 3150.9345703125, val loss None, lr 0.01
iter 1000, train loss 3074.49853515625, val loss None, lr 0.003333
iter 1250, train loss 3057.360107421875, val loss None, lr 0.003333
iter 1500, train loss 3041.11669921875, val loss None, lr 0.003333
iter 1750, train loss 3036.770263671875, val loss None, lr 0.003333
iter 2000, train loss 3023.5322265625, val loss None, lr 0.001111
iter 2250, train loss 3017.13134765625, val loss None, lr 0.001111
best loss 3010.530029296875
running bpv: 0.3412301975792097
layer14: mlp.up_proj
weight shape torch.Size([11008, 4096])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([22, 8, 8, 8]), -3, -2
iter 0, train loss 8876.13671875, val loss None, lr 0.01
iter 250, train loss 3055.7470703125, val loss None, lr 0.01
iter 500, train loss 2991.3076171875, val loss None, lr 0.01
iter 750, train loss 2948.69140625, val loss None, lr 0.01
iter 1000, train loss 2931.84716796875, val loss None, lr 0.003333
iter 1250, train loss 2893.97705078125, val loss None, lr 0.003333
iter 1500, train loss 2886.5009765625, val loss None, lr 0.001111
iter 1750, train loss 2882.369384765625, val loss None, lr 0.001111
iter 2000, train loss 2877.27490234375, val loss None, lr 0.00037
iter 2250, train loss 2874.22900390625, val loss None, lr 0.00037
best loss 2871.63623046875
running bpv: 0.3390904409186536
layer14: mlp.down_proj
weight shape torch.Size([4096, 11008])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 22, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 163.66725158691406, val loss None, lr 0.01
iter 250, train loss 36.56707763671875, val loss None, lr 0.01
iter 500, train loss 35.08921813964844, val loss None, lr 0.01
iter 750, train loss 35.04289245605469, val loss None, lr 0.01
iter 1000, train loss 34.18140411376953, val loss None, lr 0.003333
iter 1250, train loss 34.1261100769043, val loss None, lr 0.003333
iter 1500, train loss 33.88627243041992, val loss None, lr 0.001111
iter 1750, train loss 33.81401824951172, val loss None, lr 0.001111
iter 2000, train loss 33.78766632080078, val loss None, lr 0.001111
iter 2250, train loss 33.730892181396484, val loss None, lr 0.001111
best loss 33.707496643066406
running bpv: 0.3393579987046632
5966 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:09,  3.19it/s]Inference:   6%|▋         | 2/32 [00:00<00:10,  2.95it/s]Inference:   9%|▉         | 3/32 [00:01<00:10,  2.88it/s]Inference:  12%|█▎        | 4/32 [00:01<00:09,  2.84it/s]Inference:  16%|█▌        | 5/32 [00:01<00:09,  2.81it/s]Inference:  19%|█▉        | 6/32 [00:02<00:09,  2.79it/s]Inference:  22%|██▏       | 7/32 [00:02<00:08,  2.78it/s]Inference:  25%|██▌       | 8/32 [00:02<00:08,  2.77it/s]Inference:  28%|██▊       | 9/32 [00:03<00:08,  2.78it/s]Inference:  31%|███▏      | 10/32 [00:03<00:07,  2.77it/s]Inference:  34%|███▍      | 11/32 [00:03<00:07,  2.77it/s]Inference:  38%|███▊      | 12/32 [00:04<00:07,  2.77it/s]Inference:  41%|████      | 13/32 [00:04<00:06,  2.78it/s]Inference:  44%|████▍     | 14/32 [00:04<00:06,  2.78it/s]Inference:  47%|████▋     | 15/32 [00:05<00:06,  2.77it/s]Inference:  50%|█████     | 16/32 [00:05<00:05,  2.77it/s]Inference:  53%|█████▎    | 17/32 [00:06<00:05,  2.77it/s]Inference:  56%|█████▋    | 18/32 [00:06<00:05,  2.76it/s]Inference:  59%|█████▉    | 19/32 [00:06<00:04,  2.78it/s]Inference:  62%|██████▎   | 20/32 [00:07<00:04,  2.77it/s]Inference:  66%|██████▌   | 21/32 [00:07<00:03,  2.79it/s]Inference:  69%|██████▉   | 22/32 [00:07<00:03,  2.80it/s]Inference:  72%|███████▏  | 23/32 [00:08<00:03,  2.79it/s]Inference:  75%|███████▌  | 24/32 [00:08<00:02,  2.79it/s]Inference:  78%|███████▊  | 25/32 [00:08<00:02,  2.78it/s]Inference:  81%|████████▏ | 26/32 [00:09<00:02,  2.79it/s]Inference:  84%|████████▍ | 27/32 [00:09<00:01,  2.78it/s]Inference:  88%|████████▊ | 28/32 [00:10<00:01,  2.79it/s]Inference:  91%|█████████ | 29/32 [00:10<00:01,  2.78it/s]Inference:  94%|█████████▍| 30/32 [00:10<00:00,  2.77it/s]Inference:  97%|█████████▋| 31/32 [00:11<00:00,  2.78it/s]Inference: 100%|██████████| 32/32 [00:11<00:00,  2.78it/s]Inference: 100%|██████████| 32/32 [00:11<00:00,  2.79it/s]
1328 MiB free out of 48676 MiB total
Saved layer 14 to ./models/meta-llama/Llama-2-7b-hf/tensorized/128/64/no_finetune/layer_14.pt
after cast to cpu
5358 MiB free out of 48676 MiB total
running bpv: 0.3393579987046632
Done with layer 14 total_time elapsed: 28553 estimated time left: 32360
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:30,  1.02it/s]Inference:   6%|▋         | 2/32 [00:01<00:29,  1.01it/s]Inference:   9%|▉         | 3/32 [00:02<00:28,  1.01it/s]Inference:  12%|█▎        | 4/32 [00:03<00:27,  1.01it/s]Inference:  16%|█▌        | 5/32 [00:04<00:26,  1.00it/s]Inference:  19%|█▉        | 6/32 [00:05<00:25,  1.00it/s]Inference:  22%|██▏       | 7/32 [00:06<00:24,  1.00it/s]Inference:  25%|██▌       | 8/32 [00:07<00:23,  1.00it/s]Inference:  28%|██▊       | 9/32 [00:08<00:22,  1.00it/s]Inference:  31%|███▏      | 10/32 [00:09<00:21,  1.00it/s]Inference:  34%|███▍      | 11/32 [00:10<00:20,  1.00it/s]Inference:  38%|███▊      | 12/32 [00:11<00:19,  1.00it/s]Inference:  41%|████      | 13/32 [00:12<00:18,  1.00it/s]Inference:  44%|████▍     | 14/32 [00:13<00:17,  1.00it/s]Inference:  47%|████▋     | 15/32 [00:14<00:16,  1.00it/s]Inference:  50%|█████     | 16/32 [00:15<00:15,  1.00it/s]Inference:  53%|█████▎    | 17/32 [00:16<00:14,  1.00it/s]Inference:  56%|█████▋    | 18/32 [00:17<00:13,  1.00it/s]Inference:  59%|█████▉    | 19/32 [00:18<00:12,  1.00it/s]Inference:  62%|██████▎   | 20/32 [00:19<00:11,  1.00it/s]Inference:  66%|██████▌   | 21/32 [00:20<00:10,  1.02it/s]Inference:  69%|██████▉   | 22/32 [00:21<00:09,  1.01it/s]Inference:  72%|███████▏  | 23/32 [00:22<00:08,  1.01it/s]Inference:  75%|███████▌  | 24/32 [00:23<00:07,  1.01it/s]Inference:  78%|███████▊  | 25/32 [00:24<00:06,  1.01it/s]Inference:  81%|████████▏ | 26/32 [00:25<00:05,  1.01it/s]Inference:  84%|████████▍ | 27/32 [00:26<00:04,  1.00it/s]Inference:  88%|████████▊ | 28/32 [00:27<00:03,  1.00it/s]Inference:  91%|█████████ | 29/32 [00:28<00:02,  1.00it/s]Inference:  94%|█████████▍| 30/32 [00:29<00:01,  1.00it/s]Inference:  97%|█████████▋| 31/32 [00:30<00:00,  1.00it/s]Inference: 100%|██████████| 32/32 [00:31<00:00,  1.00it/s]Inference: 100%|██████████| 32/32 [00:31<00:00,  1.00it/s]
layer15: self_attn.q_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 73052.65625, val loss None, lr 0.01
iter 250, train loss 2984.60546875, val loss None, lr 0.01
iter 500, train loss 2414.426513671875, val loss None, lr 0.01
iter 750, train loss 2302.793212890625, val loss None, lr 0.01
iter 1000, train loss 2257.0537109375, val loss None, lr 0.01
iter 1250, train loss 2146.1455078125, val loss None, lr 0.003333
iter 1500, train loss 2140.92236328125, val loss None, lr 0.001111
iter 1750, train loss 2108.96728515625, val loss None, lr 0.001111
iter 2000, train loss 2100.815673828125, val loss None, lr 0.001111
iter 2250, train loss 2096.40087890625, val loss None, lr 0.001111
best loss 2085.240966796875
running bpv: 0.3403268314153212
layer15: self_attn.k_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 95494.953125, val loss None, lr 0.01
iter 250, train loss 2731.04248046875, val loss None, lr 0.01
iter 500, train loss 2288.13818359375, val loss None, lr 0.01
iter 750, train loss 2125.496337890625, val loss None, lr 0.003333
iter 1000, train loss 2076.23046875, val loss None, lr 0.003333
iter 1250, train loss 2039.2003173828125, val loss None, lr 0.001111
iter 1500, train loss 2024.593505859375, val loss None, lr 0.001111
iter 1750, train loss 2015.2802734375, val loss None, lr 0.001111
iter 2000, train loss 2005.571044921875, val loss None, lr 0.001111
iter 2250, train loss 1998.1500244140625, val loss None, lr 0.001111
best loss 1989.024169921875
running bpv: 0.34128507217287324
layer15: self_attn.v_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 4616.76318359375, val loss None, lr 0.01
iter 250, train loss 773.8599853515625, val loss None, lr 0.01
iter 500, train loss 746.8562622070312, val loss None, lr 0.01
iter 750, train loss 735.837158203125, val loss None, lr 0.01
iter 1000, train loss 731.2151489257812, val loss None, lr 0.01
iter 1250, train loss 722.8623657226562, val loss None, lr 0.003333
iter 1500, train loss 719.2111206054688, val loss None, lr 0.003333
iter 1750, train loss 718.255126953125, val loss None, lr 0.003333
iter 2000, train loss 715.6339111328125, val loss None, lr 0.001111
iter 2250, train loss 714.5421752929688, val loss None, lr 0.001111
best loss 713.4912109375
running bpv: 0.34223289373088683
layer15: self_attn.o_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 777.14892578125, val loss None, lr 0.01
iter 250, train loss 65.607421875, val loss None, lr 0.01
iter 500, train loss 55.69279098510742, val loss None, lr 0.01
iter 750, train loss 55.54678726196289, val loss None, lr 0.01
iter 1000, train loss 52.415687561035156, val loss None, lr 0.003333
iter 1250, train loss 51.99909973144531, val loss None, lr 0.003333
iter 1500, train loss 51.67626953125, val loss None, lr 0.003333
iter 1750, train loss 51.31322479248047, val loss None, lr 0.001111
iter 2000, train loss 50.950721740722656, val loss None, lr 0.001111
iter 2250, train loss 50.830291748046875, val loss None, lr 0.001111
best loss 50.69770050048828
running bpv: 0.3431704651064549
layer15: mlp.gate_proj
weight shape torch.Size([11008, 4096])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([22, 8, 8, 8]), -3, -2
iter 0, train loss 14831.2958984375, val loss None, lr 0.01
iter 250, train loss 3565.83056640625, val loss None, lr 0.01
iter 500, train loss 3481.42431640625, val loss None, lr 0.01
iter 750, train loss 3439.213623046875, val loss None, lr 0.01
iter 1000, train loss 3354.73779296875, val loss None, lr 0.003333
iter 1250, train loss 3324.675048828125, val loss None, lr 0.003333
iter 1500, train loss 3310.62890625, val loss None, lr 0.001111
iter 1750, train loss 3301.270751953125, val loss None, lr 0.001111
iter 2000, train loss 3294.8515625, val loss None, lr 0.00037
iter 2250, train loss 3289.697265625, val loss None, lr 0.00037
best loss 3286.39404296875
running bpv: 0.34110983302798137
layer15: mlp.up_proj
weight shape torch.Size([11008, 4096])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([22, 8, 8, 8]), -3, -2
iter 0, train loss 9912.859375, val loss None, lr 0.01
iter 250, train loss 3320.203125, val loss None, lr 0.01
iter 500, train loss 3253.787109375, val loss None, lr 0.01
iter 750, train loss 3240.89892578125, val loss None, lr 0.01
iter 1000, train loss 3173.006103515625, val loss None, lr 0.003333
iter 1250, train loss 3168.745361328125, val loss None, lr 0.003333
iter 1500, train loss 3151.0556640625, val loss None, lr 0.001111
iter 1750, train loss 3144.876220703125, val loss None, lr 0.00037
iter 2000, train loss 3140.42822265625, val loss None, lr 0.00037
iter 2250, train loss 3136.79638671875, val loss None, lr 0.00037
best loss 3134.2841796875
running bpv: 0.33910739942528734
layer15: mlp.down_proj
weight shape torch.Size([4096, 11008])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 22, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 248.3515625, val loss None, lr 0.01
iter 250, train loss 49.89616394042969, val loss None, lr 0.01
iter 500, train loss 47.83494567871094, val loss None, lr 0.01
iter 750, train loss 46.92136001586914, val loss None, lr 0.01
iter 1000, train loss 46.19779586791992, val loss None, lr 0.003333
iter 1250, train loss 45.9622802734375, val loss None, lr 0.003333
iter 1500, train loss 45.85096740722656, val loss None, lr 0.001111
iter 1750, train loss 45.71796798706055, val loss None, lr 0.001111
iter 2000, train loss 45.63451385498047, val loss None, lr 0.00037
iter 2250, train loss 45.57093811035156, val loss None, lr 0.00037
best loss 45.532264709472656
running bpv: 0.3393579987046632
5358 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:06,  4.91it/s]Inference:   6%|▋         | 2/32 [00:00<00:06,  4.70it/s]Inference:   9%|▉         | 3/32 [00:00<00:06,  4.59it/s]Inference:  12%|█▎        | 4/32 [00:00<00:06,  4.53it/s]Inference:  16%|█▌        | 5/32 [00:01<00:05,  4.50it/s]Inference:  19%|█▉        | 6/32 [00:01<00:05,  4.46it/s]Inference:  22%|██▏       | 7/32 [00:01<00:05,  4.44it/s]Inference:  25%|██▌       | 8/32 [00:01<00:05,  4.40it/s]Inference:  28%|██▊       | 9/32 [00:02<00:05,  4.40it/s]Inference:  31%|███▏      | 10/32 [00:02<00:04,  4.41it/s]Inference:  34%|███▍      | 11/32 [00:02<00:04,  4.44it/s]Inference:  38%|███▊      | 12/32 [00:02<00:04,  4.42it/s]Inference:  41%|████      | 13/32 [00:02<00:04,  4.41it/s]Inference:  44%|████▍     | 14/32 [00:03<00:04,  4.40it/s]Inference:  47%|████▋     | 15/32 [00:03<00:03,  4.40it/s]Inference:  50%|█████     | 16/32 [00:03<00:03,  4.37it/s]Inference:  53%|█████▎    | 17/32 [00:03<00:03,  4.37it/s]Inference:  56%|█████▋    | 18/32 [00:04<00:03,  4.37it/s]Inference:  59%|█████▉    | 19/32 [00:04<00:02,  4.39it/s]Inference:  62%|██████▎   | 20/32 [00:04<00:02,  4.38it/s]Inference:  66%|██████▌   | 21/32 [00:04<00:02,  4.40it/s]Inference:  69%|██████▉   | 22/32 [00:04<00:02,  4.39it/s]Inference:  72%|███████▏  | 23/32 [00:05<00:02,  4.40it/s]Inference:  75%|███████▌  | 24/32 [00:05<00:01,  4.37it/s]Inference:  78%|███████▊  | 25/32 [00:05<00:01,  4.38it/s]Inference:  81%|████████▏ | 26/32 [00:05<00:01,  4.36it/s]Inference:  84%|████████▍ | 27/32 [00:06<00:01,  4.38it/s]Inference:  88%|████████▊ | 28/32 [00:06<00:00,  4.36it/s]Inference:  91%|█████████ | 29/32 [00:06<00:00,  4.37it/s]Inference:  94%|█████████▍| 30/32 [00:06<00:00,  4.39it/s]Inference:  97%|█████████▋| 31/32 [00:07<00:00,  4.36it/s]Inference: 100%|██████████| 32/32 [00:07<00:00,  4.35it/s]Inference: 100%|██████████| 32/32 [00:07<00:00,  4.41it/s]
23435 MiB free out of 48676 MiB total
Saved layer 15 to ./models/meta-llama/Llama-2-7b-hf/tensorized/128/64/no_finetune/layer_15.pt
after cast to cpu
27465 MiB free out of 48676 MiB total
running bpv: 0.3393579987046632
Done with layer 15 total_time elapsed: 29471 estimated time left: 29471
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:15,  1.98it/s]Inference:   6%|▋         | 2/32 [00:01<00:15,  1.94it/s]Inference:   9%|▉         | 3/32 [00:01<00:15,  1.92it/s]Inference:  12%|█▎        | 4/32 [00:02<00:14,  1.90it/s]Inference:  16%|█▌        | 5/32 [00:02<00:14,  1.92it/s]Inference:  19%|█▉        | 6/32 [00:03<00:13,  1.96it/s]Inference:  22%|██▏       | 7/32 [00:03<00:12,  1.95it/s]Inference:  25%|██▌       | 8/32 [00:04<00:12,  1.94it/s]Inference:  28%|██▊       | 9/32 [00:04<00:11,  1.92it/s]Inference:  31%|███▏      | 10/32 [00:05<00:11,  1.94it/s]Inference:  34%|███▍      | 11/32 [00:05<00:10,  1.96it/s]Inference:  38%|███▊      | 12/32 [00:06<00:10,  1.94it/s]Inference:  41%|████      | 13/32 [00:06<00:09,  1.93it/s]Inference:  44%|████▍     | 14/32 [00:07<00:09,  1.94it/s]Inference:  47%|████▋     | 15/32 [00:07<00:08,  1.96it/s]Inference:  50%|█████     | 16/32 [00:08<00:08,  1.97it/s]Inference:  53%|█████▎    | 17/32 [00:08<00:07,  1.94it/s]Inference:  56%|█████▋    | 18/32 [00:09<00:07,  1.93it/s]Inference:  59%|█████▉    | 19/32 [00:09<00:06,  1.92it/s]Inference:  62%|██████▎   | 20/32 [00:10<00:06,  1.92it/s]Inference:  66%|██████▌   | 21/32 [00:10<00:05,  1.93it/s]Inference:  69%|██████▉   | 22/32 [00:11<00:05,  1.95it/s]Inference:  72%|███████▏  | 23/32 [00:11<00:04,  1.93it/s]Inference:  75%|███████▌  | 24/32 [00:12<00:04,  1.92it/s]Inference:  78%|███████▊  | 25/32 [00:12<00:03,  1.91it/s]Inference:  81%|████████▏ | 26/32 [00:13<00:03,  1.92it/s]Inference:  84%|████████▍ | 27/32 [00:13<00:02,  1.94it/s]Inference:  88%|████████▊ | 28/32 [00:14<00:02,  1.96it/s]Inference:  91%|█████████ | 29/32 [00:14<00:01,  1.94it/s]Inference:  94%|█████████▍| 30/32 [00:15<00:01,  1.93it/s]Inference:  97%|█████████▋| 31/32 [00:16<00:00,  1.92it/s]Inference: 100%|██████████| 32/32 [00:16<00:00,  1.93it/s]Inference: 100%|██████████| 32/32 [00:16<00:00,  1.94it/s]
layer16: self_attn.q_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 71694.546875, val loss None, lr 0.01
iter 250, train loss 2446.451171875, val loss None, lr 0.01
iter 500, train loss 2266.5498046875, val loss None, lr 0.01
iter 750, train loss 2173.41552734375, val loss None, lr 0.003333
iter 1000, train loss 2109.818359375, val loss None, lr 0.003333
iter 1250, train loss 2036.9705810546875, val loss None, lr 0.003333
iter 1500, train loss 2006.259765625, val loss None, lr 0.001111
iter 1750, train loss 1995.8802490234375, val loss None, lr 0.001111
iter 2000, train loss 1986.5299072265625, val loss None, lr 0.001111
iter 2250, train loss 1978.01220703125, val loss None, lr 0.001111
best loss 1972.42919921875
running bpv: 0.34026659149484534
layer16: self_attn.k_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 111650.453125, val loss None, lr 0.01
iter 250, train loss 2394.98681640625, val loss None, lr 0.01
iter 500, train loss 2377.93408203125, val loss None, lr 0.01
iter 750, train loss 2024.027587890625, val loss None, lr 0.003333
iter 1000, train loss 1989.78662109375, val loss None, lr 0.003333
iter 1250, train loss 1938.8780517578125, val loss None, lr 0.001111
iter 1500, train loss 1920.810791015625, val loss None, lr 0.001111
iter 1750, train loss 1909.131103515625, val loss None, lr 0.001111
iter 2000, train loss 1895.7987060546875, val loss None, lr 0.00037
iter 2250, train loss 1887.11962890625, val loss None, lr 0.00037
best loss 1880.1617431640625
running bpv: 0.3411658653846154
layer16: self_attn.v_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 4732.7431640625, val loss None, lr 0.01
iter 250, train loss 774.0106811523438, val loss None, lr 0.01
iter 500, train loss 748.0068969726562, val loss None, lr 0.01
iter 750, train loss 743.4801025390625, val loss None, lr 0.01
iter 1000, train loss 732.5535888671875, val loss None, lr 0.01
iter 1250, train loss 726.0731201171875, val loss None, lr 0.01
iter 1500, train loss 717.9869384765625, val loss None, lr 0.003333
iter 1750, train loss 716.0758666992188, val loss None, lr 0.003333
iter 2000, train loss 713.7852783203125, val loss None, lr 0.001111
iter 2250, train loss 712.5926513671875, val loss None, lr 0.001111
best loss 711.5372314453125
running bpv: 0.3420559630102041
layer16: self_attn.o_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 755.1622314453125, val loss None, lr 0.01
iter 250, train loss 47.05609130859375, val loss None, lr 0.01
iter 500, train loss 44.599342346191406, val loss None, lr 0.003333
iter 750, train loss 42.95476531982422, val loss None, lr 0.003333
iter 1000, train loss 42.35123062133789, val loss None, lr 0.001111
iter 1250, train loss 42.020626068115234, val loss None, lr 0.001111
iter 1500, train loss 41.822654724121094, val loss None, lr 0.001111
iter 1750, train loss 41.624778747558594, val loss None, lr 0.001111
iter 2000, train loss 41.500083923339844, val loss None, lr 0.001111
iter 2250, train loss 41.50518035888672, val loss None, lr 0.001111
best loss 41.263607025146484
running bpv: 0.34293702411167515
layer16: mlp.gate_proj
weight shape torch.Size([11008, 4096])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([22, 8, 8, 8]), -3, -2
iter 0, train loss 16512.3984375, val loss None, lr 0.01
iter 250, train loss 3763.33544921875, val loss None, lr 0.01
iter 500, train loss 3642.9794921875, val loss None, lr 0.01
iter 750, train loss 3519.0537109375, val loss None, lr 0.003333
iter 1000, train loss 3477.543701171875, val loss None, lr 0.003333
iter 1250, train loss 3467.399658203125, val loss None, lr 0.003333
iter 1500, train loss 3448.5732421875, val loss None, lr 0.001111
iter 1750, train loss 3437.1982421875, val loss None, lr 0.001111
iter 2000, train loss 3428.7392578125, val loss None, lr 0.00037
iter 2250, train loss 3422.89306640625, val loss None, lr 0.00037
best loss 3418.744140625
running bpv: 0.341004010172144
layer16: mlp.up_proj
weight shape torch.Size([11008, 4096])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([22, 8, 8, 8]), -3, -2
iter 0, train loss 10390.201171875, val loss None, lr 0.01
iter 250, train loss 3388.96728515625, val loss None, lr 0.01
iter 500, train loss 3330.79150390625, val loss None, lr 0.01
iter 750, train loss 3280.2216796875, val loss None, lr 0.01
iter 1000, train loss 3262.603515625, val loss None, lr 0.01
iter 1250, train loss 3260.39453125, val loss None, lr 0.01
iter 1500, train loss 3231.28125, val loss None, lr 0.01
iter 1750, train loss 3204.05517578125, val loss None, lr 0.003333
iter 2000, train loss 3192.55322265625, val loss None, lr 0.001111
iter 2250, train loss 3187.960693359375, val loss None, lr 0.001111
best loss 3182.77734375
running bpv: 0.33912233631871525
layer16: mlp.down_proj
weight shape torch.Size([4096, 11008])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 22, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 309.6053466796875, val loss None, lr 0.01
iter 250, train loss 49.054901123046875, val loss None, lr 0.01
iter 500, train loss 47.54457092285156, val loss None, lr 0.01
iter 750, train loss 46.250465393066406, val loss None, lr 0.01
iter 1000, train loss 45.52617263793945, val loss None, lr 0.003333
iter 1250, train loss 45.392601013183594, val loss None, lr 0.003333
iter 1500, train loss 45.541358947753906, val loss None, lr 0.003333
iter 1750, train loss 45.013397216796875, val loss None, lr 0.001111
iter 2000, train loss 45.16740798950195, val loss None, lr 0.001111
iter 2250, train loss 44.82691955566406, val loss None, lr 0.00037
best loss 44.77059555053711
running bpv: 0.3393579987046632
27465 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:08,  3.85it/s]Inference:   6%|▋         | 2/32 [00:00<00:07,  4.21it/s]Inference:   9%|▉         | 3/32 [00:00<00:06,  4.34it/s]Inference:  12%|█▎        | 4/32 [00:00<00:06,  4.34it/s]Inference:  16%|█▌        | 5/32 [00:01<00:06,  4.34it/s]Inference:  19%|█▉        | 6/32 [00:01<00:06,  4.31it/s]Inference:  22%|██▏       | 7/32 [00:01<00:05,  4.29it/s]Inference:  25%|██▌       | 8/32 [00:01<00:05,  4.23it/s]Inference:  28%|██▊       | 9/32 [00:02<00:05,  4.24it/s]Inference:  31%|███▏      | 10/32 [00:02<00:05,  4.24it/s]Inference:  34%|███▍      | 11/32 [00:02<00:04,  4.25it/s]Inference:  38%|███▊      | 12/32 [00:02<00:04,  4.26it/s]Inference:  41%|████      | 13/32 [00:03<00:04,  4.26it/s]Inference:  44%|████▍     | 14/32 [00:03<00:04,  4.27it/s]Inference:  47%|████▋     | 15/32 [00:03<00:03,  4.28it/s]Inference:  50%|█████     | 16/32 [00:03<00:03,  4.30it/s]Inference:  53%|█████▎    | 17/32 [00:03<00:03,  4.30it/s]Inference:  56%|█████▋    | 18/32 [00:04<00:03,  4.28it/s]Inference:  59%|█████▉    | 19/32 [00:04<00:03,  4.26it/s]Inference:  62%|██████▎   | 20/32 [00:04<00:02,  4.23it/s]Inference:  66%|██████▌   | 21/32 [00:04<00:02,  4.15it/s]Inference:  69%|██████▉   | 22/32 [00:05<00:02,  4.26it/s]Inference:  72%|███████▏  | 23/32 [00:05<00:02,  4.21it/s]Inference:  75%|███████▌  | 24/32 [00:05<00:01,  4.20it/s]Inference:  78%|███████▊  | 25/32 [00:05<00:01,  4.20it/s]Inference:  81%|████████▏ | 26/32 [00:06<00:01,  4.22it/s]Inference:  84%|████████▍ | 27/32 [00:06<00:01,  4.30it/s]Inference:  88%|████████▊ | 28/32 [00:06<00:00,  4.26it/s]Inference:  91%|█████████ | 29/32 [00:06<00:00,  4.29it/s]Inference:  94%|█████████▍| 30/32 [00:07<00:00,  4.25it/s]Inference:  97%|█████████▋| 31/32 [00:07<00:00,  4.33it/s]Inference: 100%|██████████| 32/32 [00:07<00:00,  4.27it/s]Inference: 100%|██████████| 32/32 [00:07<00:00,  4.26it/s]
22147 MiB free out of 48676 MiB total
Saved layer 16 to ./models/meta-llama/Llama-2-7b-hf/tensorized/128/64/no_finetune/layer_16.pt
after cast to cpu
26177 MiB free out of 48676 MiB total
running bpv: 0.3393579987046632
Done with layer 16 total_time elapsed: 30317 estimated time left: 26751
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:15,  1.98it/s]Inference:   6%|▋         | 2/32 [00:00<00:14,  2.02it/s]Inference:   9%|▉         | 3/32 [00:01<00:14,  2.02it/s]Inference:  12%|█▎        | 4/32 [00:01<00:13,  2.02it/s]Inference:  16%|█▌        | 5/32 [00:02<00:13,  2.02it/s]Inference:  19%|█▉        | 6/32 [00:02<00:12,  2.02it/s]Inference:  22%|██▏       | 7/32 [00:03<00:12,  2.01it/s]Inference:  25%|██▌       | 8/32 [00:03<00:12,  1.99it/s]Inference:  28%|██▊       | 9/32 [00:04<00:11,  1.98it/s]Inference:  31%|███▏      | 10/32 [00:05<00:11,  1.97it/s]Inference:  34%|███▍      | 11/32 [00:05<00:10,  1.96it/s]Inference:  38%|███▊      | 12/32 [00:06<00:10,  1.97it/s]Inference:  41%|████      | 13/32 [00:06<00:09,  1.99it/s]Inference:  44%|████▍     | 14/32 [00:07<00:09,  1.99it/s]Inference:  47%|████▋     | 15/32 [00:07<00:08,  2.00it/s]Inference:  50%|█████     | 16/32 [00:08<00:08,  1.96it/s]Inference:  53%|█████▎    | 17/32 [00:08<00:07,  1.94it/s]Inference:  56%|█████▋    | 18/32 [00:09<00:07,  1.94it/s]Inference:  59%|█████▉    | 19/32 [00:09<00:06,  1.96it/s]Inference:  62%|██████▎   | 20/32 [00:10<00:06,  1.95it/s]Inference:  66%|██████▌   | 21/32 [00:10<00:05,  1.98it/s]Inference:  69%|██████▉   | 22/32 [00:11<00:05,  2.00it/s]Inference:  72%|███████▏  | 23/32 [00:11<00:04,  2.01it/s]Inference:  75%|███████▌  | 24/32 [00:12<00:03,  2.02it/s]Inference:  78%|███████▊  | 25/32 [00:12<00:03,  2.02it/s]Inference:  81%|████████▏ | 26/32 [00:13<00:02,  2.02it/s]Inference:  84%|████████▍ | 27/32 [00:13<00:02,  2.01it/s]Inference:  88%|████████▊ | 28/32 [00:14<00:02,  1.99it/s]Inference:  91%|█████████ | 29/32 [00:14<00:01,  1.97it/s]Inference:  94%|█████████▍| 30/32 [00:15<00:01,  1.96it/s]Inference:  97%|█████████▋| 31/32 [00:15<00:00,  1.96it/s]Inference: 100%|██████████| 32/32 [00:16<00:00,  1.98it/s]Inference: 100%|██████████| 32/32 [00:16<00:00,  1.99it/s]
layer17: self_attn.q_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 68650.734375, val loss None, lr 0.01
iter 250, train loss 2128.64013671875, val loss None, lr 0.01
iter 500, train loss 1947.8056640625, val loss None, lr 0.01
iter 750, train loss 1830.4232177734375, val loss None, lr 0.003333
iter 1000, train loss 1795.8009033203125, val loss None, lr 0.003333
iter 1250, train loss 1808.3809814453125, val loss None, lr 0.003333
iter 1500, train loss 1744.765625, val loss None, lr 0.001111
iter 1750, train loss 1733.7421875, val loss None, lr 0.001111
iter 2000, train loss 1724.204345703125, val loss None, lr 0.001111
iter 2250, train loss 1714.9537353515625, val loss None, lr 0.00037
best loss 1708.0633544921875
running bpv: 0.3402134042311192
layer17: self_attn.k_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 102278.671875, val loss None, lr 0.01
iter 250, train loss 2097.7197265625, val loss None, lr 0.01
iter 500, train loss 1873.962646484375, val loss None, lr 0.01
iter 750, train loss 1844.9429931640625, val loss None, lr 0.01
iter 1000, train loss 1691.9775390625, val loss None, lr 0.003333
iter 1250, train loss 1651.808837890625, val loss None, lr 0.003333
iter 1500, train loss 1629.337158203125, val loss None, lr 0.001111
iter 1750, train loss 1618.81201171875, val loss None, lr 0.001111
iter 2000, train loss 1611.2572021484375, val loss None, lr 0.001111
iter 2250, train loss 1599.7427978515625, val loss None, lr 0.00037
best loss 1594.4310302734375
running bpv: 0.3410605474645337
layer17: self_attn.v_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 5303.4921875, val loss None, lr 0.01
iter 250, train loss 700.3082885742188, val loss None, lr 0.01
iter 500, train loss 664.7724609375, val loss None, lr 0.01
iter 750, train loss 657.951904296875, val loss None, lr 0.01
iter 1000, train loss 649.1073608398438, val loss None, lr 0.01
iter 1250, train loss 639.2073364257812, val loss None, lr 0.003333
iter 1500, train loss 635.2735595703125, val loss None, lr 0.001111
iter 1750, train loss 633.7994384765625, val loss None, lr 0.001111
iter 2000, train loss 632.284423828125, val loss None, lr 0.001111
iter 2250, train loss 631.1728515625, val loss None, lr 0.001111
best loss 630.314453125
running bpv: 0.34189954753679785
layer17: self_attn.o_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 835.3497924804688, val loss None, lr 0.01
iter 250, train loss 33.85352325439453, val loss None, lr 0.01
iter 500, train loss 29.725494384765625, val loss None, lr 0.01
iter 750, train loss 30.388229370117188, val loss None, lr 0.01
iter 1000, train loss 27.352439880371094, val loss None, lr 0.003333
iter 1250, train loss 26.919700622558594, val loss None, lr 0.003333
iter 1500, train loss 26.413440704345703, val loss None, lr 0.001111
iter 1750, train loss 26.276371002197266, val loss None, lr 0.001111
iter 2000, train loss 26.180419921875, val loss None, lr 0.001111
iter 2250, train loss 26.04353141784668, val loss None, lr 0.00037
best loss 25.959882736206055
running bpv: 0.34273052130044845
layer17: mlp.gate_proj
weight shape torch.Size([11008, 4096])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([22, 8, 8, 8]), -3, -2
iter 0, train loss 16502.123046875, val loss None, lr 0.01
iter 250, train loss 3778.652099609375, val loss None, lr 0.01
iter 500, train loss 3643.900390625, val loss None, lr 0.01
iter 750, train loss 3594.978759765625, val loss None, lr 0.01
iter 1000, train loss 3569.30908203125, val loss None, lr 0.01
iter 1250, train loss 3488.58154296875, val loss None, lr 0.003333
iter 1500, train loss 3482.9443359375, val loss None, lr 0.003333
iter 1750, train loss 3466.4580078125, val loss None, lr 0.001111
iter 2000, train loss 3460.77734375, val loss None, lr 0.001111
iter 2250, train loss 3452.7265625, val loss None, lr 0.001111
best loss 3444.59765625
running bpv: 0.3409102438754427
layer17: mlp.up_proj
weight shape torch.Size([11008, 4096])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([22, 8, 8, 8]), -3, -2
iter 0, train loss 10209.3642578125, val loss None, lr 0.01
iter 250, train loss 3354.04541015625, val loss None, lr 0.01
iter 500, train loss 3279.05029296875, val loss None, lr 0.01
iter 750, train loss 3247.5283203125, val loss None, lr 0.01
iter 1000, train loss 3243.771240234375, val loss None, lr 0.01
iter 1250, train loss 3184.91455078125, val loss None, lr 0.003333
iter 1500, train loss 3168.57568359375, val loss None, lr 0.001111
iter 1750, train loss 3160.649658203125, val loss None, lr 0.001111
iter 2000, train loss 3156.635986328125, val loss None, lr 0.00037
iter 2250, train loss 3152.424560546875, val loss None, lr 0.00037
best loss 3149.12451171875
running bpv: 0.33913559275721367
layer17: mlp.down_proj
weight shape torch.Size([4096, 11008])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 22, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 314.4305725097656, val loss None, lr 0.01
iter 250, train loss 44.6723747253418, val loss None, lr 0.01
iter 500, train loss 42.99467849731445, val loss None, lr 0.01
iter 750, train loss 41.7984504699707, val loss None, lr 0.01
iter 1000, train loss 41.063636779785156, val loss None, lr 0.003333
iter 1250, train loss 40.85172653198242, val loss None, lr 0.003333
iter 1500, train loss 40.6963996887207, val loss None, lr 0.003333
iter 1750, train loss 40.473873138427734, val loss None, lr 0.001111
iter 2000, train loss 40.31794357299805, val loss None, lr 0.001111
iter 2250, train loss 40.25165939331055, val loss None, lr 0.00037
best loss 40.19896697998047
running bpv: 0.3393579987046632
26177 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:10,  2.85it/s]Inference:   6%|▋         | 2/32 [00:00<00:11,  2.66it/s]Inference:   9%|▉         | 3/32 [00:01<00:11,  2.53it/s]Inference:  12%|█▎        | 4/32 [00:01<00:11,  2.52it/s]Inference:  16%|█▌        | 5/32 [00:01<00:10,  2.47it/s]Inference:  19%|█▉        | 6/32 [00:02<00:10,  2.48it/s]Inference:  22%|██▏       | 7/32 [00:02<00:10,  2.47it/s]Inference:  25%|██▌       | 8/32 [00:03<00:09,  2.51it/s]Inference:  28%|██▊       | 9/32 [00:03<00:09,  2.55it/s]Inference:  31%|███▏      | 10/32 [00:03<00:08,  2.61it/s]Inference:  34%|███▍      | 11/32 [00:04<00:07,  2.64it/s]Inference:  38%|███▊      | 12/32 [00:04<00:07,  2.71it/s]Inference:  41%|████      | 13/32 [00:05<00:07,  2.67it/s]Inference:  44%|████▍     | 14/32 [00:05<00:06,  2.67it/s]Inference:  47%|████▋     | 15/32 [00:05<00:06,  2.61it/s]Inference:  50%|█████     | 16/32 [00:06<00:06,  2.42it/s]Inference:  53%|█████▎    | 17/32 [00:06<00:06,  2.48it/s]Inference:  56%|█████▋    | 18/32 [00:07<00:05,  2.55it/s]Inference:  59%|█████▉    | 19/32 [00:07<00:05,  2.60it/s]Inference:  62%|██████▎   | 20/32 [00:07<00:04,  2.45it/s]Inference:  66%|██████▌   | 21/32 [00:08<00:05,  2.19it/s]Inference:  69%|██████▉   | 22/32 [00:08<00:04,  2.04it/s]Inference:  72%|███████▏  | 23/32 [00:09<00:04,  1.98it/s]Inference:  75%|███████▌  | 24/32 [00:09<00:03,  2.07it/s]Inference:  78%|███████▊  | 25/32 [00:10<00:03,  1.96it/s]Inference:  81%|████████▏ | 26/32 [00:11<00:03,  1.90it/s]Inference:  84%|████████▍ | 27/32 [00:11<00:02,  1.87it/s]Inference:  88%|████████▊ | 28/32 [00:12<00:02,  1.85it/s]Inference:  91%|█████████ | 29/32 [00:12<00:01,  1.87it/s]Inference:  94%|█████████▍| 30/32 [00:13<00:01,  1.90it/s]Inference:  97%|█████████▋| 31/32 [00:13<00:00,  1.91it/s]Inference: 100%|██████████| 32/32 [00:14<00:00,  1.89it/s]Inference: 100%|██████████| 32/32 [00:14<00:00,  2.24it/s]
20859 MiB free out of 48676 MiB total
Saved layer 17 to ./models/meta-llama/Llama-2-7b-hf/tensorized/128/64/no_finetune/layer_17.pt
after cast to cpu
24889 MiB free out of 48676 MiB total
running bpv: 0.3393579987046632
Done with layer 17 total_time elapsed: 31171 estimated time left: 24244
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:17,  1.79it/s]Inference:   6%|▋         | 2/32 [00:01<00:16,  1.82it/s]Inference:   9%|▉         | 3/32 [00:01<00:15,  1.83it/s]Inference:  12%|█▎        | 4/32 [00:02<00:15,  1.83it/s]Inference:  16%|█▌        | 5/32 [00:02<00:14,  1.83it/s]Inference:  19%|█▉        | 6/32 [00:03<00:14,  1.84it/s]Inference:  22%|██▏       | 7/32 [00:03<00:13,  1.85it/s]Inference:  25%|██▌       | 8/32 [00:04<00:13,  1.84it/s]Inference:  28%|██▊       | 9/32 [00:04<00:12,  1.84it/s]Inference:  31%|███▏      | 10/32 [00:05<00:12,  1.83it/s]Inference:  34%|███▍      | 11/32 [00:06<00:11,  1.82it/s]Inference:  38%|███▊      | 12/32 [00:06<00:10,  1.83it/s]Inference:  41%|████      | 13/32 [00:07<00:10,  1.78it/s]Inference:  44%|████▍     | 14/32 [00:07<00:10,  1.80it/s]Inference:  47%|████▋     | 15/32 [00:08<00:09,  1.79it/s]Inference:  50%|█████     | 16/32 [00:08<00:08,  1.80it/s]Inference:  53%|█████▎    | 17/32 [00:09<00:08,  1.81it/s]Inference:  56%|█████▋    | 18/32 [00:09<00:07,  1.79it/s]Inference:  59%|█████▉    | 19/32 [00:10<00:07,  1.80it/s]Inference:  62%|██████▎   | 20/32 [00:11<00:06,  1.80it/s]Inference:  66%|██████▌   | 21/32 [00:11<00:06,  1.79it/s]Inference:  69%|██████▉   | 22/32 [00:12<00:05,  1.80it/s]Inference:  72%|███████▏  | 23/32 [00:12<00:04,  1.81it/s]Inference:  75%|███████▌  | 24/32 [00:13<00:04,  1.81it/s]Inference:  78%|███████▊  | 25/32 [00:13<00:03,  1.82it/s]Inference:  81%|████████▏ | 26/32 [00:14<00:03,  1.81it/s]Inference:  84%|████████▍ | 27/32 [00:14<00:02,  1.81it/s]Inference:  88%|████████▊ | 28/32 [00:15<00:02,  1.81it/s]Inference:  91%|█████████ | 29/32 [00:16<00:01,  1.81it/s]Inference:  94%|█████████▍| 30/32 [00:16<00:01,  1.82it/s]Inference:  97%|█████████▋| 31/32 [00:17<00:00,  1.87it/s]Inference: 100%|██████████| 32/32 [00:17<00:00,  1.87it/s]Inference: 100%|██████████| 32/32 [00:17<00:00,  1.82it/s]
layer18: self_attn.q_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 72112.484375, val loss None, lr 0.01
iter 250, train loss 2036.5458984375, val loss None, lr 0.01
iter 500, train loss 1846.974853515625, val loss None, lr 0.01
iter 750, train loss 1789.9483642578125, val loss None, lr 0.01
iter 1000, train loss 1685.9342041015625, val loss None, lr 0.003333
iter 1250, train loss 1665.16943359375, val loss None, lr 0.003333
iter 1500, train loss 1660.050537109375, val loss None, lr 0.003333
iter 1750, train loss 1625.7615966796875, val loss None, lr 0.001111
iter 2000, train loss 1616.76171875, val loss None, lr 0.00037
iter 2250, train loss 1610.546630859375, val loss None, lr 0.00037
best loss 1605.3115234375
running bpv: 0.34016609957020055
layer18: self_attn.k_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 101851.765625, val loss None, lr 0.01
iter 250, train loss 1981.291259765625, val loss None, lr 0.01
iter 500, train loss 1729.7935791015625, val loss None, lr 0.003333
iter 750, train loss 1680.4022216796875, val loss None, lr 0.003333
iter 1000, train loss 1639.347412109375, val loss None, lr 0.001111
iter 1250, train loss 1591.50390625, val loss None, lr 0.001111
iter 1500, train loss 1581.50439453125, val loss None, lr 0.001111
iter 1750, train loss 1568.8350830078125, val loss None, lr 0.001111
iter 2000, train loss 1557.5386962890625, val loss None, lr 0.001111
iter 2250, train loss 1546.01953125, val loss None, lr 0.00037
best loss 1540.1795654296875
running bpv: 0.34096682472903594
layer18: self_attn.v_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 6488.39990234375, val loss None, lr 0.01
iter 250, train loss 787.1506958007812, val loss None, lr 0.01
iter 500, train loss 755.8284912109375, val loss None, lr 0.01
iter 750, train loss 730.907470703125, val loss None, lr 0.01
iter 1000, train loss 729.4174194335938, val loss None, lr 0.01
iter 1250, train loss 722.8704833984375, val loss None, lr 0.01
iter 1500, train loss 711.9445190429688, val loss None, lr 0.003333
iter 1750, train loss 709.9088134765625, val loss None, lr 0.003333
iter 2000, train loss 707.8680419921875, val loss None, lr 0.003333
iter 2250, train loss 704.5400390625, val loss None, lr 0.001111
best loss 703.3994750976562
running bpv: 0.3417602747018739
layer18: self_attn.o_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 832.0153198242188, val loss None, lr 0.01
iter 250, train loss 21.69036865234375, val loss None, lr 0.01
iter 500, train loss 21.18996238708496, val loss None, lr 0.01
iter 750, train loss 17.321577072143555, val loss None, lr 0.003333
iter 1000, train loss 19.103734970092773, val loss None, lr 0.003333
iter 1250, train loss 16.626832962036133, val loss None, lr 0.003333
iter 1500, train loss 16.420536041259766, val loss None, lr 0.003333
iter 1750, train loss 16.163583755493164, val loss None, lr 0.001111
iter 2000, train loss 15.964903831481934, val loss None, lr 0.001111
iter 2250, train loss 15.892822265625, val loss None, lr 0.001111
best loss 15.791399955749512
running bpv: 0.3425465481910684
layer18: mlp.gate_proj
weight shape torch.Size([11008, 4096])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([22, 8, 8, 8]), -3, -2
iter 0, train loss 19818.16796875, val loss None, lr 0.01
iter 250, train loss 4257.80859375, val loss None, lr 0.01
iter 500, train loss 3986.700927734375, val loss None, lr 0.01
iter 750, train loss 3979.94873046875, val loss None, lr 0.01
iter 1000, train loss 3873.21142578125, val loss None, lr 0.003333
iter 1250, train loss 3854.007080078125, val loss None, lr 0.003333
iter 1500, train loss 3838.945556640625, val loss None, lr 0.003333
iter 1750, train loss 3812.555908203125, val loss None, lr 0.001111
iter 2000, train loss 3807.212890625, val loss None, lr 0.001111
iter 2250, train loss 3801.177734375, val loss None, lr 0.001111
best loss 3791.793701171875
running bpv: 0.3408265847528623
layer18: mlp.up_proj
weight shape torch.Size([11008, 4096])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([22, 8, 8, 8]), -3, -2
iter 0, train loss 11081.875, val loss None, lr 0.01
iter 250, train loss 3676.658203125, val loss None, lr 0.01
iter 500, train loss 3532.461181640625, val loss None, lr 0.01
iter 750, train loss 3446.782470703125, val loss None, lr 0.003333
iter 1000, train loss 3434.02587890625, val loss None, lr 0.003333
iter 1250, train loss 3423.790771484375, val loss None, lr 0.003333
iter 1500, train loss 3408.60009765625, val loss None, lr 0.001111
iter 1750, train loss 3395.100830078125, val loss None, lr 0.001111
iter 2000, train loss 3388.756103515625, val loss None, lr 0.00037
iter 2250, train loss 3384.353271484375, val loss None, lr 0.00037
best loss 3380.322998046875
running bpv: 0.3391474372240618
layer18: mlp.down_proj
weight shape torch.Size([4096, 11008])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 22, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 280.8638000488281, val loss None, lr 0.01
iter 250, train loss 46.90488052368164, val loss None, lr 0.01
iter 500, train loss 44.4853515625, val loss None, lr 0.01
iter 750, train loss 42.92580032348633, val loss None, lr 0.01
iter 1000, train loss 42.67461013793945, val loss None, lr 0.01
iter 1250, train loss 42.10868835449219, val loss None, lr 0.01
iter 1500, train loss 41.57061767578125, val loss None, lr 0.003333
iter 1750, train loss 41.43021774291992, val loss None, lr 0.001111
iter 2000, train loss 41.39603042602539, val loss None, lr 0.001111
iter 2250, train loss 41.28567123413086, val loss None, lr 0.00037
best loss 41.24643325805664
running bpv: 0.3393579987046632
24889 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:07,  4.07it/s]Inference:   6%|▋         | 2/32 [00:00<00:07,  4.05it/s]Inference:   9%|▉         | 3/32 [00:00<00:07,  4.06it/s]Inference:  12%|█▎        | 4/32 [00:00<00:06,  4.11it/s]Inference:  16%|█▌        | 5/32 [00:01<00:06,  4.13it/s]Inference:  19%|█▉        | 6/32 [00:01<00:06,  4.16it/s]Inference:  22%|██▏       | 7/32 [00:01<00:05,  4.17it/s]Inference:  25%|██▌       | 8/32 [00:01<00:05,  4.19it/s]Inference:  28%|██▊       | 9/32 [00:02<00:05,  4.16it/s]Inference:  31%|███▏      | 10/32 [00:02<00:05,  4.17it/s]Inference:  34%|███▍      | 11/32 [00:02<00:05,  4.17it/s]Inference:  38%|███▊      | 12/32 [00:02<00:04,  4.16it/s]Inference:  41%|████      | 13/32 [00:03<00:04,  4.17it/s]Inference:  44%|████▍     | 14/32 [00:03<00:04,  4.21it/s]Inference:  47%|████▋     | 15/32 [00:03<00:04,  4.24it/s]Inference:  50%|█████     | 16/32 [00:03<00:03,  4.26it/s]Inference:  53%|█████▎    | 17/32 [00:04<00:03,  4.27it/s]Inference:  56%|█████▋    | 18/32 [00:04<00:03,  4.30it/s]Inference:  59%|█████▉    | 19/32 [00:04<00:03,  4.29it/s]Inference:  62%|██████▎   | 20/32 [00:04<00:02,  4.32it/s]Inference:  66%|██████▌   | 21/32 [00:04<00:02,  4.31it/s]Inference:  69%|██████▉   | 22/32 [00:05<00:02,  4.33it/s]Inference:  72%|███████▏  | 23/32 [00:05<00:02,  4.33it/s]Inference:  75%|███████▌  | 24/32 [00:05<00:01,  4.35it/s]Inference:  78%|███████▊  | 25/32 [00:05<00:01,  4.34it/s]Inference:  81%|████████▏ | 26/32 [00:06<00:01,  4.34it/s]Inference:  84%|████████▍ | 27/32 [00:06<00:01,  4.33it/s]Inference:  88%|████████▊ | 28/32 [00:06<00:00,  4.34it/s]Inference:  91%|█████████ | 29/32 [00:06<00:00,  4.33it/s]Inference:  94%|█████████▍| 30/32 [00:07<00:00,  4.35it/s]Inference:  97%|█████████▋| 31/32 [00:07<00:00,  4.46it/s]Inference: 100%|██████████| 32/32 [00:07<00:00,  4.39it/s]Inference: 100%|██████████| 32/32 [00:07<00:00,  4.26it/s]
19635 MiB free out of 48676 MiB total
Saved layer 18 to ./models/meta-llama/Llama-2-7b-hf/tensorized/128/64/no_finetune/layer_18.pt
after cast to cpu
23665 MiB free out of 48676 MiB total
running bpv: 0.3393579987046632
Done with layer 18 total_time elapsed: 32049 estimated time left: 21928
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:15,  1.97it/s]Inference:   6%|▋         | 2/32 [00:01<00:15,  1.96it/s]Inference:   9%|▉         | 3/32 [00:01<00:14,  1.98it/s]Inference:  12%|█▎        | 4/32 [00:02<00:14,  1.99it/s]Inference:  16%|█▌        | 5/32 [00:02<00:13,  1.99it/s]Inference:  19%|█▉        | 6/32 [00:03<00:13,  2.00it/s]Inference:  22%|██▏       | 7/32 [00:03<00:12,  2.00it/s]Inference:  25%|██▌       | 8/32 [00:04<00:11,  2.01it/s]Inference:  28%|██▊       | 9/32 [00:04<00:11,  2.01it/s]Inference:  31%|███▏      | 10/32 [00:04<00:10,  2.01it/s]Inference:  34%|███▍      | 11/32 [00:05<00:10,  2.01it/s]Inference:  38%|███▊      | 12/32 [00:05<00:09,  2.01it/s]Inference:  41%|████      | 13/32 [00:06<00:09,  2.02it/s]Inference:  44%|████▍     | 14/32 [00:06<00:08,  2.02it/s]Inference:  47%|████▋     | 15/32 [00:07<00:08,  2.02it/s]Inference:  50%|█████     | 16/32 [00:07<00:07,  2.02it/s]Inference:  53%|█████▎    | 17/32 [00:08<00:07,  2.02it/s]Inference:  56%|█████▋    | 18/32 [00:08<00:06,  2.01it/s]Inference:  59%|█████▉    | 19/32 [00:09<00:06,  2.01it/s]Inference:  62%|██████▎   | 20/32 [00:09<00:06,  2.00it/s]Inference:  66%|██████▌   | 21/32 [00:10<00:05,  2.01it/s]Inference:  69%|██████▉   | 22/32 [00:10<00:05,  2.00it/s]Inference:  72%|███████▏  | 23/32 [00:11<00:04,  2.00it/s]Inference:  75%|███████▌  | 24/32 [00:11<00:04,  2.00it/s]Inference:  78%|███████▊  | 25/32 [00:12<00:03,  2.00it/s]Inference:  81%|████████▏ | 26/32 [00:12<00:03,  1.99it/s]Inference:  84%|████████▍ | 27/32 [00:13<00:02,  2.00it/s]Inference:  88%|████████▊ | 28/32 [00:13<00:02,  2.00it/s]Inference:  91%|█████████ | 29/32 [00:14<00:01,  2.00it/s]Inference:  94%|█████████▍| 30/32 [00:14<00:01,  2.00it/s]Inference:  97%|█████████▋| 31/32 [00:15<00:00,  2.00it/s]Inference: 100%|██████████| 32/32 [00:15<00:00,  2.00it/s]Inference: 100%|██████████| 32/32 [00:15<00:00,  2.00it/s]
layer19: self_attn.q_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 72943.359375, val loss None, lr 0.01
iter 250, train loss 2106.61083984375, val loss None, lr 0.01
iter 500, train loss 1860.7689208984375, val loss None, lr 0.01
iter 750, train loss 1767.525390625, val loss None, lr 0.01
iter 1000, train loss 1676.761474609375, val loss None, lr 0.003333
iter 1250, train loss 1630.4208984375, val loss None, lr 0.001111
iter 1500, train loss 1618.3974609375, val loss None, lr 0.001111
iter 1750, train loss 1608.7545166015625, val loss None, lr 0.001111
iter 2000, train loss 1600.189208984375, val loss None, lr 0.001111
iter 2250, train loss 1589.715087890625, val loss None, lr 0.00037
best loss 1584.1561279296875
running bpv: 0.34012375271517786
layer19: self_attn.k_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 111001.375, val loss None, lr 0.01
iter 250, train loss 1901.9208984375, val loss None, lr 0.01
iter 500, train loss 1848.80517578125, val loss None, lr 0.01
iter 750, train loss 1719.2537841796875, val loss None, lr 0.01
iter 1000, train loss 1559.0869140625, val loss None, lr 0.003333
iter 1250, train loss 1515.104736328125, val loss None, lr 0.001111
iter 1500, train loss 1500.5498046875, val loss None, lr 0.001111
iter 1750, train loss 1487.02294921875, val loss None, lr 0.00037
iter 2000, train loss 1478.14111328125, val loss None, lr 0.00037
iter 2250, train loss 1470.3856201171875, val loss None, lr 0.00037
best loss 1465.445556640625
running bpv: 0.3408828821978913
layer19: self_attn.v_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 6964.2763671875, val loss None, lr 0.01
iter 250, train loss 784.1982421875, val loss None, lr 0.01
iter 500, train loss 746.9937744140625, val loss None, lr 0.01
iter 750, train loss 730.1607055664062, val loss None, lr 0.01
iter 1000, train loss 723.0517578125, val loss None, lr 0.01
iter 1250, train loss 710.643798828125, val loss None, lr 0.003333
iter 1500, train loss 706.2711791992188, val loss None, lr 0.001111
iter 1750, train loss 704.295654296875, val loss None, lr 0.001111
iter 2000, train loss 702.74267578125, val loss None, lr 0.001111
iter 2250, train loss 701.678466796875, val loss None, lr 0.001111
best loss 699.9915771484375
running bpv: 0.34163547274562583
layer19: self_attn.o_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 1017.1942138671875, val loss None, lr 0.01
iter 250, train loss 25.32614517211914, val loss None, lr 0.01
iter 500, train loss 25.764629364013672, val loss None, lr 0.01
iter 750, train loss 21.03234100341797, val loss None, lr 0.01
iter 1000, train loss 20.572704315185547, val loss None, lr 0.003333
iter 1250, train loss 19.48485565185547, val loss None, lr 0.003333
iter 1500, train loss 19.778968811035156, val loss None, lr 0.003333
iter 1750, train loss 19.01812171936035, val loss None, lr 0.001111
iter 2000, train loss 18.968971252441406, val loss None, lr 0.001111
iter 2250, train loss 18.740798950195312, val loss None, lr 0.001111
best loss 18.662174224853516
running bpv: 0.34238160848298044
layer19: mlp.gate_proj
weight shape torch.Size([11008, 4096])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([22, 8, 8, 8]), -3, -2
iter 0, train loss 20490.3671875, val loss None, lr 0.01
iter 250, train loss 4354.88720703125, val loss None, lr 0.01
iter 500, train loss 4237.6962890625, val loss None, lr 0.01
iter 750, train loss 4191.2890625, val loss None, lr 0.01
iter 1000, train loss 4088.348876953125, val loss None, lr 0.003333
iter 1250, train loss 4063.956787109375, val loss None, lr 0.003333
iter 1500, train loss 4045.891845703125, val loss None, lr 0.001111
iter 1750, train loss 4041.55517578125, val loss None, lr 0.00037
iter 2000, train loss 4028.8291015625, val loss None, lr 0.00037
iter 2250, train loss 4023.056396484375, val loss None, lr 0.00037
best loss 4018.43994140625
running bpv: 0.3407514821807101
layer19: mlp.up_proj
weight shape torch.Size([11008, 4096])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([22, 8, 8, 8]), -3, -2
iter 0, train loss 11919.794921875, val loss None, lr 0.01
iter 250, train loss 3823.9404296875, val loss None, lr 0.01
iter 500, train loss 3709.580078125, val loss None, lr 0.01
iter 750, train loss 3647.834228515625, val loss None, lr 0.003333
iter 1000, train loss 3608.632080078125, val loss None, lr 0.003333
iter 1250, train loss 3591.51708984375, val loss None, lr 0.001111
iter 1500, train loss 3580.35400390625, val loss None, lr 0.001111
iter 1750, train loss 3575.9462890625, val loss None, lr 0.001111
iter 2000, train loss 3571.9130859375, val loss None, lr 0.001111
iter 2250, train loss 3564.97998046875, val loss None, lr 0.00037
best loss 3561.162841796875
running bpv: 0.33915808390096935
layer19: mlp.down_proj
weight shape torch.Size([4096, 11008])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 22, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 256.07208251953125, val loss None, lr 0.01
iter 250, train loss 46.77406311035156, val loss None, lr 0.01
iter 500, train loss 45.729583740234375, val loss None, lr 0.01
iter 750, train loss 44.419158935546875, val loss None, lr 0.01
iter 1000, train loss 44.28704833984375, val loss None, lr 0.01
iter 1250, train loss 44.219661712646484, val loss None, lr 0.01
iter 1500, train loss 43.50840759277344, val loss None, lr 0.003333
iter 1750, train loss 43.2796630859375, val loss None, lr 0.001111
iter 2000, train loss 43.232322692871094, val loss None, lr 0.001111
iter 2250, train loss 43.19780349731445, val loss None, lr 0.00037
best loss 43.1078987121582
running bpv: 0.3393579987046632
23665 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:06,  4.72it/s]Inference:   6%|▋         | 2/32 [00:00<00:06,  4.74it/s]Inference:   9%|▉         | 3/32 [00:00<00:06,  4.70it/s]Inference:  12%|█▎        | 4/32 [00:00<00:06,  4.60it/s]Inference:  16%|█▌        | 5/32 [00:01<00:06,  4.50it/s]Inference:  19%|█▉        | 6/32 [00:01<00:05,  4.44it/s]Inference:  22%|██▏       | 7/32 [00:01<00:05,  4.33it/s]Inference:  25%|██▌       | 8/32 [00:01<00:05,  4.35it/s]Inference:  28%|██▊       | 9/32 [00:02<00:05,  4.34it/s]Inference:  31%|███▏      | 10/32 [00:02<00:05,  4.27it/s]Inference:  34%|███▍      | 11/32 [00:02<00:04,  4.28it/s]Inference:  38%|███▊      | 12/32 [00:02<00:04,  4.29it/s]Inference:  41%|████      | 13/32 [00:02<00:04,  4.32it/s]Inference:  44%|████▍     | 14/32 [00:03<00:04,  4.32it/s]Inference:  47%|████▋     | 15/32 [00:03<00:03,  4.30it/s]Inference:  50%|█████     | 16/32 [00:03<00:03,  4.31it/s]Inference:  53%|█████▎    | 17/32 [00:03<00:03,  4.30it/s]Inference:  56%|█████▋    | 18/32 [00:04<00:03,  4.29it/s]Inference:  59%|█████▉    | 19/32 [00:04<00:03,  4.28it/s]Inference:  62%|██████▎   | 20/32 [00:04<00:02,  4.29it/s]Inference:  66%|██████▌   | 21/32 [00:04<00:02,  4.28it/s]Inference:  69%|██████▉   | 22/32 [00:05<00:02,  4.27it/s]Inference:  72%|███████▏  | 23/32 [00:05<00:02,  4.26it/s]Inference:  75%|███████▌  | 24/32 [00:05<00:01,  4.25it/s]Inference:  78%|███████▊  | 25/32 [00:05<00:01,  4.24it/s]Inference:  81%|████████▏ | 26/32 [00:06<00:01,  4.25it/s]Inference:  84%|████████▍ | 27/32 [00:06<00:01,  4.23it/s]Inference:  88%|████████▊ | 28/32 [00:06<00:00,  4.23it/s]Inference:  91%|█████████ | 29/32 [00:06<00:00,  4.22it/s]Inference:  94%|█████████▍| 30/32 [00:06<00:00,  4.21it/s]Inference:  97%|█████████▋| 31/32 [00:07<00:00,  4.23it/s]Inference: 100%|██████████| 32/32 [00:07<00:00,  4.25it/s]Inference: 100%|██████████| 32/32 [00:07<00:00,  4.31it/s]
18347 MiB free out of 48676 MiB total
Saved layer 19 to ./models/meta-llama/Llama-2-7b-hf/tensorized/128/64/no_finetune/layer_19.pt
after cast to cpu
22377 MiB free out of 48676 MiB total
running bpv: 0.3393579987046632
Done with layer 19 total_time elapsed: 32894 estimated time left: 19736
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:15,  2.04it/s]Inference:   6%|▋         | 2/32 [00:00<00:14,  2.06it/s]Inference:   9%|▉         | 3/32 [00:01<00:14,  2.03it/s]Inference:  12%|█▎        | 4/32 [00:01<00:14,  2.00it/s]Inference:  16%|█▌        | 5/32 [00:02<00:13,  1.99it/s]Inference:  19%|█▉        | 6/32 [00:02<00:13,  1.99it/s]Inference:  22%|██▏       | 7/32 [00:03<00:12,  1.99it/s]Inference:  25%|██▌       | 8/32 [00:04<00:12,  1.98it/s]Inference:  28%|██▊       | 9/32 [00:04<00:11,  1.99it/s]Inference:  31%|███▏      | 10/32 [00:05<00:11,  1.98it/s]Inference:  34%|███▍      | 11/32 [00:05<00:10,  1.99it/s]Inference:  38%|███▊      | 12/32 [00:06<00:10,  1.99it/s]Inference:  41%|████      | 13/32 [00:06<00:09,  2.01it/s]Inference:  44%|████▍     | 14/32 [00:06<00:08,  2.02it/s]Inference:  47%|████▋     | 15/32 [00:07<00:08,  2.01it/s]Inference:  50%|█████     | 16/32 [00:07<00:07,  2.02it/s]Inference:  53%|█████▎    | 17/32 [00:08<00:07,  2.02it/s]Inference:  56%|█████▋    | 18/32 [00:08<00:06,  2.02it/s]Inference:  59%|█████▉    | 19/32 [00:09<00:06,  2.02it/s]Inference:  62%|██████▎   | 20/32 [00:09<00:05,  2.02it/s]Inference:  66%|██████▌   | 21/32 [00:10<00:05,  2.02it/s]Inference:  69%|██████▉   | 22/32 [00:10<00:04,  2.02it/s]Inference:  72%|███████▏  | 23/32 [00:11<00:04,  2.02it/s]Inference:  75%|███████▌  | 24/32 [00:11<00:03,  2.02it/s]Inference:  78%|███████▊  | 25/32 [00:12<00:03,  2.02it/s]Inference:  81%|████████▏ | 26/32 [00:12<00:02,  2.02it/s]Inference:  84%|████████▍ | 27/32 [00:13<00:02,  2.01it/s]Inference:  88%|████████▊ | 28/32 [00:13<00:01,  2.01it/s]Inference:  91%|█████████ | 29/32 [00:14<00:01,  2.02it/s]Inference:  94%|█████████▍| 30/32 [00:14<00:00,  2.02it/s]Inference:  97%|█████████▋| 31/32 [00:15<00:00,  2.02it/s]Inference: 100%|██████████| 32/32 [00:15<00:00,  2.03it/s]Inference: 100%|██████████| 32/32 [00:15<00:00,  2.01it/s]
layer20: self_attn.q_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 78807.109375, val loss None, lr 0.01
iter 250, train loss 2071.365966796875, val loss None, lr 0.01
iter 500, train loss 1833.138427734375, val loss None, lr 0.01
iter 750, train loss 1750.57080078125, val loss None, lr 0.01
iter 1000, train loss 1732.778076171875, val loss None, lr 0.01
iter 1250, train loss 1626.438232421875, val loss None, lr 0.003333
iter 1500, train loss 1596.33935546875, val loss None, lr 0.001111
iter 1750, train loss 1586.48974609375, val loss None, lr 0.001111
iter 2000, train loss 1578.6898193359375, val loss None, lr 0.001111
iter 2250, train loss 1567.9998779296875, val loss None, lr 0.00037
best loss 1561.030517578125
running bpv: 0.3400856230650155
layer20: self_attn.k_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 118319.046875, val loss None, lr 0.01
iter 250, train loss 1906.97119140625, val loss None, lr 0.01
iter 500, train loss 1800.584716796875, val loss None, lr 0.01
iter 750, train loss 1639.1611328125, val loss None, lr 0.01
iter 1000, train loss 1580.270263671875, val loss None, lr 0.01
iter 1250, train loss 1488.23193359375, val loss None, lr 0.001111
iter 1500, train loss 1472.3212890625, val loss None, lr 0.001111
iter 1750, train loss 1457.3857421875, val loss None, lr 0.001111
iter 2000, train loss 1450.3505859375, val loss None, lr 0.001111
iter 2250, train loss 1440.8602294921875, val loss None, lr 0.00037
best loss 1433.7581787109375
running bpv: 0.34080726490236385
layer20: self_attn.v_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 6475.4853515625, val loss None, lr 0.01
iter 250, train loss 762.7452392578125, val loss None, lr 0.01
iter 500, train loss 734.2528076171875, val loss None, lr 0.01
iter 750, train loss 713.07373046875, val loss None, lr 0.01
iter 1000, train loss 697.254150390625, val loss None, lr 0.003333
iter 1250, train loss 692.6000366210938, val loss None, lr 0.003333
iter 1500, train loss 688.1491088867188, val loss None, lr 0.001111
iter 1750, train loss 685.9517211914062, val loss None, lr 0.001111
iter 2000, train loss 684.3837890625, val loss None, lr 0.001111
iter 2250, train loss 682.8920288085938, val loss None, lr 0.001111
best loss 681.4727783203125
running bpv: 0.3415229976970317
layer20: self_attn.o_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 1134.3343505859375, val loss None, lr 0.01
iter 250, train loss 16.95738410949707, val loss None, lr 0.01
iter 500, train loss 18.013912200927734, val loss None, lr 0.01
iter 750, train loss 11.533378601074219, val loss None, lr 0.003333
iter 1000, train loss 11.115821838378906, val loss None, lr 0.003333
iter 1250, train loss 11.510400772094727, val loss None, lr 0.003333
iter 1500, train loss 10.450435638427734, val loss None, lr 0.001111
iter 1750, train loss 10.187106132507324, val loss None, lr 0.001111
iter 2000, train loss 10.161248207092285, val loss None, lr 0.001111
iter 2250, train loss 10.004781723022461, val loss None, lr 0.00037
best loss 9.902198791503906
running bpv: 0.34223289373088683
layer20: mlp.gate_proj
weight shape torch.Size([11008, 4096])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([22, 8, 8, 8]), -3, -2
iter 0, train loss 19093.7578125, val loss None, lr 0.01
iter 250, train loss 4731.1669921875, val loss None, lr 0.01
iter 500, train loss 4603.2724609375, val loss None, lr 0.01
iter 750, train loss 4558.4228515625, val loss None, lr 0.01
iter 1000, train loss 4503.76806640625, val loss None, lr 0.01
iter 1250, train loss 4430.64453125, val loss None, lr 0.003333
iter 1500, train loss 4410.32421875, val loss None, lr 0.003333
iter 1750, train loss 4399.587890625, val loss None, lr 0.003333
iter 2000, train loss 4382.0625, val loss None, lr 0.001111
iter 2250, train loss 4372.67919921875, val loss None, lr 0.00037
best loss 4368.353515625
running bpv: 0.3406836872951853
layer20: mlp.up_proj
weight shape torch.Size([11008, 4096])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([22, 8, 8, 8]), -3, -2
iter 0, train loss 12307.3671875, val loss None, lr 0.01
iter 250, train loss 4080.55078125, val loss None, lr 0.01
iter 500, train loss 3976.541015625, val loss None, lr 0.01
iter 750, train loss 3931.351318359375, val loss None, lr 0.01
iter 1000, train loss 3941.31298828125, val loss None, lr 0.01
iter 1250, train loss 3839.143798828125, val loss None, lr 0.003333
iter 1500, train loss 3826.783935546875, val loss None, lr 0.003333
iter 1750, train loss 3815.476318359375, val loss None, lr 0.001111
iter 2000, train loss 3813.763671875, val loss None, lr 0.001111
iter 2250, train loss 3805.918701171875, val loss None, lr 0.001111
best loss 3800.42822265625
running bpv: 0.33916770573566085
layer20: mlp.down_proj
weight shape torch.Size([4096, 11008])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 22, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 282.87652587890625, val loss None, lr 0.01
iter 250, train loss 52.421165466308594, val loss None, lr 0.01
iter 500, train loss 50.47310256958008, val loss None, lr 0.01
iter 750, train loss 49.960243225097656, val loss None, lr 0.01
iter 1000, train loss 49.08982849121094, val loss None, lr 0.003333
iter 1250, train loss 49.055824279785156, val loss None, lr 0.003333
iter 1500, train loss 48.68016815185547, val loss None, lr 0.001111
iter 1750, train loss 48.600318908691406, val loss None, lr 0.001111
iter 2000, train loss 48.514076232910156, val loss None, lr 0.001111
iter 2250, train loss 48.508140563964844, val loss None, lr 0.001111
best loss 48.39312744140625
running bpv: 0.3393579987046632
22377 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:07,  4.25it/s]Inference:   6%|▋         | 2/32 [00:00<00:07,  4.08it/s]Inference:   9%|▉         | 3/32 [00:00<00:07,  4.10it/s]Inference:  12%|█▎        | 4/32 [00:00<00:06,  4.14it/s]Inference:  16%|█▌        | 5/32 [00:01<00:06,  4.18it/s]Inference:  19%|█▉        | 6/32 [00:01<00:06,  4.20it/s]Inference:  22%|██▏       | 7/32 [00:01<00:05,  4.23it/s]Inference:  25%|██▌       | 8/32 [00:01<00:05,  4.25it/s]Inference:  28%|██▊       | 9/32 [00:02<00:05,  4.26it/s]Inference:  31%|███▏      | 10/32 [00:02<00:05,  4.24it/s]Inference:  34%|███▍      | 11/32 [00:02<00:04,  4.26it/s]Inference:  38%|███▊      | 12/32 [00:02<00:04,  4.28it/s]Inference:  41%|████      | 13/32 [00:03<00:04,  4.30it/s]Inference:  44%|████▍     | 14/32 [00:03<00:04,  4.29it/s]Inference:  47%|████▋     | 15/32 [00:03<00:03,  4.31it/s]Inference:  50%|█████     | 16/32 [00:03<00:03,  4.32it/s]Inference:  53%|█████▎    | 17/32 [00:03<00:03,  4.32it/s]Inference:  56%|█████▋    | 18/32 [00:04<00:03,  4.33it/s]Inference:  59%|█████▉    | 19/32 [00:04<00:02,  4.45it/s]Inference:  62%|██████▎   | 20/32 [00:04<00:02,  4.37it/s]Inference:  66%|██████▌   | 21/32 [00:04<00:02,  4.34it/s]Inference:  69%|██████▉   | 22/32 [00:05<00:02,  4.35it/s]Inference:  72%|███████▏  | 23/32 [00:05<00:02,  4.34it/s]Inference:  75%|███████▌  | 24/32 [00:05<00:01,  4.34it/s]Inference:  78%|███████▊  | 25/32 [00:05<00:01,  4.33it/s]Inference:  81%|████████▏ | 26/32 [00:06<00:01,  4.41it/s]Inference:  84%|████████▍ | 27/32 [00:06<00:01,  4.37it/s]Inference:  88%|████████▊ | 28/32 [00:06<00:00,  4.37it/s]Inference:  91%|█████████ | 29/32 [00:06<00:00,  4.34it/s]Inference:  94%|█████████▍| 30/32 [00:06<00:00,  4.34it/s]Inference:  97%|█████████▋| 31/32 [00:07<00:00,  4.34it/s]Inference: 100%|██████████| 32/32 [00:07<00:00,  4.34it/s]Inference: 100%|██████████| 32/32 [00:07<00:00,  4.30it/s]
17059 MiB free out of 48676 MiB total
Saved layer 20 to ./models/meta-llama/Llama-2-7b-hf/tensorized/128/64/no_finetune/layer_20.pt
after cast to cpu
21089 MiB free out of 48676 MiB total
running bpv: 0.3393579987046632
Done with layer 20 total_time elapsed: 33739 estimated time left: 17673
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:17,  1.81it/s]Inference:   6%|▋         | 2/32 [00:01<00:16,  1.82it/s]Inference:   9%|▉         | 3/32 [00:01<00:15,  1.87it/s]Inference:  12%|█▎        | 4/32 [00:02<00:14,  1.89it/s]Inference:  16%|█▌        | 5/32 [00:02<00:14,  1.92it/s]Inference:  19%|█▉        | 6/32 [00:03<00:13,  1.89it/s]Inference:  22%|██▏       | 7/32 [00:03<00:13,  1.82it/s]Inference:  25%|██▌       | 8/32 [00:04<00:12,  1.85it/s]Inference:  28%|██▊       | 9/32 [00:04<00:12,  1.88it/s]Inference:  31%|███▏      | 10/32 [00:05<00:11,  1.91it/s]Inference:  34%|███▍      | 11/32 [00:05<00:11,  1.88it/s]Inference:  38%|███▊      | 12/32 [00:06<00:10,  1.86it/s]Inference:  41%|████      | 13/32 [00:06<00:10,  1.86it/s]Inference:  44%|████▍     | 14/32 [00:07<00:09,  1.87it/s]Inference:  47%|████▋     | 15/32 [00:08<00:08,  1.89it/s]Inference:  50%|█████     | 16/32 [00:08<00:08,  1.87it/s]Inference:  53%|█████▎    | 17/32 [00:09<00:08,  1.86it/s]Inference:  56%|█████▋    | 18/32 [00:09<00:07,  1.88it/s]Inference:  59%|█████▉    | 19/32 [00:10<00:06,  1.89it/s]Inference:  62%|██████▎   | 20/32 [00:10<00:06,  1.89it/s]Inference:  66%|██████▌   | 21/32 [00:11<00:05,  1.86it/s]Inference:  69%|██████▉   | 22/32 [00:11<00:05,  1.88it/s]Inference:  72%|███████▏  | 23/32 [00:12<00:04,  1.87it/s]Inference:  75%|███████▌  | 24/32 [00:12<00:04,  1.89it/s]Inference:  78%|███████▊  | 25/32 [00:13<00:03,  1.86it/s]Inference:  81%|████████▏ | 26/32 [00:13<00:03,  1.86it/s]Inference:  84%|████████▍ | 27/32 [00:14<00:02,  1.87it/s]Inference:  88%|████████▊ | 28/32 [00:14<00:02,  1.88it/s]Inference:  91%|█████████ | 29/32 [00:15<00:01,  1.87it/s]Inference:  94%|█████████▍| 30/32 [00:16<00:01,  1.84it/s]Inference:  97%|█████████▋| 31/32 [00:16<00:00,  1.85it/s]Inference: 100%|██████████| 32/32 [00:17<00:00,  1.87it/s]Inference: 100%|██████████| 32/32 [00:17<00:00,  1.87it/s]
layer21: self_attn.q_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 80029.453125, val loss None, lr 0.01
iter 250, train loss 2103.1650390625, val loss None, lr 0.01
iter 500, train loss 1896.592529296875, val loss None, lr 0.01
iter 750, train loss 1809.6031494140625, val loss None, lr 0.01
iter 1000, train loss 1793.3193359375, val loss None, lr 0.01
iter 1250, train loss 1657.030029296875, val loss None, lr 0.003333
iter 1500, train loss 1626.35302734375, val loss None, lr 0.001111
iter 1750, train loss 1613.8359375, val loss None, lr 0.001111
iter 2000, train loss 1608.091064453125, val loss None, lr 0.001111
iter 2250, train loss 1598.49658203125, val loss None, lr 0.001111
best loss 1589.4122314453125
running bpv: 0.34005111053084297
layer21: self_attn.k_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 117884.9140625, val loss None, lr 0.01
iter 250, train loss 1949.2230224609375, val loss None, lr 0.01
iter 500, train loss 1697.454345703125, val loss None, lr 0.003333
iter 750, train loss 1605.47607421875, val loss None, lr 0.003333
iter 1000, train loss 1544.005859375, val loss None, lr 0.003333
iter 1250, train loss 1507.6348876953125, val loss None, lr 0.001111
iter 1500, train loss 1496.0562744140625, val loss None, lr 0.001111
iter 1750, train loss 1483.261474609375, val loss None, lr 0.001111
iter 2000, train loss 1470.33056640625, val loss None, lr 0.001111
iter 2250, train loss 1460.759521484375, val loss None, lr 0.00037
best loss 1452.994873046875
running bpv: 0.3407387928396573
layer21: self_attn.v_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 8362.5771484375, val loss None, lr 0.01
iter 250, train loss 889.708251953125, val loss None, lr 0.01
iter 500, train loss 844.6783447265625, val loss None, lr 0.01
iter 750, train loss 824.0656127929688, val loss None, lr 0.01
iter 1000, train loss 813.985107421875, val loss None, lr 0.01
iter 1250, train loss 803.6298217773438, val loss None, lr 0.003333
iter 1500, train loss 796.2457275390625, val loss None, lr 0.001111
iter 1750, train loss 794.0318603515625, val loss None, lr 0.001111
iter 2000, train loss 792.4486694335938, val loss None, lr 0.001111
iter 2250, train loss 790.6365966796875, val loss None, lr 0.001111
best loss 789.093017578125
running bpv: 0.34142110918068763
layer21: self_attn.o_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 1774.112060546875, val loss None, lr 0.01
iter 250, train loss 36.791343688964844, val loss None, lr 0.01
iter 500, train loss 35.602474212646484, val loss None, lr 0.01
iter 750, train loss 28.999662399291992, val loss None, lr 0.003333
iter 1000, train loss 28.767290115356445, val loss None, lr 0.003333
iter 1250, train loss 27.706836700439453, val loss None, lr 0.001111
iter 1500, train loss 27.375207901000977, val loss None, lr 0.001111
iter 1750, train loss 27.073299407958984, val loss None, lr 0.001111
iter 2000, train loss 26.88772964477539, val loss None, lr 0.001111
iter 2250, train loss 26.663917541503906, val loss None, lr 0.00037
best loss 26.528919219970703
running bpv: 0.3420981221156182
layer21: mlp.gate_proj
weight shape torch.Size([11008, 4096])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([22, 8, 8, 8]), -3, -2
iter 0, train loss 21015.94921875, val loss None, lr 0.01
iter 250, train loss 4961.5849609375, val loss None, lr 0.01
iter 500, train loss 4696.20556640625, val loss None, lr 0.01
iter 750, train loss 4558.71435546875, val loss None, lr 0.003333
iter 1000, train loss 4516.64404296875, val loss None, lr 0.001111
iter 1250, train loss 4499.17138671875, val loss None, lr 0.001111
iter 1500, train loss 4487.92578125, val loss None, lr 0.001111
iter 1750, train loss 4476.24609375, val loss None, lr 0.001111
iter 2000, train loss 4465.4931640625, val loss None, lr 0.001111
iter 2250, train loss 4457.51904296875, val loss None, lr 0.00037
best loss 4452.43017578125
running bpv: 0.34062218299278846
layer21: mlp.up_proj
weight shape torch.Size([11008, 4096])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([22, 8, 8, 8]), -3, -2
iter 0, train loss 12662.2373046875, val loss None, lr 0.01
iter 250, train loss 4145.6474609375, val loss None, lr 0.01
iter 500, train loss 3986.580078125, val loss None, lr 0.01
iter 750, train loss 3911.9384765625, val loss None, lr 0.003333
iter 1000, train loss 3883.500244140625, val loss None, lr 0.003333
iter 1250, train loss 3868.764404296875, val loss None, lr 0.001111
iter 1500, train loss 3862.2490234375, val loss None, lr 0.001111
iter 1750, train loss 3853.603271484375, val loss None, lr 0.00037
iter 2000, train loss 3848.388427734375, val loss None, lr 0.00037
iter 2250, train loss 3843.938232421875, val loss None, lr 0.00037
best loss 3839.69189453125
running bpv: 0.33917644390911256
layer21: mlp.down_proj
weight shape torch.Size([4096, 11008])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 22, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 293.0260009765625, val loss None, lr 0.01
iter 250, train loss 49.72949981689453, val loss None, lr 0.01
iter 500, train loss 48.013671875, val loss None, lr 0.01
iter 750, train loss 47.117000579833984, val loss None, lr 0.01
iter 1000, train loss 46.78645706176758, val loss None, lr 0.01
iter 1250, train loss 46.049110412597656, val loss None, lr 0.003333
iter 1500, train loss 45.80256652832031, val loss None, lr 0.001111
iter 1750, train loss 45.715736389160156, val loss None, lr 0.001111
iter 2000, train loss 45.66204833984375, val loss None, lr 0.001111
iter 2250, train loss 45.569061279296875, val loss None, lr 0.001111
best loss 45.529727935791016
running bpv: 0.3393579987046632
21089 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:07,  3.89it/s]Inference:   6%|▋         | 2/32 [00:00<00:07,  3.82it/s]Inference:   9%|▉         | 3/32 [00:00<00:07,  3.77it/s]Inference:  12%|█▎        | 4/32 [00:01<00:07,  3.67it/s]Inference:  16%|█▌        | 5/32 [00:01<00:07,  3.60it/s]Inference:  19%|█▉        | 6/32 [00:01<00:07,  3.56it/s]Inference:  22%|██▏       | 7/32 [00:01<00:06,  3.65it/s]Inference:  25%|██▌       | 8/32 [00:02<00:06,  3.62it/s]Inference:  28%|██▊       | 9/32 [00:02<00:06,  3.57it/s]Inference:  31%|███▏      | 10/32 [00:02<00:06,  3.55it/s]Inference:  34%|███▍      | 11/32 [00:03<00:05,  3.55it/s]Inference:  38%|███▊      | 12/32 [00:03<00:05,  3.55it/s]Inference:  41%|████      | 13/32 [00:03<00:05,  3.54it/s]Inference:  44%|████▍     | 14/32 [00:03<00:04,  3.71it/s]Inference:  47%|████▋     | 15/32 [00:04<00:04,  3.65it/s]Inference:  50%|█████     | 16/32 [00:04<00:04,  3.59it/s]Inference:  53%|█████▎    | 17/32 [00:04<00:04,  3.55it/s]Inference:  56%|█████▋    | 18/32 [00:05<00:03,  3.51it/s]Inference:  59%|█████▉    | 19/32 [00:05<00:03,  3.45it/s]Inference:  62%|██████▎   | 20/32 [00:05<00:03,  3.42it/s]Inference:  66%|██████▌   | 21/32 [00:05<00:03,  3.39it/s]Inference:  69%|██████▉   | 22/32 [00:06<00:02,  3.39it/s]Inference:  72%|███████▏  | 23/32 [00:06<00:02,  3.54it/s]Inference:  75%|███████▌  | 24/32 [00:06<00:02,  3.51it/s]Inference:  78%|███████▊  | 25/32 [00:07<00:02,  3.48it/s]Inference:  81%|████████▏ | 26/32 [00:07<00:01,  3.46it/s]Inference:  84%|████████▍ | 27/32 [00:07<00:01,  3.41it/s]Inference:  88%|████████▊ | 28/32 [00:07<00:01,  3.48it/s]Inference:  91%|█████████ | 29/32 [00:08<00:00,  3.43it/s]Inference:  94%|█████████▍| 30/32 [00:08<00:00,  3.40it/s]Inference:  97%|█████████▋| 31/32 [00:08<00:00,  3.38it/s]Inference: 100%|██████████| 32/32 [00:09<00:00,  3.37it/s]Inference: 100%|██████████| 32/32 [00:09<00:00,  3.51it/s]
15835 MiB free out of 48676 MiB total
Saved layer 21 to ./models/meta-llama/Llama-2-7b-hf/tensorized/128/64/no_finetune/layer_21.pt
after cast to cpu
19865 MiB free out of 48676 MiB total
running bpv: 0.3393579987046632
Done with layer 21 total_time elapsed: 34603 estimated time left: 15728
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:16,  1.85it/s]Inference:   6%|▋         | 2/32 [00:01<00:16,  1.80it/s]Inference:   9%|▉         | 3/32 [00:01<00:16,  1.78it/s]Inference:  12%|█▎        | 4/32 [00:02<00:15,  1.77it/s]Inference:  16%|█▌        | 5/32 [00:02<00:15,  1.78it/s]Inference:  19%|█▉        | 6/32 [00:03<00:14,  1.79it/s]Inference:  22%|██▏       | 7/32 [00:03<00:14,  1.78it/s]Inference:  25%|██▌       | 8/32 [00:04<00:13,  1.82it/s]Inference:  28%|██▊       | 9/32 [00:05<00:12,  1.80it/s]Inference:  31%|███▏      | 10/32 [00:05<00:12,  1.79it/s]Inference:  34%|███▍      | 11/32 [00:06<00:11,  1.78it/s]Inference:  38%|███▊      | 12/32 [00:06<00:11,  1.78it/s]Inference:  41%|████      | 13/32 [00:07<00:10,  1.77it/s]Inference:  44%|████▍     | 14/32 [00:07<00:10,  1.77it/s]Inference:  47%|████▋     | 15/32 [00:08<00:09,  1.77it/s]Inference:  50%|█████     | 16/32 [00:08<00:09,  1.77it/s]Inference:  53%|█████▎    | 17/32 [00:09<00:08,  1.76it/s]Inference:  56%|█████▋    | 18/32 [00:10<00:07,  1.76it/s]Inference:  59%|█████▉    | 19/32 [00:10<00:07,  1.76it/s]Inference:  62%|██████▎   | 20/32 [00:11<00:06,  1.76it/s]Inference:  66%|██████▌   | 21/32 [00:11<00:06,  1.76it/s]Inference:  69%|██████▉   | 22/32 [00:12<00:05,  1.76it/s]Inference:  72%|███████▏  | 23/32 [00:12<00:05,  1.76it/s]Inference:  75%|███████▌  | 24/32 [00:13<00:04,  1.76it/s]Inference:  78%|███████▊  | 25/32 [00:14<00:03,  1.76it/s]Inference:  81%|████████▏ | 26/32 [00:14<00:03,  1.76it/s]Inference:  84%|████████▍ | 27/32 [00:15<00:02,  1.76it/s]Inference:  88%|████████▊ | 28/32 [00:15<00:02,  1.76it/s]Inference:  91%|█████████ | 29/32 [00:16<00:01,  1.76it/s]Inference:  94%|█████████▍| 30/32 [00:16<00:01,  1.76it/s]Inference:  97%|█████████▋| 31/32 [00:17<00:00,  1.78it/s]Inference: 100%|██████████| 32/32 [00:18<00:00,  1.78it/s]Inference: 100%|██████████| 32/32 [00:18<00:00,  1.77it/s]
layer22: self_attn.q_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 83749.234375, val loss None, lr 0.01
iter 250, train loss 2100.076171875, val loss None, lr 0.01
iter 500, train loss 1872.1123046875, val loss None, lr 0.01
iter 750, train loss 1773.408203125, val loss None, lr 0.01
iter 1000, train loss 1723.875732421875, val loss None, lr 0.01
iter 1250, train loss 1705.336669921875, val loss None, lr 0.01
iter 1500, train loss 1634.1217041015625, val loss None, lr 0.003333
iter 1750, train loss 1639.0299072265625, val loss None, lr 0.001111
iter 2000, train loss 1596.739501953125, val loss None, lr 0.001111
iter 2250, train loss 1589.42333984375, val loss None, lr 0.001111
best loss 1582.5185546875
running bpv: 0.3400197237212576
layer22: self_attn.k_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 118699.4921875, val loss None, lr 0.01
iter 250, train loss 2034.4346923828125, val loss None, lr 0.01
iter 500, train loss 1817.842529296875, val loss None, lr 0.01
iter 750, train loss 1585.044677734375, val loss None, lr 0.003333
iter 1000, train loss 1557.8841552734375, val loss None, lr 0.003333
iter 1250, train loss 1564.7708740234375, val loss None, lr 0.001111
iter 1500, train loss 1497.80712890625, val loss None, lr 0.001111
iter 1750, train loss 1482.480712890625, val loss None, lr 0.00037
iter 2000, train loss 1472.93994140625, val loss None, lr 0.00037
iter 2250, train loss 1463.8433837890625, val loss None, lr 0.00037
best loss 1457.389404296875
running bpv: 0.3406764989481066
layer22: self_attn.v_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 8537.630859375, val loss None, lr 0.01
iter 250, train loss 883.6190185546875, val loss None, lr 0.01
iter 500, train loss 818.5629272460938, val loss None, lr 0.01
iter 750, train loss 798.5428466796875, val loss None, lr 0.01
iter 1000, train loss 785.5148315429688, val loss None, lr 0.003333
iter 1250, train loss 781.4675903320312, val loss None, lr 0.003333
iter 1500, train loss 778.6768188476562, val loss None, lr 0.003333
iter 1750, train loss 771.9459228515625, val loss None, lr 0.001111
iter 2000, train loss 769.9093627929688, val loss None, lr 0.001111
iter 2250, train loss 768.6043090820312, val loss None, lr 0.001111
best loss 766.6714477539062
running bpv: 0.34132837971588265
layer22: self_attn.o_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 2349.04736328125, val loss None, lr 0.01
iter 250, train loss 65.19103240966797, val loss None, lr 0.01
iter 500, train loss 66.50662231445312, val loss None, lr 0.01
iter 750, train loss 48.26874542236328, val loss None, lr 0.003333
iter 1000, train loss 46.65040588378906, val loss None, lr 0.003333
iter 1250, train loss 50.89202117919922, val loss None, lr 0.003333
iter 1500, train loss 45.623661041259766, val loss None, lr 0.003333
iter 1750, train loss 44.95050048828125, val loss None, lr 0.001111
iter 2000, train loss 44.8723258972168, val loss None, lr 0.001111
iter 2250, train loss 44.06440353393555, val loss None, lr 0.00037
best loss 43.81684494018555
running bpv: 0.3419754205336427
layer22: mlp.gate_proj
weight shape torch.Size([11008, 4096])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([22, 8, 8, 8]), -3, -2
iter 0, train loss 20258.701171875, val loss None, lr 0.01
iter 250, train loss 4880.2275390625, val loss None, lr 0.01
iter 500, train loss 4709.56103515625, val loss None, lr 0.01
iter 750, train loss 4646.40087890625, val loss None, lr 0.01
iter 1000, train loss 4703.62548828125, val loss None, lr 0.01
iter 1250, train loss 4581.408203125, val loss None, lr 0.003333
iter 1500, train loss 4565.7802734375, val loss None, lr 0.003333
iter 1750, train loss 4529.921875, val loss None, lr 0.001111
iter 2000, train loss 4516.517578125, val loss None, lr 0.001111
iter 2250, train loss 4509.73193359375, val loss None, lr 0.00037
best loss 4505.09375
running bpv: 0.3405661325522628
layer22: mlp.up_proj
weight shape torch.Size([11008, 4096])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([22, 8, 8, 8]), -3, -2
iter 0, train loss 13053.8349609375, val loss None, lr 0.01
iter 250, train loss 4077.363525390625, val loss None, lr 0.01
iter 500, train loss 3987.822265625, val loss None, lr 0.01
iter 750, train loss 3989.23681640625, val loss None, lr 0.01
iter 1000, train loss 3871.087890625, val loss None, lr 0.003333
iter 1250, train loss 3844.904296875, val loss None, lr 0.001111
iter 1500, train loss 3833.775390625, val loss None, lr 0.001111
iter 1750, train loss 3829.10595703125, val loss None, lr 0.001111
iter 2000, train loss 3819.24560546875, val loss None, lr 0.00037
iter 2250, train loss 3814.492431640625, val loss None, lr 0.00037
best loss 3810.09423828125
running bpv: 0.3391844148089172
layer22: mlp.down_proj
weight shape torch.Size([4096, 11008])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 22, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 296.7564392089844, val loss None, lr 0.01
iter 250, train loss 52.777095794677734, val loss None, lr 0.01
iter 500, train loss 51.396549224853516, val loss None, lr 0.01
iter 750, train loss 50.250587463378906, val loss None, lr 0.01
iter 1000, train loss 49.904754638671875, val loss None, lr 0.003333
iter 1250, train loss 49.39673614501953, val loss None, lr 0.003333
iter 1500, train loss 49.06541442871094, val loss None, lr 0.001111
iter 1750, train loss 49.00188446044922, val loss None, lr 0.001111
iter 2000, train loss 48.91721725463867, val loss None, lr 0.001111
iter 2250, train loss 48.90069580078125, val loss None, lr 0.001111
best loss 48.7725715637207
running bpv: 0.3393579987046632
19865 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:07,  4.21it/s]Inference:   6%|▋         | 2/32 [00:00<00:07,  3.85it/s]Inference:   9%|▉         | 3/32 [00:00<00:07,  3.72it/s]Inference:  12%|█▎        | 4/32 [00:01<00:07,  3.65it/s]Inference:  16%|█▌        | 5/32 [00:01<00:07,  3.63it/s]Inference:  19%|█▉        | 6/32 [00:01<00:07,  3.62it/s]Inference:  22%|██▏       | 7/32 [00:01<00:06,  3.82it/s]Inference:  25%|██▌       | 8/32 [00:02<00:06,  3.75it/s]Inference:  28%|██▊       | 9/32 [00:02<00:06,  3.70it/s]Inference:  31%|███▏      | 10/32 [00:02<00:05,  3.86it/s]Inference:  34%|███▍      | 11/32 [00:02<00:05,  3.76it/s]Inference:  38%|███▊      | 12/32 [00:03<00:05,  3.71it/s]Inference:  41%|████      | 13/32 [00:03<00:05,  3.67it/s]Inference:  44%|████▍     | 14/32 [00:03<00:04,  3.68it/s]Inference:  47%|████▋     | 15/32 [00:04<00:04,  3.66it/s]Inference:  50%|█████     | 16/32 [00:04<00:04,  3.65it/s]Inference:  53%|█████▎    | 17/32 [00:04<00:04,  3.63it/s]Inference:  56%|█████▋    | 18/32 [00:04<00:03,  3.63it/s]Inference:  59%|█████▉    | 19/32 [00:05<00:03,  3.61it/s]Inference:  62%|██████▎   | 20/32 [00:05<00:03,  3.73it/s]Inference:  66%|██████▌   | 21/32 [00:05<00:02,  3.69it/s]Inference:  69%|██████▉   | 22/32 [00:05<00:02,  3.67it/s]Inference:  72%|███████▏  | 23/32 [00:06<00:02,  3.64it/s]Inference:  75%|███████▌  | 24/32 [00:06<00:02,  3.63it/s]Inference:  78%|███████▊  | 25/32 [00:06<00:01,  3.62it/s]Inference:  81%|████████▏ | 26/32 [00:07<00:01,  3.62it/s]Inference:  84%|████████▍ | 27/32 [00:07<00:01,  3.60it/s]Inference:  88%|████████▊ | 28/32 [00:07<00:01,  3.60it/s]Inference:  91%|█████████ | 29/32 [00:07<00:00,  3.61it/s]Inference:  94%|█████████▍| 30/32 [00:08<00:00,  3.60it/s]Inference:  97%|█████████▋| 31/32 [00:08<00:00,  3.60it/s]Inference: 100%|██████████| 32/32 [00:08<00:00,  3.61it/s]Inference: 100%|██████████| 32/32 [00:08<00:00,  3.67it/s]
14547 MiB free out of 48676 MiB total
Saved layer 22 to ./models/meta-llama/Llama-2-7b-hf/tensorized/128/64/no_finetune/layer_22.pt
after cast to cpu
18577 MiB free out of 48676 MiB total
running bpv: 0.3393579987046632
Done with layer 22 total_time elapsed: 35452 estimated time left: 13873
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:16,  1.89it/s]Inference:   6%|▋         | 2/32 [00:01<00:16,  1.82it/s]Inference:   9%|▉         | 3/32 [00:01<00:15,  1.83it/s]Inference:  12%|█▎        | 4/32 [00:02<00:15,  1.79it/s]Inference:  16%|█▌        | 5/32 [00:02<00:15,  1.80it/s]Inference:  19%|█▉        | 6/32 [00:03<00:14,  1.79it/s]Inference:  22%|██▏       | 7/32 [00:03<00:14,  1.78it/s]Inference:  25%|██▌       | 8/32 [00:04<00:13,  1.78it/s]Inference:  28%|██▊       | 9/32 [00:05<00:12,  1.78it/s]Inference:  31%|███▏      | 10/32 [00:05<00:12,  1.77it/s]Inference:  34%|███▍      | 11/32 [00:06<00:11,  1.78it/s]Inference:  38%|███▊      | 12/32 [00:06<00:11,  1.78it/s]Inference:  41%|████      | 13/32 [00:07<00:10,  1.78it/s]Inference:  44%|████▍     | 14/32 [00:07<00:09,  1.82it/s]Inference:  47%|████▋     | 15/32 [00:08<00:09,  1.83it/s]Inference:  50%|█████     | 16/32 [00:08<00:08,  1.81it/s]Inference:  53%|█████▎    | 17/32 [00:09<00:08,  1.80it/s]Inference:  56%|█████▋    | 18/32 [00:09<00:07,  1.83it/s]Inference:  59%|█████▉    | 19/32 [00:10<00:07,  1.81it/s]Inference:  62%|██████▎   | 20/32 [00:11<00:06,  1.83it/s]Inference:  66%|██████▌   | 21/32 [00:11<00:06,  1.81it/s]Inference:  69%|██████▉   | 22/32 [00:12<00:05,  1.79it/s]Inference:  72%|███████▏  | 23/32 [00:12<00:05,  1.79it/s]Inference:  75%|███████▌  | 24/32 [00:13<00:04,  1.80it/s]Inference:  78%|███████▊  | 25/32 [00:13<00:03,  1.79it/s]Inference:  81%|████████▏ | 26/32 [00:14<00:03,  1.78it/s]Inference:  84%|████████▍ | 27/32 [00:15<00:02,  1.78it/s]Inference:  88%|████████▊ | 28/32 [00:15<00:02,  1.77it/s]Inference:  91%|█████████ | 29/32 [00:16<00:01,  1.77it/s]Inference:  94%|█████████▍| 30/32 [00:16<00:01,  1.77it/s]Inference:  97%|█████████▋| 31/32 [00:17<00:00,  1.77it/s]Inference: 100%|██████████| 32/32 [00:17<00:00,  1.81it/s]Inference: 100%|██████████| 32/32 [00:17<00:00,  1.80it/s]
layer23: self_attn.q_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 84158.828125, val loss None, lr 0.01
iter 250, train loss 2176.10546875, val loss None, lr 0.01
iter 500, train loss 1795.992919921875, val loss None, lr 0.01
iter 750, train loss 1724.20458984375, val loss None, lr 0.01
iter 1000, train loss 1715.376708984375, val loss None, lr 0.01
iter 1250, train loss 1595.5242919921875, val loss None, lr 0.003333
iter 1500, train loss 1570.5528564453125, val loss None, lr 0.003333
iter 1750, train loss 1558.0125732421875, val loss None, lr 0.003333
iter 2000, train loss 1550.198974609375, val loss None, lr 0.001111
iter 2250, train loss 1536.5982666015625, val loss None, lr 0.001111
best loss 1529.4840087890625
running bpv: 0.3399910563973064
layer23: self_attn.k_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 115288.3203125, val loss None, lr 0.01
iter 250, train loss 1901.0267333984375, val loss None, lr 0.01
iter 500, train loss 1760.069091796875, val loss None, lr 0.01
iter 750, train loss 1765.31982421875, val loss None, lr 0.01
iter 1000, train loss 1555.9083251953125, val loss None, lr 0.003333
iter 1250, train loss 1532.763671875, val loss None, lr 0.003333
iter 1500, train loss 1490.2659912109375, val loss None, lr 0.001111
iter 1750, train loss 1486.3365478515625, val loss None, lr 0.001111
iter 2000, train loss 1465.3118896484375, val loss None, lr 0.00037
iter 2250, train loss 1456.929931640625, val loss None, lr 0.00037
best loss 1451.5059814453125
running bpv: 0.340619583146947
layer23: self_attn.v_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 10140.51171875, val loss None, lr 0.01
iter 250, train loss 1011.4931030273438, val loss None, lr 0.01
iter 500, train loss 945.80126953125, val loss None, lr 0.01
iter 750, train loss 938.7929077148438, val loss None, lr 0.01
iter 1000, train loss 918.0263671875, val loss None, lr 0.01
iter 1250, train loss 902.214111328125, val loss None, lr 0.003333
iter 1500, train loss 918.065185546875, val loss None, lr 0.003333
iter 1750, train loss 891.870849609375, val loss None, lr 0.001111
iter 2000, train loss 888.740478515625, val loss None, lr 0.001111
iter 2250, train loss 887.0673828125, val loss None, lr 0.001111
best loss 885.1401977539062
running bpv: 0.34124362742366837
layer23: self_attn.o_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 3086.023681640625, val loss None, lr 0.01
iter 250, train loss 67.3331527709961, val loss None, lr 0.01
iter 500, train loss 57.931549072265625, val loss None, lr 0.01
iter 750, train loss 48.277679443359375, val loss None, lr 0.003333
iter 1000, train loss 46.98626708984375, val loss None, lr 0.003333
iter 1250, train loss 46.00641632080078, val loss None, lr 0.001111
iter 1500, train loss 44.864505767822266, val loss None, lr 0.001111
iter 1750, train loss 44.52450180053711, val loss None, lr 0.001111
iter 2000, train loss 44.491432189941406, val loss None, lr 0.001111
iter 2250, train loss 43.90345001220703, val loss None, lr 0.00037
best loss 43.714637756347656
running bpv: 0.3418632370086609
layer23: mlp.gate_proj
weight shape torch.Size([11008, 4096])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([22, 8, 8, 8]), -3, -2
iter 0, train loss 23126.912109375, val loss None, lr 0.01
iter 250, train loss 5280.20751953125, val loss None, lr 0.01
iter 500, train loss 5168.330078125, val loss None, lr 0.01
iter 750, train loss 5097.31884765625, val loss None, lr 0.01
iter 1000, train loss 4952.22216796875, val loss None, lr 0.003333
iter 1250, train loss 4933.96337890625, val loss None, lr 0.003333
iter 1500, train loss 4907.42431640625, val loss None, lr 0.001111
iter 1750, train loss 4894.7568359375, val loss None, lr 0.001111
iter 2000, train loss 4885.5087890625, val loss None, lr 0.001111
iter 2250, train loss 4877.6806640625, val loss None, lr 0.001111
best loss 4870.5576171875
running bpv: 0.3405148413440387
layer23: mlp.up_proj
weight shape torch.Size([11008, 4096])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([22, 8, 8, 8]), -3, -2
iter 0, train loss 14820.181640625, val loss None, lr 0.01
iter 250, train loss 4492.51611328125, val loss None, lr 0.01
iter 500, train loss 4418.16796875, val loss None, lr 0.01
iter 750, train loss 4323.06298828125, val loss None, lr 0.01
iter 1000, train loss 4325.62939453125, val loss None, lr 0.01
iter 1250, train loss 4236.50830078125, val loss None, lr 0.003333
iter 1500, train loss 4229.09423828125, val loss None, lr 0.003333
iter 1750, train loss 4206.7197265625, val loss None, lr 0.001111
iter 2000, train loss 4199.154296875, val loss None, lr 0.001111
iter 2250, train loss 4194.796875, val loss None, lr 0.00037
best loss 4187.361328125
running bpv: 0.3391917152429723
layer23: mlp.down_proj
weight shape torch.Size([4096, 11008])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 22, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 385.41986083984375, val loss None, lr 0.01
iter 250, train loss 62.856536865234375, val loss None, lr 0.01
iter 500, train loss 60.46503448486328, val loss None, lr 0.01
iter 750, train loss 59.515602111816406, val loss None, lr 0.01
iter 1000, train loss 59.050621032714844, val loss None, lr 0.01
iter 1250, train loss 58.370140075683594, val loss None, lr 0.003333
iter 1500, train loss 58.08341598510742, val loss None, lr 0.001111
iter 1750, train loss 57.910404205322266, val loss None, lr 0.001111
iter 2000, train loss 57.804168701171875, val loss None, lr 0.001111
iter 2250, train loss 57.69428253173828, val loss None, lr 0.00037
best loss 57.628631591796875
running bpv: 0.3393579987046632
18577 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:07,  4.17it/s]Inference:   6%|▋         | 2/32 [00:00<00:07,  3.90it/s]Inference:   9%|▉         | 3/32 [00:00<00:07,  3.76it/s]Inference:  12%|█▎        | 4/32 [00:01<00:07,  3.69it/s]Inference:  16%|█▌        | 5/32 [00:01<00:07,  3.64it/s]Inference:  19%|█▉        | 6/32 [00:01<00:07,  3.63it/s]Inference:  22%|██▏       | 7/32 [00:01<00:06,  3.61it/s]Inference:  25%|██▌       | 8/32 [00:02<00:06,  3.60it/s]Inference:  28%|██▊       | 9/32 [00:02<00:06,  3.60it/s]Inference:  31%|███▏      | 10/32 [00:02<00:06,  3.60it/s]Inference:  34%|███▍      | 11/32 [00:03<00:05,  3.59it/s]Inference:  38%|███▊      | 12/32 [00:03<00:05,  3.59it/s]Inference:  41%|████      | 13/32 [00:03<00:05,  3.59it/s]Inference:  44%|████▍     | 14/32 [00:03<00:05,  3.58it/s]Inference:  47%|████▋     | 15/32 [00:04<00:04,  3.72it/s]Inference:  50%|█████     | 16/32 [00:04<00:04,  3.67it/s]Inference:  53%|█████▎    | 17/32 [00:04<00:04,  3.65it/s]Inference:  56%|█████▋    | 18/32 [00:04<00:03,  3.62it/s]Inference:  59%|█████▉    | 19/32 [00:05<00:03,  3.61it/s]Inference:  62%|██████▎   | 20/32 [00:05<00:03,  3.59it/s]Inference:  66%|██████▌   | 21/32 [00:05<00:03,  3.59it/s]Inference:  69%|██████▉   | 22/32 [00:06<00:02,  3.58it/s]Inference:  72%|███████▏  | 23/32 [00:06<00:02,  3.58it/s]Inference:  75%|███████▌  | 24/32 [00:06<00:02,  3.60it/s]Inference:  78%|███████▊  | 25/32 [00:06<00:01,  3.60it/s]Inference:  81%|████████▏ | 26/32 [00:07<00:01,  3.59it/s]Inference:  84%|████████▍ | 27/32 [00:07<00:01,  3.59it/s]Inference:  88%|████████▊ | 28/32 [00:07<00:01,  3.57it/s]Inference:  91%|█████████ | 29/32 [00:08<00:00,  3.57it/s]Inference:  94%|█████████▍| 30/32 [00:08<00:00,  3.56it/s]Inference:  97%|█████████▋| 31/32 [00:08<00:00,  3.59it/s]Inference: 100%|██████████| 32/32 [00:08<00:00,  3.56it/s]Inference: 100%|██████████| 32/32 [00:08<00:00,  3.61it/s]
13259 MiB free out of 48676 MiB total
Saved layer 23 to ./models/meta-llama/Llama-2-7b-hf/tensorized/128/64/no_finetune/layer_23.pt
after cast to cpu
17289 MiB free out of 48676 MiB total
running bpv: 0.3393579987046632
Done with layer 23 total_time elapsed: 36302 estimated time left: 12101
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:16,  1.88it/s]Inference:   6%|▋         | 2/32 [00:01<00:16,  1.81it/s]Inference:   9%|▉         | 3/32 [00:01<00:16,  1.79it/s]Inference:  12%|█▎        | 4/32 [00:02<00:15,  1.78it/s]Inference:  16%|█▌        | 5/32 [00:02<00:15,  1.77it/s]Inference:  19%|█▉        | 6/32 [00:03<00:14,  1.78it/s]Inference:  22%|██▏       | 7/32 [00:03<00:14,  1.77it/s]Inference:  25%|██▌       | 8/32 [00:04<00:13,  1.77it/s]Inference:  28%|██▊       | 9/32 [00:05<00:13,  1.77it/s]Inference:  31%|███▏      | 10/32 [00:05<00:12,  1.77it/s]Inference:  34%|███▍      | 11/32 [00:06<00:11,  1.77it/s]Inference:  38%|███▊      | 12/32 [00:06<00:11,  1.77it/s]Inference:  41%|████      | 13/32 [00:07<00:10,  1.77it/s]Inference:  44%|████▍     | 14/32 [00:07<00:10,  1.79it/s]Inference:  47%|████▋     | 15/32 [00:08<00:09,  1.77it/s]Inference:  50%|█████     | 16/32 [00:09<00:09,  1.76it/s]Inference:  53%|█████▎    | 17/32 [00:09<00:08,  1.79it/s]Inference:  56%|█████▋    | 18/32 [00:10<00:07,  1.77it/s]Inference:  59%|█████▉    | 19/32 [00:10<00:07,  1.76it/s]Inference:  62%|██████▎   | 20/32 [00:11<00:06,  1.75it/s]Inference:  66%|██████▌   | 21/32 [00:11<00:06,  1.75it/s]Inference:  69%|██████▉   | 22/32 [00:12<00:05,  1.75it/s]Inference:  72%|███████▏  | 23/32 [00:13<00:05,  1.74it/s]Inference:  75%|███████▌  | 24/32 [00:13<00:04,  1.75it/s]Inference:  78%|███████▊  | 25/32 [00:14<00:04,  1.74it/s]Inference:  81%|████████▏ | 26/32 [00:14<00:03,  1.74it/s]Inference:  84%|████████▍ | 27/32 [00:15<00:02,  1.74it/s]Inference:  88%|████████▊ | 28/32 [00:15<00:02,  1.74it/s]Inference:  91%|█████████ | 29/32 [00:16<00:01,  1.74it/s]Inference:  94%|█████████▍| 30/32 [00:16<00:01,  1.77it/s]Inference:  97%|█████████▋| 31/32 [00:17<00:00,  1.76it/s]Inference: 100%|██████████| 32/32 [00:18<00:00,  1.80it/s]Inference: 100%|██████████| 32/32 [00:18<00:00,  1.77it/s]
layer24: self_attn.q_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 79860.640625, val loss None, lr 0.01
iter 250, train loss 1944.41943359375, val loss None, lr 0.01
iter 500, train loss 1694.353759765625, val loss None, lr 0.01
iter 750, train loss 1666.402099609375, val loss None, lr 0.01
iter 1000, train loss 1562.2955322265625, val loss None, lr 0.01
iter 1250, train loss 1595.4459228515625, val loss None, lr 0.01
iter 1500, train loss 1464.07470703125, val loss None, lr 0.003333
iter 1750, train loss 1442.46044921875, val loss None, lr 0.001111
iter 2000, train loss 1428.809814453125, val loss None, lr 0.001111
iter 2250, train loss 1423.303955078125, val loss None, lr 0.001111
best loss 1414.9595947265625
running bpv: 0.3399647697934596
layer24: self_attn.k_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 117270.8359375, val loss None, lr 0.01
iter 250, train loss 1880.7412109375, val loss None, lr 0.01
iter 500, train loss 1688.265625, val loss None, lr 0.01
iter 750, train loss 1475.7747802734375, val loss None, lr 0.01
iter 1000, train loss 1518.4658203125, val loss None, lr 0.01
iter 1250, train loss 1356.945068359375, val loss None, lr 0.003333
iter 1500, train loss 1333.768310546875, val loss None, lr 0.003333
iter 1750, train loss 1306.3154296875, val loss None, lr 0.001111
iter 2000, train loss 1298.8695068359375, val loss None, lr 0.001111
iter 2250, train loss 1291.501708984375, val loss None, lr 0.001111
best loss 1284.63720703125
running bpv: 0.34056737778730706
layer24: self_attn.v_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 10488.1484375, val loss None, lr 0.01
iter 250, train loss 918.777587890625, val loss None, lr 0.01
iter 500, train loss 871.6463012695312, val loss None, lr 0.01
iter 750, train loss 860.0810546875, val loss None, lr 0.01
iter 1000, train loss 841.7482299804688, val loss None, lr 0.01
iter 1250, train loss 826.848876953125, val loss None, lr 0.003333
iter 1500, train loss 820.2578125, val loss None, lr 0.001111
iter 1750, train loss 817.3733520507812, val loss None, lr 0.001111
iter 2000, train loss 814.8402099609375, val loss None, lr 0.001111
iter 2250, train loss 813.1834106445312, val loss None, lr 0.001111
best loss 810.9541015625
running bpv: 0.3411658653846154
layer24: self_attn.o_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 3457.8818359375, val loss None, lr 0.01
iter 250, train loss 101.13863372802734, val loss None, lr 0.01
iter 500, train loss 90.01105499267578, val loss None, lr 0.01
iter 750, train loss 95.54722595214844, val loss None, lr 0.003333
iter 1000, train loss 78.70008850097656, val loss None, lr 0.003333
iter 1250, train loss 79.65449523925781, val loss None, lr 0.003333
iter 1500, train loss 76.9767837524414, val loss None, lr 0.003333
iter 1750, train loss 76.28291320800781, val loss None, lr 0.003333
iter 2000, train loss 75.66030883789062, val loss None, lr 0.001111
iter 2250, train loss 74.98023223876953, val loss None, lr 0.001111
best loss 74.48208618164062
running bpv: 0.3417602747018739
layer24: mlp.gate_proj
weight shape torch.Size([11008, 4096])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([22, 8, 8, 8]), -3, -2
iter 0, train loss 25460.423828125, val loss None, lr 0.01
iter 250, train loss 5329.5634765625, val loss None, lr 0.01
iter 500, train loss 5200.5380859375, val loss None, lr 0.01
iter 750, train loss 5073.7333984375, val loss None, lr 0.01
iter 1000, train loss 5053.8134765625, val loss None, lr 0.01
iter 1250, train loss 5054.4921875, val loss None, lr 0.01
iter 1500, train loss 4913.0947265625, val loss None, lr 0.003333
iter 1750, train loss 4883.1279296875, val loss None, lr 0.001111
iter 2000, train loss 4872.7978515625, val loss None, lr 0.001111
iter 2250, train loss 4865.32421875, val loss None, lr 0.00037
best loss 4859.583984375
running bpv: 0.3404677278961806
layer24: mlp.up_proj
weight shape torch.Size([11008, 4096])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([22, 8, 8, 8]), -3, -2
iter 0, train loss 16100.865234375, val loss None, lr 0.01
iter 250, train loss 4491.669921875, val loss None, lr 0.01
iter 500, train loss 4385.1689453125, val loss None, lr 0.01
iter 750, train loss 4347.73388671875, val loss None, lr 0.01
iter 1000, train loss 4239.5966796875, val loss None, lr 0.003333
iter 1250, train loss 4244.75927734375, val loss None, lr 0.001111
iter 1500, train loss 4193.3359375, val loss None, lr 0.001111
iter 1750, train loss 4185.69677734375, val loss None, lr 0.00037
iter 2000, train loss 4175.0087890625, val loss None, lr 0.00037
iter 2250, train loss 4169.44970703125, val loss None, lr 0.00037
best loss 4164.31591796875
running bpv: 0.3391984263906315
layer24: mlp.down_proj
weight shape torch.Size([4096, 11008])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 22, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 406.504150390625, val loss None, lr 0.01
iter 250, train loss 64.78577423095703, val loss None, lr 0.01
iter 500, train loss 62.21141052246094, val loss None, lr 0.01
iter 750, train loss 61.01300811767578, val loss None, lr 0.01
iter 1000, train loss 60.49755096435547, val loss None, lr 0.01
iter 1250, train loss 60.002532958984375, val loss None, lr 0.01
iter 1500, train loss 59.99873352050781, val loss None, lr 0.003333
iter 1750, train loss 59.16697311401367, val loss None, lr 0.003333
iter 2000, train loss 58.85134506225586, val loss None, lr 0.001111
iter 2250, train loss 58.75635528564453, val loss None, lr 0.001111
best loss 58.68021774291992
running bpv: 0.3393579987046632
17289 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:07,  4.27it/s]Inference:   6%|▋         | 2/32 [00:00<00:07,  3.85it/s]Inference:   9%|▉         | 3/32 [00:00<00:07,  3.73it/s]Inference:  12%|█▎        | 4/32 [00:01<00:07,  3.95it/s]Inference:  16%|█▌        | 5/32 [00:01<00:07,  3.81it/s]Inference:  19%|█▉        | 6/32 [00:01<00:06,  3.75it/s]Inference:  22%|██▏       | 7/32 [00:01<00:06,  3.69it/s]Inference:  25%|██▌       | 8/32 [00:02<00:06,  3.66it/s]Inference:  28%|██▊       | 9/32 [00:02<00:06,  3.83it/s]Inference:  31%|███▏      | 10/32 [00:02<00:05,  3.74it/s]Inference:  34%|███▍      | 11/32 [00:02<00:05,  3.69it/s]Inference:  38%|███▊      | 12/32 [00:03<00:05,  3.79it/s]Inference:  41%|████      | 13/32 [00:03<00:05,  3.70it/s]Inference:  44%|████▍     | 14/32 [00:03<00:04,  3.63it/s]Inference:  47%|████▋     | 15/32 [00:04<00:04,  3.58it/s]Inference:  50%|█████     | 16/32 [00:04<00:04,  3.55it/s]Inference:  53%|█████▎    | 17/32 [00:04<00:04,  3.53it/s]Inference:  56%|█████▋    | 18/32 [00:04<00:03,  3.51it/s]Inference:  59%|█████▉    | 19/32 [00:05<00:03,  3.51it/s]Inference:  62%|██████▎   | 20/32 [00:05<00:03,  3.51it/s]Inference:  66%|██████▌   | 21/32 [00:05<00:03,  3.50it/s]Inference:  69%|██████▉   | 22/32 [00:06<00:02,  3.49it/s]Inference:  72%|███████▏  | 23/32 [00:06<00:02,  3.49it/s]Inference:  75%|███████▌  | 24/32 [00:06<00:02,  3.48it/s]Inference:  78%|███████▊  | 25/32 [00:06<00:01,  3.60it/s]Inference:  81%|████████▏ | 26/32 [00:07<00:01,  3.79it/s]Inference:  84%|████████▍ | 27/32 [00:07<00:01,  3.72it/s]Inference:  88%|████████▊ | 28/32 [00:07<00:01,  3.70it/s]Inference:  91%|█████████ | 29/32 [00:07<00:00,  3.67it/s]Inference:  94%|█████████▍| 30/32 [00:08<00:00,  3.66it/s]Inference:  97%|█████████▋| 31/32 [00:08<00:00,  3.63it/s]Inference: 100%|██████████| 32/32 [00:08<00:00,  3.63it/s]Inference: 100%|██████████| 32/32 [00:08<00:00,  3.65it/s]
12035 MiB free out of 48676 MiB total
Saved layer 24 to ./models/meta-llama/Llama-2-7b-hf/tensorized/128/64/no_finetune/layer_24.pt
after cast to cpu
16065 MiB free out of 48676 MiB total
running bpv: 0.3393579987046632
Done with layer 24 total_time elapsed: 37151 estimated time left: 10402
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:16,  1.90it/s]Inference:   6%|▋         | 2/32 [00:01<00:16,  1.83it/s]Inference:   9%|▉         | 3/32 [00:01<00:16,  1.80it/s]Inference:  12%|█▎        | 4/32 [00:02<00:15,  1.79it/s]Inference:  16%|█▌        | 5/32 [00:02<00:15,  1.79it/s]Inference:  19%|█▉        | 6/32 [00:03<00:14,  1.78it/s]Inference:  22%|██▏       | 7/32 [00:03<00:14,  1.78it/s]Inference:  25%|██▌       | 8/32 [00:04<00:13,  1.79it/s]Inference:  28%|██▊       | 9/32 [00:05<00:12,  1.78it/s]Inference:  31%|███▏      | 10/32 [00:05<00:12,  1.78it/s]Inference:  34%|███▍      | 11/32 [00:06<00:11,  1.78it/s]Inference:  38%|███▊      | 12/32 [00:06<00:11,  1.78it/s]Inference:  41%|████      | 13/32 [00:07<00:10,  1.78it/s]Inference:  44%|████▍     | 14/32 [00:07<00:10,  1.77it/s]Inference:  47%|████▋     | 15/32 [00:08<00:09,  1.81it/s]Inference:  50%|█████     | 16/32 [00:08<00:08,  1.80it/s]Inference:  53%|█████▎    | 17/32 [00:09<00:08,  1.79it/s]Inference:  56%|█████▋    | 18/32 [00:10<00:07,  1.79it/s]Inference:  59%|█████▉    | 19/32 [00:10<00:07,  1.78it/s]Inference:  62%|██████▎   | 20/32 [00:11<00:06,  1.82it/s]Inference:  66%|██████▌   | 21/32 [00:11<00:06,  1.82it/s]Inference:  69%|██████▉   | 22/32 [00:12<00:05,  1.80it/s]Inference:  72%|███████▏  | 23/32 [00:12<00:05,  1.79it/s]Inference:  75%|███████▌  | 24/32 [00:13<00:04,  1.78it/s]Inference:  78%|███████▊  | 25/32 [00:13<00:03,  1.78it/s]Inference:  81%|████████▏ | 26/32 [00:14<00:03,  1.77it/s]Inference:  84%|████████▍ | 27/32 [00:15<00:02,  1.77it/s]Inference:  88%|████████▊ | 28/32 [00:15<00:02,  1.77it/s]Inference:  91%|█████████ | 29/32 [00:16<00:01,  1.77it/s]Inference:  94%|█████████▍| 30/32 [00:16<00:01,  1.77it/s]Inference:  97%|█████████▋| 31/32 [00:17<00:00,  1.77it/s]Inference: 100%|██████████| 32/32 [00:17<00:00,  1.78it/s]Inference: 100%|██████████| 32/32 [00:17<00:00,  1.79it/s]
layer25: self_attn.q_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 75037.8203125, val loss None, lr 0.01
iter 250, train loss 1795.548095703125, val loss None, lr 0.01
iter 500, train loss 1661.0660400390625, val loss None, lr 0.01
iter 750, train loss 1600.263671875, val loss None, lr 0.01
iter 1000, train loss 1467.878662109375, val loss None, lr 0.003333
iter 1250, train loss 1422.667236328125, val loss None, lr 0.001111
iter 1500, train loss 1411.31396484375, val loss None, lr 0.001111
iter 1750, train loss 1404.19775390625, val loss None, lr 0.001111
iter 2000, train loss 1391.4378662109375, val loss None, lr 0.00037
iter 2250, train loss 1385.0548095703125, val loss None, lr 0.00037
best loss 1380.04296875
running bpv: 0.33994057916752735
layer25: self_attn.k_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 107584.2734375, val loss None, lr 0.01
iter 250, train loss 1777.375244140625, val loss None, lr 0.01
iter 500, train loss 1566.5419921875, val loss None, lr 0.01
iter 750, train loss 1594.5322265625, val loss None, lr 0.01
iter 1000, train loss 1398.9056396484375, val loss None, lr 0.003333
iter 1250, train loss 1366.48046875, val loss None, lr 0.001111
iter 1500, train loss 1351.67333984375, val loss None, lr 0.001111
iter 1750, train loss 1340.3623046875, val loss None, lr 0.00037
iter 2000, train loss 1328.7738037109375, val loss None, lr 0.00037
iter 2250, train loss 1321.6766357421875, val loss None, lr 0.00037
best loss 1315.4178466796875
running bpv: 0.3405193213403335
layer25: self_attn.v_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 12996.185546875, val loss None, lr 0.01
iter 250, train loss 1062.85205078125, val loss None, lr 0.01
iter 500, train loss 1000.5709228515625, val loss None, lr 0.01
iter 750, train loss 979.205322265625, val loss None, lr 0.01
iter 1000, train loss 957.5523681640625, val loss None, lr 0.01
iter 1250, train loss 938.9590454101562, val loss None, lr 0.003333
iter 1500, train loss 933.6592407226562, val loss None, lr 0.001111
iter 1750, train loss 926.6534423828125, val loss None, lr 0.001111
iter 2000, train loss 923.9357299804688, val loss None, lr 0.001111
iter 2250, train loss 922.0274658203125, val loss None, lr 0.001111
best loss 919.0045166015625
running bpv: 0.34109426303098705
layer25: self_attn.o_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 3733.6875, val loss None, lr 0.01
iter 250, train loss 83.53147888183594, val loss None, lr 0.01
iter 500, train loss 72.48448181152344, val loss None, lr 0.01
iter 750, train loss 68.16770935058594, val loss None, lr 0.003333
iter 1000, train loss 63.9804801940918, val loss None, lr 0.001111
iter 1250, train loss 61.47621536254883, val loss None, lr 0.001111
iter 1500, train loss 60.5968017578125, val loss None, lr 0.001111
iter 1750, train loss 59.9680061340332, val loss None, lr 0.001111
iter 2000, train loss 59.67737579345703, val loss None, lr 0.001111
iter 2250, train loss 59.21839904785156, val loss None, lr 0.00037
best loss 58.914756774902344
running bpv: 0.3416654415524647
layer25: mlp.gate_proj
weight shape torch.Size([11008, 4096])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([22, 8, 8, 8]), -3, -2
iter 0, train loss 30324.91796875, val loss None, lr 0.01
iter 250, train loss 5483.66650390625, val loss None, lr 0.01
iter 500, train loss 5307.7431640625, val loss None, lr 0.01
iter 750, train loss 5096.6240234375, val loss None, lr 0.003333
iter 1000, train loss 5049.8525390625, val loss None, lr 0.003333
iter 1250, train loss 5030.16796875, val loss None, lr 0.003333
iter 1500, train loss 5002.0986328125, val loss None, lr 0.001111
iter 1750, train loss 4986.427734375, val loss None, lr 0.001111
iter 2000, train loss 4976.28515625, val loss None, lr 0.00037
iter 2250, train loss 4968.5146484375, val loss None, lr 0.00037
best loss 4961.1884765625
running bpv: 0.3404243017538524
layer25: mlp.up_proj
weight shape torch.Size([11008, 4096])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([22, 8, 8, 8]), -3, -2
iter 0, train loss 18012.8515625, val loss None, lr 0.01
iter 250, train loss 4658.72607421875, val loss None, lr 0.01
iter 500, train loss 4535.625, val loss None, lr 0.01
iter 750, train loss 4487.45947265625, val loss None, lr 0.01
iter 1000, train loss 4432.5419921875, val loss None, lr 0.003333
iter 1250, train loss 4416.01513671875, val loss None, lr 0.003333
iter 1500, train loss 4317.6689453125, val loss None, lr 0.001111
iter 1750, train loss 4316.98193359375, val loss None, lr 0.001111
iter 2000, train loss 4302.66796875, val loss None, lr 0.001111
iter 2250, train loss 4294.6181640625, val loss None, lr 0.00037
best loss 4287.82763671875
running bpv: 0.33920461683417086
layer25: mlp.down_proj
weight shape torch.Size([4096, 11008])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 22, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 775.027099609375, val loss None, lr 0.01
iter 250, train loss 73.43685913085938, val loss None, lr 0.01
iter 500, train loss 68.99317932128906, val loss None, lr 0.01
iter 750, train loss 66.9681167602539, val loss None, lr 0.003333
iter 1000, train loss 65.47808837890625, val loss None, lr 0.003333
iter 1250, train loss 64.75361633300781, val loss None, lr 0.001111
iter 1500, train loss 64.33061981201172, val loss None, lr 0.001111
iter 1750, train loss 64.17684936523438, val loss None, lr 0.001111
iter 2000, train loss 63.977333068847656, val loss None, lr 0.001111
iter 2250, train loss 63.89995574951172, val loss None, lr 0.001111
best loss 63.7153434753418
running bpv: 0.3393579987046632
16065 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:07,  4.23it/s]Inference:   6%|▋         | 2/32 [00:00<00:07,  3.85it/s]Inference:   9%|▉         | 3/32 [00:00<00:07,  3.74it/s]Inference:  12%|█▎        | 4/32 [00:01<00:07,  3.69it/s]Inference:  16%|█▌        | 5/32 [00:01<00:07,  3.66it/s]Inference:  19%|█▉        | 6/32 [00:01<00:07,  3.63it/s]Inference:  22%|██▏       | 7/32 [00:01<00:06,  3.61it/s]Inference:  25%|██▌       | 8/32 [00:02<00:06,  3.56it/s]Inference:  28%|██▊       | 9/32 [00:02<00:06,  3.56it/s]Inference:  31%|███▏      | 10/32 [00:02<00:06,  3.53it/s]Inference:  34%|███▍      | 11/32 [00:03<00:05,  3.53it/s]Inference:  38%|███▊      | 12/32 [00:03<00:05,  3.52it/s]Inference:  41%|████      | 13/32 [00:03<00:05,  3.51it/s]Inference:  44%|████▍     | 14/32 [00:03<00:05,  3.50it/s]Inference:  47%|████▋     | 15/32 [00:04<00:04,  3.50it/s]Inference:  50%|█████     | 16/32 [00:04<00:04,  3.50it/s]Inference:  53%|█████▎    | 17/32 [00:04<00:04,  3.51it/s]Inference:  56%|█████▋    | 18/32 [00:05<00:04,  3.49it/s]Inference:  59%|█████▉    | 19/32 [00:05<00:03,  3.49it/s]Inference:  62%|██████▎   | 20/32 [00:05<00:03,  3.48it/s]Inference:  66%|██████▌   | 21/32 [00:05<00:03,  3.49it/s]Inference:  69%|██████▉   | 22/32 [00:06<00:02,  3.68it/s]Inference:  72%|███████▏  | 23/32 [00:06<00:02,  3.68it/s]Inference:  75%|███████▌  | 24/32 [00:06<00:02,  3.66it/s]Inference:  78%|███████▊  | 25/32 [00:06<00:01,  3.65it/s]Inference:  81%|████████▏ | 26/32 [00:07<00:01,  3.62it/s]Inference:  84%|████████▍ | 27/32 [00:07<00:01,  3.63it/s]Inference:  88%|████████▊ | 28/32 [00:07<00:01,  3.61it/s]Inference:  91%|█████████ | 29/32 [00:08<00:00,  3.62it/s]Inference:  94%|█████████▍| 30/32 [00:08<00:00,  3.61it/s]Inference:  97%|█████████▋| 31/32 [00:08<00:00,  3.64it/s]Inference: 100%|██████████| 32/32 [00:08<00:00,  3.61it/s]Inference: 100%|██████████| 32/32 [00:08<00:00,  3.59it/s]
10747 MiB free out of 48676 MiB total
Saved layer 25 to ./models/meta-llama/Llama-2-7b-hf/tensorized/128/64/no_finetune/layer_25.pt
after cast to cpu
14777 MiB free out of 48676 MiB total
running bpv: 0.3393579987046632
Done with layer 25 total_time elapsed: 38000 estimated time left: 8769
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:16,  1.91it/s]Inference:   6%|▋         | 2/32 [00:01<00:16,  1.83it/s]Inference:   9%|▉         | 3/32 [00:01<00:16,  1.81it/s]Inference:  12%|█▎        | 4/32 [00:02<00:15,  1.79it/s]Inference:  16%|█▌        | 5/32 [00:02<00:15,  1.78it/s]Inference:  19%|█▉        | 6/32 [00:03<00:14,  1.78it/s]Inference:  22%|██▏       | 7/32 [00:03<00:14,  1.78it/s]Inference:  25%|██▌       | 8/32 [00:04<00:13,  1.78it/s]Inference:  28%|██▊       | 9/32 [00:05<00:12,  1.78it/s]Inference:  31%|███▏      | 10/32 [00:05<00:12,  1.78it/s]Inference:  34%|███▍      | 11/32 [00:06<00:11,  1.78it/s]Inference:  38%|███▊      | 12/32 [00:06<00:11,  1.77it/s]Inference:  41%|████      | 13/32 [00:07<00:10,  1.78it/s]Inference:  44%|████▍     | 14/32 [00:07<00:10,  1.78it/s]Inference:  47%|████▋     | 15/32 [00:08<00:09,  1.78it/s]Inference:  50%|█████     | 16/32 [00:08<00:08,  1.80it/s]Inference:  53%|█████▎    | 17/32 [00:09<00:08,  1.79it/s]Inference:  56%|█████▋    | 18/32 [00:10<00:07,  1.77it/s]Inference:  59%|█████▉    | 19/32 [00:10<00:07,  1.76it/s]Inference:  62%|██████▎   | 20/32 [00:11<00:06,  1.76it/s]Inference:  66%|██████▌   | 21/32 [00:11<00:06,  1.75it/s]Inference:  69%|██████▉   | 22/32 [00:12<00:05,  1.75it/s]Inference:  72%|███████▏  | 23/32 [00:12<00:05,  1.75it/s]Inference:  75%|███████▌  | 24/32 [00:13<00:04,  1.76it/s]Inference:  78%|███████▊  | 25/32 [00:14<00:03,  1.76it/s]Inference:  81%|████████▏ | 26/32 [00:14<00:03,  1.76it/s]Inference:  84%|████████▍ | 27/32 [00:15<00:02,  1.80it/s]Inference:  88%|████████▊ | 28/32 [00:15<00:02,  1.79it/s]Inference:  91%|█████████ | 29/32 [00:16<00:01,  1.79it/s]Inference:  94%|█████████▍| 30/32 [00:16<00:01,  1.78it/s]Inference:  97%|█████████▋| 31/32 [00:17<00:00,  1.78it/s]Inference: 100%|██████████| 32/32 [00:18<00:00,  1.78it/s]Inference: 100%|██████████| 32/32 [00:18<00:00,  1.78it/s]
layer26: self_attn.q_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 84538.84375, val loss None, lr 0.01
iter 250, train loss 1832.42578125, val loss None, lr 0.01
iter 500, train loss 1583.7733154296875, val loss None, lr 0.01
iter 750, train loss 1521.6488037109375, val loss None, lr 0.01
iter 1000, train loss 1460.485595703125, val loss None, lr 0.01
iter 1250, train loss 1350.4219970703125, val loss None, lr 0.003333
iter 1500, train loss 1338.94189453125, val loss None, lr 0.001111
iter 1750, train loss 1312.3489990234375, val loss None, lr 0.001111
iter 2000, train loss 1301.8648681640625, val loss None, lr 0.001111
iter 2250, train loss 1295.2608642578125, val loss None, lr 0.00037
best loss 1290.50927734375
running bpv: 0.3399182434445769
layer26: self_attn.k_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 119663.953125, val loss None, lr 0.01
iter 250, train loss 1849.5145263671875, val loss None, lr 0.01
iter 500, train loss 1447.2645263671875, val loss None, lr 0.01
iter 750, train loss 1394.19775390625, val loss None, lr 0.01
iter 1000, train loss 1396.337158203125, val loss None, lr 0.003333
iter 1250, train loss 1308.1220703125, val loss None, lr 0.003333
iter 1500, train loss 1232.3841552734375, val loss None, lr 0.001111
iter 1750, train loss 1208.8675537109375, val loss None, lr 0.001111
iter 2000, train loss 1201.7037353515625, val loss None, lr 0.001111
iter 2250, train loss 1189.4307861328125, val loss None, lr 0.00037
best loss 1183.683349609375
running bpv: 0.34047493811881185
layer26: self_attn.v_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 12585.8466796875, val loss None, lr 0.01
iter 250, train loss 951.1380004882812, val loss None, lr 0.01
iter 500, train loss 891.6932373046875, val loss None, lr 0.01
iter 750, train loss 902.4606323242188, val loss None, lr 0.01
iter 1000, train loss 845.1355590820312, val loss None, lr 0.003333
iter 1250, train loss 840.21875, val loss None, lr 0.001111
iter 1500, train loss 831.4442749023438, val loss None, lr 0.001111
iter 1750, train loss 828.6004638671875, val loss None, lr 0.001111
iter 2000, train loss 825.8714599609375, val loss None, lr 0.001111
iter 2250, train loss 824.103271484375, val loss None, lr 0.001111
best loss 821.1646118164062
running bpv: 0.34102811636399527
layer26: self_attn.o_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 6157.4462890625, val loss None, lr 0.01
iter 250, train loss 176.61508178710938, val loss None, lr 0.01
iter 500, train loss 150.3822021484375, val loss None, lr 0.003333
iter 750, train loss 143.11978149414062, val loss None, lr 0.003333
iter 1000, train loss 138.13307189941406, val loss None, lr 0.003333
iter 1250, train loss 134.88900756835938, val loss None, lr 0.001111
iter 1500, train loss 133.81561279296875, val loss None, lr 0.001111
iter 1750, train loss 132.5057373046875, val loss None, lr 0.001111
iter 2000, train loss 131.97760009765625, val loss None, lr 0.001111
iter 2250, train loss 132.2578125, val loss None, lr 0.00037
best loss 130.28195190429688
running bpv: 0.3415778113931523
layer26: mlp.gate_proj
weight shape torch.Size([11008, 4096])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([22, 8, 8, 8]), -3, -2
iter 0, train loss 43122.75, val loss None, lr 0.01
iter 250, train loss 5668.884765625, val loss None, lr 0.01
iter 500, train loss 5501.98828125, val loss None, lr 0.01
iter 750, train loss 5183.17578125, val loss None, lr 0.003333
iter 1000, train loss 5104.783203125, val loss None, lr 0.001111
iter 1250, train loss 5083.9599609375, val loss None, lr 0.001111
iter 1500, train loss 5062.3046875, val loss None, lr 0.00037
iter 1750, train loss 5050.54248046875, val loss None, lr 0.00037
iter 2000, train loss 5039.6572265625, val loss None, lr 0.00037
iter 2250, train loss 5032.11767578125, val loss None, lr 0.00037
best loss 5024.681640625
running bpv: 0.3403841463414634
layer26: mlp.up_proj
weight shape torch.Size([11008, 4096])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([22, 8, 8, 8]), -3, -2
iter 0, train loss 22825.669921875, val loss None, lr 0.01
iter 250, train loss 4874.17919921875, val loss None, lr 0.01
iter 500, train loss 4735.3076171875, val loss None, lr 0.01
iter 750, train loss 4513.0634765625, val loss None, lr 0.003333
iter 1000, train loss 4481.95703125, val loss None, lr 0.001111
iter 1250, train loss 4464.328125, val loss None, lr 0.001111
iter 1500, train loss 4457.5263671875, val loss None, lr 0.001111
iter 1750, train loss 4437.3876953125, val loss None, lr 0.00037
iter 2000, train loss 4430.0009765625, val loss None, lr 0.00037
iter 2250, train loss 4423.89501953125, val loss None, lr 0.00037
best loss 4417.15234375
running bpv: 0.3392103449109907
layer26: mlp.down_proj
weight shape torch.Size([4096, 11008])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 22, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 727.9127197265625, val loss None, lr 0.01
iter 250, train loss 74.51921844482422, val loss None, lr 0.01
iter 500, train loss 70.30189514160156, val loss None, lr 0.01
iter 750, train loss 69.27897644042969, val loss None, lr 0.01
iter 1000, train loss 70.02493286132812, val loss None, lr 0.01
iter 1250, train loss 68.07335662841797, val loss None, lr 0.003333
iter 1500, train loss 66.51321411132812, val loss None, lr 0.003333
iter 1750, train loss 65.5758056640625, val loss None, lr 0.001111
iter 2000, train loss 65.40586853027344, val loss None, lr 0.001111
iter 2250, train loss 65.24752044677734, val loss None, lr 0.00037
best loss 65.12013244628906
running bpv: 0.3393579987046632
14777 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:07,  4.23it/s]Inference:   6%|▋         | 2/32 [00:00<00:07,  3.90it/s]Inference:   9%|▉         | 3/32 [00:00<00:07,  3.78it/s]Inference:  12%|█▎        | 4/32 [00:01<00:07,  3.77it/s]Inference:  16%|█▌        | 5/32 [00:01<00:07,  3.71it/s]Inference:  19%|█▉        | 6/32 [00:01<00:07,  3.65it/s]Inference:  22%|██▏       | 7/32 [00:01<00:06,  3.63it/s]Inference:  25%|██▌       | 8/32 [00:02<00:06,  3.61it/s]Inference:  28%|██▊       | 9/32 [00:02<00:06,  3.63it/s]Inference:  31%|███▏      | 10/32 [00:02<00:06,  3.61it/s]Inference:  34%|███▍      | 11/32 [00:02<00:05,  3.61it/s]Inference:  38%|███▊      | 12/32 [00:03<00:05,  3.60it/s]Inference:  41%|████      | 13/32 [00:03<00:05,  3.59it/s]Inference:  44%|████▍     | 14/32 [00:03<00:05,  3.60it/s]Inference:  47%|████▋     | 15/32 [00:04<00:04,  3.60it/s]Inference:  50%|█████     | 16/32 [00:04<00:04,  3.61it/s]Inference:  53%|█████▎    | 17/32 [00:04<00:03,  3.80it/s]Inference:  56%|█████▋    | 18/32 [00:04<00:03,  3.74it/s]Inference:  59%|█████▉    | 19/32 [00:05<00:03,  3.71it/s]Inference:  62%|██████▎   | 20/32 [00:05<00:03,  3.68it/s]Inference:  66%|██████▌   | 21/32 [00:05<00:03,  3.67it/s]Inference:  69%|██████▉   | 22/32 [00:05<00:02,  3.64it/s]Inference:  72%|███████▏  | 23/32 [00:06<00:02,  3.64it/s]Inference:  75%|███████▌  | 24/32 [00:06<00:02,  3.64it/s]Inference:  78%|███████▊  | 25/32 [00:06<00:01,  3.64it/s]Inference:  81%|████████▏ | 26/32 [00:07<00:01,  3.64it/s]Inference:  84%|████████▍ | 27/32 [00:07<00:01,  3.63it/s]Inference:  88%|████████▊ | 28/32 [00:07<00:01,  3.81it/s]Inference:  91%|█████████ | 29/32 [00:07<00:00,  3.90it/s]Inference:  94%|█████████▍| 30/32 [00:08<00:00,  3.78it/s]Inference:  97%|█████████▋| 31/32 [00:08<00:00,  3.92it/s]Inference: 100%|██████████| 32/32 [00:08<00:00,  3.87it/s]Inference: 100%|██████████| 32/32 [00:08<00:00,  3.71it/s]
9459 MiB free out of 48676 MiB total
Saved layer 26 to ./models/meta-llama/Llama-2-7b-hf/tensorized/128/64/no_finetune/layer_26.pt
after cast to cpu
13489 MiB free out of 48676 MiB total
running bpv: 0.3393579987046632
Done with layer 26 total_time elapsed: 38848 estimated time left: 7194
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:17,  1.79it/s]Inference:   6%|▋         | 2/32 [00:01<00:16,  1.78it/s]Inference:   9%|▉         | 3/32 [00:01<00:16,  1.78it/s]Inference:  12%|█▎        | 4/32 [00:02<00:15,  1.77it/s]Inference:  16%|█▌        | 5/32 [00:02<00:15,  1.77it/s]Inference:  19%|█▉        | 6/32 [00:03<00:14,  1.77it/s]Inference:  22%|██▏       | 7/32 [00:03<00:14,  1.77it/s]Inference:  25%|██▌       | 8/32 [00:04<00:13,  1.78it/s]Inference:  28%|██▊       | 9/32 [00:05<00:12,  1.77it/s]Inference:  31%|███▏      | 10/32 [00:05<00:12,  1.77it/s]Inference:  34%|███▍      | 11/32 [00:06<00:11,  1.78it/s]Inference:  38%|███▊      | 12/32 [00:06<00:11,  1.77it/s]Inference:  41%|████      | 13/32 [00:07<00:10,  1.77it/s]Inference:  44%|████▍     | 14/32 [00:07<00:09,  1.84it/s]Inference:  47%|████▋     | 15/32 [00:08<00:09,  1.80it/s]Inference:  50%|█████     | 16/32 [00:08<00:09,  1.77it/s]Inference:  53%|█████▎    | 17/32 [00:09<00:08,  1.77it/s]Inference:  56%|█████▋    | 18/32 [00:10<00:07,  1.81it/s]Inference:  59%|█████▉    | 19/32 [00:10<00:07,  1.80it/s]Inference:  62%|██████▎   | 20/32 [00:11<00:06,  1.82it/s]Inference:  66%|██████▌   | 21/32 [00:11<00:06,  1.80it/s]Inference:  69%|██████▉   | 22/32 [00:12<00:05,  1.79it/s]Inference:  72%|███████▏  | 23/32 [00:12<00:05,  1.79it/s]Inference:  75%|███████▌  | 24/32 [00:13<00:04,  1.82it/s]Inference:  78%|███████▊  | 25/32 [00:13<00:03,  1.81it/s]Inference:  81%|████████▏ | 26/32 [00:14<00:03,  1.79it/s]Inference:  84%|████████▍ | 27/32 [00:15<00:02,  1.79it/s]Inference:  88%|████████▊ | 28/32 [00:15<00:02,  1.78it/s]Inference:  91%|█████████ | 29/32 [00:16<00:01,  1.85it/s]Inference:  94%|█████████▍| 30/32 [00:16<00:01,  1.81it/s]Inference:  97%|█████████▋| 31/32 [00:17<00:00,  1.82it/s]Inference: 100%|██████████| 32/32 [00:17<00:00,  1.80it/s]Inference: 100%|██████████| 32/32 [00:17<00:00,  1.79it/s]
layer27: self_attn.q_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 78734.96875, val loss None, lr 0.01
iter 250, train loss 1523.6485595703125, val loss None, lr 0.01
iter 500, train loss 1270.5809326171875, val loss None, lr 0.01
iter 750, train loss 1176.158447265625, val loss None, lr 0.003333
iter 1000, train loss 1136.8302001953125, val loss None, lr 0.003333
iter 1250, train loss 1105.00439453125, val loss None, lr 0.001111
iter 1500, train loss 1095.190673828125, val loss None, lr 0.001111
iter 1750, train loss 1085.8377685546875, val loss None, lr 0.001111
iter 2000, train loss 1076.7490234375, val loss None, lr 0.001111
iter 2250, train loss 1067.649169921875, val loss None, lr 0.00037
best loss 1061.4541015625
running bpv: 0.3398975571551559
layer27: self_attn.k_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 111769.609375, val loss None, lr 0.01
iter 250, train loss 1476.13037109375, val loss None, lr 0.01
iter 500, train loss 1599.4833984375, val loss None, lr 0.01
iter 750, train loss 1158.3165283203125, val loss None, lr 0.003333
iter 1000, train loss 1129.3997802734375, val loss None, lr 0.001111
iter 1250, train loss 1093.09814453125, val loss None, lr 0.001111
iter 1500, train loss 1080.58251953125, val loss None, lr 0.001111
iter 1750, train loss 1070.8316650390625, val loss None, lr 0.001111
iter 2000, train loss 1070.06005859375, val loss None, lr 0.001111
iter 2250, train loss 1055.4365234375, val loss None, lr 0.001111
best loss 1048.1058349609375
running bpv: 0.3404338224775892
layer27: self_attn.v_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 13302.10546875, val loss None, lr 0.01
iter 250, train loss 819.5104370117188, val loss None, lr 0.01
iter 500, train loss 762.1087646484375, val loss None, lr 0.01
iter 750, train loss 741.9860229492188, val loss None, lr 0.01
iter 1000, train loss 720.860595703125, val loss None, lr 0.003333
iter 1250, train loss 713.3928833007812, val loss None, lr 0.003333
iter 1500, train loss 709.8663940429688, val loss None, lr 0.003333
iter 1750, train loss 705.5078125, val loss None, lr 0.003333
iter 2000, train loss 705.88818359375, val loss None, lr 0.001111
iter 2250, train loss 699.5873413085938, val loss None, lr 0.001111
best loss 697.0628662109375
running bpv: 0.34096682472903594
layer27: self_attn.o_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 5417.45556640625, val loss None, lr 0.01
iter 250, train loss 121.3985595703125, val loss None, lr 0.01
iter 500, train loss 103.0528793334961, val loss None, lr 0.01
iter 750, train loss 103.08850860595703, val loss None, lr 0.01
iter 1000, train loss 93.52861022949219, val loss None, lr 0.01
iter 1250, train loss 89.81874084472656, val loss None, lr 0.003333
iter 1500, train loss 88.0920181274414, val loss None, lr 0.003333
iter 1750, train loss 89.07396697998047, val loss None, lr 0.003333
iter 2000, train loss 85.64430236816406, val loss None, lr 0.001111
iter 2250, train loss 85.48077392578125, val loss None, lr 0.001111
best loss 84.82913208007812
running bpv: 0.3414965936018957
layer27: mlp.gate_proj
weight shape torch.Size([11008, 4096])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([22, 8, 8, 8]), -3, -2
iter 0, train loss 49369.9375, val loss None, lr 0.01
iter 250, train loss 5549.9169921875, val loss None, lr 0.01
iter 500, train loss 5364.95751953125, val loss None, lr 0.01
iter 750, train loss 5319.8359375, val loss None, lr 0.01
iter 1000, train loss 5196.69189453125, val loss None, lr 0.01
iter 1250, train loss 5025.455078125, val loss None, lr 0.003333
iter 1500, train loss 5048.7294921875, val loss None, lr 0.003333
iter 1750, train loss 4946.86865234375, val loss None, lr 0.001111
iter 2000, train loss 4934.40380859375, val loss None, lr 0.001111
iter 2250, train loss 4921.3203125, val loss None, lr 0.001111
best loss 4909.85595703125
running bpv: 0.34034690555660024
layer27: mlp.up_proj
weight shape torch.Size([11008, 4096])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([22, 8, 8, 8]), -3, -2
iter 0, train loss 28424.48046875, val loss None, lr 0.01
iter 250, train loss 4881.4873046875, val loss None, lr 0.01
iter 500, train loss 4724.5478515625, val loss None, lr 0.01
iter 750, train loss 4539.7099609375, val loss None, lr 0.003333
iter 1000, train loss 4483.68798828125, val loss None, lr 0.003333
iter 1250, train loss 4460.330078125, val loss None, lr 0.001111
iter 1500, train loss 4445.4619140625, val loss None, lr 0.001111
iter 1750, train loss 4433.71630859375, val loss None, lr 0.001111
iter 2000, train loss 4420.8876953125, val loss None, lr 0.00037
iter 2250, train loss 4414.4951171875, val loss None, lr 0.00037
best loss 4407.6953125
running bpv: 0.3392156605577318
layer27: mlp.down_proj
weight shape torch.Size([4096, 11008])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 22, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 993.60009765625, val loss None, lr 0.01
iter 250, train loss 78.6064224243164, val loss None, lr 0.01
iter 500, train loss 71.51310729980469, val loss None, lr 0.01
iter 750, train loss 71.330322265625, val loss None, lr 0.01
iter 1000, train loss 68.38008880615234, val loss None, lr 0.003333
iter 1250, train loss 67.703857421875, val loss None, lr 0.003333
iter 1500, train loss 66.88369750976562, val loss None, lr 0.001111
iter 1750, train loss 66.63638305664062, val loss None, lr 0.001111
iter 2000, train loss 66.38744354248047, val loss None, lr 0.00037
iter 2250, train loss 66.23500061035156, val loss None, lr 0.00037
best loss 66.10598754882812
running bpv: 0.3393579987046632
13489 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:07,  4.28it/s]Inference:   6%|▋         | 2/32 [00:00<00:07,  4.24it/s]Inference:   9%|▉         | 3/32 [00:00<00:06,  4.44it/s]Inference:  12%|█▎        | 4/32 [00:00<00:06,  4.08it/s]Inference:  16%|█▌        | 5/32 [00:01<00:06,  3.91it/s]Inference:  19%|█▉        | 6/32 [00:01<00:06,  3.79it/s]Inference:  22%|██▏       | 7/32 [00:01<00:06,  3.74it/s]Inference:  25%|██▌       | 8/32 [00:02<00:06,  3.69it/s]Inference:  28%|██▊       | 9/32 [00:02<00:06,  3.66it/s]Inference:  31%|███▏      | 10/32 [00:02<00:06,  3.65it/s]Inference:  34%|███▍      | 11/32 [00:02<00:05,  3.64it/s]Inference:  38%|███▊      | 12/32 [00:03<00:05,  3.63it/s]Inference:  41%|████      | 13/32 [00:03<00:05,  3.62it/s]Inference:  44%|████▍     | 14/32 [00:03<00:04,  3.61it/s]Inference:  47%|████▋     | 15/32 [00:04<00:04,  3.60it/s]Inference:  50%|█████     | 16/32 [00:04<00:04,  3.59it/s]Inference:  53%|█████▎    | 17/32 [00:04<00:04,  3.63it/s]Inference:  56%|█████▋    | 18/32 [00:04<00:03,  3.61it/s]Inference:  59%|█████▉    | 19/32 [00:05<00:03,  3.61it/s]Inference:  62%|██████▎   | 20/32 [00:05<00:03,  3.60it/s]Inference:  66%|██████▌   | 21/32 [00:05<00:03,  3.60it/s]Inference:  69%|██████▉   | 22/32 [00:05<00:02,  3.59it/s]Inference:  72%|███████▏  | 23/32 [00:06<00:02,  3.66it/s]Inference:  75%|███████▌  | 24/32 [00:06<00:02,  3.64it/s]Inference:  78%|███████▊  | 25/32 [00:06<00:01,  3.63it/s]Inference:  81%|████████▏ | 26/32 [00:07<00:01,  3.62it/s]Inference:  84%|████████▍ | 27/32 [00:07<00:01,  3.63it/s]Inference:  88%|████████▊ | 28/32 [00:07<00:01,  3.62it/s]Inference:  91%|█████████ | 29/32 [00:07<00:00,  3.62it/s]Inference:  94%|█████████▍| 30/32 [00:08<00:00,  3.62it/s]Inference:  97%|█████████▋| 31/32 [00:08<00:00,  3.63it/s]Inference: 100%|██████████| 32/32 [00:08<00:00,  3.73it/s]Inference: 100%|██████████| 32/32 [00:08<00:00,  3.69it/s]
8235 MiB free out of 48676 MiB total
Saved layer 27 to ./models/meta-llama/Llama-2-7b-hf/tensorized/128/64/no_finetune/layer_27.pt
after cast to cpu
12265 MiB free out of 48676 MiB total
running bpv: 0.3393579987046632
Done with layer 27 total_time elapsed: 39697 estimated time left: 5671
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:16,  1.89it/s]Inference:   6%|▋         | 2/32 [00:01<00:15,  1.90it/s]Inference:   9%|▉         | 3/32 [00:01<00:15,  1.92it/s]Inference:  12%|█▎        | 4/32 [00:02<00:14,  1.88it/s]Inference:  16%|█▌        | 5/32 [00:02<00:14,  1.86it/s]Inference:  19%|█▉        | 6/32 [00:03<00:14,  1.82it/s]Inference:  22%|██▏       | 7/32 [00:03<00:13,  1.81it/s]Inference:  25%|██▌       | 8/32 [00:04<00:13,  1.80it/s]Inference:  28%|██▊       | 9/32 [00:04<00:12,  1.79it/s]Inference:  31%|███▏      | 10/32 [00:05<00:12,  1.82it/s]Inference:  34%|███▍      | 11/32 [00:05<00:11,  1.84it/s]Inference:  38%|███▊      | 12/32 [00:06<00:11,  1.82it/s]Inference:  41%|████      | 13/32 [00:07<00:10,  1.80it/s]Inference:  44%|████▍     | 14/32 [00:07<00:10,  1.79it/s]Inference:  47%|████▋     | 15/32 [00:08<00:09,  1.79it/s]Inference:  50%|█████     | 16/32 [00:08<00:08,  1.79it/s]Inference:  53%|█████▎    | 17/32 [00:09<00:08,  1.78it/s]Inference:  56%|█████▋    | 18/32 [00:09<00:07,  1.78it/s]Inference:  59%|█████▉    | 19/32 [00:10<00:07,  1.78it/s]Inference:  62%|██████▎   | 20/32 [00:11<00:06,  1.78it/s]Inference:  66%|██████▌   | 21/32 [00:11<00:06,  1.77it/s]Inference:  69%|██████▉   | 22/32 [00:12<00:05,  1.78it/s]Inference:  72%|███████▏  | 23/32 [00:12<00:05,  1.77it/s]Inference:  75%|███████▌  | 24/32 [00:13<00:04,  1.77it/s]Inference:  78%|███████▊  | 25/32 [00:13<00:03,  1.81it/s]Inference:  81%|████████▏ | 26/32 [00:14<00:03,  1.79it/s]Inference:  84%|████████▍ | 27/32 [00:14<00:02,  1.77it/s]Inference:  88%|████████▊ | 28/32 [00:15<00:02,  1.76it/s]Inference:  91%|█████████ | 29/32 [00:16<00:01,  1.76it/s]Inference:  94%|█████████▍| 30/32 [00:16<00:01,  1.75it/s]Inference:  97%|█████████▋| 31/32 [00:17<00:00,  1.80it/s]Inference: 100%|██████████| 32/32 [00:17<00:00,  1.80it/s]Inference: 100%|██████████| 32/32 [00:17<00:00,  1.80it/s]
layer28: self_attn.q_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 78653.875, val loss None, lr 0.01
iter 250, train loss 1426.3203125, val loss None, lr 0.01
iter 500, train loss 1250.62255859375, val loss None, lr 0.01
iter 750, train loss 1096.5965576171875, val loss None, lr 0.01
iter 1000, train loss 1112.4166259765625, val loss None, lr 0.01
iter 1250, train loss 1039.9957275390625, val loss None, lr 0.003333
iter 1500, train loss 999.7579345703125, val loss None, lr 0.001111
iter 1750, train loss 989.6969604492188, val loss None, lr 0.001111
iter 2000, train loss 983.4337158203125, val loss None, lr 0.001111
iter 2250, train loss 977.50146484375, val loss None, lr 0.001111
best loss 971.12646484375
running bpv: 0.339878344095941
layer28: self_attn.k_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 112775.9453125, val loss None, lr 0.01
iter 250, train loss 1401.3018798828125, val loss None, lr 0.01
iter 500, train loss 1145.84375, val loss None, lr 0.003333
iter 750, train loss 1123.93798828125, val loss None, lr 0.003333
iter 1000, train loss 1036.4952392578125, val loss None, lr 0.001111
iter 1250, train loss 1013.883056640625, val loss None, lr 0.001111
iter 1500, train loss 998.8042602539062, val loss None, lr 0.001111
iter 1750, train loss 993.466552734375, val loss None, lr 0.001111
iter 2000, train loss 986.2879638671875, val loss None, lr 0.001111
iter 2250, train loss 993.6744995117188, val loss None, lr 0.001111
best loss 966.7757568359375
running bpv: 0.34039562637969095
layer28: self_attn.v_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 15327.130859375, val loss None, lr 0.01
iter 250, train loss 871.1239624023438, val loss None, lr 0.01
iter 500, train loss 793.5012817382812, val loss None, lr 0.01
iter 750, train loss 777.2661743164062, val loss None, lr 0.01
iter 1000, train loss 740.0614013671875, val loss None, lr 0.003333
iter 1250, train loss 738.8594360351562, val loss None, lr 0.003333
iter 1500, train loss 725.9501953125, val loss None, lr 0.001111
iter 1750, train loss 722.0390014648438, val loss None, lr 0.001111
iter 2000, train loss 719.71923828125, val loss None, lr 0.001111
iter 2250, train loss 716.6871337890625, val loss None, lr 0.00037
best loss 714.83251953125
running bpv: 0.34090987252384447
layer28: self_attn.o_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 7331.6572265625, val loss None, lr 0.01
iter 250, train loss 164.14865112304688, val loss None, lr 0.01
iter 500, train loss 155.32298278808594, val loss None, lr 0.01
iter 750, train loss 133.58575439453125, val loss None, lr 0.01
iter 1000, train loss 126.13353729248047, val loss None, lr 0.003333
iter 1250, train loss 123.97596740722656, val loss None, lr 0.003333
iter 1500, train loss 121.3631591796875, val loss None, lr 0.001111
iter 1750, train loss 120.23460388183594, val loss None, lr 0.001111
iter 2000, train loss 118.91869354248047, val loss None, lr 0.00037
iter 2250, train loss 118.31953430175781, val loss None, lr 0.00037
best loss 117.92919921875
running bpv: 0.34142110918068763
layer28: mlp.gate_proj
weight shape torch.Size([11008, 4096])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([22, 8, 8, 8]), -3, -2
iter 0, train loss 51705.2578125, val loss None, lr 0.01
iter 250, train loss 5753.93603515625, val loss None, lr 0.01
iter 500, train loss 5588.7568359375, val loss None, lr 0.01
iter 750, train loss 5410.3740234375, val loss None, lr 0.01
iter 1000, train loss 5270.68017578125, val loss None, lr 0.01
iter 1250, train loss 5137.99169921875, val loss None, lr 0.003333
iter 1500, train loss 5061.50341796875, val loss None, lr 0.001111
iter 1750, train loss 5041.7265625, val loss None, lr 0.001111
iter 2000, train loss 5026.142578125, val loss None, lr 0.00037
iter 2250, train loss 5016.6552734375, val loss None, lr 0.00037
best loss 5008.5341796875
running bpv: 0.3403122731809109
layer28: mlp.up_proj
weight shape torch.Size([11008, 4096])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([22, 8, 8, 8]), -3, -2
iter 0, train loss 38680.84375, val loss None, lr 0.01
iter 250, train loss 5248.28955078125, val loss None, lr 0.01
iter 500, train loss 5064.7060546875, val loss None, lr 0.01
iter 750, train loss 5050.12890625, val loss None, lr 0.01
iter 1000, train loss 5000.1005859375, val loss None, lr 0.003333
iter 1250, train loss 4737.24951171875, val loss None, lr 0.003333
iter 1500, train loss 4749.87109375, val loss None, lr 0.003333
iter 1750, train loss 4688.20703125, val loss None, lr 0.001111
iter 2000, train loss 4674.8994140625, val loss None, lr 0.001111
iter 2250, train loss 4665.31884765625, val loss None, lr 0.001111
best loss 4657.2216796875
running bpv: 0.33922060676989557
layer28: mlp.down_proj
weight shape torch.Size([4096, 11008])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 22, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 1383.302490234375, val loss None, lr 0.01
iter 250, train loss 96.43458557128906, val loss None, lr 0.01
iter 500, train loss 90.68888854980469, val loss None, lr 0.01
iter 750, train loss 85.05945587158203, val loss None, lr 0.003333
iter 1000, train loss 83.18184661865234, val loss None, lr 0.001111
iter 1250, train loss 82.51034545898438, val loss None, lr 0.001111
iter 1500, train loss 82.08448791503906, val loss None, lr 0.001111
iter 1750, train loss 81.64923095703125, val loss None, lr 0.001111
iter 2000, train loss 81.36646270751953, val loss None, lr 0.001111
iter 2250, train loss 81.00588989257812, val loss None, lr 0.00037
best loss 80.79100036621094
running bpv: 0.3393579987046632
12265 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:07,  4.05it/s]Inference:   6%|▋         | 2/32 [00:00<00:08,  3.70it/s]Inference:   9%|▉         | 3/32 [00:00<00:08,  3.61it/s]Inference:  12%|█▎        | 4/32 [00:01<00:07,  3.57it/s]Inference:  16%|█▌        | 5/32 [00:01<00:07,  3.81it/s]Inference:  19%|█▉        | 6/32 [00:01<00:06,  3.77it/s]Inference:  22%|██▏       | 7/32 [00:01<00:06,  3.72it/s]Inference:  25%|██▌       | 8/32 [00:02<00:06,  3.80it/s]Inference:  28%|██▊       | 9/32 [00:02<00:06,  3.76it/s]Inference:  31%|███▏      | 10/32 [00:02<00:05,  3.71it/s]Inference:  34%|███▍      | 11/32 [00:02<00:05,  3.68it/s]Inference:  38%|███▊      | 12/32 [00:03<00:05,  3.66it/s]Inference:  41%|████      | 13/32 [00:03<00:05,  3.65it/s]Inference:  44%|████▍     | 14/32 [00:03<00:04,  3.64it/s]Inference:  47%|████▋     | 15/32 [00:04<00:04,  3.64it/s]Inference:  50%|█████     | 16/32 [00:04<00:04,  3.63it/s]Inference:  53%|█████▎    | 17/32 [00:04<00:04,  3.63it/s]Inference:  56%|█████▋    | 18/32 [00:04<00:03,  3.64it/s]Inference:  59%|█████▉    | 19/32 [00:05<00:03,  3.64it/s]Inference:  62%|██████▎   | 20/32 [00:05<00:03,  3.64it/s]Inference:  66%|██████▌   | 21/32 [00:05<00:03,  3.64it/s]Inference:  69%|██████▉   | 22/32 [00:05<00:02,  3.64it/s]Inference:  72%|███████▏  | 23/32 [00:06<00:02,  3.63it/s]Inference:  75%|███████▌  | 24/32 [00:06<00:02,  3.62it/s]Inference:  78%|███████▊  | 25/32 [00:06<00:01,  3.68it/s]Inference:  81%|████████▏ | 26/32 [00:07<00:01,  3.62it/s]Inference:  84%|████████▍ | 27/32 [00:07<00:01,  3.57it/s]Inference:  88%|████████▊ | 28/32 [00:07<00:01,  3.55it/s]Inference:  91%|█████████ | 29/32 [00:07<00:00,  3.53it/s]Inference:  94%|█████████▍| 30/32 [00:08<00:00,  3.52it/s]Inference:  97%|█████████▋| 31/32 [00:08<00:00,  3.50it/s]Inference: 100%|██████████| 32/32 [00:08<00:00,  3.72it/s]Inference: 100%|██████████| 32/32 [00:08<00:00,  3.65it/s]
6947 MiB free out of 48676 MiB total
Saved layer 28 to ./models/meta-llama/Llama-2-7b-hf/tensorized/128/64/no_finetune/layer_28.pt
after cast to cpu
10977 MiB free out of 48676 MiB total
running bpv: 0.3393579987046632
Done with layer 28 total_time elapsed: 40549 estimated time left: 4195
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:17,  1.79it/s]Inference:   6%|▋         | 2/32 [00:01<00:16,  1.79it/s]Inference:   9%|▉         | 3/32 [00:01<00:16,  1.78it/s]Inference:  12%|█▎        | 4/32 [00:02<00:15,  1.78it/s]Inference:  16%|█▌        | 5/32 [00:02<00:14,  1.83it/s]Inference:  19%|█▉        | 6/32 [00:03<00:14,  1.80it/s]Inference:  22%|██▏       | 7/32 [00:03<00:13,  1.83it/s]Inference:  25%|██▌       | 8/32 [00:04<00:13,  1.80it/s]Inference:  28%|██▊       | 9/32 [00:05<00:12,  1.78it/s]Inference:  31%|███▏      | 10/32 [00:05<00:12,  1.77it/s]Inference:  34%|███▍      | 11/32 [00:06<00:11,  1.76it/s]Inference:  38%|███▊      | 12/32 [00:06<00:11,  1.79it/s]Inference:  41%|████      | 13/32 [00:07<00:10,  1.79it/s]Inference:  44%|████▍     | 14/32 [00:07<00:09,  1.81it/s]Inference:  47%|████▋     | 15/32 [00:08<00:09,  1.82it/s]Inference:  50%|█████     | 16/32 [00:08<00:08,  1.81it/s]Inference:  53%|█████▎    | 17/32 [00:09<00:08,  1.83it/s]Inference:  56%|█████▋    | 18/32 [00:10<00:07,  1.80it/s]Inference:  59%|█████▉    | 19/32 [00:10<00:07,  1.80it/s]Inference:  62%|██████▎   | 20/32 [00:11<00:06,  1.84it/s]Inference:  66%|██████▌   | 21/32 [00:11<00:05,  1.87it/s]Inference:  69%|██████▉   | 22/32 [00:12<00:05,  1.91it/s]Inference:  72%|███████▏  | 23/32 [00:12<00:04,  1.89it/s]Inference:  75%|███████▌  | 24/32 [00:13<00:04,  1.90it/s]Inference:  78%|███████▊  | 25/32 [00:13<00:03,  1.94it/s]Inference:  81%|████████▏ | 26/32 [00:14<00:03,  1.96it/s]Inference:  84%|████████▍ | 27/32 [00:14<00:02,  1.92it/s]Inference:  88%|████████▊ | 28/32 [00:15<00:02,  1.94it/s]Inference:  91%|█████████ | 29/32 [00:15<00:01,  1.95it/s]Inference:  94%|█████████▍| 30/32 [00:16<00:01,  1.92it/s]Inference:  97%|█████████▋| 31/32 [00:16<00:00,  1.95it/s]Inference: 100%|██████████| 32/32 [00:17<00:00,  1.94it/s]Inference: 100%|██████████| 32/32 [00:17<00:00,  1.85it/s]
layer29: self_attn.q_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 72094.265625, val loss None, lr 0.01
iter 250, train loss 1176.7342529296875, val loss None, lr 0.01
iter 500, train loss 1027.2708740234375, val loss None, lr 0.01
iter 750, train loss 896.010009765625, val loss None, lr 0.003333
iter 1000, train loss 898.0858154296875, val loss None, lr 0.003333
iter 1250, train loss 840.7158813476562, val loss None, lr 0.001111
iter 1500, train loss 829.9306640625, val loss None, lr 0.001111
iter 1750, train loss 820.761474609375, val loss None, lr 0.00037
iter 2000, train loss 813.4031982421875, val loss None, lr 0.00037
iter 2250, train loss 809.2633666992188, val loss None, lr 0.00037
best loss 805.8221435546875
running bpv: 0.33986045229823625
layer29: self_attn.k_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 108128.1015625, val loss None, lr 0.01
iter 250, train loss 1233.662109375, val loss None, lr 0.01
iter 500, train loss 1034.7515869140625, val loss None, lr 0.01
iter 750, train loss 891.7381591796875, val loss None, lr 0.003333
iter 1000, train loss 847.9468994140625, val loss None, lr 0.001111
iter 1250, train loss 828.4112548828125, val loss None, lr 0.001111
iter 1500, train loss 815.7823486328125, val loss None, lr 0.00037
iter 1750, train loss 802.2509765625, val loss None, lr 0.00037
iter 2000, train loss 795.4507446289062, val loss None, lr 0.00037
iter 2250, train loss 790.4099731445312, val loss None, lr 0.00037
best loss 785.3441162109375
running bpv: 0.3403600495203411
layer29: self_attn.v_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 14203.873046875, val loss None, lr 0.01
iter 250, train loss 730.3697509765625, val loss None, lr 0.01
iter 500, train loss 682.8804931640625, val loss None, lr 0.01
iter 750, train loss 660.0293579101562, val loss None, lr 0.01
iter 1000, train loss 631.27490234375, val loss None, lr 0.003333
iter 1250, train loss 624.0435791015625, val loss None, lr 0.003333
iter 1500, train loss 615.6041259765625, val loss None, lr 0.001111
iter 1750, train loss 612.3536376953125, val loss None, lr 0.001111
iter 2000, train loss 609.848876953125, val loss None, lr 0.001111
iter 2250, train loss 607.7548217773438, val loss None, lr 0.00037
best loss 605.8966064453125
running bpv: 0.34085681465899026
layer29: self_attn.o_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 10342.8046875, val loss None, lr 0.01
iter 250, train loss 223.73947143554688, val loss None, lr 0.01
iter 500, train loss 179.56704711914062, val loss None, lr 0.003333
iter 750, train loss 171.39566040039062, val loss None, lr 0.003333
iter 1000, train loss 164.14381408691406, val loss None, lr 0.003333
iter 1250, train loss 161.67849731445312, val loss None, lr 0.001111
iter 1500, train loss 158.1240234375, val loss None, lr 0.001111
iter 1750, train loss 156.13409423828125, val loss None, lr 0.001111
iter 2000, train loss 155.04510498046875, val loss None, lr 0.001111
iter 2250, train loss 153.77835083007812, val loss None, lr 0.00037
best loss 152.52365112304688
running bpv: 0.34135077172760997
layer29: mlp.gate_proj
weight shape torch.Size([11008, 4096])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([22, 8, 8, 8]), -3, -2
iter 0, train loss 62031.99609375, val loss None, lr 0.01
iter 250, train loss 5736.6240234375, val loss None, lr 0.01
iter 500, train loss 5497.04150390625, val loss None, lr 0.01
iter 750, train loss 5225.96240234375, val loss None, lr 0.003333
iter 1000, train loss 5118.146484375, val loss None, lr 0.001111
iter 1250, train loss 5077.3427734375, val loss None, lr 0.001111
iter 1500, train loss 5054.01416015625, val loss None, lr 0.001111
iter 1750, train loss 5029.4677734375, val loss None, lr 0.001111
iter 2000, train loss 5020.087890625, val loss None, lr 0.001111
iter 2250, train loss 5023.48046875, val loss None, lr 0.001111
best loss 4986.37890625
running bpv: 0.3402799844407433
layer29: mlp.up_proj
weight shape torch.Size([11008, 4096])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([22, 8, 8, 8]), -3, -2
iter 0, train loss 55230.34375, val loss None, lr 0.01
iter 250, train loss 5360.8701171875, val loss None, lr 0.01
iter 500, train loss 5117.1875, val loss None, lr 0.01
iter 750, train loss 5006.884765625, val loss None, lr 0.01
iter 1000, train loss 5031.4619140625, val loss None, lr 0.01
iter 1250, train loss 4805.0947265625, val loss None, lr 0.003333
iter 1500, train loss 4811.9033203125, val loss None, lr 0.003333
iter 1750, train loss 4763.55712890625, val loss None, lr 0.001111
iter 2000, train loss 4744.47802734375, val loss None, lr 0.00037
iter 2250, train loss 4737.68212890625, val loss None, lr 0.00037
best loss 4731.37646484375
running bpv: 0.3392252207673569
layer29: mlp.down_proj
weight shape torch.Size([4096, 11008])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 22, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 2134.22119140625, val loss None, lr 0.01
iter 250, train loss 112.51362609863281, val loss None, lr 0.01
iter 500, train loss 103.09225463867188, val loss None, lr 0.01
iter 750, train loss 98.86761474609375, val loss None, lr 0.01
iter 1000, train loss 94.22230529785156, val loss None, lr 0.003333
iter 1250, train loss 93.04478454589844, val loss None, lr 0.003333
iter 1500, train loss 92.14741516113281, val loss None, lr 0.001111
iter 1750, train loss 90.998046875, val loss None, lr 0.001111
iter 2000, train loss 90.86528778076172, val loss None, lr 0.001111
iter 2250, train loss 90.14157104492188, val loss None, lr 0.00037
best loss 89.83585357666016
running bpv: 0.3393579987046632
10977 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:07,  4.17it/s]Inference:   6%|▋         | 2/32 [00:00<00:07,  3.98it/s]Inference:   9%|▉         | 3/32 [00:00<00:07,  3.89it/s]Inference:  12%|█▎        | 4/32 [00:01<00:07,  3.84it/s]Inference:  16%|█▌        | 5/32 [00:01<00:07,  3.82it/s]Inference:  19%|█▉        | 6/32 [00:01<00:06,  3.77it/s]Inference:  22%|██▏       | 7/32 [00:01<00:06,  3.72it/s]Inference:  25%|██▌       | 8/32 [00:02<00:06,  3.68it/s]Inference:  28%|██▊       | 9/32 [00:02<00:06,  3.65it/s]Inference:  31%|███▏      | 10/32 [00:02<00:06,  3.62it/s]Inference:  34%|███▍      | 11/32 [00:02<00:05,  3.70it/s]Inference:  38%|███▊      | 12/32 [00:03<00:05,  3.68it/s]Inference:  41%|████      | 13/32 [00:03<00:05,  3.67it/s]Inference:  44%|████▍     | 14/32 [00:03<00:04,  3.66it/s]Inference:  47%|████▋     | 15/32 [00:04<00:04,  3.65it/s]Inference:  50%|█████     | 16/32 [00:04<00:04,  3.69it/s]Inference:  53%|█████▎    | 17/32 [00:04<00:04,  3.72it/s]Inference:  56%|█████▋    | 18/32 [00:04<00:03,  3.68it/s]Inference:  59%|█████▉    | 19/32 [00:05<00:03,  3.68it/s]Inference:  62%|██████▎   | 20/32 [00:05<00:03,  3.65it/s]Inference:  66%|██████▌   | 21/32 [00:05<00:03,  3.65it/s]Inference:  69%|██████▉   | 22/32 [00:05<00:02,  3.63it/s]Inference:  72%|███████▏  | 23/32 [00:06<00:02,  3.67it/s]Inference:  75%|███████▌  | 24/32 [00:06<00:02,  3.65it/s]Inference:  78%|███████▊  | 25/32 [00:06<00:01,  3.65it/s]Inference:  81%|████████▏ | 26/32 [00:07<00:01,  3.64it/s]Inference:  84%|████████▍ | 27/32 [00:07<00:01,  3.63it/s]Inference:  88%|████████▊ | 28/32 [00:07<00:01,  3.65it/s]Inference:  91%|█████████ | 29/32 [00:07<00:00,  3.64it/s]Inference:  94%|█████████▍| 30/32 [00:08<00:00,  3.63it/s]Inference:  97%|█████████▋| 31/32 [00:08<00:00,  3.62it/s]Inference: 100%|██████████| 32/32 [00:08<00:00,  3.61it/s]Inference: 100%|██████████| 32/32 [00:08<00:00,  3.68it/s]
5659 MiB free out of 48676 MiB total
Saved layer 29 to ./models/meta-llama/Llama-2-7b-hf/tensorized/128/64/no_finetune/layer_29.pt
after cast to cpu
9689 MiB free out of 48676 MiB total
running bpv: 0.3393579987046632
Done with layer 29 total_time elapsed: 41398 estimated time left: 2760
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:15,  1.94it/s]Inference:   6%|▋         | 2/32 [00:01<00:15,  1.92it/s]Inference:   9%|▉         | 3/32 [00:01<00:15,  1.92it/s]Inference:  12%|█▎        | 4/32 [00:02<00:14,  1.95it/s]Inference:  16%|█▌        | 5/32 [00:02<00:13,  1.95it/s]Inference:  19%|█▉        | 6/32 [00:03<00:13,  1.98it/s]Inference:  22%|██▏       | 7/32 [00:03<00:12,  1.99it/s]Inference:  25%|██▌       | 8/32 [00:04<00:12,  1.95it/s]Inference:  28%|██▊       | 9/32 [00:04<00:12,  1.88it/s]Inference:  31%|███▏      | 10/32 [00:05<00:11,  1.89it/s]Inference:  34%|███▍      | 11/32 [00:05<00:11,  1.90it/s]Inference:  38%|███▊      | 12/32 [00:06<00:10,  1.92it/s]Inference:  41%|████      | 13/32 [00:06<00:09,  1.91it/s]Inference:  44%|████▍     | 14/32 [00:07<00:09,  1.89it/s]Inference:  47%|████▋     | 15/32 [00:07<00:09,  1.88it/s]Inference:  50%|█████     | 16/32 [00:08<00:08,  1.88it/s]Inference:  53%|█████▎    | 17/32 [00:08<00:07,  1.92it/s]Inference:  56%|█████▋    | 18/32 [00:09<00:07,  1.91it/s]Inference:  59%|█████▉    | 19/32 [00:09<00:06,  1.90it/s]Inference:  62%|██████▎   | 20/32 [00:10<00:06,  1.91it/s]Inference:  66%|██████▌   | 21/32 [00:10<00:05,  1.89it/s]Inference:  69%|██████▉   | 22/32 [00:11<00:05,  1.85it/s]Inference:  72%|███████▏  | 23/32 [00:12<00:04,  1.90it/s]Inference:  75%|███████▌  | 24/32 [00:12<00:04,  1.92it/s]Inference:  78%|███████▊  | 25/32 [00:13<00:03,  1.90it/s]Inference:  81%|████████▏ | 26/32 [00:13<00:03,  1.92it/s]Inference:  84%|████████▍ | 27/32 [00:14<00:02,  1.89it/s]Inference:  88%|████████▊ | 28/32 [00:14<00:02,  1.90it/s]Inference:  91%|█████████ | 29/32 [00:15<00:01,  1.93it/s]Inference:  94%|█████████▍| 30/32 [00:15<00:01,  1.96it/s]Inference:  97%|█████████▋| 31/32 [00:16<00:00,  1.93it/s]Inference: 100%|██████████| 32/32 [00:16<00:00,  1.89it/s]Inference: 100%|██████████| 32/32 [00:16<00:00,  1.91it/s]
layer30: self_attn.q_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 80376.921875, val loss None, lr 0.01
iter 250, train loss 1274.067138671875, val loss None, lr 0.01
iter 500, train loss 882.6716918945312, val loss None, lr 0.003333
iter 750, train loss 831.9962768554688, val loss None, lr 0.003333
iter 1000, train loss 789.7386474609375, val loss None, lr 0.001111
iter 1250, train loss 775.7960205078125, val loss None, lr 0.001111
iter 1500, train loss 765.1920166015625, val loss None, lr 0.001111
iter 1750, train loss 753.2244873046875, val loss None, lr 0.00037
iter 2000, train loss 748.3674926757812, val loss None, lr 0.00037
iter 2250, train loss 743.3662109375, val loss None, lr 0.00037
best loss 737.9544067382812
running bpv: 0.33984375
layer30: self_attn.k_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 110421.6328125, val loss None, lr 0.01
iter 250, train loss 1168.954833984375, val loss None, lr 0.01
iter 500, train loss 943.828125, val loss None, lr 0.01
iter 750, train loss 838.1544189453125, val loss None, lr 0.003333
iter 1000, train loss 805.7379150390625, val loss None, lr 0.003333
iter 1250, train loss 773.3792724609375, val loss None, lr 0.001111
iter 1500, train loss 760.6552734375, val loss None, lr 0.001111
iter 1750, train loss 752.7015991210938, val loss None, lr 0.001111
iter 2000, train loss 744.172119140625, val loss None, lr 0.001111
iter 2250, train loss 735.467529296875, val loss None, lr 0.00037
best loss 729.1934814453125
running bpv: 0.3403268314153212
layer30: self_attn.v_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 17131.90234375, val loss None, lr 0.01
iter 250, train loss 724.2630004882812, val loss None, lr 0.01
iter 500, train loss 643.9415283203125, val loss None, lr 0.01
iter 750, train loss 622.572998046875, val loss None, lr 0.01
iter 1000, train loss 597.3927612304688, val loss None, lr 0.003333
iter 1250, train loss 590.114990234375, val loss None, lr 0.003333
iter 1500, train loss 581.671630859375, val loss None, lr 0.001111
iter 1750, train loss 578.8428955078125, val loss None, lr 0.001111
iter 2000, train loss 575.8077392578125, val loss None, lr 0.001111
iter 2250, train loss 574.3218383789062, val loss None, lr 0.001111
best loss 571.703125
running bpv: 0.34080726490236385
layer30: self_attn.o_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 10942.66796875, val loss None, lr 0.01
iter 250, train loss 192.7438201904297, val loss None, lr 0.01
iter 500, train loss 160.0083770751953, val loss None, lr 0.01
iter 750, train loss 163.2889862060547, val loss None, lr 0.01
iter 1000, train loss 137.99417114257812, val loss None, lr 0.001111
iter 1250, train loss 135.39732360839844, val loss None, lr 0.001111
iter 1500, train loss 133.4622802734375, val loss None, lr 0.001111
iter 1750, train loss 132.04580688476562, val loss None, lr 0.001111
iter 2000, train loss 130.86474609375, val loss None, lr 0.00037
iter 2250, train loss 129.9178466796875, val loss None, lr 0.00037
best loss 129.3395538330078
running bpv: 0.34128507217287324
layer30: mlp.gate_proj
weight shape torch.Size([11008, 4096])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([22, 8, 8, 8]), -3, -2
iter 0, train loss 81475.4765625, val loss None, lr 0.01
iter 250, train loss 6307.16845703125, val loss None, lr 0.01
iter 500, train loss 6039.6787109375, val loss None, lr 0.01
iter 750, train loss 5434.908203125, val loss None, lr 0.003333
iter 1000, train loss 5535.548828125, val loss None, lr 0.003333
iter 1250, train loss 5339.83203125, val loss None, lr 0.003333
iter 1500, train loss 5297.06640625, val loss None, lr 0.003333
iter 1750, train loss 5285.7646484375, val loss None, lr 0.001111
iter 2000, train loss 5238.78271484375, val loss None, lr 0.001111
iter 2250, train loss 5210.6943359375, val loss None, lr 0.00037
best loss 5196.4873046875
running bpv: 0.3402498092250297
layer30: mlp.up_proj
weight shape torch.Size([11008, 4096])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([22, 8, 8, 8]), -3, -2
iter 0, train loss 85719.0625, val loss None, lr 0.01
iter 250, train loss 6458.140625, val loss None, lr 0.01
iter 500, train loss 5915.11669921875, val loss None, lr 0.01
iter 750, train loss 5108.009765625, val loss None, lr 0.003333
iter 1000, train loss 5038.8359375, val loss None, lr 0.003333
iter 1250, train loss 4965.361328125, val loss None, lr 0.001111
iter 1500, train loss 4920.18603515625, val loss None, lr 0.001111
iter 1750, train loss 4905.3720703125, val loss None, lr 0.001111
iter 2000, train loss 4893.04833984375, val loss None, lr 0.001111
iter 2250, train loss 4882.55859375, val loss None, lr 0.001111
best loss 4868.90478515625
running bpv: 0.33922953493265995
layer30: mlp.down_proj
weight shape torch.Size([4096, 11008])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 22, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 3303252.0, val loss None, lr 0.01
iter 250, train loss 2148.42578125, val loss None, lr 0.01
iter 500, train loss 821.9157104492188, val loss None, lr 0.003333
iter 750, train loss 316.53955078125, val loss None, lr 0.001111
iter 1000, train loss 258.0646667480469, val loss None, lr 0.00037
iter 1250, train loss 206.18728637695312, val loss None, lr 0.00037
iter 1500, train loss 186.9996337890625, val loss None, lr 0.00037
iter 1750, train loss 177.34181213378906, val loss None, lr 0.00037
iter 2000, train loss 168.26614379882812, val loss None, lr 0.00037
iter 2250, train loss 163.63992309570312, val loss None, lr 0.00037
best loss 157.03221130371094
running bpv: 0.3393579987046632
9689 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:07,  4.22it/s]Inference:   6%|▋         | 2/32 [00:00<00:07,  3.89it/s]Inference:   9%|▉         | 3/32 [00:00<00:07,  3.78it/s]Inference:  12%|█▎        | 4/32 [00:01<00:07,  3.72it/s]Inference:  16%|█▌        | 5/32 [00:01<00:07,  3.69it/s]Inference:  19%|█▉        | 6/32 [00:01<00:07,  3.66it/s]Inference:  22%|██▏       | 7/32 [00:01<00:06,  3.66it/s]Inference:  25%|██▌       | 8/32 [00:02<00:06,  3.64it/s]Inference:  28%|██▊       | 9/32 [00:02<00:06,  3.64it/s]Inference:  31%|███▏      | 10/32 [00:02<00:06,  3.66it/s]Inference:  34%|███▍      | 11/32 [00:02<00:05,  3.66it/s]Inference:  38%|███▊      | 12/32 [00:03<00:05,  3.65it/s]Inference:  41%|████      | 13/32 [00:03<00:05,  3.65it/s]Inference:  44%|████▍     | 14/32 [00:03<00:04,  3.64it/s]Inference:  47%|████▋     | 15/32 [00:04<00:04,  3.64it/s]Inference:  50%|█████     | 16/32 [00:04<00:04,  3.63it/s]Inference:  53%|█████▎    | 17/32 [00:04<00:04,  3.65it/s]Inference:  56%|█████▋    | 18/32 [00:04<00:03,  3.65it/s]Inference:  59%|█████▉    | 19/32 [00:05<00:03,  3.65it/s]Inference:  62%|██████▎   | 20/32 [00:05<00:03,  3.65it/s]Inference:  66%|██████▌   | 21/32 [00:05<00:03,  3.65it/s]Inference:  69%|██████▉   | 22/32 [00:05<00:02,  3.64it/s]Inference:  72%|███████▏  | 23/32 [00:06<00:02,  3.64it/s]Inference:  75%|███████▌  | 24/32 [00:06<00:02,  3.64it/s]Inference:  78%|███████▊  | 25/32 [00:06<00:01,  3.65it/s]Inference:  81%|████████▏ | 26/32 [00:07<00:01,  3.65it/s]Inference:  84%|████████▍ | 27/32 [00:07<00:01,  3.65it/s]Inference:  88%|████████▊ | 28/32 [00:07<00:01,  3.64it/s]Inference:  91%|█████████ | 29/32 [00:07<00:00,  3.64it/s]Inference:  94%|█████████▍| 30/32 [00:08<00:00,  3.63it/s]Inference:  97%|█████████▋| 31/32 [00:08<00:00,  3.64it/s]Inference: 100%|██████████| 32/32 [00:08<00:00,  3.66it/s]Inference: 100%|██████████| 32/32 [00:08<00:00,  3.66it/s]
4435 MiB free out of 48676 MiB total
Saved layer 30 to ./models/meta-llama/Llama-2-7b-hf/tensorized/128/64/no_finetune/layer_30.pt
after cast to cpu
8465 MiB free out of 48676 MiB total
running bpv: 0.3393579987046632
Done with layer 30 total_time elapsed: 42246 estimated time left: 1363
layer original dtype torch.float16
sequential [['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj']]
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=4096, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=4096, out_features=11008, bias=False)
sublayer Linear(in_features=11008, out_features=4096, bias=False)
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:17,  1.79it/s]Inference:   6%|▋         | 2/32 [00:01<00:16,  1.79it/s]Inference:   9%|▉         | 3/32 [00:01<00:16,  1.79it/s]Inference:  12%|█▎        | 4/32 [00:02<00:15,  1.78it/s]Inference:  16%|█▌        | 5/32 [00:02<00:15,  1.78it/s]Inference:  19%|█▉        | 6/32 [00:03<00:14,  1.78it/s]Inference:  22%|██▏       | 7/32 [00:03<00:14,  1.78it/s]Inference:  25%|██▌       | 8/32 [00:04<00:13,  1.79it/s]Inference:  28%|██▊       | 9/32 [00:05<00:12,  1.78it/s]Inference:  31%|███▏      | 10/32 [00:05<00:12,  1.82it/s]Inference:  34%|███▍      | 11/32 [00:06<00:11,  1.82it/s]Inference:  38%|███▊      | 12/32 [00:06<00:11,  1.80it/s]Inference:  41%|████      | 13/32 [00:07<00:10,  1.78it/s]Inference:  44%|████▍     | 14/32 [00:07<00:10,  1.77it/s]Inference:  47%|████▋     | 15/32 [00:08<00:09,  1.76it/s]Inference:  50%|█████     | 16/32 [00:08<00:08,  1.80it/s]Inference:  53%|█████▎    | 17/32 [00:09<00:08,  1.79it/s]Inference:  56%|█████▋    | 18/32 [00:10<00:07,  1.79it/s]Inference:  59%|█████▉    | 19/32 [00:10<00:07,  1.79it/s]Inference:  62%|██████▎   | 20/32 [00:11<00:06,  1.82it/s]Inference:  66%|██████▌   | 21/32 [00:11<00:06,  1.81it/s]Inference:  69%|██████▉   | 22/32 [00:12<00:05,  1.80it/s]Inference:  72%|███████▏  | 23/32 [00:12<00:05,  1.80it/s]Inference:  75%|███████▌  | 24/32 [00:13<00:04,  1.83it/s]Inference:  78%|███████▊  | 25/32 [00:13<00:03,  1.81it/s]Inference:  81%|████████▏ | 26/32 [00:14<00:03,  1.80it/s]Inference:  84%|████████▍ | 27/32 [00:15<00:02,  1.79it/s]Inference:  88%|████████▊ | 28/32 [00:15<00:02,  1.79it/s]Inference:  91%|█████████ | 29/32 [00:16<00:01,  1.79it/s]Inference:  94%|█████████▍| 30/32 [00:16<00:01,  1.79it/s]Inference:  97%|█████████▋| 31/32 [00:17<00:00,  1.82it/s]Inference: 100%|██████████| 32/32 [00:17<00:00,  1.81it/s]Inference: 100%|██████████| 32/32 [00:17<00:00,  1.80it/s]
layer31: self_attn.q_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 65415.6875, val loss None, lr 0.01
iter 250, train loss 751.5718383789062, val loss None, lr 0.01
iter 500, train loss 559.2595825195312, val loss None, lr 0.003333
iter 750, train loss 503.86865234375, val loss None, lr 0.003333
iter 1000, train loss 489.6720275878906, val loss None, lr 0.003333
iter 1250, train loss 459.06378173828125, val loss None, lr 0.001111
iter 1500, train loss 452.02593994140625, val loss None, lr 0.001111
iter 1750, train loss 443.3725891113281, val loss None, lr 0.00037
iter 2000, train loss 439.01239013671875, val loss None, lr 0.00037
iter 2250, train loss 434.6233215332031, val loss None, lr 0.00037
best loss 431.3074951171875
running bpv: 0.3398281223953992
layer31: self_attn.k_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 93209.65625, val loss None, lr 0.01
iter 250, train loss 868.279052734375, val loss None, lr 0.01
iter 500, train loss 744.6305541992188, val loss None, lr 0.01
iter 750, train loss 653.3790283203125, val loss None, lr 0.01
iter 1000, train loss 491.04345703125, val loss None, lr 0.003333
iter 1250, train loss 467.13818359375, val loss None, lr 0.001111
iter 1500, train loss 459.0265197753906, val loss None, lr 0.001111
iter 1750, train loss 487.5605163574219, val loss None, lr 0.001111
iter 2000, train loss 448.98046875, val loss None, lr 0.001111
iter 2250, train loss 441.68206787109375, val loss None, lr 0.00037
best loss 438.59710693359375
running bpv: 0.34029574501246884
layer31: self_attn.v_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 9795.107421875, val loss None, lr 0.01
iter 250, train loss 330.31475830078125, val loss None, lr 0.01
iter 500, train loss 296.75482177734375, val loss None, lr 0.01
iter 750, train loss 283.54681396484375, val loss None, lr 0.01
iter 1000, train loss 274.1065673828125, val loss None, lr 0.003333
iter 1250, train loss 259.8694763183594, val loss None, lr 0.003333
iter 1500, train loss 256.22735595703125, val loss None, lr 0.001111
iter 1750, train loss 254.6143798828125, val loss None, lr 0.00037
iter 2000, train loss 253.19393920898438, val loss None, lr 0.00037
iter 2250, train loss 252.17813110351562, val loss None, lr 0.00037
best loss 251.23812866210938
running bpv: 0.340760886461615
layer31: self_attn.o_proj
weight shape torch.Size([4096, 4096])
[8, 8, 64]
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 94314.2734375, val loss None, lr 0.01
iter 250, train loss 533.6666259765625, val loss None, lr 0.01
iter 500, train loss 317.668212890625, val loss None, lr 0.01
iter 750, train loss 446.0802307128906, val loss None, lr 0.01
iter 1000, train loss 176.08412170410156, val loss None, lr 0.003333
iter 1250, train loss 162.7987060546875, val loss None, lr 0.003333
iter 1500, train loss 157.37998962402344, val loss None, lr 0.003333
iter 1750, train loss 147.59506225585938, val loss None, lr 0.001111
iter 2000, train loss 159.5223388671875, val loss None, lr 0.001111
iter 2250, train loss 142.23521423339844, val loss None, lr 0.001111
best loss 137.34950256347656
running bpv: 0.3412235664379031
layer31: mlp.gate_proj
weight shape torch.Size([11008, 4096])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([22, 8, 8, 8]), -3, -2
iter 0, train loss 124479.6875, val loss None, lr 0.01
iter 250, train loss 5660.8623046875, val loss None, lr 0.01
iter 500, train loss 5234.5556640625, val loss None, lr 0.01
iter 750, train loss 5030.373046875, val loss None, lr 0.01
iter 1000, train loss 4759.470703125, val loss None, lr 0.003333
iter 1250, train loss 4613.59375, val loss None, lr 0.003333
iter 1500, train loss 4555.6767578125, val loss None, lr 0.001111
iter 1750, train loss 4521.88818359375, val loss None, lr 0.001111
iter 2000, train loss 4505.04296875, val loss None, lr 0.001111
iter 2250, train loss 4481.21923828125, val loss None, lr 0.00037
best loss 4471.28759765625
running bpv: 0.34022154659277504
layer31: mlp.up_proj
weight shape torch.Size([11008, 4096])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 8, 64]), -3, -1
gate 1 torch.Size([22, 8, 8, 8]), -3, -2
iter 0, train loss 141390.984375, val loss None, lr 0.01
iter 250, train loss 5147.8671875, val loss None, lr 0.01
iter 500, train loss 4673.31494140625, val loss None, lr 0.01
iter 750, train loss 4293.2958984375, val loss None, lr 0.003333
iter 1000, train loss 4180.37646484375, val loss None, lr 0.001111
iter 1250, train loss 4116.2880859375, val loss None, lr 0.001111
iter 1500, train loss 4083.821533203125, val loss None, lr 0.001111
iter 1750, train loss 4062.58984375, val loss None, lr 0.00037
iter 2000, train loss 4047.515380859375, val loss None, lr 0.00037
iter 2250, train loss 4034.469970703125, val loss None, lr 0.00037
best loss 4024.650146484375
running bpv: 0.33923357757215067
layer31: mlp.down_proj
weight shape torch.Size([4096, 11008])
[8, 8, 64]
paddding 256
gate 3 torch.Size([8, 64, 8, 64]), -2, -1
gate 2 torch.Size([8, 64, 22, 64]), -3, -1
gate 1 torch.Size([8, 8, 8, 8]), -3, -2
iter 0, train loss 289122.25, val loss None, lr 0.01
iter 250, train loss 1134.7080078125, val loss None, lr 0.003333
iter 500, train loss 1239.92431640625, val loss None, lr 0.003333
iter 750, train loss 524.744140625, val loss None, lr 0.003333
iter 1000, train loss 422.5550842285156, val loss None, lr 0.003333
iter 1250, train loss 429.97161865234375, val loss None, lr 0.003333
iter 1500, train loss 364.11175537109375, val loss None, lr 0.001111
iter 1750, train loss 324.7261047363281, val loss None, lr 0.001111
iter 2000, train loss 315.57330322265625, val loss None, lr 0.001111
iter 2250, train loss 299.890869140625, val loss None, lr 0.001111
best loss 292.1294250488281
running bpv: 0.3393579987046632
8465 MiB free out of 48676 MiB total
trying to convert back to original dtype, current dtype: torch.float16
torch.Size([128, 4096, 4096]) batch size 4
Inference:   0%|          | 0/32 [00:00<?, ?it/s]Inference:   3%|▎         | 1/32 [00:00<00:07,  4.28it/s]Inference:   6%|▋         | 2/32 [00:00<00:07,  3.90it/s]Inference:   9%|▉         | 3/32 [00:00<00:07,  3.82it/s]Inference:  12%|█▎        | 4/32 [00:01<00:07,  3.77it/s]Inference:  16%|█▌        | 5/32 [00:01<00:07,  3.74it/s]Inference:  19%|█▉        | 6/32 [00:01<00:07,  3.71it/s]Inference:  22%|██▏       | 7/32 [00:01<00:06,  3.69it/s]Inference:  25%|██▌       | 8/32 [00:02<00:06,  3.67it/s]Inference:  28%|██▊       | 9/32 [00:02<00:06,  3.66it/s]Inference:  31%|███▏      | 10/32 [00:02<00:06,  3.65it/s]Inference:  34%|███▍      | 11/32 [00:02<00:05,  3.66it/s]Inference:  38%|███▊      | 12/32 [00:03<00:05,  3.66it/s]Inference:  41%|████      | 13/32 [00:03<00:05,  3.65it/s]Inference:  44%|████▍     | 14/32 [00:03<00:04,  3.64it/s]Inference:  47%|████▋     | 15/32 [00:04<00:04,  3.64it/s]Inference:  50%|█████     | 16/32 [00:04<00:04,  3.63it/s]Inference:  53%|█████▎    | 17/32 [00:04<00:04,  3.63it/s]Inference:  56%|█████▋    | 18/32 [00:04<00:03,  3.64it/s]Inference:  59%|█████▉    | 19/32 [00:05<00:03,  3.66it/s]Inference:  62%|██████▎   | 20/32 [00:05<00:03,  3.64it/s]Inference:  66%|██████▌   | 21/32 [00:05<00:03,  3.63it/s]Inference:  69%|██████▉   | 22/32 [00:05<00:02,  3.62it/s]Inference:  72%|███████▏  | 23/32 [00:06<00:02,  3.61it/s]Inference:  75%|███████▌  | 24/32 [00:06<00:02,  3.60it/s]Inference:  78%|███████▊  | 25/32 [00:06<00:01,  3.86it/s]Inference:  81%|████████▏ | 26/32 [00:07<00:01,  3.74it/s]Inference:  84%|████████▍ | 27/32 [00:07<00:01,  3.65it/s]Inference:  88%|████████▊ | 28/32 [00:07<00:01,  3.60it/s]Inference:  91%|█████████ | 29/32 [00:07<00:00,  3.56it/s]Inference:  94%|█████████▍| 30/32 [00:08<00:00,  3.53it/s]Inference:  97%|█████████▋| 31/32 [00:08<00:00,  3.51it/s]Inference: 100%|██████████| 32/32 [00:08<00:00,  3.50it/s]Inference: 100%|██████████| 32/32 [00:08<00:00,  3.64it/s]
3147 MiB free out of 48676 MiB total
Saved layer 31 to ./models/meta-llama/Llama-2-7b-hf/tensorized/128/64/no_finetune/layer_31.pt
after cast to cpu
7177 MiB free out of 48676 MiB total
running bpv: 0.3393579987046632
Done with layer 31 total_time elapsed: 43096 estimated time left: 0
Total bits: 2197684224 Total params: 6476005376
average bits per value: 0.3393579987046632
total time taken: 43096.40978908539
/data/lliu/huffman/scripts/tensorize.bash: 15: 28: not found
