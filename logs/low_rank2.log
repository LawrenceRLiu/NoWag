/home/lliu/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.08s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.36it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.27it/s]
Model loaded. LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
4096
Loading tokenizer for meta-llama/Llama-2-7b-hf
Starting...
position_ids torch.Size([1, 4096])
attention_mask torch.Size([1, 1, 4096, 4096])
Ready.
Layer 0
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
total size:  202375168 total Megabytes:  tensor(272.3271, device='cuda:0') bits per value:  tensor(11.2882, device='cuda:0')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
weights.shape =  torch.Size([4096, 11008])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:0') bits per value:  tensor(0.8228, device='cuda:0')
Pre fine tuning: 56380 MiB free out of 81050 MiB total
Found 17 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 38736 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:0 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:0 torch.float32
----------
epoch=0
train loss=7.28e-05	
42036 MiB free out of 81050 MiB total
----------
epoch=1
train loss=5.79e-05	
42036 MiB free out of 81050 MiB total
----------
epoch=2
train loss=4.75e-05	
42036 MiB free out of 81050 MiB total
----------
epoch=3
train loss=4.17e-05	
42036 MiB free out of 81050 MiB total
----------
epoch=4
train loss=3.86e-05	
42036 MiB free out of 81050 MiB total
post_fine_tuning 42036 MiB free out of 81050 MiB total
42036 MiB free out of 81050 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:0') bits per value:  tensor(0.8228, device='cuda:0')
42028 MiB free out of 81050 MiB total
layer: 0
done in 0:08:25.387752 overall time: 0:08:25.387776 estimated time left: 4:21:07.021068
after cast to cpu
71488 MiB free out of 81050 MiB total
Layer 1
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
total size:  202375168 total Megabytes:  tensor(272.3271, device='cuda:0') bits per value:  tensor(11.2882, device='cuda:0')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
weights.shape =  torch.Size([4096, 11008])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:0') bits per value:  tensor(0.8228, device='cuda:0')
Pre fine tuning: 55858 MiB free out of 81050 MiB total
Found 17 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 38736 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:0 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:0 torch.float32
----------
epoch=0
train loss=7.70e-02	
43570 MiB free out of 81050 MiB total
----------
epoch=1
train loss=7.50e-02	
43570 MiB free out of 81050 MiB total
----------
epoch=2
train loss=7.26e-02	
43570 MiB free out of 81050 MiB total
----------
epoch=3
train loss=7.01e-02	
43570 MiB free out of 81050 MiB total
----------
epoch=4
train loss=6.74e-02	
43570 MiB free out of 81050 MiB total
post_fine_tuning 43570 MiB free out of 81050 MiB total
43570 MiB free out of 81050 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:0') bits per value:  tensor(0.8228, device='cuda:0')
43570 MiB free out of 81050 MiB total
layer: 1
done in 0:08:25.862215 overall time: 0:16:51.331572 estimated time left: 4:12:49.973577
after cast to cpu
71488 MiB free out of 81050 MiB total
Layer 2
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
total size:  202375168 total Megabytes:  tensor(272.3271, device='cuda:0') bits per value:  tensor(11.2882, device='cuda:0')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
weights.shape =  torch.Size([4096, 11008])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:0') bits per value:  tensor(0.8228, device='cuda:0')
Pre fine tuning: 55858 MiB free out of 81050 MiB total
Found 17 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 38736 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:0 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:0 torch.float32
----------
epoch=0
train loss=1.06e-03	
43570 MiB free out of 81050 MiB total
----------
epoch=1
train loss=9.11e-04	
43570 MiB free out of 81050 MiB total
----------
epoch=2
train loss=8.13e-04	
43150 MiB free out of 81050 MiB total
----------
epoch=3
train loss=7.48e-04	
43150 MiB free out of 81050 MiB total
----------
epoch=4
train loss=7.04e-04	
43150 MiB free out of 81050 MiB total
post_fine_tuning 43150 MiB free out of 81050 MiB total
43150 MiB free out of 81050 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:0') bits per value:  tensor(0.8228, device='cuda:0')
43150 MiB free out of 81050 MiB total
layer: 2
done in 0:08:24.019789 overall time: 0:25:15.472757 estimated time left: 4:04:09.569983
after cast to cpu
71068 MiB free out of 81050 MiB total
Layer 3
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
total size:  202375168 total Megabytes:  tensor(272.3271, device='cuda:0') bits per value:  tensor(11.2882, device='cuda:0')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
weights.shape =  torch.Size([4096, 11008])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:0') bits per value:  tensor(0.8228, device='cuda:0')
Pre fine tuning: 55438 MiB free out of 81050 MiB total
Found 17 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 38736 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:0 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:0 torch.float32
----------
epoch=0
train loss=1.70e-02	
43150 MiB free out of 81050 MiB total
----------
epoch=1
train loss=1.48e-02	
43150 MiB free out of 81050 MiB total
----------
epoch=2
train loss=1.31e-02	
43150 MiB free out of 81050 MiB total
----------
epoch=3
train loss=1.17e-02	
43150 MiB free out of 81050 MiB total
----------
epoch=4
train loss=1.06e-02	
43150 MiB free out of 81050 MiB total
post_fine_tuning 43150 MiB free out of 81050 MiB total
43150 MiB free out of 81050 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:0') bits per value:  tensor(0.8228, device='cuda:0')
43150 MiB free out of 81050 MiB total
layer: 3
done in 0:08:22.839801 overall time: 0:33:38.431653 estimated time left: 3:55:29.021573
after cast to cpu
71068 MiB free out of 81050 MiB total
Layer 4
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
total size:  202375168 total Megabytes:  tensor(272.3271, device='cuda:0') bits per value:  tensor(11.2882, device='cuda:0')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
weights.shape =  torch.Size([4096, 11008])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:0') bits per value:  tensor(0.8228, device='cuda:0')
Pre fine tuning: 55440 MiB free out of 81050 MiB total
Found 17 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 38736 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:0 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:0 torch.float32
----------
epoch=0
train loss=2.76e-02	
43152 MiB free out of 81050 MiB total
----------
epoch=1
train loss=2.40e-02	
43152 MiB free out of 81050 MiB total
----------
epoch=2
train loss=2.14e-02	
43152 MiB free out of 81050 MiB total
----------
epoch=3
train loss=1.91e-02	
43152 MiB free out of 81050 MiB total
----------
epoch=4
train loss=1.72e-02	
43152 MiB free out of 81050 MiB total
post_fine_tuning 43152 MiB free out of 81050 MiB total
43152 MiB free out of 81050 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:0') bits per value:  tensor(0.8228, device='cuda:0')
43152 MiB free out of 81050 MiB total
layer: 4
done in 0:08:22.100918 overall time: 0:42:00.652494 estimated time left: 3:46:51.523466
after cast to cpu
71068 MiB free out of 81050 MiB total
Layer 5
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
total size:  202375168 total Megabytes:  tensor(272.3271, device='cuda:0') bits per value:  tensor(11.2882, device='cuda:0')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
weights.shape =  torch.Size([4096, 11008])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:0') bits per value:  tensor(0.8228, device='cuda:0')
Pre fine tuning: 55438 MiB free out of 81050 MiB total
Found 17 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 38736 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:0 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:0 torch.float32
----------
epoch=0
train loss=1.75e-02	
43150 MiB free out of 81050 MiB total
----------
epoch=1
train loss=1.50e-02	
43150 MiB free out of 81050 MiB total
----------
epoch=2
train loss=1.32e-02	
43150 MiB free out of 81050 MiB total
----------
epoch=3
train loss=1.18e-02	
43150 MiB free out of 81050 MiB total
----------
epoch=4
train loss=1.06e-02	
43150 MiB free out of 81050 MiB total
post_fine_tuning 43150 MiB free out of 81050 MiB total
43150 MiB free out of 81050 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:0') bits per value:  tensor(0.8228, device='cuda:0')
43150 MiB free out of 81050 MiB total
layer: 5
done in 0:08:22.514561 overall time: 0:50:23.294164 estimated time left: 3:38:20.941378
after cast to cpu
71068 MiB free out of 81050 MiB total
Layer 6
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
total size:  202375168 total Megabytes:  tensor(272.3271, device='cuda:0') bits per value:  tensor(11.2882, device='cuda:0')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
weights.shape =  torch.Size([4096, 11008])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:0') bits per value:  tensor(0.8228, device='cuda:0')
Pre fine tuning: 55438 MiB free out of 81050 MiB total
Found 17 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 38736 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:0 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:0 torch.float32
----------
epoch=0
train loss=7.60e-03	
43150 MiB free out of 81050 MiB total
----------
epoch=1
train loss=6.48e-03	
43150 MiB free out of 81050 MiB total
----------
epoch=2
train loss=5.77e-03	
43150 MiB free out of 81050 MiB total
----------
epoch=3
train loss=5.23e-03	
43150 MiB free out of 81050 MiB total
----------
epoch=4
train loss=4.80e-03	
43150 MiB free out of 81050 MiB total
post_fine_tuning 43150 MiB free out of 81050 MiB total
43150 MiB free out of 81050 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:0') bits per value:  tensor(0.8228, device='cuda:0')
43150 MiB free out of 81050 MiB total
layer: 6
done in 0:08:21.773585 overall time: 0:58:45.191894 estimated time left: 3:29:49.971049
after cast to cpu
71068 MiB free out of 81050 MiB total
Layer 7
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
total size:  202375168 total Megabytes:  tensor(272.3271, device='cuda:0') bits per value:  tensor(11.2882, device='cuda:0')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
weights.shape =  torch.Size([4096, 11008])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:0') bits per value:  tensor(0.8228, device='cuda:0')
Pre fine tuning: 55438 MiB free out of 81050 MiB total
Found 17 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 38736 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:0 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:0 torch.float32
----------
epoch=0
train loss=1.03e-02	
43150 MiB free out of 81050 MiB total
----------
epoch=1
train loss=8.72e-03	
43150 MiB free out of 81050 MiB total
----------
epoch=2
train loss=7.80e-03	
43150 MiB free out of 81050 MiB total
----------
epoch=3
train loss=7.11e-03	
43150 MiB free out of 81050 MiB total
----------
epoch=4
train loss=6.54e-03	
43150 MiB free out of 81050 MiB total
post_fine_tuning 43150 MiB free out of 81050 MiB total
43150 MiB free out of 81050 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:0') bits per value:  tensor(0.8228, device='cuda:0')
43150 MiB free out of 81050 MiB total
layer: 7
done in 0:08:21.773597 overall time: 1:07:07.091218 estimated time left: 3:21:21.273654
after cast to cpu
71068 MiB free out of 81050 MiB total
Layer 8
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
total size:  202375168 total Megabytes:  tensor(272.3271, device='cuda:0') bits per value:  tensor(11.2882, device='cuda:0')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
weights.shape =  torch.Size([4096, 11008])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:0') bits per value:  tensor(0.8228, device='cuda:0')
Pre fine tuning: 55438 MiB free out of 81050 MiB total
Found 17 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 38736 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:0 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:0 torch.float32
----------
epoch=0
train loss=1.42e-02	
43150 MiB free out of 81050 MiB total
----------
epoch=1
train loss=1.26e-02	
43150 MiB free out of 81050 MiB total
----------
epoch=2
train loss=1.14e-02	
43150 MiB free out of 81050 MiB total
----------
epoch=3
train loss=1.05e-02	
43150 MiB free out of 81050 MiB total
----------
epoch=4
train loss=9.70e-03	
43150 MiB free out of 81050 MiB total
post_fine_tuning 43150 MiB free out of 81050 MiB total
43150 MiB free out of 81050 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:0') bits per value:  tensor(0.8228, device='cuda:0')
43150 MiB free out of 81050 MiB total
layer: 8
done in 0:08:21.621526 overall time: 1:15:28.837562 estimated time left: 3:12:53.695991
after cast to cpu
71068 MiB free out of 81050 MiB total
Layer 9
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
total size:  202375168 total Megabytes:  tensor(272.3271, device='cuda:0') bits per value:  tensor(11.2882, device='cuda:0')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
weights.shape =  torch.Size([4096, 11008])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:0') bits per value:  tensor(0.8228, device='cuda:0')
Pre fine tuning: 55438 MiB free out of 81050 MiB total
Found 17 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 38736 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:0 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:0 torch.float32
----------
epoch=0
train loss=1.50e-02	
43150 MiB free out of 81050 MiB total
----------
epoch=1
train loss=1.31e-02	
43150 MiB free out of 81050 MiB total
----------
epoch=2
train loss=1.19e-02	
43150 MiB free out of 81050 MiB total
----------
epoch=3
train loss=1.11e-02	
43150 MiB free out of 81050 MiB total
----------
epoch=4
train loss=1.04e-02	
43150 MiB free out of 81050 MiB total
post_fine_tuning 43150 MiB free out of 81050 MiB total
43150 MiB free out of 81050 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:0') bits per value:  tensor(0.8228, device='cuda:0')
43150 MiB free out of 81050 MiB total
layer: 9
done in 0:08:21.184161 overall time: 1:23:50.143492 estimated time left: 3:04:26.315683
after cast to cpu
71068 MiB free out of 81050 MiB total
Layer 10
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
total size:  202375168 total Megabytes:  tensor(272.3271, device='cuda:0') bits per value:  tensor(11.2882, device='cuda:0')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
weights.shape =  torch.Size([4096, 11008])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:0') bits per value:  tensor(0.8228, device='cuda:0')
Pre fine tuning: 55438 MiB free out of 81050 MiB total
Found 17 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 38736 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:0 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:0 torch.float32
----------
epoch=0
train loss=1.50e-02	
43150 MiB free out of 81050 MiB total
----------
epoch=1
train loss=1.31e-02	
43150 MiB free out of 81050 MiB total
----------
epoch=2
train loss=1.18e-02	
43150 MiB free out of 81050 MiB total
----------
epoch=3
train loss=1.09e-02	
43150 MiB free out of 81050 MiB total
----------
epoch=4
train loss=1.02e-02	
43150 MiB free out of 81050 MiB total
post_fine_tuning 43150 MiB free out of 81050 MiB total
43150 MiB free out of 81050 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:0') bits per value:  tensor(0.8228, device='cuda:0')
43150 MiB free out of 81050 MiB total
layer: 10
done in 0:08:21.402690 overall time: 1:32:11.677126 estimated time left: 2:56:00.474514
after cast to cpu
71068 MiB free out of 81050 MiB total
Layer 11
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
total size:  202375168 total Megabytes:  tensor(272.3271, device='cuda:0') bits per value:  tensor(11.2882, device='cuda:0')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
weights.shape =  torch.Size([4096, 11008])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:0') bits per value:  tensor(0.8228, device='cuda:0')
Pre fine tuning: 55438 MiB free out of 81050 MiB total
Found 17 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 38736 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:0 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:0 torch.float32
----------
epoch=0
train loss=2.42e-02	
43150 MiB free out of 81050 MiB total
----------
epoch=1
train loss=2.15e-02	
43150 MiB free out of 81050 MiB total
----------
epoch=2
train loss=1.96e-02	
43150 MiB free out of 81050 MiB total
----------
epoch=3
train loss=1.81e-02	
43150 MiB free out of 81050 MiB total
----------
epoch=4
train loss=1.70e-02	
43150 MiB free out of 81050 MiB total
post_fine_tuning 43150 MiB free out of 81050 MiB total
43150 MiB free out of 81050 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:0') bits per value:  tensor(0.8228, device='cuda:0')
43150 MiB free out of 81050 MiB total
layer: 11
done in 0:08:21.778761 overall time: 1:40:33.576582 estimated time left: 2:47:35.960970
after cast to cpu
71068 MiB free out of 81050 MiB total
Layer 12
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
total size:  202375168 total Megabytes:  tensor(272.3271, device='cuda:0') bits per value:  tensor(11.2882, device='cuda:0')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
weights.shape =  torch.Size([4096, 11008])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:0') bits per value:  tensor(0.8228, device='cuda:0')
Pre fine tuning: 55438 MiB free out of 81050 MiB total
Found 17 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 38736 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:0 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:0 torch.float32
----------
epoch=0
train loss=2.33e-02	
43150 MiB free out of 81050 MiB total
----------
epoch=1
train loss=2.03e-02	
43150 MiB free out of 81050 MiB total
----------
epoch=2
train loss=1.83e-02	
43150 MiB free out of 81050 MiB total
----------
epoch=3
train loss=1.68e-02	
43150 MiB free out of 81050 MiB total
----------
epoch=4
train loss=1.56e-02	
43150 MiB free out of 81050 MiB total
post_fine_tuning 43150 MiB free out of 81050 MiB total
43150 MiB free out of 81050 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:0') bits per value:  tensor(0.8228, device='cuda:0')
43150 MiB free out of 81050 MiB total
layer: 12
done in 0:08:21.769758 overall time: 1:48:55.476080 estimated time left: 2:39:11.849656
after cast to cpu
71068 MiB free out of 81050 MiB total
Layer 13
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
total size:  202375168 total Megabytes:  tensor(272.3271, device='cuda:0') bits per value:  tensor(11.2882, device='cuda:0')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
weights.shape =  torch.Size([4096, 11008])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:0') bits per value:  tensor(0.8228, device='cuda:0')
Pre fine tuning: 55438 MiB free out of 81050 MiB total
Found 17 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 38736 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:0 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:0 torch.float32
----------
epoch=0
train loss=2.46e-02	
43150 MiB free out of 81050 MiB total
----------
epoch=1
train loss=2.18e-02	
43150 MiB free out of 81050 MiB total
----------
epoch=2
train loss=1.99e-02	
43150 MiB free out of 81050 MiB total
----------
epoch=3
train loss=1.85e-02	
43150 MiB free out of 81050 MiB total
----------
epoch=4
train loss=1.73e-02	
43150 MiB free out of 81050 MiB total
post_fine_tuning 43150 MiB free out of 81050 MiB total
43150 MiB free out of 81050 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:0') bits per value:  tensor(0.8228, device='cuda:0')
43150 MiB free out of 81050 MiB total
layer: 13
done in 0:08:22.597770 overall time: 1:57:18.196916 estimated time left: 2:30:49.110320
after cast to cpu
71068 MiB free out of 81050 MiB total
Layer 14
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
total size:  202375168 total Megabytes:  tensor(272.3271, device='cuda:0') bits per value:  tensor(11.2882, device='cuda:0')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
weights.shape =  torch.Size([4096, 11008])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:0') bits per value:  tensor(0.8228, device='cuda:0')
Pre fine tuning: 55438 MiB free out of 81050 MiB total
Found 17 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 38736 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:0 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:0 torch.float32
----------
epoch=0
train loss=3.26e-02	
43150 MiB free out of 81050 MiB total
----------
epoch=1
train loss=2.91e-02	
43150 MiB free out of 81050 MiB total
----------
epoch=2
train loss=2.64e-02	
43150 MiB free out of 81050 MiB total
----------
epoch=3
train loss=2.43e-02	
43150 MiB free out of 81050 MiB total
----------
epoch=4
train loss=2.25e-02	
43150 MiB free out of 81050 MiB total
post_fine_tuning 43150 MiB free out of 81050 MiB total
43150 MiB free out of 81050 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:0') bits per value:  tensor(0.8228, device='cuda:0')
43150 MiB free out of 81050 MiB total
layer: 14
done in 0:08:23.578961 overall time: 2:05:41.896998 estimated time left: 2:22:27.483264
after cast to cpu
71068 MiB free out of 81050 MiB total
Layer 15
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
total size:  202375168 total Megabytes:  tensor(272.3271, device='cuda:0') bits per value:  tensor(11.2882, device='cuda:0')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
weights.shape =  torch.Size([4096, 11008])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:0') bits per value:  tensor(0.8228, device='cuda:0')
Pre fine tuning: 55438 MiB free out of 81050 MiB total
Found 17 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 38736 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:0 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:0 torch.float32
----------
epoch=0
train loss=3.42e-02	
43150 MiB free out of 81050 MiB total
----------
epoch=1
train loss=3.07e-02	
43150 MiB free out of 81050 MiB total
----------
epoch=2
train loss=2.80e-02	
43150 MiB free out of 81050 MiB total
----------
epoch=3
train loss=2.58e-02	
43150 MiB free out of 81050 MiB total
----------
epoch=4
train loss=2.41e-02	
43150 MiB free out of 81050 MiB total
post_fine_tuning 43150 MiB free out of 81050 MiB total
43150 MiB free out of 81050 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:0') bits per value:  tensor(0.8228, device='cuda:0')
43150 MiB free out of 81050 MiB total
layer: 15
done in 0:08:23.627280 overall time: 2:14:05.643614 estimated time left: 2:14:05.643614
after cast to cpu
71068 MiB free out of 81050 MiB total
Layer 16
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
total size:  202375168 total Megabytes:  tensor(272.3271, device='cuda:0') bits per value:  tensor(11.2882, device='cuda:0')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
weights.shape =  torch.Size([4096, 11008])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:0') bits per value:  tensor(0.8228, device='cuda:0')
Pre fine tuning: 55438 MiB free out of 81050 MiB total
Found 17 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 38736 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:0 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:0 torch.float32
----------
epoch=0
train loss=5.25e-02	
43150 MiB free out of 81050 MiB total
----------
epoch=1
train loss=4.62e-02	
43150 MiB free out of 81050 MiB total
----------
epoch=2
train loss=4.11e-02	
43150 MiB free out of 81050 MiB total
----------
epoch=3
train loss=3.73e-02	
43150 MiB free out of 81050 MiB total
----------
epoch=4
train loss=3.45e-02	
43150 MiB free out of 81050 MiB total
post_fine_tuning 43150 MiB free out of 81050 MiB total
43150 MiB free out of 81050 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:0') bits per value:  tensor(0.8228, device='cuda:0')
43150 MiB free out of 81050 MiB total
layer: 16
done in 0:08:21.271633 overall time: 2:22:27.036646 estimated time left: 2:05:41.502923
after cast to cpu
71068 MiB free out of 81050 MiB total
Layer 17
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
total size:  202375168 total Megabytes:  tensor(272.3271, device='cuda:0') bits per value:  tensor(11.2882, device='cuda:0')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
weights.shape =  torch.Size([4096, 11008])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:0') bits per value:  tensor(0.8228, device='cuda:0')
Pre fine tuning: 55438 MiB free out of 81050 MiB total
Found 17 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 38736 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:0 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:0 torch.float32
----------
epoch=0
train loss=5.68e-02	
43150 MiB free out of 81050 MiB total
----------
epoch=1
train loss=5.09e-02	
43150 MiB free out of 81050 MiB total
----------
epoch=2
train loss=4.66e-02	
43150 MiB free out of 81050 MiB total
----------
epoch=3
train loss=4.31e-02	
43150 MiB free out of 81050 MiB total
----------
epoch=4
train loss=4.02e-02	
43150 MiB free out of 81050 MiB total
post_fine_tuning 43150 MiB free out of 81050 MiB total
43150 MiB free out of 81050 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:0') bits per value:  tensor(0.8228, device='cuda:0')
43150 MiB free out of 81050 MiB total
layer: 17
done in 0:08:21.053207 overall time: 2:30:48.211478 estimated time left: 1:57:17.497816
after cast to cpu
71068 MiB free out of 81050 MiB total
Layer 18
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
total size:  202375168 total Megabytes:  tensor(272.3271, device='cuda:0') bits per value:  tensor(11.2882, device='cuda:0')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
weights.shape =  torch.Size([4096, 11008])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:0') bits per value:  tensor(0.8228, device='cuda:0')
Pre fine tuning: 55438 MiB free out of 81050 MiB total
Found 17 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 38736 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:0 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:0 torch.float32
----------
epoch=0
train loss=5.86e-02	
43150 MiB free out of 81050 MiB total
----------
epoch=1
train loss=5.09e-02	
43150 MiB free out of 81050 MiB total
----------
epoch=2
train loss=4.64e-02	
43150 MiB free out of 81050 MiB total
----------
epoch=3
train loss=4.30e-02	
43150 MiB free out of 81050 MiB total
----------
epoch=4
train loss=4.02e-02	
43150 MiB free out of 81050 MiB total
post_fine_tuning 43150 MiB free out of 81050 MiB total
43150 MiB free out of 81050 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:0') bits per value:  tensor(0.8228, device='cuda:0')
43150 MiB free out of 81050 MiB total
layer: 18
done in 0:08:19.858856 overall time: 2:39:08.189327 estimated time left: 1:48:52.971645
after cast to cpu
71068 MiB free out of 81050 MiB total
Layer 19
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
total size:  202375168 total Megabytes:  tensor(272.3271, device='cuda:0') bits per value:  tensor(11.2882, device='cuda:0')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
weights.shape =  torch.Size([4096, 11008])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:0') bits per value:  tensor(0.8228, device='cuda:0')
Pre fine tuning: 55438 MiB free out of 81050 MiB total
Found 17 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 38736 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:0 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:0 torch.float32
----------
epoch=0
train loss=4.68e-02	
43150 MiB free out of 81050 MiB total
----------
epoch=1
train loss=4.14e-02	
43150 MiB free out of 81050 MiB total
----------
epoch=2
train loss=3.81e-02	
43150 MiB free out of 81050 MiB total
----------
epoch=3
train loss=3.54e-02	
43150 MiB free out of 81050 MiB total
----------
epoch=4
train loss=3.33e-02	
43150 MiB free out of 81050 MiB total
post_fine_tuning 43150 MiB free out of 81050 MiB total
43150 MiB free out of 81050 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:0') bits per value:  tensor(0.8228, device='cuda:0')
43150 MiB free out of 81050 MiB total
layer: 19
done in 0:08:20.808298 overall time: 2:47:29.120566 estimated time left: 1:40:29.472340
after cast to cpu
71068 MiB free out of 81050 MiB total
Layer 20
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
total size:  202375168 total Megabytes:  tensor(272.3271, device='cuda:0') bits per value:  tensor(11.2882, device='cuda:0')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
weights.shape =  torch.Size([4096, 11008])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:0') bits per value:  tensor(0.8228, device='cuda:0')
Pre fine tuning: 55438 MiB free out of 81050 MiB total
Found 17 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 38736 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:0 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:0 torch.float32
----------
epoch=0
train loss=5.52e-02	
43150 MiB free out of 81050 MiB total
----------
epoch=1
train loss=4.88e-02	
43150 MiB free out of 81050 MiB total
----------
epoch=2
train loss=4.46e-02	
43150 MiB free out of 81050 MiB total
----------
epoch=3
train loss=4.13e-02	
43150 MiB free out of 81050 MiB total
----------
epoch=4
train loss=3.86e-02	
43150 MiB free out of 81050 MiB total
post_fine_tuning 43150 MiB free out of 81050 MiB total
43150 MiB free out of 81050 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:0') bits per value:  tensor(0.8228, device='cuda:0')
43150 MiB free out of 81050 MiB total
layer: 20
done in 0:08:21.793170 overall time: 2:55:51.034302 estimated time left: 1:32:06.732253
after cast to cpu
71068 MiB free out of 81050 MiB total
Layer 21
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
total size:  202375168 total Megabytes:  tensor(272.3271, device='cuda:0') bits per value:  tensor(11.2882, device='cuda:0')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
weights.shape =  torch.Size([4096, 11008])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:0') bits per value:  tensor(0.8228, device='cuda:0')
Pre fine tuning: 55438 MiB free out of 81050 MiB total
Found 17 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 38736 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:0 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:0 torch.float32
----------
epoch=0
train loss=6.51e-02	
43150 MiB free out of 81050 MiB total
----------
epoch=1
train loss=5.81e-02	
43150 MiB free out of 81050 MiB total
----------
epoch=2
train loss=5.33e-02	
43150 MiB free out of 81050 MiB total
----------
epoch=3
train loss=4.95e-02	
43150 MiB free out of 81050 MiB total
----------
epoch=4
train loss=4.63e-02	
43150 MiB free out of 81050 MiB total
post_fine_tuning 43150 MiB free out of 81050 MiB total
43150 MiB free out of 81050 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:0') bits per value:  tensor(0.8228, device='cuda:0')
43150 MiB free out of 81050 MiB total
layer: 21
done in 0:08:21.591027 overall time: 3:04:12.746205 estimated time left: 1:23:43.975548
after cast to cpu
71068 MiB free out of 81050 MiB total
Layer 22
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
total size:  202375168 total Megabytes:  tensor(272.3271, device='cuda:0') bits per value:  tensor(11.2882, device='cuda:0')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
weights.shape =  torch.Size([4096, 11008])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:0') bits per value:  tensor(0.8228, device='cuda:0')
Pre fine tuning: 55438 MiB free out of 81050 MiB total
Found 17 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 38736 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:0 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:0 torch.float32
----------
epoch=0
train loss=9.50e-02	
43150 MiB free out of 81050 MiB total
----------
epoch=1
train loss=7.74e-02	
43150 MiB free out of 81050 MiB total
----------
epoch=2
train loss=6.93e-02	
43150 MiB free out of 81050 MiB total
----------
epoch=3
train loss=6.40e-02	
43150 MiB free out of 81050 MiB total
----------
epoch=4
train loss=5.97e-02	
43150 MiB free out of 81050 MiB total
post_fine_tuning 43150 MiB free out of 81050 MiB total
43150 MiB free out of 81050 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:0') bits per value:  tensor(0.8228, device='cuda:0')
43150 MiB free out of 81050 MiB total
layer: 22
done in 0:08:20.926944 overall time: 3:12:33.793761 estimated time left: 1:15:21.049732
after cast to cpu
71068 MiB free out of 81050 MiB total
Layer 23
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
total size:  202375168 total Megabytes:  tensor(272.3271, device='cuda:0') bits per value:  tensor(11.2882, device='cuda:0')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
weights.shape =  torch.Size([4096, 11008])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:0') bits per value:  tensor(0.8228, device='cuda:0')
Pre fine tuning: 55438 MiB free out of 81050 MiB total
Found 17 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 38736 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:0 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:0 torch.float32
----------
epoch=0
train loss=6.71e-02	
43150 MiB free out of 81050 MiB total
----------
epoch=1
train loss=6.12e-02	
43150 MiB free out of 81050 MiB total
----------
epoch=2
train loss=5.66e-02	
43150 MiB free out of 81050 MiB total
----------
epoch=3
train loss=5.28e-02	
43150 MiB free out of 81050 MiB total
----------
epoch=4
train loss=4.97e-02	
43150 MiB free out of 81050 MiB total
post_fine_tuning 43150 MiB free out of 81050 MiB total
43150 MiB free out of 81050 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:0') bits per value:  tensor(0.8228, device='cuda:0')
43150 MiB free out of 81050 MiB total
layer: 23
done in 0:08:19.891798 overall time: 3:20:53.805000 estimated time left: 1:06:57.935000
after cast to cpu
71068 MiB free out of 81050 MiB total
Layer 24
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
total size:  202375168 total Megabytes:  tensor(272.3271, device='cuda:0') bits per value:  tensor(11.2882, device='cuda:0')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
weights.shape =  torch.Size([4096, 11008])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:0') bits per value:  tensor(0.8228, device='cuda:0')
Pre fine tuning: 55438 MiB free out of 81050 MiB total
Found 17 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 38736 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:0 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:0 torch.float32
----------
epoch=0
train loss=9.24e-02	
43150 MiB free out of 81050 MiB total
----------
epoch=1
train loss=8.19e-02	
43150 MiB free out of 81050 MiB total
----------
epoch=2
train loss=7.47e-02	
43150 MiB free out of 81050 MiB total
----------
epoch=3
train loss=6.95e-02	
43150 MiB free out of 81050 MiB total
----------
epoch=4
train loss=6.54e-02	
43150 MiB free out of 81050 MiB total
post_fine_tuning 43150 MiB free out of 81050 MiB total
43150 MiB free out of 81050 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:0') bits per value:  tensor(0.8228, device='cuda:0')
43150 MiB free out of 81050 MiB total
layer: 24
done in 0:08:21.418152 overall time: 3:29:15.344159 estimated time left: 0:58:35.496365
after cast to cpu
71068 MiB free out of 81050 MiB total
Layer 25
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
total size:  202375168 total Megabytes:  tensor(272.3271, device='cuda:0') bits per value:  tensor(11.2882, device='cuda:0')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
weights.shape =  torch.Size([4096, 11008])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:0') bits per value:  tensor(0.8228, device='cuda:0')
Pre fine tuning: 55438 MiB free out of 81050 MiB total
Found 17 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 38736 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:0 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:0 torch.float32
----------
epoch=0
train loss=1.04e-01	
43150 MiB free out of 81050 MiB total
----------
epoch=1
train loss=9.49e-02	
43150 MiB free out of 81050 MiB total
----------
epoch=2
train loss=8.84e-02	
43150 MiB free out of 81050 MiB total
----------
epoch=3
train loss=8.30e-02	
43150 MiB free out of 81050 MiB total
----------
epoch=4
train loss=7.84e-02	
43150 MiB free out of 81050 MiB total
post_fine_tuning 43150 MiB free out of 81050 MiB total
43150 MiB free out of 81050 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:0') bits per value:  tensor(0.8228, device='cuda:0')
43150 MiB free out of 81050 MiB total
layer: 25
done in 0:08:22.898416 overall time: 3:37:38.367724 estimated time left: 0:50:13.469475
after cast to cpu
71068 MiB free out of 81050 MiB total
Layer 26
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
total size:  202375168 total Megabytes:  tensor(272.3271, device='cuda:0') bits per value:  tensor(11.2882, device='cuda:0')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
weights.shape =  torch.Size([4096, 11008])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:0') bits per value:  tensor(0.8228, device='cuda:0')
Pre fine tuning: 55438 MiB free out of 81050 MiB total
Found 17 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 38736 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:0 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:0 torch.float32
----------
epoch=0
train loss=1.50e-01	
43150 MiB free out of 81050 MiB total
----------
epoch=1
train loss=1.37e-01	
43150 MiB free out of 81050 MiB total
----------
epoch=2
train loss=1.27e-01	
43150 MiB free out of 81050 MiB total
----------
epoch=3
train loss=1.19e-01	
43150 MiB free out of 81050 MiB total
----------
epoch=4
train loss=1.12e-01	
43150 MiB free out of 81050 MiB total
post_fine_tuning 43150 MiB free out of 81050 MiB total
43150 MiB free out of 81050 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:0') bits per value:  tensor(0.8228, device='cuda:0')
43150 MiB free out of 81050 MiB total
layer: 26
done in 0:08:20.486408 overall time: 3:45:58.974683 estimated time left: 0:41:50.921238
after cast to cpu
71068 MiB free out of 81050 MiB total
Layer 27
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
total size:  202375168 total Megabytes:  tensor(272.3271, device='cuda:0') bits per value:  tensor(11.2882, device='cuda:0')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
weights.shape =  torch.Size([4096, 11008])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:0') bits per value:  tensor(0.8228, device='cuda:0')
Pre fine tuning: 55438 MiB free out of 81050 MiB total
Found 17 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 38736 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:0 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:0 torch.float32
----------
epoch=0
train loss=1.09e-01	
43150 MiB free out of 81050 MiB total
----------
epoch=1
train loss=9.95e-02	
43150 MiB free out of 81050 MiB total
----------
epoch=2
train loss=9.24e-02	
43150 MiB free out of 81050 MiB total
----------
epoch=3
train loss=8.65e-02	
43150 MiB free out of 81050 MiB total
----------
epoch=4
train loss=8.16e-02	
43150 MiB free out of 81050 MiB total
post_fine_tuning 43150 MiB free out of 81050 MiB total
43150 MiB free out of 81050 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:0') bits per value:  tensor(0.8228, device='cuda:0')
43150 MiB free out of 81050 MiB total
layer: 27
done in 0:08:19.779939 overall time: 3:54:18.892784 estimated time left: 0:33:28.413255
after cast to cpu
71068 MiB free out of 81050 MiB total
Layer 28
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
total size:  202375168 total Megabytes:  tensor(272.3271, device='cuda:0') bits per value:  tensor(11.2882, device='cuda:0')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
weights.shape =  torch.Size([4096, 11008])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:0') bits per value:  tensor(0.8228, device='cuda:0')
Pre fine tuning: 55438 MiB free out of 81050 MiB total
Found 17 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 38736 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:0 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:0 torch.float32
----------
epoch=0
train loss=1.38e-01	
43150 MiB free out of 81050 MiB total
----------
epoch=1
train loss=1.23e-01	
43150 MiB free out of 81050 MiB total
----------
epoch=2
train loss=1.13e-01	
43150 MiB free out of 81050 MiB total
----------
epoch=3
train loss=1.04e-01	
43150 MiB free out of 81050 MiB total
----------
epoch=4
train loss=9.80e-02	
43150 MiB free out of 81050 MiB total
post_fine_tuning 43150 MiB free out of 81050 MiB total
43150 MiB free out of 81050 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:0') bits per value:  tensor(0.8228, device='cuda:0')
43150 MiB free out of 81050 MiB total
layer: 28
done in 0:08:20.693954 overall time: 4:02:39.707909 estimated time left: 0:25:06.176680
after cast to cpu
71068 MiB free out of 81050 MiB total
Layer 29
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
total size:  202375168 total Megabytes:  tensor(272.3271, device='cuda:0') bits per value:  tensor(11.2882, device='cuda:0')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
weights.shape =  torch.Size([4096, 11008])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:0') bits per value:  tensor(0.8228, device='cuda:0')
Pre fine tuning: 55438 MiB free out of 81050 MiB total
Found 17 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 38736 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:0 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:0 torch.float32
----------
epoch=0
train loss=2.43e-01	
43150 MiB free out of 81050 MiB total
----------
epoch=1
train loss=2.21e-01	
43150 MiB free out of 81050 MiB total
----------
epoch=2
train loss=2.06e-01	
43150 MiB free out of 81050 MiB total
----------
epoch=3
train loss=1.95e-01	
43150 MiB free out of 81050 MiB total
----------
epoch=4
train loss=1.85e-01	
43150 MiB free out of 81050 MiB total
post_fine_tuning 43150 MiB free out of 81050 MiB total
43150 MiB free out of 81050 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:0') bits per value:  tensor(0.8228, device='cuda:0')
43150 MiB free out of 81050 MiB total
layer: 29
done in 0:08:21.275396 overall time: 4:11:01.107997 estimated time left: 0:16:44.073866
after cast to cpu
71068 MiB free out of 81050 MiB total
Layer 30
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
total size:  202375168 total Megabytes:  tensor(272.3271, device='cuda:0') bits per value:  tensor(11.2882, device='cuda:0')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
weights.shape =  torch.Size([4096, 11008])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:0') bits per value:  tensor(0.8228, device='cuda:0')
Pre fine tuning: 55438 MiB free out of 81050 MiB total
Found 17 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 38736 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:0 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:0 torch.float32
----------
epoch=0
train loss=3.47e+01	
43150 MiB free out of 81050 MiB total
----------
epoch=1
train loss=3.36e+01	
43150 MiB free out of 81050 MiB total
----------
epoch=2
train loss=3.24e+01	
43150 MiB free out of 81050 MiB total
----------
epoch=3
train loss=3.11e+01	
43150 MiB free out of 81050 MiB total
----------
epoch=4
train loss=2.97e+01	
43150 MiB free out of 81050 MiB total
post_fine_tuning 43150 MiB free out of 81050 MiB total
43150 MiB free out of 81050 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:0') bits per value:  tensor(0.8228, device='cuda:0')
43150 MiB free out of 81050 MiB total
layer: 30
done in 0:08:21.076414 overall time: 4:19:22.302997 estimated time left: 0:08:22.009774
after cast to cpu
71068 MiB free out of 81050 MiB total
Layer 31
layer original dtype torch.float16
Performing low rank approximation
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
using low rank =  196
row mask =  4072 column mask =  4052
total size:  202375168 total Megabytes:  tensor(272.3271, device='cuda:0') bits per value:  tensor(11.2882, device='cuda:0')
Quantizing ...
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
A.shape =  torch.Size([4072, 196]) B.shape =  torch.Size([196, 4052])
weights.shape =  torch.Size([4072, 196])
quantized
weights.shape =  torch.Size([196, 4052])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
weights.shape =  torch.Size([4096, 11008])
quantized
weights.shape =  torch.Size([11008, 4096])
quantized
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:0') bits per value:  tensor(0.8228, device='cuda:0')
Pre fine tuning: 55438 MiB free out of 81050 MiB total
Found 17 differentiable parameters
differentiable parameters: dict_keys(['self_attn.q_proj.weights_norms_rowwise', 'self_attn.q_proj.A.centriods', 'self_attn.q_proj.B.centriods', 'self_attn.k_proj.weights_norms_rowwise', 'self_attn.k_proj.A.centriods', 'self_attn.k_proj.B.centriods', 'self_attn.v_proj.weights_norms_rowwise', 'self_attn.v_proj.A.centriods', 'self_attn.v_proj.B.centriods', 'self_attn.o_proj.weights_norms_rowwise', 'self_attn.o_proj.A.centriods', 'self_attn.o_proj.B.centriods', 'mlp.w1.centriods', 'mlp.w2.centriods', 'mlp.w3.centriods', 'input_layernorm.weight', 'post_attention_layernorm.weight'])
Fine-tuning 38736 parameters
train_inps[0] torch.Size([128, 4096, 4096]) cuda:0 torch.float32
train_outs[0] torch.Size([128, 4096, 4096]) cuda:0 torch.float32
----------
epoch=0
train loss=7.79e-01	
43150 MiB free out of 81050 MiB total
----------
epoch=1
train loss=5.39e-01	
43150 MiB free out of 81050 MiB total
----------
epoch=2
train loss=4.30e-01	
43150 MiB free out of 81050 MiB total
----------
epoch=3
train loss=3.88e-01	
43150 MiB free out of 81050 MiB total
----------
epoch=4
train loss=3.62e-01	
43150 MiB free out of 81050 MiB total
post_fine_tuning 43150 MiB free out of 81050 MiB total
43150 MiB free out of 81050 MiB total
trying to convert back to original dtype
total size:  202375168 total Megabytes:  tensor(19.8497, device='cuda:0') bits per value:  tensor(0.8228, device='cuda:0')
43150 MiB free out of 81050 MiB total
layer: 31
done in 0:08:24.599394 overall time: 4:27:47.027318 estimated time left: 0:00:00
after cast to cpu
71068 MiB free out of 81050 MiB total
Total bits: tensor(10656743424, device='cuda:0') Total params: 12952010752
average bits per value: tensor(0.8228, device='cuda:0')
16068.617466688156
Loading tokenizer for meta-llama/Llama-2-7b-hf
/home/lliu/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Dataset: wikitext2
Evaluating ...
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
Perplexity: 57364.011719
